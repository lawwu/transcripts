
[00:00:00.000 --> 00:00:05.080]   And now we can try something funny here, like say, the doctor and machine learning.
[00:00:05.080 --> 00:00:17.600]   The humans are being controlled by a virus, but the doctor has been experimenting with a new kind of intelligence, machine learning, that he hopes will be able to break the hold of the virus.
[00:00:17.600 --> 00:00:19.320]   OK, so it's generating.
[00:00:19.320 --> 00:00:20.200]   So let's try some more.
[00:00:20.200 --> 00:00:26.040]   Hey, what is going on, everybody?
[00:00:26.040 --> 00:00:36.480]   It's Ivan here, and this is going to be a really interesting and really practical video about fine tuning one of the most exciting ML models out there, which is OpenAI's GPT-3.
[00:00:36.480 --> 00:00:44.600]   And the funniest part about the video is that for data, it will use episode synopsis from one of my favorite TV shows of all time, Doctor Who.
[00:00:44.600 --> 00:00:50.440]   This will practically be fine tuning GPT-3 to come up with new sci-fi TV show ideas.
[00:00:51.360 --> 00:00:57.960]   If you have a problem that you want to use fine tuning for, we'll start off by discussing if GPT-3 is going to be right for your task.
[00:00:57.960 --> 00:01:05.840]   Then we'll talk about data preparation, jump into the actual fine tuning and then explore the ways in which we can test the model's predictions.
[00:01:05.840 --> 00:01:11.880]   So and if you're excited about this video, smash that like button and consider subscribing to our channel.
[00:01:11.880 --> 00:01:17.280]   And if you have any questions, as always, drop them in the comments section down below and I'll be happy to answer you.
[00:01:17.600 --> 00:01:25.160]   And so I'd like to start off by talking about a couple doubts that I've had and misconceptions maybe that I've had about fine tuning GPT-3
[00:01:25.160 --> 00:01:32.880]   in the hopes that if you're sort of doubting whether you should actually try it, that that would be useful for your task,
[00:01:32.880 --> 00:01:41.920]   that we can kind of clear up some of those doubts and that you can have a clearer idea as to whether GPT-3 is going to be suitable for your task.
[00:01:42.080 --> 00:01:49.960]   And if you watch Doctor Who, you know that it's like a really jargon heavy show with like different sci-fi futuristic tech and like aliens and stuff.
[00:01:49.960 --> 00:02:00.840]   And I was really surprised how GPT-3 picked up like on all the different like jargons from the show, because if like if you watch Doctor Who, you know that like there are different like aliens and like sci-fi tech and all that stuff.
[00:02:00.840 --> 00:02:06.880]   But it also picked up on things like British spellings, which is like really surprising because, again, just a couple of hundred data points.
[00:02:06.920 --> 00:02:13.040]   So you don't really need like a crazy large data set to fine tune GPT-3 model is I guess what I'm trying to say here.
[00:02:13.040 --> 00:02:22.240]   And so the second thing that I found surprising is that for all the fine tunes that I did through OpenAI's API, they did not cost me more than a dollar.
[00:02:22.240 --> 00:02:31.040]   In fact, like most of the fine tunes that I did, which produced amazing results, costed like 60 cents or like 70 cents or like around that amount.
[00:02:31.200 --> 00:02:41.120]   And it's obviously really important to note here that you should go to OpenAI for the most relevant like pricing information that it's going to vary depending on the size of the model you're using and the data and for how long you're training.
[00:02:41.120 --> 00:02:48.200]   But that's just me trying to say like for this task to get like this results, a fine tune costed like 70 cents.
[00:02:48.200 --> 00:02:53.760]   So that is to say, you know, checking with the OpenAI API for like the relevant pricing info.
[00:02:53.760 --> 00:02:59.600]   But if you have like a certain problem that you want to apply fine tuning GPT-3 for, it might as well be really affordable.
[00:03:00.120 --> 00:03:10.120]   And so the third kind of like doubt that I jumped into fine tuning GPT-3 with was I was thinking like, you know, what if training takes like days or like weeks or something like that?
[00:03:10.120 --> 00:03:12.880]   I was hoping that, but like I wasn't sure.
[00:03:12.880 --> 00:03:28.000]   And I was also like pleasantly surprised here because like, again, at least in the context of this task, like in my experience of running like tens of fine tunes on this data, is that not for one of them, training took longer than like 15 minutes.
[00:03:28.000 --> 00:03:30.080]   Like most of them were like under 15 minutes.
[00:03:30.080 --> 00:03:33.160]   In fact, like I don't remember like a single one that was like longer than that.
[00:03:33.160 --> 00:03:40.360]   So and of course, like needless to say that like OpenAI, like you're uploading your data to OpenAI API to do the training.
[00:03:40.360 --> 00:03:42.440]   So like it doesn't matter what kind of hardware you have.
[00:03:42.440 --> 00:03:46.080]   And it just takes, it just trains like really, really quickly.
[00:03:46.080 --> 00:03:49.280]   So I was like pleasantly surprised with that.
[00:03:49.880 --> 00:03:58.320]   So hopefully talking about the size of the data set that I used, the pricing and the training time kind of helps you decide if you want to try GPT-3.
[00:03:58.320 --> 00:04:05.240]   And so the thing that I wanted to note really quickly is that I wrote like a really detailed like guide about this whole process that I'll link to in the video description.
[00:04:05.240 --> 00:04:12.520]   So all that I say is like feel free to, you know, as I'll be diving into more of the practical stuff right now to kind of lay back and just like enjoy the video.
[00:04:12.760 --> 00:04:20.080]   And then if you want to like replicate some of the steps, you can like refer to that written guide and just like follow that for some of the steps here.
[00:04:20.080 --> 00:04:26.240]   But regardless of that, let's jump into the video and let me show you kind of how my experiments with GPT-3 went.
[00:04:26.240 --> 00:04:38.120]   So in case you haven't heard about it, OpenAI, which is like one of the largest, like most impactful AI labs out there, has collaborated with Weights and Biases, which is a really, really powerful MLabs platform.
[00:04:38.720 --> 00:04:47.840]   The result of the collaboration is that now with just one line of OpenAI Python client code, you can log all of your fine tunes and all of your experiments to Weights and Biases.
[00:04:47.840 --> 00:04:58.120]   And it's really powerful because it lets us leverage all of the Weights and Biases infrastructure of being a single system of record for your ML experiments now applied to your OpenAI fine tunes.
[00:04:58.120 --> 00:05:02.440]   But let's actually see how that and many other things look like in the code.
[00:05:02.800 --> 00:05:07.160]   So right now we're looking at a Google call up notebook, which I'll link to also in the video description.
[00:05:07.160 --> 00:05:21.120]   And the first thing we need to do is to paste our OpenAI API key, which we can get by going into our username, view API keys, and here we can copy the API key and paste it here.
[00:05:21.120 --> 00:05:28.520]   Now, what this warning is saying here is that Weights and Biases by default is going to log the source code, like this Jupyter notebook.
[00:05:28.880 --> 00:05:32.240]   And if this cell contains your API key, then it's also going to get logged.
[00:05:32.240 --> 00:05:39.360]   It might be a problem if you make your Weights and Biases project public and other people can see it and you don't want them to see your API key probably.
[00:05:39.360 --> 00:05:45.120]   So this warning says that you might just like delete your API key like that and clear the output of the cell and it's not going to get logged.
[00:05:45.120 --> 00:05:48.640]   But personally, I'm going to burn this API key after the video.
[00:05:48.640 --> 00:05:50.480]   So like, I don't really care.
[00:05:50.480 --> 00:05:52.400]   So I'll just I'll just keep it here.
[00:05:52.400 --> 00:05:56.360]   So what I'm doing next is I'm initializing a new Weights and Biases project.
[00:05:56.640 --> 00:06:03.200]   At this point, it may prompt you to log into your existing Weights and Biases account by pasting an API key or quickly creating a new one.
[00:06:03.200 --> 00:06:13.960]   And so having done that, now I'm leveraging WNB Artifact, which is a tool for data set and model versioning to pull the data set from that I have stored right here on WNB.
[00:06:13.960 --> 00:06:22.800]   Right here, it's just like a .csv file, which will visualize in a second, which weighs just like 117 kilobytes.
[00:06:23.000 --> 00:06:30.680]   And so by running this cell, I'm essentially just like downloading that from the WNB cloud to Google Colab, where we can use it for training.
[00:06:30.680 --> 00:06:34.600]   So having that now, I can see the artifacts folder appearing, which has our data set file.
[00:06:34.600 --> 00:06:43.400]   And so something that's worth noting here is that that's the exact stage in the training process in this notebook where you should like add your own data if you want to.
[00:06:43.400 --> 00:06:52.720]   So for example, I'm using artifacts to download this .csv file, but if you want to use your own data, you can either use Google Colab and just upload files with Colab,
[00:06:52.720 --> 00:06:57.880]   or you can also log in as a WNB Artifact and then just pull it from the web so you don't have to download it all the time.
[00:06:57.880 --> 00:07:06.600]   But that's the exact time when you can plug in your own data set so that you can run through the whole process and get the functioning results with your own data.
[00:07:06.960 --> 00:07:17.000]   But if you don't want to, just feel free to follow along with this fun data set, because like, you know, Stackdriver whoe episode synopsis, like it's also really, really fun to work around and to play with.
[00:07:17.000 --> 00:07:24.520]   And so now in the next stage, I'm shuffling the data set and I'm also doing something really interesting, which is that I'm logging it as a WNB table.
[00:07:24.520 --> 00:07:29.040]   And so WNB tables is a tool for interactively exploring tabular data.
[00:07:29.040 --> 00:07:36.520]   And we can click on this link right here to open the run page, which now contains a WNB table to which we have logged our data set.
[00:07:36.880 --> 00:07:45.200]   So here we can kind of click around and look at the prompts, like say that's the name of an episode and that's like it's like real completion, like it's real synopsis.
[00:07:45.200 --> 00:07:52.840]   We can like, I don't know, resize the table and just like kind of like, you know, just like explore, explore the data in this way.
[00:07:52.840 --> 00:08:00.400]   But we can also use some of the more advanced functionality of WNB tables and use like some string ops to like explore the data set a bit more.
[00:08:00.720 --> 00:08:07.400]   Like let's say, for example, we want to look for a word Dalek, which is one of the most like notarious Doctor Who monsters.
[00:08:07.400 --> 00:08:10.760]   And it's definitely a fun one to look at, to look at our data.
[00:08:10.760 --> 00:08:18.680]   So like let's say we want to find all of the completions, like all of the real episode synopsis, which have the word Dalek in them.
[00:08:18.680 --> 00:08:19.960]   And so here's how we can do it.
[00:08:19.960 --> 00:08:30.640]   I'll insert a new column on the left and we'll say that that column will be row completion, which contains the episode synopsis that lower.
[00:08:30.640 --> 00:08:37.720]   To make all of the text there lowercase that contains Dalek also in lowercase.
[00:08:37.720 --> 00:08:42.560]   We'll make the panel be Boolean and yep.
[00:08:42.560 --> 00:08:44.680]   And now we can group by this column.
[00:08:44.680 --> 00:08:46.920]   And yeah.
[00:08:46.920 --> 00:08:52.240]   And so now we've grouped by this column and we can see all the episodes that don't have the word Dalek in them.
[00:08:52.240 --> 00:08:55.200]   Like say, you know, 283 of them.
[00:08:55.200 --> 00:08:59.120]   And we can see the 21 episode synopsis, which do have the word Dalek in them.
[00:08:59.520 --> 00:09:11.720]   Like here, Daleks, Daleks, Doctor, Doctor, Dalek City, Daleks.
[00:09:11.720 --> 00:09:17.520]   Yep. And so that's kind of like a funny example of us looking for a sci-fi alien race in our data set.
[00:09:17.520 --> 00:09:25.600]   But you can imagine if your data set is like some customer service reviews, like you may want to look for the entries that contain the words, like say danger, for example.
[00:09:25.640 --> 00:09:30.120]   So like you can handle them like in some more specific way, like in your fine tuning or whatnot.
[00:09:30.120 --> 00:09:31.720]   Like it's a it's a really useful tool.
[00:09:31.720 --> 00:09:34.360]   But let's actually try doing something else as well.
[00:09:34.360 --> 00:09:37.440]   So let's now group by this column and remove it.
[00:09:37.440 --> 00:09:45.560]   I also want to show you how we can use some of that WNB table magic now to find, say, the longest or like the shortest completions.
[00:09:45.560 --> 00:09:47.880]   The longest or the shortest episode synopsis.
[00:09:47.920 --> 00:09:57.520]   So I'll go to insert again a new column on the left and we'll say that this column will be row completion that length, which will give us the length of characters.
[00:09:57.520 --> 00:10:02.520]   And now I can just like sort it saying the sending order.
[00:10:02.520 --> 00:10:08.000]   And like this will find us like the longest episode like synopsis, right?
[00:10:08.000 --> 00:10:10.920]   The longest like episode synopsis in our data set.
[00:10:10.920 --> 00:10:16.120]   And maybe now also let's try sorting it in the ascending order.
[00:10:17.040 --> 00:10:29.640]   And yeah, I mean, I sort of saw it coming, obviously, but here in the data, you can see that like there's a number of episodes which, by the way, that totally like flew past me the first time that I trained this model.
[00:10:29.640 --> 00:10:35.680]   So here we can see a number of episodes which have coming soon as their episode synopsis.
[00:10:35.680 --> 00:10:41.520]   So apparently on the website from where I got the episodes in their synopsis, some of them were titled as coming soon.
[00:10:41.680 --> 00:10:50.600]   And I actually didn't notice that at all at first, but then it was kind of surprised that when I was like playing around with the models that I trained, that some of them would just say like coming soon out of nowhere.
[00:10:50.600 --> 00:10:59.360]   And like doing this type of data set exploration can give us like a pretty, pretty nice clue as to why something like that may end up happening.
[00:10:59.360 --> 00:11:07.160]   Now, that said, the model still produced like produced great results, but there was like that coming soon anomaly which would show up now and then.
[00:11:07.600 --> 00:11:18.400]   And doing this type of data exploration can be something that can help you like catch this little irregularities in your data set so that you can have the best data to train the best possible models.
[00:11:18.400 --> 00:11:26.120]   You know. So now let's jump back into code and let's use OpenAI's Python client tool to pre-process our data set.
[00:11:26.120 --> 00:11:34.880]   So the use case of finding GPT-3 that we're going for is called conditional generation, where we have like a specific prompt, like a name of an episode.
[00:11:35.240 --> 00:11:40.400]   And we expect GPT-3 to fill in the actual synopsis and the contents of an episode.
[00:11:40.400 --> 00:11:49.200]   So for it to have an easier time doing that, the pre-processing will allow us to add a couple of suffixes and a couple of little error things, which I'll show you in a moment.
[00:11:49.200 --> 00:11:52.200]   But using the OpenAI tool is actually really, really simple.
[00:11:52.200 --> 00:11:55.600]   So you just click on it and you're passing the path to your data set file.
[00:11:55.600 --> 00:11:59.560]   And then it kind of asks you what type of pre-processing you want to apply to your data set.
[00:11:59.560 --> 00:12:01.720]   So in our case, we're going to apply all of them.
[00:12:01.760 --> 00:12:05.280]   So I'll say, add a suffix operator like this error thing.
[00:12:05.280 --> 00:12:08.960]   I'll say yes. Add a suffix ending and I'll say yes.
[00:12:08.960 --> 00:12:13.520]   White space character to the beginning of the completion.
[00:12:13.520 --> 00:12:18.600]   Yes. And data will be written to a new JSONL file.
[00:12:18.600 --> 00:12:22.040]   Yes. Cool, cool, cool.
[00:12:22.040 --> 00:12:24.720]   So let it finish doing that.
[00:12:24.720 --> 00:12:26.120]   We can kind of see what it's done.
[00:12:26.120 --> 00:12:27.680]   We'll have this new file appear.
[00:12:27.680 --> 00:12:29.880]   And here we kind of see what it is.
[00:12:29.920 --> 00:12:38.720]   So it took the name of an episode and added this error thingy, which will help the model essentially to have an easier time training.
[00:12:38.720 --> 00:12:42.280]   And an easier time, most importantly, distinguishing what is a prompt.
[00:12:42.280 --> 00:12:45.240]   Let's say a prompt is a thing that's followed by an error symbol.
[00:12:45.240 --> 00:12:48.040]   And what is the completion?
[00:12:48.040 --> 00:12:57.080]   It's a thing after an error symbol with a white space in front of it that always ends with this end character.
[00:12:57.240 --> 00:13:01.480]   And it's also really important to remember the type of pre-processing that you apply to your dataset.
[00:13:01.480 --> 00:13:06.960]   Because once you'll be performing inference, you'll kind of also need to specify some of this stuff there.
[00:13:06.960 --> 00:13:11.040]   So here we're just putting our data into training and validation.
[00:13:11.040 --> 00:13:15.360]   And now we're actually this close to starting fine tuning.
[00:13:15.360 --> 00:13:18.000]   So here we get to define the hyperparameters.
[00:13:18.000 --> 00:13:23.080]   So these three models, Ada, Babbage and Curie, are currently the ones that are available for fine tuning.
[00:13:23.080 --> 00:13:28.600]   And we can check out the OpenAI docs to learn more about what are they best used for.
[00:13:28.600 --> 00:13:32.920]   Let's say that Ada would be a lighter model than Curie and all that stuff.
[00:13:32.920 --> 00:13:36.000]   Now, go with these values for the other hyperparameters.
[00:13:36.000 --> 00:13:40.880]   So the cool thing is that, as we'll see in a couple of minutes, you can experiment with different hyperparameters.
[00:13:40.880 --> 00:13:44.240]   And then log experiments to weights and biases and see which ones work better.
[00:13:44.240 --> 00:13:49.320]   And that's kind of how you can explore which hyperparameters work best for your specific application.
[00:13:49.320 --> 00:13:52.120]   And so now we're actually ready to jump into fine tuning.
[00:13:52.520 --> 00:13:59.240]   So here we call OpenAI API fine tunes that create, pass in our training and validation data and all of our hyperparameters.
[00:13:59.240 --> 00:14:02.560]   And then pretty much just run this cell of code to start the fine tuning.
[00:14:02.560 --> 00:14:06.160]   Here I'll press enter to choose to re-upload the data.
[00:14:06.160 --> 00:14:13.320]   And as you can see, the cost for this fine tuning will just be 28 cents.
[00:14:13.320 --> 00:14:19.200]   And like, I'm not an expert in economics, but sounds like a pretty good bang for the buck, if you ask me.
[00:14:19.200 --> 00:14:21.560]   And now we just kind of gotta like wait a little, you know.
[00:14:22.000 --> 00:14:24.760]   So as you can see, our fine tune now went through four epochs.
[00:14:24.760 --> 00:14:30.560]   And if we actually time it, we can see that it took like from 17 minutes to 21 minutes, which is like what?
[00:14:30.560 --> 00:14:32.840]   Like what? Like four and a half minutes?
[00:14:32.840 --> 00:14:34.480]   Four minutes and 40 seconds.
[00:14:34.480 --> 00:14:44.440]   So it's like, apparently this number is in this, this number is even like lower than what I've talked about in the like doubts section of the video.
[00:14:44.440 --> 00:14:47.680]   Which is a good thing, which is a good thing, obviously.
[00:14:47.840 --> 00:14:54.880]   And so as we've talked about it before, now we'll run just one line of OpenAI code to sync all of our fine tune jobs to Weights & Biases.
[00:14:54.880 --> 00:14:59.160]   In this cell, we can explore the usage of this command and in the next one, we'll actually run it.
[00:14:59.160 --> 00:15:03.760]   So as you can see here, it goes like OpenAI, 1db, sync.
[00:15:03.760 --> 00:15:09.440]   And here I specify the Weights & Biases project, which I would like to log the fine tune jobs to.
[00:15:09.440 --> 00:15:16.760]   I'll select the one of which we're also liking before the WNB tables and stuff, which is GPT-3 for generating Dr. Who synopsis.
[00:15:17.440 --> 00:15:19.160]   And now let's run the cell.
[00:15:19.160 --> 00:15:24.560]   And so now OpenAI has finished syncing our fine tune jobs data to Weights & Biases.
[00:15:24.560 --> 00:15:30.720]   And this is really, as you'll see in a couple of moments, will help us take to the next level how we're analyzing our GPT-3 fine tuning experience.
[00:15:30.720 --> 00:15:33.480]   Let's click on this link to open our project.
[00:15:33.480 --> 00:15:36.880]   So this is a Weights & Biases project dashboard.
[00:15:36.880 --> 00:15:45.040]   On the left, we have our runs, which is like our instances of us training models or logging artifacts or doing like other things.
[00:15:45.360 --> 00:15:47.960]   And on the right, we have the data associated with them.
[00:15:47.960 --> 00:15:50.960]   So for example, here we can see the tables that we've logged before.
[00:15:50.960 --> 00:15:55.040]   But the new thing that we want to look at here are the training metrics.
[00:15:55.040 --> 00:15:58.240]   So maybe let's drag this panel somewhere to the top, maybe.
[00:15:58.240 --> 00:16:04.160]   And so now, as you can see, it's comparing the training and the validation metrics of all the different fine tune jobs that I've done.
[00:16:04.160 --> 00:16:13.560]   So as you can see on the left, all of the runs that are named like Floral, Butterfly or Dauntless Wood, which is like a standard naming scheme in Weights & Biases for new runs,
[00:16:13.880 --> 00:16:17.600]   are the runs that we log with like WNB tables and like model predictions.
[00:16:17.600 --> 00:16:25.480]   But all the runs that are named like FT and then like a bunch of words and letters are the runs that are corresponding to specific fine tune jobs.
[00:16:25.480 --> 00:16:38.520]   However, as you can see, it's fairly difficult to look at, say, a run that's named like FT, a bunch of stuff, and be able to reasonably say which model that run was trained on or what it even is.
[00:16:38.520 --> 00:16:43.120]   And here we can take advantage of something called Run Stable right here.
[00:16:43.240 --> 00:16:50.040]   And the fact that like along with all the training metrics, what's also getting logged are all the hyperparameters and metadata for a specific fine tune job.
[00:16:50.040 --> 00:16:53.120]   So, for example, let's take this FT guy here.
[00:16:53.120 --> 00:17:00.600]   For example, I can go and pin the hyperparameter with the name of the model, which contains...
[00:17:00.600 --> 00:17:02.680]   Let's make it a little bit larger.
[00:17:02.680 --> 00:17:05.040]   Oh, no, not the created one.
[00:17:05.040 --> 00:17:07.000]   Let's do the...
[00:17:07.000 --> 00:17:08.200]   Let's unpin this column.
[00:17:08.200 --> 00:17:13.120]   I want to do this one, fine tune model, which has the name of the model, right?
[00:17:13.840 --> 00:17:15.320]   So let's pin it.
[00:17:15.320 --> 00:17:16.880]   It has the...
[00:17:16.880 --> 00:17:19.960]   Yeah, it has the name of the model and when it was...
[00:17:19.960 --> 00:17:21.840]   When it finished fine tuning.
[00:17:21.840 --> 00:17:24.640]   OK, that's really good.
[00:17:24.640 --> 00:17:26.200]   That gives us a lot of useful information.
[00:17:26.200 --> 00:17:29.760]   First of all, it's like the name of the model, when it was fine tuned.
[00:17:29.760 --> 00:17:32.680]   So now let's maybe get the data that it was trained on.
[00:17:32.680 --> 00:17:36.640]   So this guy here, pin column.
[00:17:36.640 --> 00:17:40.720]   Yep, training file, Dr. Who train.
[00:17:41.200 --> 00:17:48.000]   And let's say we're comparing our models, we want to keep a closer watch, say, on the number of epochs that a specific fine tune job was trained for.
[00:17:48.000 --> 00:17:50.040]   So let's pin this column also.
[00:17:50.040 --> 00:17:51.560]   Maybe make it smaller.
[00:17:51.560 --> 00:18:00.040]   And so as we go back into the project page now, we can see that for each of the fine tune job runs, we can now see the name of the model that it was trained on.
[00:18:00.040 --> 00:18:05.040]   And when, which is basically the model name, it's going to be useful for us also quite soon.
[00:18:05.040 --> 00:18:08.800]   We can see the data set that it was trained on and the number of epochs.
[00:18:08.840 --> 00:18:17.360]   And we can make it like showcase any of the many, many parameters here that, you know, me or you may find useful.
[00:18:17.360 --> 00:18:19.680]   This one seems like a little bit large.
[00:18:19.680 --> 00:18:22.240]   So maybe I just fold it enough to see the name of the model.
[00:18:22.240 --> 00:18:23.360]   And that's kind of enough for me.
[00:18:23.360 --> 00:18:26.520]   Yeah. And so now that gives us a lot more data.
[00:18:26.520 --> 00:18:29.800]   But then there's still like a lot of really different runs selected.
[00:18:29.800 --> 00:18:31.080]   Maybe I don't want to look at all of them.
[00:18:31.080 --> 00:18:34.960]   So I'll just go and I'll say visualize none.
[00:18:35.320 --> 00:18:47.840]   And I'll just go in like I'll manually select all the latest one that we did so that like none of the like say older ones that might not be relevant for the current experiments displayed here.
[00:18:47.840 --> 00:18:52.440]   So, for example, I would say like all of this one are fairly recent one that I can select.
[00:18:52.440 --> 00:18:59.400]   And so now we can easily compare the validation or like training losses or accuracies of the relevant models that we have selected right here.
[00:18:59.400 --> 00:19:05.000]   And so now I can open, for example, this chart and compare the validation losses of the different fine tune jobs.
[00:19:06.000 --> 00:19:18.360]   However, one of the best ways to actually test the performance of generative models is to look at like their actual predictions on like the actual episode, like synopsis episode that they come up with.
[00:19:18.360 --> 00:19:19.920]   And here's how we can do that.
[00:19:19.920 --> 00:19:34.920]   So right now we're going to take our validation data set, unlock the model's predictions on all of the like episode synopsis from that validation data set along to like the actual ground truth real episode synopsis, which like actually aired on television.
[00:19:34.960 --> 00:19:37.600]   You know, so here we'll create an evaluation job.
[00:19:37.600 --> 00:19:40.800]   And so here's something really, really cool that I wanted to know that's happening.
[00:19:40.800 --> 00:19:56.240]   So in order for us to let the predictions we need to perform inference in Python on their validation data set and do that, we need to use open the eyes like API command in Python, which needs to know the name of the model that we're going to use for for this task.
[00:19:56.240 --> 00:20:03.520]   And here we're using like a really clever way of, you know, we have locked all of our fine tune jobs data to weights and biases.
[00:20:03.560 --> 00:20:08.160]   Now we can refer to our project there and get the latest fine tune details.
[00:20:08.160 --> 00:20:10.240]   And from there, extract the metadata.
[00:20:10.240 --> 00:20:18.440]   And from that metadata, we can get the name of our fine tune model, then do some data prep on a validation data set.
[00:20:18.440 --> 00:20:26.600]   And then knowing the name of the which like fine tune model, open API actually needs to talk to to get these predictions.
[00:20:26.600 --> 00:20:30.880]   We can run the cell, which will essentially perform the inference and validation data.
[00:20:31.680 --> 00:20:40.000]   And so as you can see, since we just have like 30 validation images in our validation data set and we've performing inference like on all of them, it didn't take really long.
[00:20:40.000 --> 00:20:43.280]   It took like under a minute to like compute all of those results.
[00:20:43.280 --> 00:20:47.120]   And now we'll go and we'll log these predictions as a WNB table against that.
[00:20:47.120 --> 00:20:48.760]   We can like interactively explore them.
[00:20:48.760 --> 00:20:56.920]   And click on this link to open the run to which our predictions table has been logged.
[00:20:57.680 --> 00:21:06.480]   And so voila, now we can see a WNB table which contains the name of an episode, the target, which is like the real episode synopsis.
[00:21:06.480 --> 00:21:08.440]   And now GPT-3 has generated.
[00:21:08.440 --> 00:21:14.720]   And so here we can click around and explore the predictions or performing all of the like string ops operations that I've shown before.
[00:21:14.720 --> 00:21:17.040]   We can kind of like maybe find some.
[00:21:17.040 --> 00:21:19.000]   I'll say that this is like a shorter one.
[00:21:19.000 --> 00:21:21.520]   Dr. Indona takes to the first alien world.
[00:21:21.520 --> 00:21:23.640]   And here's like what the like that's the real one.
[00:21:23.640 --> 00:21:24.920]   That's what the model has generated.
[00:21:25.000 --> 00:21:30.440]   You see the doctor and friends arrive like on a planet, mysterious alien wood litter.
[00:21:30.440 --> 00:21:32.280]   Not always the theme wood.
[00:21:32.280 --> 00:21:38.640]   So like sometimes some of the stuff it generates, I'm like legitimately like, wow, like that sounds even cooler than the original idea.
[00:21:38.640 --> 00:21:48.360]   But if you haven't watched the show, it's like it's like doing quite a good job on picking up like on all of the like jargon and all of the like show specific things.
[00:21:48.360 --> 00:21:52.960]   And obviously this is like a really cool way to like actually get a sense of how the model is doing.
[00:21:53.320 --> 00:21:59.200]   And so another really cool way to test and use like the models we've just fine tuned is using OpenAI's Playground.
[00:21:59.200 --> 00:22:00.720]   And here's how we can do it.
[00:22:00.720 --> 00:22:09.360]   So we go to our project page and we can use this tab right here to learn, learn the model's name.
[00:22:09.360 --> 00:22:11.520]   So like that's the last fine tune job that we've done.
[00:22:11.520 --> 00:22:12.360]   And that's its name.
[00:22:12.360 --> 00:22:18.120]   Then we go to OpenAI's Playground and here we can select the engine.
[00:22:18.120 --> 00:22:25.000]   And then I go here and I look for a career model, which ends on 21, which ends on 21, 23.
[00:22:25.000 --> 00:22:27.560]   Yep. So here it is.
[00:22:27.560 --> 00:22:35.640]   And I'll also add here as a stop sequence our word and it was added to our dataset while pre-processing.
[00:22:35.640 --> 00:22:41.600]   And so now we can show some funny here, like say the doctor and machine learning.
[00:22:46.680 --> 00:22:56.720]   The humans are being controlled by a virus, but the doctor has been experimenting with a new kind of intelligence, machine learning that he hopes will be able to break the hold of the virus.
[00:22:56.720 --> 00:22:59.160]   OK, so it's generating.
[00:22:59.160 --> 00:23:00.040]   So let's try some more.
[00:23:00.040 --> 00:23:07.960]   Technical college, computer barriers to reprogram it.
[00:23:10.440 --> 00:23:18.880]   Maybe the next victim or we can try something like, I don't know, like even here, try to do some the doctor and GPT-3.
[00:23:18.880 --> 00:23:30.920]   And so as you can see there, he encounters a crashed spaceship from Earth and the crew of a ghost ship, the GPT-3, who are the descendants of the humans on the last expedition from Earth.
[00:23:30.920 --> 00:23:38.600]   And so as you can see, it's like actually really fun to click within this OpenAI Playground UI and actually generate some new stuff using GPT-3.
[00:23:39.440 --> 00:23:41.240]   So thank you for watching this video.
[00:23:41.240 --> 00:23:46.840]   As I was saying, I wrote like a really detailed guide and the whole training process that I linked in the video description.
[00:23:46.840 --> 00:23:54.680]   But that guy also can play that game of real or fake where you get to guess which episode is actually real and which one was generated.
[00:23:54.680 --> 00:24:02.880]   So if you want to also try, whether you'd find the real one or would GPT-3 fool you, you may also enjoy reading that report for that reason.
[00:24:03.760 --> 00:24:11.840]   But yeah, as always, smash the like button if you enjoyed this video and consider subscribing to our channel to see more tutorials, interviews and talks.
[00:24:11.840 --> 00:24:13.800]   And thank you for watching this video.
[00:24:13.800 --> 00:24:15.960]   I really hope you enjoyed it and found it useful.

