
[00:00:00.000 --> 00:00:05.200]   A book that Trenton recommended, The Symbolic Species, has this really interesting argument.
[00:00:05.200 --> 00:00:11.440]   When we just think of language as this contingent and maybe suboptimal way to represent ideas.
[00:00:11.440 --> 00:00:17.280]   Actually, maybe one of the reasons that LLMs have succeeded is because language has evolved
[00:00:17.280 --> 00:00:22.960]   for tens of thousands of years to be this sort of cast in which young minds can develop.
[00:00:22.960 --> 00:00:29.120]   Certainly when you talk to multimodal or computer vision researchers versus when you talk to
[00:00:29.120 --> 00:00:34.560]   language model researchers, people who work in other modalities have to put enormous amounts
[00:00:34.560 --> 00:00:39.040]   of thought into exactly what the right representation space for the images is.
[00:00:39.040 --> 00:00:41.520]   Understanding the right level of representation there, really hard.
[00:00:41.520 --> 00:00:44.400]   In language, people are just like, "Well, I guess you just predict the next token."
[00:00:44.400 --> 00:00:52.000]   The case for a multimodal being a way to bridge the data wall or get past the data wall
[00:00:52.000 --> 00:00:58.320]   is based on the idea that the things you would have learned from more language tokens anyway,
[00:00:58.320 --> 00:00:59.760]   you can just get from YouTube.
[00:00:59.760 --> 00:01:02.480]   Has that actually been the case?
[00:01:02.480 --> 00:01:08.400]   How much positive transfer do you see between different modalities where actually the images
[00:01:08.400 --> 00:01:11.040]   are helping you be better at writing code or something?
[00:01:11.040 --> 00:01:14.960]   Just because the model is learning a latent capability is just from trying to understand
[00:01:14.960 --> 00:01:15.680]   the image.
[00:01:15.680 --> 00:01:20.480]   Yeah, I'm the wrong person to ask, but there are interesting interpretability pieces where
[00:01:20.480 --> 00:01:27.760]   if we fine tune on math problems, the model just gets better at entity recognition.
[00:01:27.760 --> 00:01:28.880]   Whoa, really?
[00:01:28.880 --> 00:01:35.280]   Yeah, so there's a paper from David Bow's lab recently where they investigate what actually
[00:01:35.280 --> 00:01:39.280]   changes in a model when I fine tune it with respect to the attention heads and these sorts
[00:01:39.280 --> 00:01:39.280]   of things.
[00:01:39.280 --> 00:01:40.080]   Fascinating.
[00:01:40.080 --> 00:01:46.960]   And they have this synthetic problem of box A has this object in it, box B has this other
[00:01:46.960 --> 00:01:47.680]   object in it.
[00:01:47.680 --> 00:01:49.760]   What was in this box?
[00:01:49.760 --> 00:01:52.400]   And it makes sense, right?
[00:01:52.400 --> 00:01:54.480]   That's beautiful.
[00:01:54.480 --> 00:01:59.040]   You're better at attending to the positions of different things, which you need for coding
[00:01:59.040 --> 00:02:00.400]   and manipulating math equations.
[00:02:00.400 --> 00:02:02.400]   I love this kind of research.
[00:02:02.400 --> 00:02:07.280]   One of the things you mentioned to me a long time ago is the evidence that when you train
[00:02:07.280 --> 00:02:10.480]   LLMs on code, they get better at reasoning in language.
[00:02:10.480 --> 00:02:14.480]   Which, unless it's the case that the comments in the code are just really high quality tokens
[00:02:14.480 --> 00:02:20.720]   or something, implies that to be able to think through how to code better, it makes you a
[00:02:20.720 --> 00:02:21.360]   better reasoner.
[00:02:21.360 --> 00:02:22.720]   That's crazy, right?
[00:02:22.720 --> 00:02:26.800]   I think that's one of the strongest pieces of evidence for scaling, just making the thing
[00:02:26.800 --> 00:02:29.680]   smart, that kind of positive transfer.
[00:02:29.680 --> 00:02:32.000]   I think this is true in two senses.
[00:02:32.000 --> 00:02:36.080]   One is just that modeling code obviously implies modeling a difficult reasoning process used
[00:02:36.080 --> 00:02:36.720]   to create it.
[00:02:36.720 --> 00:02:43.040]   But two, that code is a nice explicit structure of composed reasoning, I guess.
[00:02:43.040 --> 00:02:44.720]   If this, then that.
[00:02:44.720 --> 00:02:52.640]   Code's a lot of structure in that way that you could imagine transferring to other types
[00:02:52.640 --> 00:02:54.080]   of types of reasoning problem.
[00:02:54.080 --> 00:02:54.640]   Right.
[00:02:54.640 --> 00:03:01.680]   And crucially, the thing that makes it significant is that it's not just stochastically predicting
[00:03:01.680 --> 00:03:08.240]   the next token of words or whatever, because it's learned that Sally corresponds to murderer
[00:03:08.240 --> 00:03:10.400]   at the end of a Sherlock Holmes story.
[00:03:10.400 --> 00:03:15.520]   No, if there is some shared thing between code and language, it must be at a deeper
[00:03:15.520 --> 00:03:16.560]   level than the model has learned.
[00:03:16.560 --> 00:03:20.960]   Yeah, I think we have a lot of evidence that actual reasoning is occurring in these models
[00:03:20.960 --> 00:03:22.880]   and that they're not just stochastic parrots.
[00:03:22.880 --> 00:03:28.400]   It just feels very hard for me to believe that I haven't worked and played with these
[00:03:28.400 --> 00:03:28.960]   models.
[00:03:28.960 --> 00:03:34.320]   Yeah, my two immediate cash responses to this are one, the work on Othello and now other
[00:03:34.320 --> 00:03:38.720]   games where it's like, I give you a sequence of moves in the game and it turns out if you
[00:03:38.720 --> 00:03:44.960]   apply some pretty straightforward interpretability techniques, then you can get a board that
[00:03:44.960 --> 00:03:45.920]   the model has learned.
[00:03:45.920 --> 00:03:48.720]   And it's never seen the game board before anything, right?
[00:03:48.720 --> 00:03:49.840]   Like that's generalization.
[00:03:49.840 --> 00:03:55.280]   The other is Anthropic's influence functions paper that came out last year, where they
[00:03:55.280 --> 00:03:58.400]   look at the model outputs, like, please don't turn me off.
[00:03:58.400 --> 00:03:59.760]   I want to be helpful.
[00:03:59.760 --> 00:04:02.560]   And then they scan, like, what was the data that led to that?
[00:04:02.560 --> 00:04:08.480]   And like one of the data points that was very influential was someone dying of dehydration
[00:04:08.480 --> 00:04:11.600]   in the desert and like having like a will to keep surviving.
[00:04:13.040 --> 00:04:20.480]   And to me, that just seems like very clear generalization of motive rather than regurgitating,
[00:04:20.480 --> 00:04:21.760]   don't turn me off.

