
[00:00:00.000 --> 00:00:07.200]   In the next 10 years, we'll be building in partnership with ESA, the Fetch rover, which
[00:00:07.200 --> 00:00:12.200]   is more of a couple tricycle size rover that has to drive farther and faster because it's
[00:00:12.200 --> 00:00:16.560]   going to have to go pick up all those tubules, make it to a rendezvous point, take those
[00:00:16.560 --> 00:00:21.560]   tubules, fly them up out of the Martian atmosphere into space to a spacecraft, and then take
[00:00:21.560 --> 00:00:23.120]   that spacecraft back to Earth.
[00:00:23.120 --> 00:00:26.600]   Yes, that's ambitious, but we're NASA JPL.
[00:00:26.600 --> 00:00:30.960]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:30.960 --> 00:00:33.920]   and I'm your host, Lukas Biewald.
[00:00:33.920 --> 00:00:40.440]   Chris Battman is Chief Technology and Innovation Officer at NASA Jet Propulsion Laboratory,
[00:00:40.440 --> 00:00:44.160]   and he's the author of Machine Learning with TensorFlow, second edition.
[00:00:44.160 --> 00:00:48.800]   He recently worked on a number of space missions, including the Mars rover that just landed
[00:00:48.800 --> 00:00:52.400]   on Mars, and I could not be more excited to talk to him about that.
[00:00:52.400 --> 00:00:56.400]   All right, so I wanted to talk to you about your book and about your career, but I saw
[00:00:56.400 --> 00:01:00.760]   that you did some work on the recent NASA rover or were involved somehow, and I think
[00:01:00.760 --> 00:01:03.720]   probably everyone was watching that in the news and getting excited about it.
[00:01:03.720 --> 00:01:07.400]   So I was wondering if you could tell us what work you did and what it felt like to see
[00:01:07.400 --> 00:01:12.560]   that on Mars and how machine learning could help with projects like that.
[00:01:12.560 --> 00:01:15.720]   Yeah, Lukas, I'm really interested in the new rover.
[00:01:15.720 --> 00:01:20.820]   It's called Perseverance, a successful landing, February 18th, entry, descent, and landing.
[00:01:20.820 --> 00:01:25.940]   This is a Volkswagen bug-sized rover, very similar to the size of the 2012 Curiosity
[00:01:25.940 --> 00:01:28.980]   rover or MSL, Mars Science Laboratory.
[00:01:28.980 --> 00:01:33.140]   So it also necessitated the development of this new entry, descent, and landing that
[00:01:33.140 --> 00:01:36.020]   we piloted in 2012, which is this Skycrane.
[00:01:36.020 --> 00:01:43.880]   It's literally a robotic craft that lowers on a crane, this rover down onto the surface
[00:01:43.880 --> 00:01:47.940]   of Mars for a nice soft landing and so on and so forth.
[00:01:47.940 --> 00:01:49.220]   So that was piloted again.
[00:01:49.220 --> 00:01:50.940]   It's only the second time that's been used.
[00:01:50.940 --> 00:01:51.940]   That was amazing.
[00:01:51.940 --> 00:01:56.700]   And obviously, the 2020 rover, one of the cool parts about it, it's got this helicopter,
[00:01:56.700 --> 00:01:59.100]   this drone helicopter on it called Ingenuity.
[00:01:59.100 --> 00:02:03.460]   And so we do naming contests throughout the United States with kids in schools and ask
[00:02:03.460 --> 00:02:07.620]   them to name the rover and name, in this case, the helicopter, which is really cool.
[00:02:07.620 --> 00:02:11.620]   And I think it really symbolizes everybody's feelings, I think, during this pandemic, is
[00:02:11.620 --> 00:02:15.380]   perseverance and also just the humankind ingenuity.
[00:02:15.380 --> 00:02:18.720]   But in terms of what we were involved with, there's a couple of things.
[00:02:18.720 --> 00:02:22.140]   So I'm the Chief Technology and Innovation Officer at NASA JPL.
[00:02:22.140 --> 00:02:26.460]   I run the Artificial Intelligence, Analytics, and Innovation Division.
[00:02:26.460 --> 00:02:28.820]   We're basically cross-cutting consultants.
[00:02:28.820 --> 00:02:33.700]   We do the AI practice, so we consult out to missions, projects, and things like that,
[00:02:33.700 --> 00:02:34.700]   the cloud practice.
[00:02:34.700 --> 00:02:38.860]   And we also have some data visualization and infusion folks working in kind of new ways
[00:02:38.860 --> 00:02:39.860]   of tech.
[00:02:39.860 --> 00:02:41.780]   A couple of different areas that we helped on 2020.
[00:02:41.780 --> 00:02:44.780]   The first was in a concept that we call Drive by Science.
[00:02:44.780 --> 00:02:45.880]   It kind of works like this.
[00:02:45.880 --> 00:02:51.120]   We were partnering with the Mars Surface Mobility Group and a team led by Hiro Ohno there.
[00:02:51.120 --> 00:02:53.860]   And basically it works like this with Drive by Science.
[00:02:53.860 --> 00:02:56.380]   So Earth to Mars, 11 minutes, right?
[00:02:56.380 --> 00:03:00.620]   At least, round trip, light time, send a command there, get a message back.
[00:03:00.620 --> 00:03:02.340]   And so that's a very thin pipe.
[00:03:02.340 --> 00:03:06.620]   And so right now, Mars Surface Operations, even for the Curiosity rover, but also for
[00:03:06.620 --> 00:03:12.900]   Perseverance, it uses about 200 images a day to plan what to do the next day.
[00:03:12.900 --> 00:03:16.300]   Because thin pipe images are expensive, this and that.
[00:03:16.300 --> 00:03:21.000]   The other thing that's really important on these rovers are basically, I say, these are
[00:03:21.000 --> 00:03:25.420]   elephant-sized vehicles with pea-sized brains, unfortunately.
[00:03:25.420 --> 00:03:26.520]   And there's a reason for that.
[00:03:26.520 --> 00:03:31.900]   They're running basically a Rad 750, which is like an iPhone 1 processor in it.
[00:03:31.900 --> 00:03:34.260]   And why are they running such older technology?
[00:03:34.260 --> 00:03:35.260]   Well, cosmic radiation.
[00:03:35.260 --> 00:03:40.040]   When we put hardware up in space, cosmic radiation does wiggy stuff to the hardware.
[00:03:40.040 --> 00:03:43.040]   It flips bits from ones to zeros, zeros to ones.
[00:03:43.040 --> 00:03:47.500]   And so we typically only fly things that are radiation hardened, which pushes us on the
[00:03:47.500 --> 00:03:50.820]   technology low tick instead of the up tick for that.
[00:03:50.820 --> 00:03:56.060]   Tomorrow we'll have high performance spaceflight computing, future GPU-like processors that
[00:03:56.060 --> 00:03:57.060]   are radiation hardened.
[00:03:57.060 --> 00:04:01.100]   And we have some technology demonstrations of that today with things like the Snapdragon,
[00:04:01.100 --> 00:04:03.620]   which is on the helicopter, actually.
[00:04:03.620 --> 00:04:05.620]   It's running a Qualcomm Snapdragon.
[00:04:05.620 --> 00:04:08.680]   And so we can do that because it's a technology demonstration.
[00:04:08.680 --> 00:04:12.220]   It's not critical to the core mission of the rover and things like that.
[00:04:12.220 --> 00:04:16.700]   And so in that future, when we have big brains on these rovers and assets and things like
[00:04:16.700 --> 00:04:21.140]   that, can we run deep learning on board instead of getting 200 images back, sending it across
[00:04:21.140 --> 00:04:22.380]   that thin pipe?
[00:04:22.380 --> 00:04:24.500]   What if we could give you back a million captions?
[00:04:24.500 --> 00:04:28.260]   What if we could run Google Show and Tell or an adaptation of that using transfer learning
[00:04:28.260 --> 00:04:32.380]   like we've done, which is called Scotty for science terrain captioning?
[00:04:32.380 --> 00:04:34.060]   We name all of our stuff like Star Trek.
[00:04:34.060 --> 00:04:37.320]   And what if we're running Scotty on board and we can give you 1 million captions back?
[00:04:37.320 --> 00:04:39.420]   So we call that drive by science.
[00:04:39.420 --> 00:04:44.800]   And then another area I'll just mention, and I'll shut up, and I like to make this a conversation.
[00:04:44.800 --> 00:04:49.320]   And another area is we call energy aware optimal auto navigation.
[00:04:49.320 --> 00:04:51.180]   It's the same type of concept.
[00:04:51.180 --> 00:04:55.580]   Looking out in the distance for the rover, if it sees imagery, if it sees sand, it knows
[00:04:55.580 --> 00:04:59.480]   those wheels aren't going to catch as well on it, and it's going to use more power.
[00:04:59.480 --> 00:05:01.900]   If it sees rocky, it's going to catch the wheels better.
[00:05:01.900 --> 00:05:02.900]   It's going to use less power.
[00:05:02.900 --> 00:05:07.360]   So looking at energy aware optimal auto navigation using a similar concept.
[00:05:07.360 --> 00:05:09.780]   Those are the big things we've been working on.
[00:05:09.780 --> 00:05:11.100]   And so I guess that's really interesting.
[00:05:11.100 --> 00:05:17.060]   So do you do any kind of machine learning now on the rover?
[00:05:17.060 --> 00:05:20.680]   Is that even possible with the hardware you have?
[00:05:20.680 --> 00:05:25.980]   And if you have a Snapdragon on the helicopter, it seems like you could do some in that or
[00:05:25.980 --> 00:05:26.980]   try to do some.
[00:05:26.980 --> 00:05:31.700]   So is there any happening or is it mostly kind of older techniques for now?
[00:05:31.700 --> 00:05:36.980]   Yeah, a lot of it is human in the loop, but there are some elements of autonomy, both
[00:05:36.980 --> 00:05:39.580]   in terrain classification.
[00:05:39.580 --> 00:05:43.020]   So we have been doing a number of work to take newer modern algorithms.
[00:05:43.020 --> 00:05:49.220]   So the interesting part is DevOps at the edge where the edge is Mars, right?
[00:05:49.220 --> 00:05:53.260]   We talk about the edge today in the cloud or in IoT.
[00:05:53.260 --> 00:05:54.260]   So it's DevOps.
[00:05:54.260 --> 00:05:58.980]   So what you test terrestrially, you've got to make sure that we can uplink it and port
[00:05:58.980 --> 00:06:04.220]   it to, again, these older devices and in some cases, devices that were deployed almost eight
[00:06:04.220 --> 00:06:06.800]   years ago, like Curiosity and things like that.
[00:06:06.800 --> 00:06:08.220]   And so we have been working on that.
[00:06:08.220 --> 00:06:13.280]   There is an algorithm called Spock, again, Star Trek names, but this is a soil property
[00:06:13.280 --> 00:06:14.280]   object classifier.
[00:06:14.280 --> 00:06:19.100]   It's like a terrain classifier and we can run that on the older devices.
[00:06:19.100 --> 00:06:24.940]   Obviously the tricks with that are you don't have a GPU, you may have to quantize the models,
[00:06:24.940 --> 00:06:29.180]   trade for accuracy and performance and things like that within acceptable bounds.
[00:06:29.180 --> 00:06:32.620]   And so a lot of these things are for human subject matter expert review or for mission
[00:06:32.620 --> 00:06:35.500]   tactical ops review with human in the loop.
[00:06:35.500 --> 00:06:39.940]   The more in the future we can get that out of the loop and more autonomous decisions,
[00:06:39.940 --> 00:06:40.940]   we're going to need it.
[00:06:40.940 --> 00:06:42.460]   And I'll say one quick example.
[00:06:42.460 --> 00:06:46.620]   The next mission is called Mars Sample Return in the program.
[00:06:46.620 --> 00:06:50.480]   And the basic idea is this, this big car-sized rover driving around Perseverance.
[00:06:50.480 --> 00:06:55.320]   One of the things it does is it's coring rocks and it's going to drop tubules of those cored
[00:06:55.320 --> 00:06:59.300]   rocks as it drives over the next 10 years.
[00:06:59.300 --> 00:07:04.200]   In the next 10 years, we'll be building in partnership with ESA, the Fetch rover, which
[00:07:04.200 --> 00:07:09.000]   is more of a couple of tricycle-sized rover that has to drive farther and faster because
[00:07:09.000 --> 00:07:13.480]   it's going to have to go pick up all those tubules, make it to a rendezvous point, take
[00:07:13.480 --> 00:07:18.240]   those tubules, fly them up out of the Martian atmosphere into space to a spacecraft, and
[00:07:18.240 --> 00:07:20.080]   then take that spacecraft back to earth.
[00:07:20.080 --> 00:07:23.040]   Yes, that's ambitious, but we're NASA JPL.
[00:07:23.040 --> 00:07:29.360]   But that whole thing, you need obviously more increased autonomy with that 11 minute light
[00:07:29.360 --> 00:07:34.940]   time as well as charging and all the other stuff that we got to do on the rovers to basically
[00:07:34.940 --> 00:07:37.700]   make sure that they can operate successfully.
[00:07:37.700 --> 00:07:42.640]   Can you push firmware updates at all with the current stuff you have?
[00:07:42.640 --> 00:07:44.440]   Dive into that for me, like updates and where?
[00:07:44.440 --> 00:07:47.160]   I guess, can you update the software from earth?
[00:07:47.160 --> 00:07:51.280]   Is it like once it's in there, is it like set forever?
[00:07:51.280 --> 00:07:57.600]   Oh yeah, they do update the software from earth, but there's windows of doing that.
[00:07:57.600 --> 00:08:03.160]   There's times in the mission life cycle when that's acceptable risk, or they'll allow us
[00:08:03.160 --> 00:08:05.640]   to do that or things like that.
[00:08:05.640 --> 00:08:09.380]   And then there are times when they won't, obviously, during critical mission operations
[00:08:09.380 --> 00:08:13.840]   or associated with some science event or things like that.
[00:08:13.840 --> 00:08:17.360]   And so it really depends on the mission life cycle, but we do have the capability to uplink
[00:08:17.360 --> 00:08:19.240]   and even to update things.
[00:08:19.240 --> 00:08:23.440]   In the past, those mostly have gone well, but sometimes there have been issues with
[00:08:23.440 --> 00:08:27.800]   updating and so they're very reticent to do that a lot.
[00:08:27.800 --> 00:08:33.040]   But we do have these sort of technology opportunities to update the assets out in space.
[00:08:33.040 --> 00:08:34.560]   And sometimes they even compete them.
[00:08:34.560 --> 00:08:38.600]   They'll issue a proposal solicitation and get the best ideas and then do stuff like
[00:08:38.600 --> 00:08:39.600]   that.
[00:08:39.600 --> 00:08:40.600]   Cool.
[00:08:41.520 --> 00:08:48.560]   So I guess outside of the rover, what other ML projects are going on at the JPL right
[00:08:48.560 --> 00:08:49.560]   now?
[00:08:49.560 --> 00:08:51.680]   Yeah, there's a lot.
[00:08:51.680 --> 00:08:58.280]   I like to talk about it in sort of different pocket areas of ML and AI.
[00:08:58.280 --> 00:09:00.440]   One of the areas is cybersecurity.
[00:09:00.440 --> 00:09:06.480]   We look at signals, we do analysis with kind of like Data Lake or Delta Lake type of partners
[00:09:06.480 --> 00:09:11.800]   where we're getting signals from cyber and they're doing anomalies and stuff like that
[00:09:11.800 --> 00:09:12.800]   detection.
[00:09:12.800 --> 00:09:16.080]   Another that's really driven by computer vision is what we talked about.
[00:09:16.080 --> 00:09:21.560]   It's like the Mars surface, but not just Mars, future lunar missions, smallsats, cubesats,
[00:09:21.560 --> 00:09:23.800]   what can we do with imagery and computer vision?
[00:09:23.800 --> 00:09:27.880]   Another is basically what we call science planning and scheduling.
[00:09:27.880 --> 00:09:33.240]   Basically the idea is there is like, okay, we've got these football stadium sized dishes
[00:09:33.240 --> 00:09:39.120]   in Madrid, Spain, Canberra, Australia, Goldstone, California, we call that the deep space network.
[00:09:39.120 --> 00:09:42.280]   You can imagine these things, they're not just supporting the United States, they're
[00:09:42.280 --> 00:09:45.160]   supporting all of our international partners for missions.
[00:09:45.160 --> 00:09:49.920]   Everybody must use the DSN because they're just this international asset, this world
[00:09:49.920 --> 00:09:54.680]   asset to communicate in deep space and not everyone has built such infrastructure.
[00:09:54.680 --> 00:10:00.200]   And so in any given week, the DSN is massively oversubscribed because missions know possibly
[00:10:00.200 --> 00:10:04.000]   months, possibly years ahead of time what their critical events are and when they need
[00:10:04.000 --> 00:10:06.560]   tracks on the DSN to track things.
[00:10:06.560 --> 00:10:11.120]   And so you can imagine this very difficult scheduling problem, 80% of which can be solved
[00:10:11.120 --> 00:10:14.240]   by traditional kind of like AI scheduling and planning.
[00:10:14.240 --> 00:10:19.600]   But the last 20% of which basically boils down to managers getting into a room and horse
[00:10:19.600 --> 00:10:21.480]   trading.
[00:10:21.480 --> 00:10:25.440]   So basically we've been doing a lot of work to kind of learn what those trades are using
[00:10:25.440 --> 00:10:29.920]   like deep reinforcement learning, experimenting with quantum computing, looking at mixed integer
[00:10:29.920 --> 00:10:34.160]   linear programming or MILP, traditional ways of doing that.
[00:10:34.160 --> 00:10:39.600]   And doing that in ways where we can apply ML to actually do a couple of things, learn
[00:10:39.600 --> 00:10:42.080]   what those moves that the mission managers make.
[00:10:42.080 --> 00:10:45.800]   Because whenever you ask them, they don't tell you because to be honest, it's just innate
[00:10:45.800 --> 00:10:46.800]   to them.
[00:10:46.800 --> 00:10:51.760]   It's like, well, of course I didn't really need six hours on the track on that dish for
[00:10:51.760 --> 00:10:52.760]   my mission.
[00:10:52.760 --> 00:10:53.760]   We could have lived with four.
[00:10:53.760 --> 00:10:56.160]   Okay, well, why didn't you tell someone that?
[00:10:56.160 --> 00:10:59.320]   Well, because you always ask for more than what you can get.
[00:10:59.320 --> 00:11:00.320]   So these types of things.
[00:11:00.320 --> 00:11:05.040]   And so the agent has to learn that and they've got to learn how to generate optimal candidate
[00:11:05.040 --> 00:11:08.680]   schedules that fulfill like 46 other constraints and other things.
[00:11:08.680 --> 00:11:10.320]   So that's another big area.
[00:11:10.320 --> 00:11:15.480]   And then finally, I'd be remiss if I didn't mention, there's a ton of ML just in science
[00:11:15.480 --> 00:11:18.160]   processing related to science data and instruments.
[00:11:18.160 --> 00:11:22.380]   And JPL has a whole science and instrument section and division.
[00:11:22.380 --> 00:11:27.840]   Their job is to basically get data off the instrument, do analytics, generate data products,
[00:11:27.840 --> 00:11:33.000]   build maps, build decision products, all of these things, help science research.
[00:11:33.000 --> 00:11:38.760]   ML is at the cusp of what I would call massive infusion in those areas.
[00:11:38.760 --> 00:11:44.480]   Lots of experimentation going on and they're at that crux of turning it into ML ops.
[00:11:44.480 --> 00:11:49.520]   And that's basically where it is besides IT and business, where we also are doing it with
[00:11:49.520 --> 00:11:52.720]   RPA and some of these other areas.
[00:11:52.720 --> 00:11:54.600]   The instrument use case sounded really interesting.
[00:11:54.600 --> 00:11:56.680]   Can you give me some concrete examples of that?
[00:11:56.680 --> 00:11:59.760]   I'm just totally unfamiliar with that whole space.
[00:11:59.760 --> 00:12:00.760]   Yeah.
[00:12:00.760 --> 00:12:06.720]   So imagine JPL minting first of a kind instruments, because that's what we do in earth sciences,
[00:12:06.720 --> 00:12:08.400]   space sciences, and planetary science.
[00:12:08.400 --> 00:12:09.720]   And there's a reason for that.
[00:12:09.720 --> 00:12:14.560]   The national labs are supposed to do that work that no other, whatever, a commercial
[00:12:14.560 --> 00:12:18.700]   industry or traditional civil servant places can do.
[00:12:18.700 --> 00:12:21.720]   And then once we do it, we're supposed to transition it into industry and stuff.
[00:12:21.720 --> 00:12:27.080]   And that is very much true for hyperspectral, where the field actually was mainly defined
[00:12:27.080 --> 00:12:32.400]   in some ways at JPL by people like Rob Green and the Avarice spectrometer and things like
[00:12:32.400 --> 00:12:37.480]   that, but also in other areas, radar, LIDAR and things like that.
[00:12:37.480 --> 00:12:40.640]   And so in these instruments, there are all sorts of things.
[00:12:40.640 --> 00:12:46.760]   The traditional model for missions at JPL is a phased life cycle, where pre-phase A
[00:12:46.760 --> 00:12:53.360]   and phase A is formulation, phase B is actual real costing, phase C is where you're building
[00:12:53.360 --> 00:12:58.440]   the mission out and you're actually building it, phase D is mission launch and whatever,
[00:12:58.440 --> 00:13:00.680]   and E is standard operations.
[00:13:00.680 --> 00:13:05.320]   And so associated with that life cycle at each stage, and in particular in phases D
[00:13:05.320 --> 00:13:10.160]   and E, besides delivering the mission bits from the instruments, the science data, the
[00:13:10.160 --> 00:13:14.200]   engineering data and stuff like that, NASA competes out typically.
[00:13:14.200 --> 00:13:15.200]   It does a couple of things.
[00:13:15.200 --> 00:13:19.600]   It does some directed work, but it also does some competition for basically analytics and
[00:13:19.600 --> 00:13:22.840]   ML and things like that on the analysis.
[00:13:22.840 --> 00:13:28.440]   But even during the mission life cycle phase, it's basically go from voltages, which are
[00:13:28.440 --> 00:13:35.840]   basically electrical signals that have measurements buried into them to geo-calibrated radiances.
[00:13:35.840 --> 00:13:40.720]   So radiance is calibrated to say some space on the earth where you got to map it using
[00:13:40.720 --> 00:13:48.560]   orbital parameters to basically a full physical model in some cases to extract out from those
[00:13:48.560 --> 00:13:53.680]   calibrated, geo-calibrated, geo-referenced radiances, what the hell it was measuring.
[00:13:53.680 --> 00:13:58.560]   And that in some cases is called level two data, and there's a massive amount of it.
[00:13:58.560 --> 00:14:03.400]   And in that, in some cases, even in the mission is where some missions stop.
[00:14:03.400 --> 00:14:07.420]   And then they compete out the level three, level four product generation, which is basically
[00:14:07.420 --> 00:14:13.440]   taking those swaths of instrument, actual measurements and mapping it to a geo globally
[00:14:13.440 --> 00:14:18.560]   gridded grid, and then doing other stuff and maybe combining other products on it.
[00:14:18.560 --> 00:14:22.900]   And so even in the mission production life cycle, there's an opportunity for ML.
[00:14:22.900 --> 00:14:28.540]   Some people are looking like they say, well, can I replace my full physics model with taking
[00:14:28.540 --> 00:14:36.480]   geo-calibrated radiances and then mapping them to a map, or even to values and measurements?
[00:14:36.480 --> 00:14:39.720]   Can I say, build a neural network to do that?
[00:14:39.720 --> 00:14:43.360]   Can I learn a representation of something with an auto encoder?
[00:14:43.360 --> 00:14:50.440]   Can I do in concept wise, like a regression or even a network or a CNN to do value predictions
[00:14:50.440 --> 00:14:51.440]   and stuff like that?
[00:14:51.440 --> 00:14:55.520]   And there's a lot of experimentation during the science mission operations now for doing
[00:14:55.520 --> 00:14:59.860]   that because obviously ML has the opportunity to cost much less physically, not require
[00:14:59.860 --> 00:15:04.700]   supercomputers to do some of these things on other specialized computers, but more commercially
[00:15:04.700 --> 00:15:09.960]   available from the cloud and GPUs, TPUs and things like that.
[00:15:09.960 --> 00:15:10.960]   And finally, well, go ahead.
[00:15:10.960 --> 00:15:11.960]   You were going to say something.
[00:15:11.960 --> 00:15:17.520]   Oh, I was just wondering when you're saying compete out these steps of the process, is
[00:15:17.520 --> 00:15:22.600]   that something where if I wanted to try to build a model to do this mapping, I could
[00:15:22.600 --> 00:15:23.600]   go to a website and get involved?
[00:15:23.600 --> 00:15:27.880]   Is this like Kaggle or is this like, how does that actually work?
[00:15:27.880 --> 00:15:28.880]   This is fabulous.
[00:15:28.880 --> 00:15:31.240]   The answer is no today and there's a reason for it.
[00:15:31.240 --> 00:15:35.020]   And actually this will parlay into the other thing I was going to say.
[00:15:35.020 --> 00:15:38.260]   So basically you get the level two data out, it's massive.
[00:15:38.260 --> 00:15:40.580]   So it's petabytes in some cases.
[00:15:40.580 --> 00:15:44.380]   So people, you know, you always say people want the level two data, but you really don't
[00:15:44.380 --> 00:15:45.980]   want the level two data.
[00:15:45.980 --> 00:15:51.420]   And in some cases it's such, it's so big that there may or may not be a requirement to preserve
[00:15:51.420 --> 00:15:55.220]   it because NASA may have made the decision or even other agencies, NOAA, you look at
[00:15:55.220 --> 00:15:59.860]   this to basically, well, we could always reproduce this using a big reprocessing campaign.
[00:15:59.860 --> 00:16:04.000]   What's the minimum bits and level products that we need to store and keep around because
[00:16:04.000 --> 00:16:05.160]   there are preservation requirements.
[00:16:05.160 --> 00:16:08.200]   And they always ask that question.
[00:16:08.200 --> 00:16:13.960]   And so, you know, the answer kind of related to that, to your question is that again, you
[00:16:13.960 --> 00:16:17.080]   always want the level two products and then you don't, you know, because it's too big.
[00:16:17.080 --> 00:16:18.080]   Okay.
[00:16:18.080 --> 00:16:19.080]   So what are those level two products stored in?
[00:16:19.080 --> 00:16:26.260]   They're HDF5, HDF4 with HDF EOS metadata, they're net CDF products, GRIB.
[00:16:26.260 --> 00:16:30.620]   There's probably, you know, a half a dozen archival formats, right.
[00:16:30.620 --> 00:16:33.980]   That aren't say machine learning ready, like a big table, right.
[00:16:33.980 --> 00:16:38.840]   You know, that have everything or a multi-dimensional table, you know, SciPy, NumPy to do it.
[00:16:38.840 --> 00:16:43.020]   But there's been a massive work in the Python community and other places to like integrate
[00:16:43.020 --> 00:16:44.020]   that stuff with.
[00:16:44.020 --> 00:16:48.300]   So, you know, so believe it or not, you know, HDF5 is very popular in machine learning for
[00:16:48.300 --> 00:16:52.220]   weights and, you know, Keras and all the work Francis and others did, you know, in some
[00:16:52.220 --> 00:16:54.020]   of these things to do that.
[00:16:54.020 --> 00:16:55.740]   Where did HDF5 come from?
[00:16:55.740 --> 00:16:58.380]   It came from NASA and Earth science, actually.
[00:16:58.380 --> 00:17:03.780]   It was an Earth science archival format from investment from NASA, NOAA and the EPA in
[00:17:03.780 --> 00:17:07.260]   the HDF group, which spun out as a separate organization to do it.
[00:17:07.260 --> 00:17:12.420]   So actually the storing of matrices, scalars, vectors, named hierarchical representations
[00:17:12.420 --> 00:17:16.180]   of them actually came from representing Earth science data.
[00:17:16.180 --> 00:17:19.580]   The challenge is what you get at that level.
[00:17:19.580 --> 00:17:21.840]   Again, it's not globally gridded.
[00:17:21.840 --> 00:17:23.060]   Some people don't know what to do.
[00:17:23.060 --> 00:17:28.420]   Like if you can't do a point, you know, in a coordinate reference system and get a value,
[00:17:28.420 --> 00:17:32.020]   their heads explode, people sometimes, because they don't understand these satellites generate
[00:17:32.020 --> 00:17:37.720]   these like weird U-shaped orbital swaths, you know, where the data is only valid at
[00:17:37.720 --> 00:17:39.020]   certain times.
[00:17:39.020 --> 00:17:42.660]   Everyone just assumes you can interrogate something and say, give me the data and do
[00:17:42.660 --> 00:17:43.660]   machine learning.
[00:17:43.660 --> 00:17:47.820]   But there's so much processing and level processing that you have to do, you know, beyond that.
[00:17:47.820 --> 00:17:48.980]   And so what do people do, Lucas?
[00:17:48.980 --> 00:17:51.700]   And this was the second thing I was going to talk about.
[00:17:51.700 --> 00:17:52.820]   There's also big opportunities.
[00:17:52.820 --> 00:17:56.260]   So the science mission sometimes stops at level two, data production.
[00:17:56.260 --> 00:18:00.260]   But I'm even saying there's ML opportunity there and people are looking at it.
[00:18:00.260 --> 00:18:03.820]   But even in the archival, you go to Earth science, you go to the DACS, they're called
[00:18:03.820 --> 00:18:06.220]   the distributed archive, active archive centers.
[00:18:06.220 --> 00:18:10.020]   These are for Earth science, nine places across the U.S. that you can go get data and you
[00:18:10.020 --> 00:18:11.540]   can download it today.
[00:18:11.540 --> 00:18:13.500]   But again, does it stop at the level two products?
[00:18:13.500 --> 00:18:15.100]   Does it stop at the level three?
[00:18:15.100 --> 00:18:18.540]   Once you get to level three, we're talking about a hundred time data reduction too, because
[00:18:18.540 --> 00:18:23.540]   these maps take less because they're interpolated or they're globally averaged, not specific
[00:18:23.540 --> 00:18:25.540]   interrogatable values at a point.
[00:18:25.540 --> 00:18:29.700]   I feel like this is obvious to you, but I want to have a concrete picture in my head
[00:18:29.700 --> 00:18:34.540]   of what one of these datasets is and then what the level two version is and level three
[00:18:34.540 --> 00:18:36.660]   and where it actually goes.
[00:18:36.660 --> 00:18:38.940]   Just one where I can really picture it.
[00:18:38.940 --> 00:18:39.940]   Yeah.
[00:18:39.940 --> 00:18:43.280]   Let's take OCO, which is the Orbiting Carbon Observatory.
[00:18:43.280 --> 00:18:49.180]   And it produces a value called XCO2, which is CO2 sources and sinks.
[00:18:49.180 --> 00:18:53.100]   So a column-based measurement of CO2 in the atmosphere.
[00:18:53.100 --> 00:18:59.340]   The level two products for OCO, it's actually called the OCO2 mission.
[00:18:59.340 --> 00:19:04.700]   What they look like is kind of like a U-shaped upside down bell curve.
[00:19:04.700 --> 00:19:09.540]   Say you take a world map and you project it out in a Cartesian space, so you flatten it
[00:19:09.540 --> 00:19:10.540]   out to two dimensions.
[00:19:10.540 --> 00:19:12.220]   Have you seen those maps?
[00:19:12.220 --> 00:19:17.860]   And then if you look at the data just based on how the satellite orbits, first off, there's
[00:19:17.860 --> 00:19:22.380]   little data over water and it's like an upside down U curve.
[00:19:22.380 --> 00:19:25.220]   And then it's almost like a sine wave.
[00:19:25.220 --> 00:19:26.700]   So it's like this.
[00:19:26.700 --> 00:19:31.460]   That's the level two data because the way the satellite is orbiting and when it turns
[00:19:31.460 --> 00:19:35.500]   on and when it doesn't to get measurements, because some of these things can't see through
[00:19:35.500 --> 00:19:41.860]   clouds or whatever, ends up being this track.
[00:19:41.860 --> 00:19:47.420]   That in many cases, independent of the instrument, spectrometer, radar, whatever, but just say
[00:19:47.420 --> 00:19:51.460]   in the OCO2 case, is a level two data product.
[00:19:51.460 --> 00:19:57.460]   And it might represent, I don't know, depending on the orbit life cycle, 14 days or something
[00:19:57.460 --> 00:20:00.460]   like that before you can get the full track across that.
[00:20:00.460 --> 00:20:02.940]   It depends on how long it takes to orbit.
[00:20:02.940 --> 00:20:06.460]   So just to make sure I understand, so the satellite's kind of looking down at the earth
[00:20:06.460 --> 00:20:10.300]   and measuring the amount of CO2 with some kind of spectral camera or something.
[00:20:10.300 --> 00:20:15.380]   And then it's downloading to you the amounts of CO2, but it's only along this sort of weird
[00:20:15.380 --> 00:20:18.500]   linear track that's just like its orbit over the earth.
[00:20:18.500 --> 00:20:20.500]   But then that track, I guess, is also moving.
[00:20:20.500 --> 00:20:24.740]   So you get different measurements in different places on the earth?
[00:20:24.740 --> 00:20:26.980]   Well, that's exactly right.
[00:20:26.980 --> 00:20:36.380]   And in your mind, you imagine, "Oh, we fly a mission like OCO2 and it covers a geo-global
[00:20:36.380 --> 00:20:37.380]   grid.
[00:20:37.380 --> 00:20:44.900]   I ought to be able to go to Russia or Africa or the United States at any time in any grid
[00:20:44.900 --> 00:20:46.580]   cell and get a value."
[00:20:46.580 --> 00:20:53.020]   Well, at level two, you can't, because at level two, you only have values where that
[00:20:53.020 --> 00:20:56.380]   satellite saw data in its orbit.
[00:20:56.380 --> 00:21:02.460]   Now at level three, what people do is they take that bell curve, the upside down one,
[00:21:02.460 --> 00:21:09.220]   and they basically interpolate or they average it so that you get values in neighboring cells
[00:21:09.220 --> 00:21:10.220]   and you color the cells.
[00:21:10.220 --> 00:21:14.140]   It's almost like in machine learning terms, like a self-organizing map.
[00:21:14.140 --> 00:21:19.820]   So you go from basically like a sine wave to a self-organizing map, geo-globally grid,
[00:21:19.820 --> 00:21:23.860]   where you can interrogate the values at any point in time.
[00:21:23.860 --> 00:21:26.180]   And this is like a scientific process.
[00:21:26.180 --> 00:21:30.180]   And now is there an official level two to level three mapping?
[00:21:30.180 --> 00:21:35.100]   How do you even know, how do you compare if two people did different mappings, which one
[00:21:35.100 --> 00:21:36.100]   is the best one?
[00:21:36.100 --> 00:21:37.100]   Yeah, there you go.
[00:21:37.100 --> 00:21:38.460]   And that's a great point.
[00:21:38.460 --> 00:21:41.380]   And it's usually, it's controlled by the science team.
[00:21:41.380 --> 00:21:44.260]   There are some standards in instrument families.
[00:21:44.260 --> 00:21:49.020]   So there's a way to do this typically with spectrometers.
[00:21:49.020 --> 00:21:51.460]   There's a way to do it with radars.
[00:21:51.460 --> 00:21:55.620]   And then there are other requirements like what's the precision needed?
[00:21:55.620 --> 00:22:00.100]   What type of data density do we need at a particular pixel?
[00:22:00.100 --> 00:22:01.500]   What's the resolution?
[00:22:01.500 --> 00:22:07.500]   And those are dollars and costs because they translate to mass and power in the instrument.
[00:22:07.500 --> 00:22:13.300]   And they also translate to processing time and whatever afterwards when you get the data.
[00:22:13.300 --> 00:22:18.900]   So these are all little knobs that mission managers, science teams, the science requirements
[00:22:18.900 --> 00:22:19.900]   for the mission trade.
[00:22:19.900 --> 00:22:23.940]   But I just want to understand, so is the part that you're saying you compete out sort of
[00:22:23.940 --> 00:22:29.100]   like the mapping from level two to level three here, like going from the raw satellite data
[00:22:29.100 --> 00:22:30.500]   to the earth data?
[00:22:30.500 --> 00:22:34.660]   Or is there like a further mapping where most of the machine learning happens?
[00:22:34.660 --> 00:22:36.160]   Oh yeah, great.
[00:22:36.160 --> 00:22:40.780]   So by definition, if you look at these NASA missions and earth science, very true and
[00:22:40.780 --> 00:22:46.420]   planetary too, like the archives again, and remember the size from level two, again, a
[00:22:46.420 --> 00:22:49.860]   hundred times bigger in many cases than level three and level four.
[00:22:49.860 --> 00:22:55.220]   The dollars that they go, dollar per bit in preservation, they've got to cut off at some
[00:22:55.220 --> 00:22:56.580]   level, right?
[00:22:56.580 --> 00:23:00.260]   Because they don't have infinite money, but they're supposed to preserve this quote forever,
[00:23:00.260 --> 00:23:01.260]   right?
[00:23:01.260 --> 00:23:02.340]   In some cases.
[00:23:02.340 --> 00:23:07.060]   And so we're talking hundreds of millions of US dollars for investment in these archives
[00:23:07.060 --> 00:23:10.060]   just to keep the bits around.
[00:23:10.060 --> 00:23:17.040]   And so a lot of the archive systems will say, look, or some missions will say, we're only
[00:23:17.040 --> 00:23:19.700]   distributing up to the level two products.
[00:23:19.700 --> 00:23:22.620]   Some of them will distribute level three, but they'll have different rolling windows
[00:23:22.620 --> 00:23:27.660]   of how long they'll be available, using their standard algorithms and things like that.
[00:23:27.660 --> 00:23:29.380]   Maybe they don't keep the level three around forever.
[00:23:29.380 --> 00:23:32.060]   So now what does NASA do?
[00:23:32.060 --> 00:23:37.140]   Like I was saying with you to compete and to do analytics and really grow, rise the
[00:23:37.140 --> 00:23:39.300]   tide in ML and some of these things.
[00:23:39.300 --> 00:23:44.780]   Well, what they'll do is they'll say, okay, in a particular earth science area or whatever,
[00:23:44.780 --> 00:23:50.140]   they'll say, well, we have a number of recurring, and NASA has this thing called research opportunities
[00:23:50.140 --> 00:23:55.100]   in space and earth sciences or roses, but these programs in which they release 40 different
[00:23:55.100 --> 00:23:59.580]   programs or whatever, to basically write a proposal, compete against other NASA centers
[00:23:59.580 --> 00:24:05.780]   or universities or commercial industry to basically do higher order processing, right?
[00:24:05.780 --> 00:24:10.540]   To generate maybe improved level products, you know, at the level three and level four
[00:24:10.540 --> 00:24:15.580]   levels, maybe that costs less, that are more accurate, that didn't take as long to do the
[00:24:15.580 --> 00:24:20.500]   algorithm for, or as much scientific expertise or knowledge, and those are all the knobs
[00:24:20.500 --> 00:24:21.700]   where they'll do that on.
[00:24:21.700 --> 00:24:25.940]   It doesn't mean that such algorithms automatically get put into standard level processing or
[00:24:25.940 --> 00:24:28.820]   into the archives, but there's the opportunity to do it.
[00:24:28.820 --> 00:24:32.860]   And then some people, this is the beauty, NASA doesn't have to control everything, neither
[00:24:32.860 --> 00:24:34.620]   does NOAA, whatever.
[00:24:34.620 --> 00:24:39.460]   This creates a market and an opportunity downstream for universities, commercial partners or whatever
[00:24:39.460 --> 00:24:41.660]   to build better products.
[00:24:41.660 --> 00:24:45.820]   You know, and if they do, these could become the standards eventually, and NASA is very
[00:24:45.820 --> 00:24:50.500]   happy to do it because they've still fulfilled their mission of researching, observing, and
[00:24:50.500 --> 00:24:52.900]   making those data products for free to the world.
[00:24:52.900 --> 00:24:56.500]   So I cut you off and I was really interested in this sort of, like you keep saying kind
[00:24:56.500 --> 00:24:57.500]   of competed out.
[00:24:57.500 --> 00:25:01.820]   Like, so are you saying that the reason that I couldn't try to go build a better mapping
[00:25:01.820 --> 00:25:06.140]   and give it to NASA, is that because the data is so big, it would be hard to get, or is
[00:25:06.140 --> 00:25:09.020]   there some other bottleneck there?
[00:25:09.020 --> 00:25:10.020]   Yeah.
[00:25:10.020 --> 00:25:13.180]   Well, it's kind of like a combination of both.
[00:25:13.180 --> 00:25:17.980]   You might be able to do it because you're Lucas super powered access to massive cloud,
[00:25:17.980 --> 00:25:26.900]   whatever, you know, but it's harder for a postdoc or somebody at K through 12 or someone
[00:25:26.900 --> 00:25:31.900]   at university undergraduate to be able to get the type of, yeah, get the type of access
[00:25:31.900 --> 00:25:32.980]   to basically do this.
[00:25:32.980 --> 00:25:38.020]   And it's also part of just the way science occurs.
[00:25:38.020 --> 00:25:39.620]   There's this movement as you and I know.
[00:25:39.620 --> 00:25:44.340]   So in the context of like ML to ML ops, right, lots of people still use Jupiter.
[00:25:44.340 --> 00:25:48.940]   I still do everything like locally in some cases, they'll just do Jupiter locally to
[00:25:48.940 --> 00:25:49.940]   do stuff.
[00:25:49.940 --> 00:25:54.660]   But then there is this movement, Jupiter hub, but even beyond that, like getting stuff out
[00:25:54.660 --> 00:25:59.540]   of Jupiter, Python, ML ops, you know, frameworks, TensorFlow, you know, PyTorch, whatever, you
[00:25:59.540 --> 00:26:04.580]   know, there's this whole movement again, from like the science researcher, long tail to
[00:26:04.580 --> 00:26:10.380]   doing it in a team with DevOps, with, you know, all these things, good software engineering,
[00:26:10.380 --> 00:26:12.100]   product tags and things like that.
[00:26:12.100 --> 00:26:15.380]   The exact same thing exists in the context of science research.
[00:26:15.380 --> 00:26:20.580]   In fact, there are many scientists would rather much rather love pull all the data down to
[00:26:20.580 --> 00:26:27.820]   their laptop and crank on it with MVIDL or, you know, MATLAB or, or even Python, right?
[00:26:27.820 --> 00:26:31.260]   Like, because that's what they've learned in atmospheric science or that's what, and
[00:26:31.260 --> 00:26:36.460]   so there's almost this mentality or paradigm shift, you know, that is even undergoing there
[00:26:36.460 --> 00:26:38.940]   too, you know, so that's another part in it, Lucas.
[00:26:38.940 --> 00:26:45.380]   And then finally, the last thing I would say is that you also have to know basically how
[00:26:45.380 --> 00:26:49.800]   and some of this stuff is self documenting and some of it isn't, but what assumptions
[00:26:49.800 --> 00:26:55.080]   were made at the level two and before era to get to level two, right?
[00:26:55.080 --> 00:27:00.020]   Because they already, you've already started potentially to propagate some error bars,
[00:27:00.020 --> 00:27:06.660]   then to get to level two, to go from geo-located physically calibrated radiances, you know,
[00:27:06.660 --> 00:27:10.260]   to a level two, you know, product, you've already made some assumptions.
[00:27:10.260 --> 00:27:12.620]   And so NASA does document those.
[00:27:12.620 --> 00:27:17.060]   Those are called algorithm theoretical basis documents, and they do make them available,
[00:27:17.060 --> 00:27:23.900]   but you also need to dive into some of those to know how to then apply ML to go beyond.
[00:27:23.900 --> 00:27:24.900]   Got it.
[00:27:24.900 --> 00:27:27.220]   So it's really just a really hard problem.
[00:27:27.220 --> 00:27:31.500]   It's not that there's any resistance to letting people try it, I guess.
[00:27:31.500 --> 00:27:32.500]   Totally.
[00:27:32.500 --> 00:27:33.500]   Yeah.
[00:27:33.500 --> 00:27:35.120]   I'd say there's welcomeness for people trying it.
[00:27:35.120 --> 00:27:36.380]   It's just a really hard problem.
[00:27:36.380 --> 00:27:37.380]   You nailed it.
[00:27:37.380 --> 00:27:41.660]   You know, I'm a little shy about asking this question, but, and maybe it goes nowhere,
[00:27:41.660 --> 00:27:46.340]   but I just kind of want to, because, you know, me and my co-founders all love this game called
[00:27:46.340 --> 00:27:48.580]   Kerbal Space Program, and we all play it.
[00:27:48.580 --> 00:27:52.620]   I wonder if people at JPL are aware of this and if, you know, kind of people coming in,
[00:27:52.620 --> 00:27:57.900]   I feel like all my instincts about rockets come from this one video game that I got obsessed
[00:27:57.900 --> 00:27:59.460]   with a few years ago.
[00:27:59.460 --> 00:28:01.340]   Does this come up at all?
[00:28:01.340 --> 00:28:02.340]   Not with me.
[00:28:02.340 --> 00:28:06.300]   I'll say a lot of JPLers are playing Among Us right now, but that's not Kerbal Space
[00:28:06.300 --> 00:28:07.300]   Program.
[00:28:07.300 --> 00:28:08.300]   Tell me about it.
[00:28:08.300 --> 00:28:09.300]   Tell me, what is it?
[00:28:09.300 --> 00:28:14.420]   Oh man, it's this amazing game where it's a very, like, I think what's fun is it's very
[00:28:14.420 --> 00:28:16.940]   self-directed.
[00:28:16.940 --> 00:28:21.780]   There's not really like a clear goal, but you basically try to build rockets and put
[00:28:21.780 --> 00:28:23.100]   them into orbit.
[00:28:23.100 --> 00:28:27.500]   And so you actually learn, I feel like I learned a lot of just how complicated it is to make
[00:28:27.500 --> 00:28:32.180]   a satellite and then try to get a rocket to that satellite and the trade-offs between
[00:28:32.180 --> 00:28:36.580]   like, you know, like you want a lot of thrust at the beginning, but then that can be like
[00:28:36.580 --> 00:28:37.900]   an inefficient engine.
[00:28:37.900 --> 00:28:42.020]   And, you know, like you realize like actually like going to a planet with a high gravity
[00:28:42.020 --> 00:28:47.100]   and coming back is like way, way harder than just orbiting it, for example.
[00:28:47.100 --> 00:28:48.100]   So I don't know.
[00:28:48.100 --> 00:28:52.300]   I was curious if this, because I feel like when I talk to people at places like NASA,
[00:28:52.300 --> 00:28:55.660]   I have all these specific questions and then sometimes they're just like, "Oh my God, you
[00:28:55.660 --> 00:29:00.020]   must have played that Kerbal Space Program game."
[00:29:00.020 --> 00:29:01.020]   That's awesome.
[00:29:01.020 --> 00:29:02.580]   No, well, I'll bring it up.
[00:29:02.580 --> 00:29:07.780]   So for your audience and also for you, this isn't always clear, you know, and we even
[00:29:07.780 --> 00:29:09.660]   did it ourselves just now.
[00:29:09.660 --> 00:29:13.900]   So I work at NASA, yes, but I'm at the Jet Propulsion Laboratory, which is one of the
[00:29:13.900 --> 00:29:14.980]   nine NASA centers.
[00:29:14.980 --> 00:29:20.460]   And so typically the NASA centers have different expertises and our big, or main things, you
[00:29:20.460 --> 00:29:21.780]   know, that they do.
[00:29:21.780 --> 00:29:26.620]   Big things that JPL does and Pasadena, California, amongst all the other nine NASA centers is
[00:29:26.620 --> 00:29:29.420]   that we run autonomous exploration.
[00:29:29.420 --> 00:29:31.020]   We do a lot of autonomous exploration.
[00:29:31.020 --> 00:29:35.780]   In fact, we're a center of excellence and NASA's only federally funded and research
[00:29:35.780 --> 00:29:40.800]   and development center for that, National Lab and the Mars program.
[00:29:40.800 --> 00:29:45.980]   But we don't do very much at all, very little, like human spaceflight.
[00:29:45.980 --> 00:29:53.180]   And so those, like a lot of the other NASA centers, like Marshall or Johnson or Kennedy,
[00:29:53.180 --> 00:29:57.320]   you know, like mission control or launch pads, that's where you see a lot of that.
[00:29:57.320 --> 00:30:02.060]   But our expertise typically is in, so if it's robots and it's deep space, that's usually
[00:30:02.060 --> 00:30:03.060]   us.
[00:30:03.060 --> 00:30:04.060]   So.
[00:30:04.060 --> 00:30:07.500]   Well, just for the record, that comes up too a lot in Kerbal Space Program.
[00:30:07.500 --> 00:30:08.500]   Oh, awesome.
[00:30:08.500 --> 00:30:09.500]   Well, okay, that's it.
[00:30:09.500 --> 00:30:12.300]   I'm going to tell my whole team to play this now, Lucas.
[00:30:12.300 --> 00:30:13.300]   So we're there.
[00:30:13.300 --> 00:30:15.100]   We'll move from Among Us to that.
[00:30:15.100 --> 00:30:18.100]   So I really recommend it.
[00:30:18.100 --> 00:30:19.100]   Okay.
[00:30:19.100 --> 00:30:24.100]   So I also wanted to ask you, just because this is so impressive, you know, you also
[00:30:24.100 --> 00:30:27.980]   wrote a book in your spare time while doing your day job.
[00:30:27.980 --> 00:30:31.700]   I was curious what inspired you to write this book.
[00:30:31.700 --> 00:30:33.100]   And yeah, that's it.
[00:30:33.100 --> 00:30:35.900]   What inspired you to write this book?
[00:30:35.900 --> 00:30:42.100]   For me, it was sort of, you know, almost an appreciation and an underappreciation in a
[00:30:42.100 --> 00:30:46.700]   way of the evolution of the field of machine learning.
[00:30:46.700 --> 00:30:50.940]   So for me, like a few years ago, actually it happened right before the pandemic, I would
[00:30:50.940 --> 00:30:56.420]   talk to brilliant, you know, mench like you or people on my team.
[00:30:56.420 --> 00:30:59.900]   And then they'd be like talking to all this machine learning stuff and frameworks.
[00:30:59.900 --> 00:31:01.140]   And I'd be like, yeah, that's interesting.
[00:31:01.140 --> 00:31:04.220]   I mean, heck, like 10, 15 years ago, let's see it.
[00:31:04.220 --> 00:31:06.940]   Well, it's 13 years ago now I graduated with a PhD.
[00:31:06.940 --> 00:31:12.580]   I did a minimum amount of maybe what you would call machine learning today, k-means clustering
[00:31:12.580 --> 00:31:14.460]   in my dissertation, you know, or whatever.
[00:31:14.460 --> 00:31:17.940]   But I mean, that was kind of, I did a little bit of Bayesian inference, but I would say
[00:31:17.940 --> 00:31:20.620]   the field of machine learning wasn't back then like it was today.
[00:31:20.620 --> 00:31:23.060]   So I heard everyone talking about this stuff.
[00:31:23.060 --> 00:31:26.920]   And you know, yeah, I mean, I don't need to know stuff materially at that deep level anymore
[00:31:26.920 --> 00:31:29.740]   as much, but I said, you know, let me go pick up a book.
[00:31:29.740 --> 00:31:34.300]   I had written a book about, well, now it will in 2010, it was called Tika in Action.
[00:31:34.300 --> 00:31:38.300]   It was the Tika framework, long story short on that one.
[00:31:38.300 --> 00:31:40.620]   It's basically like, we call it the digital babblefish.
[00:31:40.620 --> 00:31:45.220]   If you read Hitchhiker's Guide to the Galaxy, you give it, you know, any language, you put
[00:31:45.220 --> 00:31:48.420]   the babblefish in your ear and out the other end, it's your interpretation.
[00:31:48.420 --> 00:31:49.580]   You can understand it.
[00:31:49.580 --> 00:31:51.300]   Tika is that for files.
[00:31:51.300 --> 00:31:55.300]   You give it any file, it tells you the file type automatically, the mind type, extracts
[00:31:55.300 --> 00:31:57.060]   the text, the metadata and the language.
[00:31:57.060 --> 00:32:00.340]   And basically for your audience, all they need to know about it is it's like, look it
[00:32:00.340 --> 00:32:01.340]   up on Wikipedia.
[00:32:01.340 --> 00:32:05.300]   It was the key technology to solve the Panama Papers and win the Pulitzer Prize.
[00:32:05.300 --> 00:32:11.060]   So I wrote that book in 2010 and I get manning books sometimes, or I talk to them and you
[00:32:11.060 --> 00:32:12.540]   know, I can get a book.
[00:32:12.540 --> 00:32:15.300]   And there was a book that came out called Machine Learning with TensorFlow.
[00:32:15.300 --> 00:32:16.860]   And I said, Hey, can I, can I get this book?
[00:32:16.860 --> 00:32:17.860]   I want to read it.
[00:32:17.860 --> 00:32:20.340]   I want to learn machine learning.
[00:32:20.340 --> 00:32:22.900]   And so I know what the hell my people are talking about.
[00:32:22.900 --> 00:32:26.660]   And so I started reading it and I pulled out a pencil.
[00:32:26.660 --> 00:32:28.180]   I started drawing matrices.
[00:32:28.180 --> 00:32:33.620]   I started like, like really trying to just, instead of read it, like do it, do the exercise
[00:32:33.620 --> 00:32:34.700]   in the book.
[00:32:34.700 --> 00:32:39.580]   And so what I arrived at after nine months, and this was like the lead up to 2020 in the
[00:32:39.580 --> 00:32:45.500]   pre pandemic world, what I arrived at was probably 50 Jupiter notebooks everywhere where
[00:32:45.500 --> 00:32:49.220]   it was a thrown out suggestion in the first edition of the book, like, Hey, you could
[00:32:49.220 --> 00:32:51.260]   try and build a facial identification system.
[00:32:51.260 --> 00:32:52.260]   I did it.
[00:32:52.260 --> 00:32:54.100]   I rebuilt the VGG face model.
[00:32:54.100 --> 00:32:57.020]   I had a publication at super computing, you know, after that.
[00:32:57.020 --> 00:33:00.500]   And I basically just had code Jupiter notebooks and everything.
[00:33:00.500 --> 00:33:03.940]   And I was like, I've got a, I've got a second edition of this book, you know, because I
[00:33:03.940 --> 00:33:07.940]   filled in all the gaps, you know, where, and I added a bunch of new chapters.
[00:33:07.940 --> 00:33:11.020]   And so I pitched it to manning, they loved it.
[00:33:11.020 --> 00:33:14.620]   And I was away, you know, running and that's how I did the machine learning with TensorFlow
[00:33:14.620 --> 00:33:16.060]   second edition book.
[00:33:16.060 --> 00:33:17.220]   And so that's how I got there.
[00:33:17.220 --> 00:33:19.340]   So that's so awesome.
[00:33:19.340 --> 00:33:23.540]   And I saw you, you made an interesting choice to use TensorFlow V1 in the book, right?
[00:33:23.540 --> 00:33:27.260]   Instead of V2 because it's still in use on super computers.
[00:33:27.260 --> 00:33:28.260]   Is that right?
[00:33:28.260 --> 00:33:29.260]   Or what was the thing there?
[00:33:29.260 --> 00:33:30.260]   Yeah.
[00:33:30.260 --> 00:33:31.260]   Yeah.
[00:33:31.260 --> 00:33:35.100]   And V2, you know, for me, you know, I never shied away from this, you know, I mean, heck,
[00:33:35.100 --> 00:33:38.220]   I was on the board of Apache, you know, we were maintaining, what is it?
[00:33:38.220 --> 00:33:40.900]   A 25 year old web server, you know, or whatever.
[00:33:40.900 --> 00:33:46.100]   You know, so it's not the oldness of the technology for me, part of it was stability.
[00:33:46.100 --> 00:33:50.560]   And so what I was finding was that the TensorFlow 2 was changing a lot, you know, at the time,
[00:33:50.560 --> 00:33:54.780]   and I made the decision in the beginning, I said, we're going to pin it to 115, you
[00:33:54.780 --> 00:33:57.900]   know, because that's stable, you know, or whatever.
[00:33:57.900 --> 00:34:02.360]   And yeah, it was still in use at big supercomputing, you know, agencies, because they hadn't been
[00:34:02.360 --> 00:34:06.540]   on the technology uptick, you know, and I was writing it a little bit for them.
[00:34:06.540 --> 00:34:10.500]   What we ended up doing, and what I promised to manning is during the book about midway
[00:34:10.500 --> 00:34:15.220]   through or at the end, I would take a look at basically porting every example, every
[00:34:15.220 --> 00:34:18.320]   notebook in the book to TensorFlow 2.
[00:34:18.320 --> 00:34:19.840]   And that's exactly what we did.
[00:34:19.840 --> 00:34:24.880]   And let me tell you something in a testament to Google, and I give them credit for this,
[00:34:24.880 --> 00:34:30.840]   it took us about two weeks to port the entire book to TensorFlow 2, myself, and a couple
[00:34:30.840 --> 00:34:35.220]   of students, you know, and folks who literally just donated their time.
[00:34:35.220 --> 00:34:37.360]   And so this wasn't a massive undertaking.
[00:34:37.360 --> 00:34:41.880]   There was a big paradigm shift mentally, in TensorFlow 1 to TensorFlow 2.
[00:34:41.880 --> 00:34:46.880]   But I would say 85% of the code from those notebooks is the same, because it's all data
[00:34:46.880 --> 00:34:51.680]   preparation, making it analytics and machine learning ready, and then doing rock analysis,
[00:34:51.680 --> 00:34:54.840]   rock, you know, receiver operating characteristics area under the curve.
[00:34:54.840 --> 00:35:00.760]   None of that stuff, the beautiful libraries in Python, you know, Matplotlib, NumPy, SciPy,
[00:35:00.760 --> 00:35:03.560]   Pandas, all of these things, they didn't change.
[00:35:03.560 --> 00:35:10.600]   You know, what changed in the inner in the inside was basically how you set up the model
[00:35:10.600 --> 00:35:16.120]   for training, you know, how you run the training step, you know, update your gradients, you
[00:35:16.120 --> 00:35:20.560]   know, if it's a neural network, all of these things, that part changed, but the other parts
[00:35:20.560 --> 00:35:21.560]   didn't.
[00:35:21.560 --> 00:35:25.240]   Scott Penberthy, the head of AI or applied AI at Google, basically made this remark in
[00:35:25.240 --> 00:35:30.840]   the intro to my book is, you know, don't worry about tracking the latest and greatest XYZ
[00:35:30.840 --> 00:35:34.680]   API update, you know, these models and the way you build them will stand the test of
[00:35:34.680 --> 00:35:36.240]   time and I agree with them.
[00:35:36.240 --> 00:35:37.240]   So.
[00:35:37.240 --> 00:35:38.240]   Cool.
[00:35:38.240 --> 00:35:43.320]   Another topic that I think you've been thinking about a lot and you kind of brought up earlier
[00:35:43.320 --> 00:35:45.680]   is, you know, MLOps and open source.
[00:35:45.680 --> 00:35:49.400]   Are there any projects you're particularly excited about kind of helping, you know, helping
[00:35:49.400 --> 00:35:51.680]   just deploy and maintain machine learning models?
[00:35:51.680 --> 00:35:55.920]   Are there any that you use at the JPL?
[00:35:55.920 --> 00:36:00.080]   There's a couple, you know, I'm impressed with a lot of the Amazonian things that are
[00:36:00.080 --> 00:36:01.080]   coming on.
[00:36:01.080 --> 00:36:04.440]   I mean, SageMaker Studio and, you know, some of the things they're doing.
[00:36:04.440 --> 00:36:09.320]   I'm impressed by some of the capabilities that, you know, WNB is building, you know,
[00:36:09.320 --> 00:36:12.680]   your company, I'm not just saying that, you know, too, there seems to be some appetite
[00:36:12.680 --> 00:36:18.360]   really to after, and for me, my exposure to this was through the DARPA data-driven discovery
[00:36:18.360 --> 00:36:24.880]   of models program, but really in kind of looking at parameters, parameter tuning in an automated
[00:36:24.880 --> 00:36:30.840]   way, optimizations of that, you know, pipeline and also SME based, subject matter experts
[00:36:30.840 --> 00:36:33.680]   based model feedback, you know?
[00:36:33.680 --> 00:36:37.480]   And I really think that's where the world's going to go soon, you know, in the next, you
[00:36:37.480 --> 00:36:40.360]   know, you look at one, three, five year timescale, AutoML is here.
[00:36:40.360 --> 00:36:44.320]   I mean, if you look at some of the capabilities, Google AutoML, DataRobot, you know, some of
[00:36:44.320 --> 00:36:45.320]   these other things.
[00:36:45.320 --> 00:36:50.400]   So really it's going to shift what people are doing, I think, from building models all
[00:36:50.400 --> 00:36:55.840]   the time, let some, let the computer put together primitives, you know, and, and, and score them
[00:36:55.840 --> 00:37:00.120]   for you, you know, and whatever, and then start there, give that feedback, change the
[00:37:00.120 --> 00:37:05.680]   job that that person is doing, you know, to that feedback.
[00:37:05.680 --> 00:37:06.680]   And I think it'll make people more optimized and, and, and things like that.
[00:37:06.680 --> 00:37:08.680]   Why do you think that hasn't happened already?
[00:37:08.680 --> 00:37:12.360]   I mean, there's, you know, AutoML has been out for a long time, you know, hyper parameter
[00:37:12.360 --> 00:37:16.680]   optimization has been around for, for quite a long time and, and good libraries exist,
[00:37:16.680 --> 00:37:20.600]   including, you know, Waste Advices has a library that, that folks use, but it doesn't seem
[00:37:20.600 --> 00:37:25.880]   like I would say maybe 20% of our users look like they're doing, you know, hyper parameter
[00:37:25.880 --> 00:37:28.360]   optimization with our stuff or somebody else's stuff.
[00:37:28.360 --> 00:37:32.040]   Why do you think it's not more widespread already?
[00:37:32.040 --> 00:37:36.400]   For me, it's kind of like a little bit of, you know, when I, in 2018, when I went to
[00:37:36.400 --> 00:37:42.060]   a blockchain conference at, at UCLA at the blockchain lab, and I sat there and I listened
[00:37:42.060 --> 00:37:47.600]   to Ethereum versus, oh God, I forget the other one versus, well, I mean, obviously there
[00:37:47.600 --> 00:37:51.800]   was Bitcoin and then there was, oh, EOS, you know, and this and that.
[00:37:51.800 --> 00:37:55.880]   And I was like, oh God, these are the early days of like ITEF or like the Internet Engineering
[00:37:55.880 --> 00:38:00.120]   Task Force where everyone was trying to build their specifications of Gopher and, you know,
[00:38:00.120 --> 00:38:01.120]   this and that.
[00:38:01.120 --> 00:38:03.760]   It feels like it's still the wild west.
[00:38:03.760 --> 00:38:09.560]   And there's always de jure competition, which is, you know, standards-based getting people
[00:38:09.560 --> 00:38:14.000]   together and saying, this is what thou shalt use, thou shalt use MapReduce or thou shalt
[00:38:14.000 --> 00:38:15.320]   use, you know, whatever.
[00:38:15.320 --> 00:38:19.880]   And then there's the de facto end with that is what is, what are people actually using,
[00:38:19.880 --> 00:38:22.320]   you know, and they build in what they're building.
[00:38:22.320 --> 00:38:26.520]   And it feels like the gap between the people that are doing the de facto development and
[00:38:26.520 --> 00:38:30.720]   that type of uptick in terms of framework things aren't meeting the people that are
[00:38:30.720 --> 00:38:35.760]   making the framework decisions for the ops and what they're going to double down and
[00:38:35.760 --> 00:38:36.760]   invest in.
[00:38:36.760 --> 00:38:39.680]   And the closer that those move, and it'll happen.
[00:38:39.680 --> 00:38:41.880]   I really do believe it'll happen.
[00:38:41.880 --> 00:38:45.520]   The pandemic accelerated that in a lot of ways, I think.
[00:38:45.520 --> 00:38:49.200]   But I think when those two things move closer, Lucas, I think you'll see it.
[00:38:49.200 --> 00:38:50.200]   So.
[00:38:50.200 --> 00:38:51.880]   Well, that's a good segue.
[00:38:51.880 --> 00:38:56.520]   To two questions that we always end with and the second to last question, which maybe you've
[00:38:56.520 --> 00:39:01.040]   kind of answered already, but maybe you could answer something else is what's an underrated
[00:39:01.040 --> 00:39:04.760]   aspect of machine learning that you feel like people should pay more attention to than they
[00:39:04.760 --> 00:39:07.120]   do today?
[00:39:07.120 --> 00:39:13.120]   Yeah, for me, the sort of, yeah, actually I won't pick that.
[00:39:13.120 --> 00:39:14.720]   I had two choices for that.
[00:39:14.720 --> 00:39:17.240]   One thing I could have picked was learning with less labels.
[00:39:17.240 --> 00:39:19.480]   That's kind of far out the zero shot, one shot learning.
[00:39:19.480 --> 00:39:25.840]   I'll just say, pay attention to that, but the soundbite here for me is ML at the edge.
[00:39:25.840 --> 00:39:31.400]   Everyone thinks you can take a machine learning model, put it onto an NVIDIA, you know, TX2
[00:39:31.400 --> 00:39:34.160]   or a Jetson, and your model's going to perform the same way.
[00:39:34.160 --> 00:39:36.240]   And it's going to be push button.
[00:39:36.240 --> 00:39:37.640]   And that's just bupkis.
[00:39:37.640 --> 00:39:38.880]   It doesn't work like that.
[00:39:38.880 --> 00:39:40.880]   There's so much engineering involved.
[00:39:40.880 --> 00:39:44.520]   You're trading so much at the model, but look, you look at CES, we're going to move from
[00:39:44.520 --> 00:39:48.200]   per capita devices, four to nine right now, per people.
[00:39:48.200 --> 00:39:53.560]   Over the next five years to 40, 50, 60, 80 devices per capita.
[00:39:53.560 --> 00:39:55.880]   So these are all going to be running machine learning.
[00:39:55.880 --> 00:39:57.360]   ML at the edge.
[00:39:57.360 --> 00:40:00.560]   Do you want to know and be involved in what's happening there?
[00:40:00.560 --> 00:40:05.280]   If you do, get involved and also realize it's not push button model deployment and your
[00:40:05.280 --> 00:40:06.880]   models perform a lot differently.
[00:40:06.880 --> 00:40:09.760]   And so there's a lot, that's an underappreciated area in my mind.
[00:40:09.760 --> 00:40:13.960]   And I want all your smart people and you and your audience to focus on that because we
[00:40:13.960 --> 00:40:14.960]   need help.
[00:40:14.960 --> 00:40:18.120]   It's funny, that kind of answers the final question that we always ask, which is, what
[00:40:18.120 --> 00:40:23.640]   is the biggest challenge making machine learning work in the real world or at your job?
[00:40:23.640 --> 00:40:27.680]   Is it actually edge deployment and models behaving differently when they're actually
[00:40:27.680 --> 00:40:28.680]   in the edge?
[00:40:28.680 --> 00:40:30.680]   Is that a fair summary?
[00:40:30.680 --> 00:40:33.000]   Yeah, absolutely.
[00:40:33.000 --> 00:40:34.840]   That's a totally fair summary, Lucas.
[00:40:34.840 --> 00:40:39.540]   And also where the edge, we definitely do a lot of IoT on campus, in clean rooms, in
[00:40:39.540 --> 00:40:44.360]   other places and all this, but also where the edge is Mars.
[00:40:44.360 --> 00:40:48.200]   Do you have any tricks to leave us with in making things work on the edge?
[00:40:48.200 --> 00:40:53.400]   Is there any best practices that you've figured out to help with that?
[00:40:53.400 --> 00:40:55.480]   Yeah.
[00:40:55.480 --> 00:40:59.400]   One best practice I'll just share with you, and this is our biggest time sink, is don't
[00:40:59.400 --> 00:41:02.640]   change the hardware midstream.
[00:41:02.640 --> 00:41:07.680]   A different thing that looks compatible is actually vastly different, even within the
[00:41:07.680 --> 00:41:09.160]   same product family.
[00:41:09.160 --> 00:41:14.040]   So stick to what you got and the computing power you have and engineer more optimizations
[00:41:14.040 --> 00:41:17.720]   there rather than thinking, "Oh, it's just because the price point of the hardware makes
[00:41:17.720 --> 00:41:18.720]   it so attractive.
[00:41:18.720 --> 00:41:21.040]   Oh, I'll just spend another 200 bucks and get a different thing."
[00:41:21.040 --> 00:41:22.360]   That's no, no, no, no.
[00:41:22.360 --> 00:41:27.560]   You'll spend 10 to 50 times that re-engineering your entire pipeline.
[00:41:27.560 --> 00:41:29.480]   So don't change the hardware midstream.
[00:41:29.480 --> 00:41:30.480]   Awesome.
[00:41:30.480 --> 00:41:31.960]   Spoken like a real engineer.
[00:41:31.960 --> 00:41:34.760]   Good note to end on.
[00:41:34.760 --> 00:41:35.760]   Thanks, Chris.
[00:41:35.760 --> 00:41:36.760]   Thank you.
[00:41:36.760 --> 00:41:37.760]   It was great to chat with you.
[00:41:37.760 --> 00:41:38.760]   Back at you.
[00:41:38.760 --> 00:41:39.760]   Thanks, Lucas.
[00:41:39.760 --> 00:41:43.000]   Thanks for listening to another episode of Gradient Dissent.
[00:41:43.000 --> 00:41:47.320]   Doing these interviews are a lot of fun, and it's especially fun for me when I can actually
[00:41:47.320 --> 00:41:50.080]   hear from the people that are listening to these episodes.
[00:41:50.080 --> 00:41:54.960]   So if you wouldn't mind leaving a comment and telling me what you think or starting a
[00:41:54.960 --> 00:41:58.100]   conversation, that would make me inspired to do more of these episodes.
[00:41:58.100 --> 00:42:01.640]   And also, if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

