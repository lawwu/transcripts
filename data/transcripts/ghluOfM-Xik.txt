
[00:00:00.000 --> 00:00:03.820]   I think the sort of like world in which we're really neck and neck, you know, we only have
[00:00:03.820 --> 00:00:06.540]   a three month lead, are incredibly dangerous, right?
[00:00:06.540 --> 00:00:11.300]   And we're in this fever struggle where like, if they get ahead, they get to dominate, maybe
[00:00:11.300 --> 00:00:15.640]   they'd get a decisive advantage, they're building clusters like crazy, they're willing to throw
[00:00:15.640 --> 00:00:19.780]   all caution to the wind, we have to keep up, there's some crazy new WMDs popping up.
[00:00:19.780 --> 00:00:22.940]   And then we're going to be in the situation where it's like, you know, crazy new military
[00:00:22.940 --> 00:00:26.640]   technology, crazy new WMDs, you know, like deterrence and mutually disturbed instruction
[00:00:26.640 --> 00:00:30.440]   like keeps changing, you know, every few weeks, and it's like, you know, completely unstable
[00:00:30.440 --> 00:00:33.400]   volatile situation is incredibly dangerous.
[00:00:33.400 --> 00:00:36.120]   So it's I think, I think, you know, both both from just the technologies are dangerous from
[00:00:36.120 --> 00:00:38.980]   the alignment point of view, you know, I think it might be really important during the intelligence
[00:00:38.980 --> 00:00:43.160]   explosion to have the sort of six month, you know, wiggle room to be like, look, we're
[00:00:43.160 --> 00:00:46.280]   gonna like, dedicate more compute to alignment during this period, because we have to get
[00:00:46.280 --> 00:00:48.760]   it right, we're feeling uneasy about how it's going.
[00:00:48.760 --> 00:00:53.360]   And so I think in some sense, like, one of the most important inputs, so whether we will
[00:00:53.360 --> 00:00:58.080]   kind of destroy ourselves, or whether we will get through this just incredibly crazy period,
[00:00:58.080 --> 00:01:03.000]   is whether we have that buffer, you know, a couple years of lead could be utterly decisive
[00:01:03.000 --> 00:01:06.520]   in say, like military competition, right, you know, if you look at like, go for one,
[00:01:06.520 --> 00:01:10.360]   right, go for one, you know, like the Western coalition forces, you know, they had, you
[00:01:10.360 --> 00:01:11.960]   know, like 100 to one kill ratio, right.
[00:01:11.960 --> 00:01:15.120]   And that was like, they had better sensors on their tanks, you know, and they had, they
[00:01:15.120 --> 00:01:20.280]   had better, more precision, precision missiles, right, like GPS, and they had, you know, stealth,
[00:01:20.280 --> 00:01:25.040]   and they had sort of a few, you know, maybe 20, 30 years of technological lead, right.
[00:01:25.040 --> 00:01:28.680]   And they, you know, just completely crushed them.
[00:01:28.680 --> 00:01:31.840]   Superintelligence applied to sort of broad fields of R&D.
[00:01:31.840 --> 00:01:34.160]   And then, you know, the sort of industrial explosion as well, you have the robots, you're
[00:01:34.160 --> 00:01:35.160]   just making lots of material.
[00:01:35.160 --> 00:01:38.920]   You know, I think that could compress, I mean, basically compress kind of like, a century
[00:01:38.920 --> 00:01:41.440]   worth of technological progress into less than a decade.
[00:01:41.440 --> 00:01:45.320]   And that means that, you know, a couple years could mean a sort of goal for one style, like,
[00:01:45.320 --> 00:01:49.480]   you know, advantage in military affairs.
[00:01:49.480 --> 00:01:54.280]   And, you know, including, like, you know, a decisive advantage that even like preempts
[00:01:54.280 --> 00:01:55.280]   nukes, right.
[00:01:55.280 --> 00:01:57.480]   Suppose, like, you know, how do you find the stealth in nuclear submarines?
[00:01:57.480 --> 00:02:00.400]   Like right now, that's a problem of like, you have sensors, you have the software to
[00:02:00.400 --> 00:02:01.400]   like detect where they are.
[00:02:01.400 --> 00:02:02.400]   You know, you can do that.
[00:02:02.400 --> 00:02:03.400]   You can find them.
[00:02:03.400 --> 00:02:07.000]   You have kind of like millions or billions of like mosquito-like, you know, sized drones.
[00:02:07.000 --> 00:02:10.640]   And you know, they take out the nuclear submarines, they take out the mobile launchers, they take
[00:02:10.640 --> 00:02:12.440]   out the other nukes.
[00:02:12.440 --> 00:02:17.240]   And anyway, so I think enormously destabilizing, enormously important for national power.
[00:02:17.240 --> 00:02:19.400]   And at some point, I think people are going to realize that.
[00:02:19.400 --> 00:02:21.440]   Not yet, but they will.
[00:02:21.440 --> 00:02:27.280]   And when they will, I think there will be sort of, you know, I don't think it'll just
[00:02:27.280 --> 00:02:30.120]   be the sort of AI researchers in charge.
[00:02:30.120 --> 00:02:34.440]   And you know, I think the CCP is going to have sort of an all-out effort to like infiltrate
[00:02:34.440 --> 00:02:35.440]   American AI labs, right.
[00:02:35.440 --> 00:02:38.680]   You know, like billions of dollars, thousands of people, you know, full force of the sort
[00:02:38.680 --> 00:02:40.200]   of, you know, Ministry of State Security.
[00:02:40.200 --> 00:02:42.560]   CCP is going to try to, you know, like outbuild us, right.
[00:02:42.560 --> 00:02:46.800]   Like they, you know, their power in China, you know, like the electric grid, you know,
[00:02:46.800 --> 00:02:50.840]   they added a U.S., you know, a complete, like they added as much power in the last decade
[00:02:50.840 --> 00:02:52.480]   as like sort of entire U.S. electric grid.
[00:02:52.480 --> 00:02:55.600]   So like the 100 gigawatt cluster, at least the 100 gigawatts is going to be a lot easier
[00:02:55.600 --> 00:02:56.720]   for them to get.
[00:02:56.720 --> 00:03:00.660]   And so I think sort of, you know, by this point, I think it's going to be like an extremely
[00:03:00.660 --> 00:03:03.200]   intense sort of international competition.
[00:03:03.200 --> 00:03:06.100]   Unlike a nukes, the data centers are nukes.
[00:03:06.100 --> 00:03:11.640]   You have obviously the submarines, planes, you have bunkers, mountains, whatever you
[00:03:11.640 --> 00:03:12.880]   have in so many different places.
[00:03:12.880 --> 00:03:16.320]   A data center that your 100 gigawatt data center, we can blow that shit up if you're
[00:03:16.320 --> 00:03:17.320]   like, we're concerned, right?
[00:03:17.320 --> 00:03:21.800]   Like just like some cruise missile or something that's like very vulnerable to sabotage.
[00:03:21.800 --> 00:03:24.960]   That gets to the sort of, I mean, that gets to the sort of insane vulnerability, the volatility
[00:03:24.960 --> 00:03:26.600]   of this period post superintelligence, right?
[00:03:26.600 --> 00:03:29.680]   Because basically I think, so you have the intelligence explosion, you have these like
[00:03:29.680 --> 00:03:33.000]   vastly superhuman things on your cluster, but you're like, you haven't done the industrial
[00:03:33.000 --> 00:03:34.000]   explosion yet.
[00:03:34.000 --> 00:03:35.000]   You don't have your robots yet.
[00:03:35.000 --> 00:03:38.000]   You haven't kind of, you haven't covered the desert in like robot factories yet.
[00:03:38.000 --> 00:03:41.960]   And that is the sort of crazy moment where, you know, say, say the United States is ahead,
[00:03:41.960 --> 00:03:43.520]   the CCP is somewhat behind.
[00:03:43.520 --> 00:03:46.120]   There's actually an enormous incentive for a first strike, right?
[00:03:46.120 --> 00:03:49.240]   Because if they can take out your data center, they, you know, they know you're about to
[00:03:49.240 --> 00:03:51.680]   have just this command decisive lead.
[00:03:51.680 --> 00:03:55.400]   They know if we can just take out this data center, you know, then we can stop it.
[00:03:55.400 --> 00:03:57.080]   And they might get desperate.
[00:03:57.080 --> 00:03:58.200]   Basically our generation, right?
[00:03:58.200 --> 00:04:02.760]   We're kind of so used to kind of, you know, basically peace and like, you know, the world,
[00:04:02.760 --> 00:04:06.560]   you know, American hegemony and nothing matters.
[00:04:06.560 --> 00:04:10.000]   But you know, the sort of like extremely intense and these extraordinary things happening in
[00:04:10.000 --> 00:04:14.440]   the world and like intense international competition is like very much the historical norm.
[00:04:14.440 --> 00:04:18.640]   Like in some sense, it's like, you know, sort of this, there's this sort of 20 year, very
[00:04:18.640 --> 00:04:19.640]   unique period.
[00:04:19.640 --> 00:04:24.960]   But like, you know, the history of the world is like, you know, you know, like in World
[00:04:24.960 --> 00:04:25.960]   War II, right?
[00:04:25.960 --> 00:04:29.120]   It was like 50% of GDP went to, you know, like, you know, war production, you know,
[00:04:29.120 --> 00:04:33.820]   the U.S. borrowed over 60% of GDP, you know, and in, you know, I think Germany and Japan
[00:04:33.820 --> 00:04:38.280]   over a hundred percent, World War I, you know, UK, Japan, sorry, UK, France, Germany all
[00:04:38.280 --> 00:04:41.320]   borrowed over a hundred percent of GDP.
[00:04:41.320 --> 00:04:47.480]   And you know, I think the sort of much more was on the line, right?
[00:04:47.480 --> 00:04:50.480]   Like, you know, and you know, people talk about World War I being so destructive and
[00:04:50.480 --> 00:04:54.520]   you know, like 20 million Soviet soldiers dying and like 20% of Poland, but you know,
[00:04:54.520 --> 00:04:56.600]   that was just the sort of like that happened all the time, right?
[00:04:56.600 --> 00:05:00.000]   You know, like seven years war, you know, like whatever, 20, 30% of Prussia died, you
[00:05:00.000 --> 00:05:04.640]   know, like 30 years war, you know, like I think like, you know, up to 50% of like large
[00:05:04.640 --> 00:05:14.440]   swath of Germany died and you know, I think the question is, will these sort of like,
[00:05:14.440 --> 00:05:18.880]   will people see that the stakes here are really, really high and that basically is sort of
[00:05:18.880 --> 00:05:21.960]   like history is actually back.
[00:05:21.960 --> 00:05:26.200]   And I think, you know, I think the American national security state thinks very seriously
[00:05:26.200 --> 00:05:27.200]   about stuff like this.
[00:05:27.200 --> 00:05:28.800]   They think very seriously about competition with China.
[00:05:28.800 --> 00:05:32.200]   I think China very much thinks of itself on this as her historical mission and rejuvenation
[00:05:32.200 --> 00:05:33.200]   of the Chinese nation.
[00:05:33.200 --> 00:05:34.320]   They think a lot about national power.
[00:05:34.320 --> 00:05:36.560]   They think a lot about like the world order.
[00:05:36.560 --> 00:05:40.440]   And then, you know, I think there's a real question on timing, right?
[00:05:40.440 --> 00:05:43.040]   Like do they, do they start taking this seriously, right?
[00:05:43.040 --> 00:05:46.040]   Like when the intelligence explosion is already happening, like quite late, or do they start
[00:05:46.040 --> 00:05:49.200]   taking this seriously, like two years earlier on that matters a lot for how things play
[00:05:49.200 --> 00:05:50.200]   out.
[00:05:50.200 --> 00:05:51.200]   But at some point they will.
[00:05:51.200 --> 00:05:57.760]   And at some point they will realize that this will be sort of utterly decisive for, you
[00:05:57.760 --> 00:06:01.360]   know, not just kind of like some proxy war somewhere, but you know, like whether liberal
[00:06:01.360 --> 00:06:07.160]   democracy can continue to thrive, whether, you know, whether the CCP will continue existing.
[00:06:07.160 --> 00:06:10.600]   And I think that will activate sort of forces that we haven't seen in a long time.
[00:06:10.600 --> 00:06:12.080]   Here's what I think we should do.
[00:06:12.080 --> 00:06:14.320]   I really don't want this volatile period.
[00:06:14.320 --> 00:06:16.960]   And so a deal with China would be nice.
[00:06:16.960 --> 00:06:21.800]   It's going to be really tough if you're in this unstable equilibrium.
[00:06:21.800 --> 00:06:25.880]   I think basically we want to get in a position where it is clear that the United States,
[00:06:25.880 --> 00:06:28.280]   that a sort of coalition of democratic allies will win.
[00:06:28.280 --> 00:06:31.000]   It's clear to the United States, it is clear to China, you know.
[00:06:31.000 --> 00:06:34.060]   That will require having locked down the secrets, that will require having built the 100 gigawatt
[00:06:34.060 --> 00:06:37.440]   cluster in the United States and having done the natural gas and doing what's necessary.
[00:06:37.440 --> 00:06:42.080]   And then when it is clear that the democratic coalition is well ahead, then you go to China
[00:06:42.080 --> 00:06:44.000]   and then you offer them a deal.
[00:06:44.000 --> 00:06:46.320]   And you know, China will know they're going to win.
[00:06:46.320 --> 00:06:50.240]   This is going to be, they're very scared of what's going to happen.
[00:06:50.240 --> 00:06:52.160]   We're going to know we're going to win, but we're also very scared of what's going to
[00:06:52.160 --> 00:06:57.240]   happen because we really want to avoid this kind of like breakneck race right at the end
[00:06:57.240 --> 00:07:01.480]   where things could really go awry.
[00:07:01.480 --> 00:07:02.680]   And so then we offer them a deal.
[00:07:02.680 --> 00:07:04.240]   I think there's an incentive to come to the table.
[00:07:04.240 --> 00:07:06.280]   I think there's a sort of more stable arrangement you can do.
[00:07:06.280 --> 00:07:08.080]   It's a sort of an atoms for peace arrangement.
[00:07:08.080 --> 00:07:09.760]   And we're like, look, we're going to respect you.
[00:07:09.760 --> 00:07:12.680]   We're not going to use superintelligence against you.
[00:07:12.680 --> 00:07:13.680]   You can do what you want.
[00:07:13.680 --> 00:07:16.880]   You're going to get your slice of the galaxy.
[00:07:16.880 --> 00:07:18.840]   We're going to benefit share with you.
[00:07:18.840 --> 00:07:21.640]   We're going to have some like compute agreement where it's like there's some ratio of compute
[00:07:21.640 --> 00:07:22.640]   that you're allowed to have.
[00:07:22.640 --> 00:07:25.680]   And that's like enforced with opposing AIs or whatever.
[00:07:25.680 --> 00:07:29.800]   And we're just not going to do we're just not going to do this kind of like volatile
[00:07:29.800 --> 00:07:31.640]   sort of WMD arms race to the death.
[00:07:31.640 --> 00:07:41.640]   [BLANK_AUDIO]

