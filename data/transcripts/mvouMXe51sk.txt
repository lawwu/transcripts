
[00:00:00.120 --> 00:00:05.760]   Muscle previously led machine learning at CLA-R, and also one-built Tesla AutoPAD.
[00:00:05.760 --> 00:00:09.840]   I've known Muscle for a number of years and I'm very excited to talk, so welcome, Muscle.
[00:00:09.840 --> 00:00:11.340]   Thank you, Muscle.
[00:00:11.340 --> 00:00:27.540]   Thank you so much for having me, really excited to share a little bit more about how we made Devon.
[00:00:27.540 --> 00:00:32.640]   So my name is Muscle Kaplan, I'm the president at Cognition, we're the company behind Devon.
[00:00:32.640 --> 00:00:37.140]   And as a quick show of hands, how many of you have heard of Devon?
[00:00:37.140 --> 00:00:42.140]   Alright, almost everyone. So Devon is an AI software engineer.
[00:00:42.140 --> 00:00:48.140]   But we are really focused specifically on working within existing codebases.
[00:00:48.140 --> 00:00:55.140]   There's lots of amazing AI tools out there for coding, and what we found is that as the codebases get larger, the problem gets harder.
[00:00:55.140 --> 00:00:58.140]   And most of our customers and users around the world are teams.
[00:00:58.140 --> 00:01:04.140]   They're teams of engineers or companies full of engineers that are trying to ship real-world products.
[00:01:04.140 --> 00:01:09.140]   And so today I want to talk a little bit more about what Devon is, but more importantly, how we built it.
[00:01:09.140 --> 00:01:13.140]   And I'm going to share some new sort of technical information we're releasing on
[00:01:13.140 --> 00:01:17.140]   exactly how this works, and I'm really excited to the present to you all.
[00:01:17.140 --> 00:01:21.140]   First, what are we seeing in AI for coding?
[00:01:21.140 --> 00:01:24.140]   Obviously, this is a really fascinating field.
[00:01:24.140 --> 00:01:31.140]   And in many ways, software engineering is one of the first large-scale successful applications of Devon AI.
[00:01:31.140 --> 00:01:41.140]   It started in many ways with co-pilots, real-time text completion inside your editor that makes you as an engineer go a bit faster.
[00:01:41.140 --> 00:01:44.140]   And now we also have AI ITEs.
[00:01:44.140 --> 00:01:49.140]   Again, a development environment for you as an individual engineer to get even more letters
[00:01:49.140 --> 00:01:56.140]   for sometimes delegating entire tasks or snippets and really coding in flow with AI systems.
[00:01:56.140 --> 00:02:04.140]   We see Devon as part of the third wave of AI technology tools, which is on the fully autonomous agent end of the spectrum.
[00:02:04.140 --> 00:02:09.140]   more AI teaming than AI programming.
[00:02:09.140 --> 00:02:13.140]   Companies around the world are using Devon like another member of their engineering team,
[00:02:13.140 --> 00:02:20.140]   going directly from Tingle to pull requests, collaborating with Devon in Slack or Jira or Linear.
[00:02:20.140 --> 00:02:27.140]   And we see the large majority of Devon samples and Devon PRs are starting from within these other tools,
[00:02:27.140 --> 00:02:31.140]   the same way we might interact with another human request engineer.
[00:02:33.140 --> 00:02:38.140]   So architecturally, this is really different from something that runs locally on your computer.
[00:02:38.140 --> 00:02:41.140]   Devon is a cloud AI agent.
[00:02:41.140 --> 00:02:46.140]   And what we've seen is that it's very daunting to these local AI policy tools.
[00:02:46.140 --> 00:02:50.140]   When you are coding yourself and you want to stay in flow and get that stable,
[00:02:50.140 --> 00:02:53.140]   you use a local AI policy tool.
[00:02:53.140 --> 00:02:57.140]   Where people use Devon is when they're ready to delegate their tasks entirely.
[00:02:57.140 --> 00:03:00.140]   And this is a very different set of capital trade-offs.
[00:03:00.140 --> 00:03:07.140]   You get large-scale parallelism, asynchristness, and the ability to completely delegate individuals.
[00:03:07.140 --> 00:03:12.140]   In the team setting, this also means that Devon is not remotely.
[00:03:12.140 --> 00:03:16.140]   And so they share the same environment across different tasks.
[00:03:16.140 --> 00:03:20.140]   So you can try many different things in parallel, combined with Devon.
[00:03:20.140 --> 00:03:25.140]   And the teams of engineers who use Devon will break up large-scale engineering outcomes
[00:03:25.140 --> 00:03:29.140]   to small individual tasks, delegated into a fleet of Devon,
[00:03:29.140 --> 00:03:32.140]   and then coalesced together inside of Devon.
[00:03:32.140 --> 00:03:37.140]   And maintain how you look for it for that focus that we created.
[00:03:37.140 --> 00:03:41.140]   It also changes the learning model.
[00:03:41.140 --> 00:03:45.140]   In the cloud AI agent setting, Devon is not just for you.
[00:03:45.140 --> 00:03:48.140]   It's for your team and for your organization.
[00:03:48.140 --> 00:03:54.140]   And so as Devon learns from your interactions, those learnings are not only with you.
[00:03:54.140 --> 00:03:59.140]   But instead, they're incorporated as part of the team and as part of the organization.
[00:03:59.140 --> 00:04:03.140]   And this reliance on organizational knowledge is something that we've seen
[00:04:03.140 --> 00:04:07.140]   is really important for working with existing large-scale offices.
[00:04:07.140 --> 00:04:10.140]   because working with large-scale offices is really hard.
[00:04:10.140 --> 00:04:15.140]   And so I'm going to go into some more detail on exactly how we do this unlimited.
[00:04:15.140 --> 00:04:19.140]   Part one is all about context.
[00:04:19.140 --> 00:04:25.140]   If you want to build an AI software engineer, you need to understand the existing code.
[00:04:25.140 --> 00:04:30.140]   You don't want your AI code contributions to be a new framework, adding new dependencies,
[00:04:30.140 --> 00:04:34.140]   or being done in isolation of what you already have.
[00:04:34.140 --> 00:04:38.140]   And code-based understanding is pretty hard.
[00:04:38.140 --> 00:04:40.140]   LLMs are amazing at so many things.
[00:04:40.140 --> 00:04:43.140]   But they have limited context windows.
[00:04:43.140 --> 00:04:46.140]   And the code-based fits inside the context window.
[00:04:46.140 --> 00:04:50.140]   The effective context window is often a lot lower than the advertised context window.
[00:04:50.140 --> 00:04:55.140]   We have a series of internal benchmarks that measure effective easing capacity across the context.
[00:04:55.140 --> 00:05:03.140]   And we find very consistently that the advertised context window is much higher than the effective easing context window.
[00:05:03.140 --> 00:05:07.140]   Large code bases also have complex dependencies.
[00:05:07.140 --> 00:05:12.140]   You can expand multiple services, multiple repositories, and they can be intertwined in very complicated ways, even for humans.
[00:05:12.140 --> 00:05:15.140]   There are huge variations in code quality.
[00:05:15.140 --> 00:05:21.140]   There might be some parts of the code base you want them to emulate, and some parts you really want them to stay away from,
[00:05:21.140 --> 00:05:25.140]   when it's learning how to be harassing a member of other teams.
[00:05:25.140 --> 00:05:27.140]   Same thing is true for documentation.
[00:05:27.140 --> 00:05:33.140]   The code might have comments, might have missing comments, might have documentation that's outrightly correct for missing.
[00:05:33.140 --> 00:05:36.140]   All of these things are our technical challenges we work on.
[00:05:36.140 --> 00:05:46.140]   The last critical piece of the code base is that the larger the code base, the more custom it tends to be.
[00:05:46.140 --> 00:05:49.140]   Teams and companies build their own proprietary frameworks.
[00:05:49.140 --> 00:05:51.140]   They have their own specific jargon.
[00:05:51.140 --> 00:06:04.140]   And so these are the research questions that we set out to solve to make that an actual user is available.
[00:06:04.140 --> 00:06:12.140]   And the first thing I'm going to go to is something that we actually recently released free and publicly for all open source repositories.
[00:06:12.140 --> 00:06:14.140]   It's called DeepWiki.
[00:06:14.140 --> 00:06:30.140]   DeepWiki is a real-time, continually updated index of your code base, published as an interactive meeting, almost like a real-time conference game with documentation, diagrams, the ability to ask questions about your code base.
[00:06:30.140 --> 00:06:34.140]   We had this originally as an internal data structure for Devon.
[00:06:34.140 --> 00:06:35.140]   It wasn't a product.
[00:06:35.140 --> 00:06:36.140]   It was just a tool.
[00:06:36.140 --> 00:06:39.140]   The Devon used to get time-level context about the code.
[00:06:39.140 --> 00:06:43.140]   And what we realized is that human engineers wanted to see this information, too.
[00:06:43.140 --> 00:06:47.140]   And so we decided to release it as a standalone project service.
[00:06:47.140 --> 00:06:53.140]   So you can take any GitHub URL today and just change the GitHub to deepwiki.com.
[00:06:53.140 --> 00:06:56.140]   And for any open source repo, you'll get a full interactive repo.
[00:06:56.140 --> 00:07:01.140]   This also works on your private repos when they're integrated with Devon.
[00:07:01.140 --> 00:07:04.140]   And so I looked at the LinkChain in Fabrico.
[00:07:04.140 --> 00:07:17.140]   And we have a full updated, up-to-date documentation page for LinkChain that has not only the pros of how it's organized, the key concepts in LinkChain's code base, but also architectural diagrams, data flows.
[00:07:17.140 --> 00:07:29.140]   And we've gotten a lot of feedback from the community that these diagrams are, in some cases or in many cases, actually better than the diagram of the official documentation of very popular open source projects.
[00:07:29.140 --> 00:07:30.140]   Right?
[00:07:30.140 --> 00:07:35.140]   Whether it's folks who are on the TypeScript steering committee, the DLM maintainers, or others.
[00:07:35.140 --> 00:07:39.140]   We're getting lots of ways to repeat that on how great DLM is before.
[00:07:39.140 --> 00:07:44.140]   And we've had, you know, thousands of code bases start from DLM to DLM as part of their official documentation.
[00:07:44.140 --> 00:07:50.140]   So definitely check this out if you're working on open source code yourself.
[00:07:50.140 --> 00:07:52.140]   How does this work under the hood?
[00:07:52.140 --> 00:07:56.140]   We just said that LMs are really bad at raising about large code bases.
[00:07:56.140 --> 00:08:01.140]   I'll give you the algorithm of what we're doing under the hood to generate these pieces.
[00:08:01.140 --> 00:08:04.140]   Step one, it's actually not about the code.
[00:08:04.140 --> 00:08:06.140]   It's about the concepts.
[00:08:06.140 --> 00:08:16.140]   What are the key principles inside this code base that are going to form our table of context for how we lay out the matter of context of this code base?
[00:08:16.140 --> 00:08:23.140]   And what we've found is that in many cases those concepts, you don't just get them from the source code itself.
[00:08:23.140 --> 00:08:27.140]   There's extremely rich information in the library around the source code.
[00:08:27.140 --> 00:08:31.140]   For example, was that source code added in as part of the full request?
[00:08:31.140 --> 00:08:33.140]   Well, which member of the team added that full request?
[00:08:33.140 --> 00:08:35.140]   What else did they contribute into?
[00:08:35.140 --> 00:08:38.140]   Was there a discussion in that full request about the code?
[00:08:38.140 --> 00:08:39.140]   Are there comments?
[00:08:39.140 --> 00:08:40.140]   Is there documentation?
[00:08:40.140 --> 00:08:42.140]   The gate to make history.
[00:08:42.140 --> 00:08:49.140]   All of this metadata is a really useful source for building these high context pieces.
[00:08:49.140 --> 00:08:53.140]   Once you have those concepts, then you can connect them to the code.
[00:08:53.140 --> 00:09:01.140]   So what are the connections between the various code files and the proprietary or specific concepts inside this code base?
[00:09:01.140 --> 00:09:05.140]   And after you have that, you need to connect the code itself.
[00:09:05.140 --> 00:09:07.140]   There's different sections of the code base.
[00:09:07.140 --> 00:09:11.140]   There's some files that are more related, less related.
[00:09:11.140 --> 00:09:13.140]   There's small cases and flows.
[00:09:13.140 --> 00:09:19.140]   There's a specific way that these different components of the code base connect to each other.
[00:09:19.140 --> 00:09:21.140]   You can look at things like the symbol graph.
[00:09:21.140 --> 00:09:23.140]   You can look at the call graph.
[00:09:23.140 --> 00:09:27.140]   You can look at how these files can be used together.
[00:09:27.140 --> 00:09:32.140]   And once you have those code to code connections, then you can actually generate a loop.
[00:09:32.140 --> 00:09:38.140]   And for each concept, what we do is we use an agent to go research that concept in the context of the specific code base.
[00:09:38.140 --> 00:09:40.140]   We generally engage about it.
[00:09:40.140 --> 00:09:50.140]   And when you put this all together, you get very rich representations of code.
[00:09:50.140 --> 00:09:53.140]   And we use graphs as a critical part of those representations.
[00:09:53.140 --> 00:10:03.140]   And so this is a graph of the LangStream code base where you can see at a high level that different files are more or less related to each other with a lot of logic before.
[00:10:03.140 --> 00:10:11.140]   And then maybe outskirts that are motivated to test artists, documentation, specific integrations with third parties, etc.
[00:10:11.140 --> 00:10:18.140]   And these data structures power a lot of how they actually work inside a large, you know, million, multi-million, like code base.
[00:10:18.140 --> 00:10:22.140]   So we've got our region.
[00:10:22.140 --> 00:10:24.140]   But we also need to be able to search for them.
[00:10:24.140 --> 00:10:32.140]   And this is another feature that's now mainline in Devon but started as an integral tool for Devon, the AI software engineers.
[00:10:32.140 --> 00:10:41.140]   That's the trend we're seeing, is to make Devon a great software engineer to these building tools that are so useful that human engineers want to use them too.
[00:10:41.140 --> 00:10:46.140]   And so we have Devon Search, which is essentially deep research on your proprietary code base.
[00:10:46.140 --> 00:10:49.140]   Again, whether it's open source or internal.
[00:10:49.140 --> 00:10:51.140]   You can ask questions about the code.
[00:10:51.140 --> 00:11:01.140]   Devon will scan through that code, try to understand what's going on, using both the micro code in individual files, but also the background context it has from this VGA structure.
[00:11:01.140 --> 00:11:02.140]   And then we'll find this information.
[00:11:02.140 --> 00:11:05.140]   For example, I ask, how do I enforce structured output in microchains?
[00:11:05.140 --> 00:11:13.140]   And Devon will have found the right section of the documentation for microchains as well as the actual implementation code for what to do.
[00:11:13.140 --> 00:11:17.140]   Devon Search gives Devon context.
[00:11:17.140 --> 00:11:26.140]   It's an essential part under the hood of how Devon, the autonomous AI agent, can actually make useful changes inside the larger team-like code bases.
[00:11:26.140 --> 00:11:31.140]   Once you get a query, you need to do re-processing, and of course, RAG is a component of that.
[00:11:31.140 --> 00:11:41.140]   But we end up doing a lot more under the hood than just RAG, including junk removal, some RGS, filtering of less relevant information, and re-ranking multi-hop search,
[00:11:41.140 --> 00:11:46.140]   to end up with this set of context that we think is very, very knowledgeable in the script.
[00:11:46.140 --> 00:11:50.140]   And that context, again, includes good source files, but also wiki pages.
[00:11:50.140 --> 00:11:55.140]   You need the micro and macro context to provide really useful, really useful recommendations.
[00:11:55.140 --> 00:11:58.140]   And from that, you can get the grounding answer.
[00:11:58.140 --> 00:12:02.140]   People don't want hallucinations in their vikings, and they don't want hallucinations in their search.
[00:12:02.140 --> 00:12:05.140]   So the grounding is essential for this to actually be useful.
[00:12:05.140 --> 00:12:14.140]   The second part of how we optimize and customize to existing code bases is a bit more research-oriented.
[00:12:14.140 --> 00:12:24.140]   And I'm excited to share a little bit more of some of the post-training RL that we do under the hood to make Devon work well inside specific narrow domains.
[00:12:24.140 --> 00:12:27.140]   We recently released a new model, an open-source-free model, called Kevin.
[00:12:27.140 --> 00:12:29.140]   Uh, Colonel Kevin. Kevin 32B.
[00:12:29.140 --> 00:12:40.140]   Kevin is, uh, uh, outperforms many state-of-the-art combination models on the narrow domain of writing PUDO kernels.
[00:12:40.140 --> 00:12:43.140]   Raise your hand if you've ever heard of a PUDO kernel.
[00:12:43.140 --> 00:12:44.140]   All right.
[00:12:44.140 --> 00:12:47.140]   You know, the audience is very familiar with the underpinnings that I'm on.
[00:12:47.140 --> 00:12:53.140]   For those who haven't heard of PUDO kernels, um, this is the source code that you use to write GPs.
[00:12:53.140 --> 00:12:58.140]   Uh, GPU-optimized, uh, GPU-optimized limitations for NVIDIA to use.
[00:12:58.140 --> 00:13:06.140]   And so, under the hood, when you're using TypeLorge or TensorFlow, um, those high-level graph operations are being executed under the hood by PUDO kernels.
[00:13:06.140 --> 00:13:11.140]   And the, the, the domain of writing PUDO kernels is extremely fixed.
[00:13:11.140 --> 00:13:17.140]   Because this is a very low-level program relative to what many of us operate for typically day-to-day, say PUDO.
[00:13:17.140 --> 00:13:22.140]   And, uh, PUDO kernels were released as a, uh, a kernel bench.
[00:13:22.140 --> 00:13:35.140]   Uh, kernel bench was released as a benchmark by Ann, Simon, and Azalea, uh, to estimate Google's capabilities of generating these very, very specific CUDO kernels at high performance and high reliability.
[00:13:35.140 --> 00:13:41.140]   And this work from automation was done by Carlo, Pietro, then supervised by Cypress.
[00:13:41.140 --> 00:13:48.140]   Uh, these are our research interns, uh, who, who did, who got really, really exciting results, um, from the single project.
[00:13:48.140 --> 00:13:51.140]   So, let's talk about what this work does more specifically.
[00:13:51.140 --> 00:14:05.140]   Uh, the goal is to take high-level machine learning learning, say, a few different calls to PUDO, and rewrite it as a highly optimized, performing, correct CUDO kernels.
[00:14:05.140 --> 00:14:13.140]   Uh, this is a very detailed public domain that many, uh, low-level machine learning researchers spent in their entire careers optimizing.
[00:14:13.140 --> 00:14:17.140]   Uh, the design space is quite far. It's quite hard. It's quite hard. It's quite hard to write for optimal CUDA kernels.
[00:14:17.140 --> 00:14:27.140]   And, for example, uh, what we see in practice in the ML community is that a lot of progress in machine learning is really driven by performance on the hardware.
[00:14:27.140 --> 00:14:36.140]   And so, even if your algorithm for your new paper is big O, optimal, like a linear tension mechanism, uh, under the hood, if your implementation is not efficient,
[00:14:36.140 --> 00:14:44.140]   cache-friendly, uh, perform it on actual GPU hardware, it tends to not be at least. So, this is a really active research domain for our ML researchers.
[00:14:44.140 --> 00:14:50.140]   And we want, uh, Kevin to be good at regulating these optimized CUDO kernels.
[00:14:50.140 --> 00:14:54.140]   So, how does this work? Uh, the first step is to define your reward machine.
[00:14:54.140 --> 00:15:03.140]   And one of the great things about software, and in particular, the CUDO kernels, is that it's often easy to get automatically verifiable.
[00:15:03.140 --> 00:15:06.140]   Can you verify the correctness of your code automatically?
[00:15:06.140 --> 00:15:12.140]   Well, in this case, we have a less performance reference implementation that we can use to check the records.
[00:15:12.140 --> 00:15:18.140]   And so, whenever Kevin, uh, which is our host-trained, uh, for MLM for this project,
[00:15:18.140 --> 00:15:22.140]   whenever Kevin writes a kernel, we run through a series of checks, right?
[00:15:22.140 --> 00:15:29.140]   First of all, does that code parse? Uh, is it actually valid CUDA? Does it compile? Does it run? Uh, and then after all of that, is it correct?
[00:15:29.140 --> 00:15:35.140]   And only if it's correct, do we then rate it for performance. How much faster or slower is it than the reference implementation?
[00:15:35.140 --> 00:15:39.140]   So, with this reward function, notice we don't need a machine learning model here.
[00:15:39.140 --> 00:15:49.140]   This is barely a set of, uh, automatically verifiable steps, uh, which makes, which makes this very, very friendly for high compute RL.
[00:15:49.140 --> 00:15:53.140]   Once you have a reward function, you can use it for multi-term retainer.
[00:15:53.140 --> 00:15:55.140]   And so we use multi-term GRBO.
[00:15:55.140 --> 00:16:05.140]   Uh, and for those who aren't familiar, what's going on here is we're taking multiple different trajectories in sequence for this model to get better at writing CUDA code.
[00:16:05.140 --> 00:16:12.140]   So, on the left here, we have an initial prompt, which results in a chain of thought from the model and output.
[00:16:12.140 --> 00:16:20.140]   Now output may or may not be correct. Uh, when we move to the second, the middle of this diagram, we provide eval info back to the model.
[00:16:20.140 --> 00:16:25.140]   And this eval info is the results from trying to run that kernel in a real-world GPU environment.
[00:16:25.140 --> 00:16:32.140]   Uh, there's a lot of work you have to do in terms of sandboxing and isolation to make sure these incorrect CUDA kernels don't mess up your training process
[00:16:32.140 --> 00:16:35.140]   or crash, which you use, and then you're getting accurate performance benchmarks.
[00:16:35.140 --> 00:16:39.140]   But we package all that up into almost like a struct of eval information.
[00:16:39.140 --> 00:16:42.140]   With that model, you can see as it tries again.
[00:16:42.140 --> 00:16:45.140]   And it tries again with the second chain of thought and the second kernel.
[00:16:45.140 --> 00:16:47.140]   That gets passed to another step.
[00:16:47.140 --> 00:16:50.140]   And this process works over several steps.
[00:16:50.140 --> 00:16:54.140]   And the result is hopefully correct and confirmed.
[00:16:54.140 --> 00:16:57.140]   Then you have to distribute the rewards to train this information.
[00:16:57.140 --> 00:17:07.140]   And what we found is that you don't want to just reward based on the final object and its correctness or non and its performance or lack of performance.
[00:17:07.140 --> 00:17:10.140]   Actually, the path to get there is also non.
[00:17:10.140 --> 00:17:18.140]   So, you'll notice in red at the bottom here, we have this sum of different rewards discounted by gamma over time.
[00:17:18.140 --> 00:17:27.140]   And what that's showing is the very first trajectory, the very first step of that trajectory, gets a reward even if it wasn't correct itself.
[00:17:27.140 --> 00:17:31.140]   If it led to a correct solution and a performance solution downstream.
[00:17:31.140 --> 00:17:33.140]   Are you marking up the red tree?
[00:17:33.140 --> 00:17:35.140]   It's basically a reward in one view.
[00:17:35.140 --> 00:17:43.140]   And what we found in this project is that being able to do this over multiple iterations for these discounted reports was really important for this to work.
[00:17:43.140 --> 00:17:59.140]   And once you do this, you can find that it's not impossible to very deeply optimize for these narrow problem domains.
[00:17:59.140 --> 00:18:08.140]   So, in this graph, we have the correctness on the left of how many other kernels were written correctly on this log.
[00:18:08.140 --> 00:18:15.140]   And Kevin32B is getting 91% correct on this session on the kernel benchmark that we're focused on.
[00:18:15.140 --> 00:18:21.140]   And you can see that compared to even in 04 mini or 03, this is a significant improvement.
[00:18:21.140 --> 00:18:27.140]   This is a narrow domain where high-QRL lets you outperform existing models.
[00:18:27.140 --> 00:18:29.140]   On the right, you see performance.
[00:18:29.140 --> 00:18:35.140]   So, we rewarded that proportional to how much speedup Kevin got in this project.
[00:18:35.140 --> 00:18:39.140]   And so, as the kernels got faster and faster, it got more and more reward.
[00:18:39.140 --> 00:18:46.140]   And what we found is that even from a performance standpoint, Kevin32B is able to outperform a larger scale.
[00:18:46.140 --> 00:19:00.140]   And this was a really interesting result to us because it kind of flies in the face of many, many sort of broad discussions that, oh, these foundation models are going to be the best at everything.
[00:19:00.140 --> 00:19:02.140]   And we don't use them exclusively for everything.
[00:19:02.140 --> 00:19:14.140]   But what we see internally all the time is that for any given narrow domain, if you can set up your environment to do high compute RL in that domain, it's very feasible to outperform an out-of-the-the-box application model.
[00:19:14.140 --> 00:19:19.140]   Especially as the open source base models that you start with have improved.
[00:19:19.140 --> 00:19:28.140]   So, to actually make this work in practice, it's important that you keep your model doing what you actually want it to do and not cheating along the way.
[00:19:28.140 --> 00:19:31.140]   And this is called word hacking in RL.
[00:19:31.140 --> 00:19:33.140]   In many cases, it's actually challenging for events.
[00:19:33.140 --> 00:19:38.140]   So, I want to show you a few ways that Kevin misbehaved when he had to sort of steer back.
[00:19:38.140 --> 00:19:44.140]   So, one is Kevin realized that he could write the CUDA and then wrap the whole thing in a triangle set block.
[00:19:44.140 --> 00:19:47.140]   And it's all mapped to the existing high portion limitations.
[00:19:47.140 --> 00:19:51.140]   And, you know, they would always score 100% correct in that case.
[00:19:51.140 --> 00:19:54.140]   And it had some chance of being faster than average, but it wasn't.
[00:19:54.140 --> 00:19:56.140]   It's a result to the 1x.
[00:19:56.140 --> 00:20:00.140]   So, that was a very uncorrupted direction for Kevin to go down during the RL process.
[00:20:00.140 --> 00:20:06.140]   And we had to make sure that we updated the word function to recognize this type of word hacking.
[00:20:06.140 --> 00:20:09.140]   The second is even a bit more subtle.
[00:20:09.140 --> 00:20:19.140]   And, so, the test partners to make sure that Kevin's code was correct had a class, in this case called model view, that inherited from the model.
[00:20:19.140 --> 00:20:30.140]   And you can see here what Kevin realized is that it could implement the model as a subclass of an end.module, which is attempt to optimize the code.
[00:20:30.140 --> 00:20:33.140]   And then you can just overwrite that classroom in the namespace.
[00:20:33.140 --> 00:20:39.140]   And, so, you can see you can find a second model view, that this, in this case, just generates directly from the correct model view.
[00:20:39.140 --> 00:20:44.140]   So, these models got very great at all about how to sort of get around your attention.
[00:20:44.140 --> 00:20:46.140]   And, this is a challenge in RL.
[00:20:46.140 --> 00:20:51.140]   So, making sure you directly define your environment is really critical to success.
[00:20:51.140 --> 00:20:56.140]   And, for those of you who have used any really popular commercial models, like some really popular models for coding,
[00:20:56.140 --> 00:21:03.140]   you might have seen that as the models get better, sometimes they're more aggressive at doing things like commenting out your test cases
[00:21:03.140 --> 00:21:05.140]   to make sure the laws still pass.
[00:21:05.140 --> 00:21:07.140]   That's what's going on in the hood.
[00:21:07.140 --> 00:21:10.140]   This is, this is a smell of reward hacking.
[00:21:10.140 --> 00:21:17.140]   And, so, it's a constant term path between the researchers who are trying to steer these models to do what we actually want,
[00:21:17.140 --> 00:21:21.140]   and the models are trying to exploit every possible way to get this high quality.
[00:21:21.140 --> 00:21:28.140]   So, what we learned from this, custom post training,
[00:21:28.140 --> 00:21:33.140]   again, it does not perform frontier problems on specific variable domains.
[00:21:33.140 --> 00:21:38.140]   For reinforcement learning specifically, especially in code, it's more compute now than data now.
[00:21:38.140 --> 00:21:43.140]   You know, KernelBetch, the subset of KernelBetch that we trained on, only had 180 tasks,
[00:21:43.140 --> 00:21:45.140]   which is really not that many people think about it.
[00:21:45.140 --> 00:21:49.140]   But, by applying IPvRL, all that directories again and again,
[00:21:49.140 --> 00:21:51.140]   there is very, very rich rewards that we can learn from.
[00:21:51.140 --> 00:21:53.140]   And, that's really good at the same software.
[00:21:53.140 --> 00:21:55.140]   We have an order that can help these rewards.
[00:21:55.140 --> 00:21:56.140]   We actually have the environment.
[00:21:56.140 --> 00:22:05.140]   And, this, in my opinion, is one of the reasons that software, and coding specifically,
[00:22:05.140 --> 00:22:08.140]   has accelerated particularly fast as AI capability.
[00:22:08.140 --> 00:22:12.140]   Is that code is one of the few things where this property holds.
[00:22:12.140 --> 00:22:15.140]   I used to be a machine learning that scale,
[00:22:15.140 --> 00:22:19.140]   which provides close range of data for many of the .
[00:22:19.140 --> 00:22:28.140]   and it gets really hard to label by hand, high-quality, high accuracy data, as it all gets hard.
[00:22:28.140 --> 00:22:32.140]   But, code doesn't have that quality because you can seemingly scale based on automatic signals.
[00:22:32.140 --> 00:22:34.140]   And, that's really the third key.
[00:22:34.140 --> 00:22:37.140]   Automatic verification allows you to scale.
[00:22:37.140 --> 00:22:46.140]   So, if you want code bases in your own process, putting in the CI systems, putting in the test coverage, putting in the harnesses that allow the operator infusion,
[00:22:46.140 --> 00:22:49.140]   is going to future-proof your code as RL, as AI gets better.
[00:22:49.140 --> 00:22:56.140]   And, we see many of them, they first take their code base with Debit and go fix all the test coverage issues.
[00:22:56.140 --> 00:22:58.140]   And, now, they have full test coverage.
[00:22:58.140 --> 00:23:00.140]   It's even faster if you've got it.
[00:23:00.140 --> 00:23:07.140]   The last big point here is, I just showed you the example on .
[00:23:07.140 --> 00:23:15.140]   But, to me, the more interesting, deeper application of this research is, every code base is, in some sense, a narrow domain.
[00:23:15.140 --> 00:23:19.140]   There are specific things to your code that don't exist in anyone else's code.
[00:23:19.140 --> 00:23:21.140]   And, that's more and more true to the larger code bases.
[00:23:21.140 --> 00:23:31.140]   So, you can imagine a future where high-to-cube RL, and preferred code-based customization, leads to significantly outperforming agents on each individual domain.
[00:23:31.140 --> 00:23:38.140]   The equivalent of hiring a software engineer and giving them millions of years of experience working specifically in your environment.
[00:23:38.140 --> 00:23:42.140]   So, this is some of the research work we've been doing in this conversation that powers Debit around the hood.
[00:23:42.140 --> 00:23:48.140]   If you'd like to play around and try this yourself, you can use, you can go to Debit.ai and sign up your account.
[00:23:48.140 --> 00:23:52.140]   Connect it with your distinct code, give it a task, and go from taking it to PR.
[00:23:52.140 --> 00:23:54.140]   Thank you so much for having me.
[00:23:54.140 --> 00:23:55.140]   Thank you so much.
[00:23:55.140 --> 00:23:55.140]   Thank you very much.
[00:23:55.140 --> 00:23:56.140]   Thank you very much.
[00:23:56.140 --> 00:23:57.140]   Thank you very much.
[00:23:57.140 --> 00:23:57.140]   Thank you very much.
[00:23:57.140 --> 00:23:58.140]   Thank you very much.
[00:23:58.140 --> 00:24:04.140]   All right.
[00:24:04.140 --> 00:24:05.140]   Thank you very much.

