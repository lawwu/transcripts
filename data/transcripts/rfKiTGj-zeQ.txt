
[00:00:00.000 --> 00:00:02.880]   The following is a conversation with Nick Bostrom,
[00:00:02.880 --> 00:00:05.520]   a philosopher at University of Oxford
[00:00:05.520 --> 00:00:08.700]   and the director of the Future of Humanity Institute.
[00:00:08.700 --> 00:00:11.900]   He has worked on fascinating and important ideas
[00:00:11.900 --> 00:00:14.960]   in existential risk, simulation hypothesis,
[00:00:14.960 --> 00:00:16.860]   human enhancement ethics,
[00:00:16.860 --> 00:00:19.940]   and the risks of superintelligent AI systems,
[00:00:19.940 --> 00:00:23.200]   including in his book, "Superintelligence."
[00:00:23.200 --> 00:00:26.200]   I can see talking to Nick multiple times in this podcast,
[00:00:26.200 --> 00:00:27.640]   many hours each time,
[00:00:27.640 --> 00:00:30.440]   because he has done some incredible work
[00:00:30.440 --> 00:00:33.520]   in artificial intelligence, in technology space,
[00:00:33.520 --> 00:00:36.440]   science, and really philosophy in general.
[00:00:36.440 --> 00:00:38.800]   But we have to start somewhere.
[00:00:38.800 --> 00:00:40.480]   This conversation was recorded
[00:00:40.480 --> 00:00:43.500]   before the outbreak of the coronavirus pandemic,
[00:00:43.500 --> 00:00:45.800]   that both Nick and I, I'm sure,
[00:00:45.800 --> 00:00:48.680]   will have a lot to say about next time we speak.
[00:00:48.680 --> 00:00:50.680]   And perhaps that is for the best,
[00:00:50.680 --> 00:00:52.920]   because the deepest lessons can be learned
[00:00:52.920 --> 00:00:55.840]   only in retrospect, when the storm has passed.
[00:00:56.680 --> 00:00:58.800]   I do recommend you read many of his papers
[00:00:58.800 --> 00:01:00.720]   on the topic of existential risk,
[00:01:00.720 --> 00:01:02.400]   including the technical report
[00:01:02.400 --> 00:01:05.720]   titled "Global Catastrophic Risks Survey"
[00:01:05.720 --> 00:01:09.360]   that he co-authored with Anders Sandberg.
[00:01:09.360 --> 00:01:11.600]   For everyone feeling the medical, psychological,
[00:01:11.600 --> 00:01:13.640]   and financial burden of this crisis,
[00:01:13.640 --> 00:01:15.480]   I'm sending love your way.
[00:01:15.480 --> 00:01:16.680]   Stay strong.
[00:01:16.680 --> 00:01:18.100]   We're in this together.
[00:01:18.100 --> 00:01:19.200]   We'll beat this thing.
[00:01:19.200 --> 00:01:23.320]   This is the Artificial Intelligence Podcast.
[00:01:23.320 --> 00:01:25.560]   If you enjoy it, subscribe on YouTube,
[00:01:25.560 --> 00:01:27.760]   review it with five stars on Apple Podcast,
[00:01:27.760 --> 00:01:29.080]   support it on Patreon,
[00:01:29.080 --> 00:01:32.200]   or simply connect with me on Twitter at Lex Friedman,
[00:01:32.200 --> 00:01:34.840]   spelled F-R-I-D-M-A-N.
[00:01:34.840 --> 00:01:37.480]   As usual, I'll do one or two minutes of ads now,
[00:01:37.480 --> 00:01:39.020]   and never any ads in the middle
[00:01:39.020 --> 00:01:41.120]   that can break the flow of the conversation.
[00:01:41.120 --> 00:01:42.440]   I hope that works for you
[00:01:42.440 --> 00:01:44.800]   and doesn't hurt the listening experience.
[00:01:44.800 --> 00:01:48.000]   This show is presented by Cash App,
[00:01:48.000 --> 00:01:50.200]   the number one finance app in the App Store.
[00:01:50.200 --> 00:01:53.600]   When you get it, use code LEXPODCAST.
[00:01:53.600 --> 00:01:55.480]   Cash App lets you send money to friends,
[00:01:55.480 --> 00:01:57.800]   buy Bitcoin, and invest in the stock market
[00:01:57.800 --> 00:01:58.800]   with as little as $1.
[00:01:58.800 --> 00:02:02.240]   Since Cash App does fractional share trading,
[00:02:02.240 --> 00:02:05.240]   let me mention that the order execution algorithm
[00:02:05.240 --> 00:02:06.480]   that works behind the scenes
[00:02:06.480 --> 00:02:09.200]   to create the abstraction of fractional orders
[00:02:09.200 --> 00:02:11.000]   is an algorithmic marvel.
[00:02:11.000 --> 00:02:13.320]   So big props to the Cash App engineers
[00:02:13.320 --> 00:02:15.840]   for solving a hard problem that in the end
[00:02:15.840 --> 00:02:18.480]   provides an easy interface that takes a step up
[00:02:18.480 --> 00:02:21.120]   to the next layer of abstraction over the stock market,
[00:02:21.120 --> 00:02:23.640]   making trading more accessible for new investors
[00:02:23.640 --> 00:02:26.840]   and diversification much easier.
[00:02:26.840 --> 00:02:28.600]   So again, if you get Cash App
[00:02:28.600 --> 00:02:30.120]   from the App Store or Google Play,
[00:02:30.120 --> 00:02:34.320]   and use the code LEXPODCAST, you get $10,
[00:02:34.320 --> 00:02:37.040]   and Cash App will also donate $10 to FIRST,
[00:02:37.040 --> 00:02:39.880]   an organization that is helping to advance robotics
[00:02:39.880 --> 00:02:42.720]   and STEM education for young people around the world.
[00:02:42.720 --> 00:02:48.080]   And now, here's my conversation with Nick Bostrom.
[00:02:48.080 --> 00:02:51.600]   At the risk of asking the Beatles to play "Yesterday"
[00:02:51.600 --> 00:02:54.080]   or the Rolling Stones to play "Satisfaction,"
[00:02:54.080 --> 00:02:56.240]   let me ask you the basics.
[00:02:56.240 --> 00:02:59.320]   What is the simulation hypothesis?
[00:02:59.320 --> 00:03:01.840]   - That we are living in a computer simulation.
[00:03:01.840 --> 00:03:04.280]   - What is a computer simulation?
[00:03:04.280 --> 00:03:07.240]   How are we supposed to even think about that?
[00:03:07.240 --> 00:03:10.680]   - Well, so the hypothesis is meant to be understood
[00:03:10.680 --> 00:03:12.640]   in a literal sense,
[00:03:12.640 --> 00:03:17.160]   not that we can kind of metaphorically view the universe
[00:03:17.160 --> 00:03:20.200]   as an information processing physical system,
[00:03:20.200 --> 00:03:24.040]   but that there is some advanced civilization
[00:03:24.040 --> 00:03:26.240]   who built a lot of computers,
[00:03:26.240 --> 00:03:30.560]   and that what we experience is an effect
[00:03:30.560 --> 00:03:33.720]   of what's going on inside one of those computers,
[00:03:33.720 --> 00:03:37.520]   so that the world around us, our own brains,
[00:03:37.520 --> 00:03:41.000]   everything we see and perceive and think and feel
[00:03:41.000 --> 00:03:45.720]   would exist because this computer
[00:03:45.720 --> 00:03:49.080]   is running certain programs.
[00:03:49.080 --> 00:03:51.000]   - So do you think of this computer
[00:03:51.000 --> 00:03:54.200]   as something similar to the computers of today,
[00:03:54.200 --> 00:03:58.080]   these deterministic sort of Turing machine type things?
[00:03:58.080 --> 00:03:59.520]   Is that what we're supposed to imagine,
[00:03:59.520 --> 00:04:01.800]   or we're supposed to think of something more
[00:04:01.800 --> 00:04:06.760]   like a quantum mechanical system,
[00:04:06.760 --> 00:04:09.160]   something much bigger, something much more complicated,
[00:04:09.160 --> 00:04:12.800]   something much more mysterious from our current perspective?
[00:04:12.800 --> 00:04:14.320]   - The ones we have today would define,
[00:04:14.320 --> 00:04:15.240]   I mean, bigger, certainly.
[00:04:15.240 --> 00:04:17.080]   You'd need more-- - It's all about size.
[00:04:17.080 --> 00:04:18.760]   - More memory and more processing power.
[00:04:18.760 --> 00:04:21.680]   I don't think anything else would be required.
[00:04:21.680 --> 00:04:24.360]   Now, it might well be that they do have,
[00:04:24.360 --> 00:04:27.080]   maybe they have quantum computers and other things
[00:04:27.080 --> 00:04:29.440]   that would give them even more oomph.
[00:04:29.440 --> 00:04:30.600]   It seems kind of plausible,
[00:04:30.600 --> 00:04:33.560]   but I don't think it's a necessary assumption
[00:04:33.560 --> 00:04:37.400]   in order to get to the conclusion
[00:04:37.400 --> 00:04:40.760]   that a technologically mature civilization
[00:04:40.760 --> 00:04:44.000]   would be able to create these kinds of computer simulations
[00:04:44.000 --> 00:04:46.520]   with conscious beings inside them.
[00:04:46.520 --> 00:04:49.440]   - So do you think the simulation hypothesis
[00:04:49.440 --> 00:04:52.520]   is an idea that's most useful in philosophy,
[00:04:52.520 --> 00:04:54.880]   computer science, physics?
[00:04:54.880 --> 00:04:59.520]   Sort of where do you see it having valuable
[00:04:59.520 --> 00:05:02.880]   kind of starting point
[00:05:02.880 --> 00:05:05.200]   in terms of a thought experiment of it?
[00:05:05.200 --> 00:05:06.040]   - Is it useful?
[00:05:06.040 --> 00:05:11.040]   I guess it's more informative and interesting
[00:05:11.040 --> 00:05:12.200]   and maybe important,
[00:05:12.200 --> 00:05:16.320]   but it's not designed to be useful for something else.
[00:05:16.320 --> 00:05:18.320]   - Well, okay, interesting, sure.
[00:05:18.320 --> 00:05:20.840]   But is it philosophically interesting
[00:05:20.840 --> 00:05:23.080]   or is there some kind of implications
[00:05:23.080 --> 00:05:24.920]   of computer science and physics?
[00:05:24.920 --> 00:05:27.880]   - I think not so much for computer science
[00:05:27.880 --> 00:05:29.160]   or physics per se.
[00:05:29.160 --> 00:05:32.240]   Certainly it would be of interest in philosophy,
[00:05:32.240 --> 00:05:37.240]   I think also to say cosmology or physics
[00:05:37.240 --> 00:05:38.920]   in as much as you're interested
[00:05:38.920 --> 00:05:43.080]   in the fundamental building blocks of the world
[00:05:43.080 --> 00:05:44.880]   and the rules that govern it.
[00:05:45.920 --> 00:05:47.000]   If we are in a simulation,
[00:05:47.000 --> 00:05:49.760]   there is then the possibility that say physics
[00:05:49.760 --> 00:05:52.960]   at the level where the computer running the simulation
[00:05:52.960 --> 00:05:58.120]   could be different from the physics governing phenomena
[00:05:58.120 --> 00:05:59.600]   in the simulation.
[00:05:59.600 --> 00:06:02.200]   So I think it might be interesting
[00:06:02.200 --> 00:06:04.760]   from point of view of religion
[00:06:04.760 --> 00:06:06.960]   or just for kind of trying to figure out
[00:06:06.960 --> 00:06:08.680]   what the heck is going on.
[00:06:08.680 --> 00:06:14.600]   So we mentioned the simulation hypothesis so far.
[00:06:14.600 --> 00:06:16.760]   There is also the simulation argument,
[00:06:16.760 --> 00:06:19.720]   which I tend to make a distinction.
[00:06:19.720 --> 00:06:20.960]   So simulation hypothesis,
[00:06:20.960 --> 00:06:22.680]   we are living in a computer simulation.
[00:06:22.680 --> 00:06:23.560]   Simulation argument,
[00:06:23.560 --> 00:06:25.840]   this argument that tries to show
[00:06:25.840 --> 00:06:27.960]   that one of three propositions is true.
[00:06:27.960 --> 00:06:30.800]   One of which is the simulation hypothesis,
[00:06:30.800 --> 00:06:32.320]   but there are two alternatives
[00:06:32.320 --> 00:06:35.800]   in the original simulation argument,
[00:06:35.800 --> 00:06:36.720]   which we can get to.
[00:06:36.720 --> 00:06:37.720]   - Yeah, let's go there.
[00:06:37.720 --> 00:06:39.080]   By the way, confusing terms
[00:06:39.080 --> 00:06:41.920]   because people will, I think,
[00:06:41.920 --> 00:06:43.760]   probably naturally think simulation argument
[00:06:43.760 --> 00:06:47.040]   equals simulation hypothesis, just terminology wise.
[00:06:47.040 --> 00:06:48.120]   But let's go there.
[00:06:48.120 --> 00:06:49.760]   So simulation hypothesis means
[00:06:49.760 --> 00:06:51.600]   that we are living in a simulation.
[00:06:51.600 --> 00:06:53.600]   So the hypothesis that we're living in a simulation,
[00:06:53.600 --> 00:06:58.600]   simulation argument has these three complete possibilities
[00:06:58.600 --> 00:07:00.400]   that cover all possibilities.
[00:07:00.400 --> 00:07:01.240]   So what are they?
[00:07:01.240 --> 00:07:02.280]   - So it's like a disjunction.
[00:07:02.280 --> 00:07:05.640]   It says at least one of these three is true.
[00:07:05.640 --> 00:07:08.760]   Although it doesn't on its own tell us which one.
[00:07:10.640 --> 00:07:15.240]   So the first one is that almost all civilizations
[00:07:15.240 --> 00:07:17.800]   at our current stage of technological development
[00:07:17.800 --> 00:07:23.680]   go extinct before they reach technological maturity.
[00:07:23.680 --> 00:07:25.960]   So there is some great filter
[00:07:25.960 --> 00:07:31.640]   that makes it so that basically
[00:07:31.640 --> 00:07:35.200]   none of the civilizations throughout,
[00:07:35.200 --> 00:07:37.640]   you know, maybe vast cosmos
[00:07:37.640 --> 00:07:41.080]   will ever get to realize the full potential
[00:07:41.080 --> 00:07:42.200]   of technological development.
[00:07:42.200 --> 00:07:44.720]   - And this could be, theoretically speaking,
[00:07:44.720 --> 00:07:47.480]   this could be because most civilizations
[00:07:47.480 --> 00:07:50.600]   kill themselves too eagerly or destroy themselves too eagerly
[00:07:50.600 --> 00:07:55.080]   or it might be super difficult to build a simulation.
[00:07:55.080 --> 00:07:56.840]   So the span of time.
[00:07:56.840 --> 00:07:58.320]   - Theoretically, it could be both.
[00:07:58.320 --> 00:08:02.080]   Now, I think it looks like we would technologically
[00:08:02.080 --> 00:08:04.360]   be able to get there in a time span
[00:08:04.360 --> 00:08:09.360]   that is short compared to, say, the lifetime of planets
[00:08:09.360 --> 00:08:13.680]   and other sort of astronomical processes.
[00:08:13.680 --> 00:08:16.920]   - So your intuition is to build a simulation is not...
[00:08:16.920 --> 00:08:18.280]   - Well, so there's this interesting concept
[00:08:18.280 --> 00:08:20.240]   of technological maturity.
[00:08:20.240 --> 00:08:22.440]   It's kind of an interesting concept
[00:08:22.440 --> 00:08:25.040]   to have for other purposes as well.
[00:08:25.040 --> 00:08:29.240]   We can see, even based on our current limited understanding,
[00:08:29.240 --> 00:08:32.680]   what some lower bound would be on the capabilities
[00:08:32.680 --> 00:08:36.960]   that you could realize by just developing technologies
[00:08:36.960 --> 00:08:38.480]   that we already see are possible.
[00:08:38.480 --> 00:08:41.960]   So for example, one of my research fellows here,
[00:08:41.960 --> 00:08:44.680]   Eric Drexler, back in the '80s,
[00:08:44.680 --> 00:08:48.320]   studied molecular manufacturing.
[00:08:48.320 --> 00:08:53.280]   That is, you could analyze using theoretical tools
[00:08:53.280 --> 00:08:55.600]   and computer modeling the performance
[00:08:55.600 --> 00:08:58.080]   of various molecularly precise structures
[00:08:58.080 --> 00:09:01.040]   that we didn't then and still don't today
[00:09:01.040 --> 00:09:03.960]   have the ability to actually fabricate.
[00:09:03.960 --> 00:09:04.960]   But you could say that, well,
[00:09:04.960 --> 00:09:07.040]   if we could put these atoms together in this way,
[00:09:07.040 --> 00:09:08.840]   then the system would be stable
[00:09:08.840 --> 00:09:12.200]   and it would rotate at this speed
[00:09:12.200 --> 00:09:16.120]   and have these computational characteristics.
[00:09:16.120 --> 00:09:18.240]   And he also outlined some pathways
[00:09:18.240 --> 00:09:20.840]   that would enable us to get to this kind
[00:09:20.840 --> 00:09:25.440]   of molecularly manufacturing in the fullness of time.
[00:09:25.440 --> 00:09:28.200]   You could do other studies we've done.
[00:09:28.200 --> 00:09:30.240]   You could look at the speed at which, say,
[00:09:30.240 --> 00:09:33.440]   it would be possible to colonize the galaxy
[00:09:33.440 --> 00:09:35.880]   if you had mature technology.
[00:09:35.880 --> 00:09:38.280]   We have an upper limit, which is the speed of light.
[00:09:38.280 --> 00:09:40.320]   We have sort of a lower current limit,
[00:09:40.320 --> 00:09:42.120]   which is how fast current rockets go.
[00:09:42.120 --> 00:09:45.840]   We know we can go faster than that by just, you know,
[00:09:45.840 --> 00:09:47.920]   making them bigger and have more fuel and stuff.
[00:09:47.920 --> 00:09:52.080]   And you can then start to describe
[00:09:52.080 --> 00:09:54.880]   the technological affordances that would exist
[00:09:54.880 --> 00:09:58.400]   once a civilization has had enough time to develop,
[00:09:58.400 --> 00:10:00.880]   at least those technologies we already know are possible.
[00:10:00.880 --> 00:10:01.960]   Then maybe they would discover
[00:10:01.960 --> 00:10:04.880]   other new physical phenomena as well
[00:10:04.880 --> 00:10:05.840]   that we haven't realized
[00:10:05.840 --> 00:10:07.800]   that would enable them to do even more.
[00:10:07.800 --> 00:10:12.520]   But at least there is this kind of basic set of capabilities.
[00:10:12.520 --> 00:10:14.320]   - Can you just linger on that?
[00:10:14.320 --> 00:10:17.560]   How do we jump from molecular manufacturing
[00:10:17.560 --> 00:10:22.160]   to deep space exploration to mature technology?
[00:10:22.160 --> 00:10:23.680]   Like, what's the connection there?
[00:10:23.680 --> 00:10:26.720]   - Well, so these would be two examples
[00:10:26.720 --> 00:10:29.400]   of technological capability sets
[00:10:29.400 --> 00:10:32.280]   that we can have a high degree of confidence
[00:10:32.280 --> 00:10:35.600]   are physically possible in our universe
[00:10:35.600 --> 00:10:39.280]   and that a civilization that was allowed to continue
[00:10:39.280 --> 00:10:41.800]   to develop its science and technology
[00:10:41.800 --> 00:10:42.640]   would eventually attain.
[00:10:42.640 --> 00:10:45.240]   - You can intuit, like, we can kind of see
[00:10:45.240 --> 00:10:49.120]   the set of breakthroughs that are likely to happen.
[00:10:49.120 --> 00:10:50.400]   So you can see, like,
[00:10:50.400 --> 00:10:53.120]   what did you call it, the technological set?
[00:10:53.120 --> 00:10:54.920]   - With computers, maybe it's easier.
[00:10:55.640 --> 00:10:58.160]   The one is we could just imagine bigger computers
[00:10:58.160 --> 00:11:00.000]   using exactly the same parts that we have.
[00:11:00.000 --> 00:11:02.680]   So you can kind of scale things that way, right?
[00:11:02.680 --> 00:11:06.120]   But you could also make processors a bit faster.
[00:11:06.120 --> 00:11:08.240]   If you had this molecular nanotechnology
[00:11:08.240 --> 00:11:09.600]   that Eric Drexler described,
[00:11:09.600 --> 00:11:13.840]   he characterized a kind of crude computer
[00:11:13.840 --> 00:11:16.760]   built with these parts that would perform
[00:11:16.760 --> 00:11:18.920]   at a million times the human brain
[00:11:18.920 --> 00:11:21.000]   while being significantly smaller,
[00:11:21.000 --> 00:11:22.440]   the size of a sugar cube.
[00:11:23.640 --> 00:11:24.640]   And he may not claim that
[00:11:24.640 --> 00:11:26.640]   that's the optimum computing structure,
[00:11:26.640 --> 00:11:28.360]   like for what, you know,
[00:11:28.360 --> 00:11:30.480]   we could build a faster computer
[00:11:30.480 --> 00:11:31.320]   that would be more efficient,
[00:11:31.320 --> 00:11:32.440]   but at least you could do that
[00:11:32.440 --> 00:11:34.080]   if you had the ability to do things
[00:11:34.080 --> 00:11:35.880]   that were atomically precise.
[00:11:35.880 --> 00:11:36.720]   - Yes.
[00:11:36.720 --> 00:11:37.960]   - I mean, so you can then combine these two.
[00:11:37.960 --> 00:11:40.960]   You could have this kind of nanomolecular ability
[00:11:40.960 --> 00:11:42.600]   to build things atom by atom,
[00:11:42.600 --> 00:11:46.080]   and then say at this, as a spatial scale,
[00:11:46.080 --> 00:11:47.960]   that would be attainable
[00:11:47.960 --> 00:11:51.200]   through space colonizing technology.
[00:11:51.200 --> 00:11:52.440]   You could then start to build
[00:11:52.440 --> 00:11:54.320]   and you could then start, for example,
[00:11:54.320 --> 00:11:56.000]   to characterize a lower bound
[00:11:56.000 --> 00:11:58.200]   on the amount of computing power
[00:11:58.200 --> 00:12:01.760]   that a technologically mature civilization would have.
[00:12:01.760 --> 00:12:04.360]   If it could grab resources,
[00:12:04.360 --> 00:12:06.160]   you know, planets and so forth,
[00:12:06.160 --> 00:12:08.520]   and then use this molecular nanotechnology
[00:12:08.520 --> 00:12:10.280]   to optimize them for computing,
[00:12:10.280 --> 00:12:15.200]   you'd get a very, very high lower bound
[00:12:15.200 --> 00:12:16.880]   on the amount of compute.
[00:12:16.880 --> 00:12:19.080]   - So, sorry, just to define some terms.
[00:12:19.080 --> 00:12:21.000]   So technologically mature civilization
[00:12:21.000 --> 00:12:23.160]   is one that took that piece of technology
[00:12:23.160 --> 00:12:26.080]   to its lower bound.
[00:12:26.080 --> 00:12:27.480]   What is a technologically mature civilization?
[00:12:27.480 --> 00:12:29.080]   - Well, okay, so that means it's a stronger concept
[00:12:29.080 --> 00:12:31.160]   than we really need for the simulation hypothesis.
[00:12:31.160 --> 00:12:33.920]   I just think it's interesting in its own right.
[00:12:33.920 --> 00:12:35.680]   So it would be the idea that there is
[00:12:35.680 --> 00:12:38.560]   some stage of technological development
[00:12:38.560 --> 00:12:40.880]   where you've basically maxed out,
[00:12:40.880 --> 00:12:43.920]   that you developed all those general purpose,
[00:12:43.920 --> 00:12:47.080]   widely useful technologies that could be developed,
[00:12:47.080 --> 00:12:50.040]   or at least kind of come very close to the,
[00:12:50.040 --> 00:12:52.360]   you're 99.9% there or something.
[00:12:52.360 --> 00:12:55.120]   So that's an independent question.
[00:12:55.120 --> 00:12:57.640]   You can think either that there is such a ceiling,
[00:12:57.640 --> 00:12:59.440]   or you might think it just goes,
[00:12:59.440 --> 00:13:02.400]   the technology tree just goes on forever.
[00:13:02.400 --> 00:13:04.640]   - Where does your sense fall?
[00:13:04.640 --> 00:13:07.240]   - I would guess that there is a maximum
[00:13:07.240 --> 00:13:09.880]   that you would start to asymptote towards.
[00:13:09.880 --> 00:13:13.800]   - So new things won't keep springing up, new ceilings.
[00:13:13.800 --> 00:13:16.320]   - In terms of basic technological capabilities,
[00:13:16.320 --> 00:13:19.840]   I think that, yeah, there is like a finite set of those
[00:13:19.840 --> 00:13:22.080]   that can exist in this universe.
[00:13:22.080 --> 00:13:27.080]   Moreover, I mean, I wouldn't be that surprised
[00:13:27.080 --> 00:13:30.080]   if we actually reached close to that level
[00:13:30.080 --> 00:13:33.880]   fairly shortly after we have, say, machine superintelligence.
[00:13:33.880 --> 00:13:36.360]   So I don't think it would take millions of years
[00:13:36.360 --> 00:13:42.280]   for a human originating civilization to begin to do this.
[00:13:42.280 --> 00:13:44.800]   It, I think, is more likely to happen
[00:13:44.800 --> 00:13:46.520]   on historical timescales.
[00:13:46.520 --> 00:13:49.480]   But that's an independent speculation
[00:13:49.480 --> 00:13:51.320]   from the simulation argument.
[00:13:51.320 --> 00:13:54.000]   I mean, for the purpose of the simulation argument,
[00:13:54.000 --> 00:13:56.080]   it doesn't really matter whether it goes
[00:13:56.080 --> 00:13:58.280]   indefinitely far up or whether there's a ceiling,
[00:13:58.280 --> 00:14:01.600]   as long as we know we can at least get to a certain level.
[00:14:01.600 --> 00:14:04.400]   And it also doesn't matter whether that's gonna happen
[00:14:04.400 --> 00:14:08.360]   in 100 years or 5,000 years or 50 million years.
[00:14:08.360 --> 00:14:11.240]   Like the timescales really don't make any difference
[00:14:11.240 --> 00:14:12.080]   for the simulation.
[00:14:12.080 --> 00:14:13.200]   - Can you linger on that a little bit?
[00:14:13.200 --> 00:14:16.480]   Like there's a big difference between 100 years
[00:14:16.480 --> 00:14:19.000]   and 10 million years.
[00:14:19.000 --> 00:14:21.640]   So does it really not matter?
[00:14:21.640 --> 00:14:22.960]   Because you just said,
[00:14:22.960 --> 00:14:25.680]   does it matter if we jump scales
[00:14:25.680 --> 00:14:28.440]   to beyond historical scales?
[00:14:28.440 --> 00:14:30.160]   So will you describe that?
[00:14:30.160 --> 00:14:34.880]   So for the simulation argument,
[00:14:34.880 --> 00:14:38.400]   sort of, doesn't it matter that we,
[00:14:38.400 --> 00:14:40.800]   if it takes 10 million years,
[00:14:40.800 --> 00:14:42.680]   it gives us a lot more opportunity
[00:14:42.680 --> 00:14:44.760]   to destroy civilization in the meantime?
[00:14:44.760 --> 00:14:47.320]   - Yeah, well, so it would shift around the probabilities
[00:14:47.320 --> 00:14:49.680]   between these three alternatives.
[00:14:49.680 --> 00:14:52.400]   That is, if we are very, very far away
[00:14:52.400 --> 00:14:54.680]   from being able to create these simulations,
[00:14:54.680 --> 00:14:56.880]   if it's like say the billions of years into the future,
[00:14:56.880 --> 00:14:59.760]   then it's more likely that we will fail ever to get there.
[00:14:59.760 --> 00:15:02.320]   There are more time for us to kind of,
[00:15:02.320 --> 00:15:03.960]   you know, go extinct along the way.
[00:15:03.960 --> 00:15:06.280]   And so similarly for other civilizations.
[00:15:06.280 --> 00:15:08.440]   - So it is important to think about how hard it is
[00:15:08.440 --> 00:15:09.880]   to build a simulation.
[00:15:09.880 --> 00:15:14.400]   - In terms of, yeah, figuring out which of the disjuncts.
[00:15:14.400 --> 00:15:16.320]   But for the simulation argument itself,
[00:15:16.320 --> 00:15:19.440]   which is agnostic as to which of these three alternatives
[00:15:19.440 --> 00:15:20.280]   is true.
[00:15:20.280 --> 00:15:21.120]   - Yeah, yeah, okay.
[00:15:21.120 --> 00:15:22.720]   - Is it like, you don't have to,
[00:15:22.720 --> 00:15:24.800]   like the simulation argument would be true
[00:15:24.800 --> 00:15:28.200]   whether or not we thought this could be done in 500 years
[00:15:28.200 --> 00:15:29.880]   or it would take 500 million years.
[00:15:29.880 --> 00:15:30.720]   - No, for sure.
[00:15:30.720 --> 00:15:31.800]   The simulation argument stands.
[00:15:31.800 --> 00:15:34.560]   I mean, I'm sure there might be some people who oppose it,
[00:15:34.560 --> 00:15:36.120]   but it doesn't matter.
[00:15:36.120 --> 00:15:39.360]   I mean, it's very nice those three cases cover it.
[00:15:39.360 --> 00:15:42.240]   But the fun part is at least not saying
[00:15:42.240 --> 00:15:43.520]   what the probabilities are,
[00:15:43.560 --> 00:15:44.960]   but kind of thinking about,
[00:15:44.960 --> 00:15:46.960]   kind of intuiting reasoning about like,
[00:15:46.960 --> 00:15:48.800]   what's more likely,
[00:15:48.800 --> 00:15:51.280]   what are the kinds of things that would make
[00:15:51.280 --> 00:15:54.200]   some of the arguments less and more so like,
[00:15:54.200 --> 00:15:56.400]   but let's actually, I don't think we went through them.
[00:15:56.400 --> 00:15:58.760]   So number one is we destroy ourselves
[00:15:58.760 --> 00:16:00.640]   before we ever create simulated.
[00:16:00.640 --> 00:16:02.440]   - Right, so that's kind of sad,
[00:16:02.440 --> 00:16:07.440]   but we have to think not just what might destroy us.
[00:16:07.440 --> 00:16:09.560]   I mean, so that could be some,
[00:16:09.560 --> 00:16:13.400]   whatever disasters or meteorites slamming the earth
[00:16:13.400 --> 00:16:16.040]   a few years from now that could destroy us, right?
[00:16:16.040 --> 00:16:18.720]   But you'd have to postulate
[00:16:18.720 --> 00:16:23.320]   in order for this first disjunct to be true,
[00:16:23.320 --> 00:16:27.760]   that almost all civilizations throughout the cosmos
[00:16:27.760 --> 00:16:32.040]   also failed to reach technological maturity.
[00:16:32.040 --> 00:16:35.040]   - And the underlying assumption there is that there is
[00:16:35.040 --> 00:16:37.080]   likely a very large number
[00:16:37.080 --> 00:16:40.120]   of other intelligent civilizations.
[00:16:40.120 --> 00:16:41.880]   - Well, if there are, yeah,
[00:16:41.880 --> 00:16:44.720]   then they would virtually all have to succumb
[00:16:44.720 --> 00:16:45.640]   in the same way.
[00:16:45.640 --> 00:16:48.640]   I mean, then that leads off another,
[00:16:48.640 --> 00:16:50.040]   I guess there are a lot of little digressions
[00:16:50.040 --> 00:16:50.880]   that are interesting.
[00:16:50.880 --> 00:16:52.000]   - Definitely, let's go there, let's go there.
[00:16:52.000 --> 00:16:53.920]   - Yeah, he means- - I'm dragging us back.
[00:16:53.920 --> 00:16:55.040]   - Well, there are these,
[00:16:55.040 --> 00:16:58.080]   there is a set of basic questions that always come up
[00:16:58.080 --> 00:17:01.400]   in conversations with interesting people,
[00:17:01.400 --> 00:17:03.600]   like the Fermi paradox, like there's like,
[00:17:03.600 --> 00:17:07.360]   you could almost define whether a person is interesting,
[00:17:07.360 --> 00:17:09.120]   whether at some point the question
[00:17:09.120 --> 00:17:10.920]   of the Fermi paradox comes up, like.
[00:17:11.120 --> 00:17:12.840]   (both laughing)
[00:17:12.840 --> 00:17:14.480]   Well, so for what it's worth,
[00:17:14.480 --> 00:17:17.920]   it looks to me that the universe is very big.
[00:17:17.920 --> 00:17:21.480]   I mean, in fact, according to the most popular
[00:17:21.480 --> 00:17:24.040]   current cosmological theories, infinitely big.
[00:17:24.040 --> 00:17:28.360]   And so then it would follow pretty trivially
[00:17:28.360 --> 00:17:31.360]   that it would contain a lot of other civilizations,
[00:17:31.360 --> 00:17:32.600]   in fact, infinitely many.
[00:17:32.600 --> 00:17:37.800]   If you have some local stochasticity and infinitely many,
[00:17:37.800 --> 00:17:40.480]   it's like, you know, infinitely many lumps of matter,
[00:17:40.480 --> 00:17:41.320]   one next to another,
[00:17:41.320 --> 00:17:42.880]   there's kind of random stuff in each one,
[00:17:42.880 --> 00:17:45.800]   then you're gonna get all possible outcomes
[00:17:45.800 --> 00:17:49.120]   with probability one, infinitely repeated.
[00:17:49.120 --> 00:17:52.560]   So then certainly that would be a lot
[00:17:52.560 --> 00:17:54.200]   of extraterrestrials out there.
[00:17:54.200 --> 00:17:57.920]   Even short of that, if the universe is very big,
[00:17:57.920 --> 00:18:00.560]   that might be a finite, but large number.
[00:18:00.560 --> 00:18:04.120]   If we were literally the only one, yeah,
[00:18:04.120 --> 00:18:08.000]   then of course, if we went extinct,
[00:18:08.000 --> 00:18:12.160]   then all of civilizations at our current stage
[00:18:12.160 --> 00:18:13.000]   would have gone extinct
[00:18:13.000 --> 00:18:14.760]   before becoming technological material.
[00:18:14.760 --> 00:18:17.440]   So then it kind of becomes trivially true
[00:18:17.440 --> 00:18:21.240]   that a very high fraction of those went extinct.
[00:18:21.240 --> 00:18:24.520]   But if we think there are many, I mean, it's interesting
[00:18:24.520 --> 00:18:26.920]   because there are certain things that plausibly
[00:18:26.920 --> 00:18:32.320]   could kill us, like if you look at existential risks.
[00:18:32.320 --> 00:18:37.440]   And it might be a different, like the best answer
[00:18:37.440 --> 00:18:39.000]   to what would be most likely to kill us
[00:18:39.000 --> 00:18:40.440]   might be a different answer
[00:18:40.440 --> 00:18:43.640]   than the best answer to the question,
[00:18:43.640 --> 00:18:46.640]   if there is something that kills almost everyone,
[00:18:46.640 --> 00:18:47.720]   what would that be?
[00:18:47.720 --> 00:18:49.880]   'Cause that would have to be some risk factor
[00:18:49.880 --> 00:18:53.520]   that was kind of uniform overall possible civilization.
[00:18:53.520 --> 00:18:56.320]   - Yeah, so in this, for the sake of this argument,
[00:18:56.320 --> 00:18:58.920]   you have to think about not just us,
[00:18:58.920 --> 00:19:01.720]   but like every civilization dies out
[00:19:01.720 --> 00:19:02.920]   before they create the simulation.
[00:19:02.920 --> 00:19:07.680]   - Yeah, or something very close to everybody.
[00:19:07.680 --> 00:19:10.080]   - Okay, so what's number two in the-
[00:19:10.080 --> 00:19:13.040]   - Well, so number two is the convergence hypothesis
[00:19:13.040 --> 00:19:14.960]   that is that maybe like a lot of,
[00:19:14.960 --> 00:19:16.880]   some of these civilizations do make it
[00:19:16.880 --> 00:19:18.600]   through to technological maturity,
[00:19:18.600 --> 00:19:21.320]   but out of those who do get there,
[00:19:21.320 --> 00:19:26.320]   they all lose interest in creating these simulations.
[00:19:26.320 --> 00:19:29.920]   So they just, they have the capability of doing it,
[00:19:29.920 --> 00:19:31.120]   but they choose not to.
[00:19:32.880 --> 00:19:34.600]   Not just a few of them decide not to,
[00:19:34.600 --> 00:19:39.360]   but out of a million,
[00:19:39.360 --> 00:19:41.520]   maybe not even a single one of them would do it.
[00:19:41.520 --> 00:19:44.840]   - And I think when you say lose interest,
[00:19:44.840 --> 00:19:46.920]   that sounds like unlikely
[00:19:46.920 --> 00:19:49.400]   because it's like they get bored or whatever,
[00:19:49.400 --> 00:19:52.680]   but it could be so many possibilities within that.
[00:19:52.680 --> 00:19:56.900]   I mean, losing interest could be,
[00:19:56.900 --> 00:20:01.240]   it could be anything from it being
[00:20:01.240 --> 00:20:03.880]   exceptionally difficult to do,
[00:20:03.880 --> 00:20:07.680]   to fundamentally changing the sort of,
[00:20:07.680 --> 00:20:10.080]   the fabric of reality if you do it,
[00:20:10.080 --> 00:20:12.760]   to ethical concerns, all those kinds of things
[00:20:12.760 --> 00:20:15.200]   could be exceptionally strong pressures.
[00:20:15.200 --> 00:20:18.440]   - Well, certainly, I mean, yeah, ethical concerns.
[00:20:18.440 --> 00:20:21.160]   I mean, not really too difficult to do.
[00:20:21.160 --> 00:20:23.640]   I mean, in a sense, that's the first assumption
[00:20:23.640 --> 00:20:25.880]   that you get to technological maturity
[00:20:25.880 --> 00:20:27.920]   where you would have the ability,
[00:20:27.920 --> 00:20:31.200]   using only a tiny fraction of your resources,
[00:20:31.200 --> 00:20:34.400]   to create many, many simulations.
[00:20:34.400 --> 00:20:36.960]   So it wouldn't be the case that they would need
[00:20:36.960 --> 00:20:39.640]   to spend half of their GDP forever
[00:20:39.640 --> 00:20:41.280]   in order to create one simulation.
[00:20:41.280 --> 00:20:42.880]   And they had this difficult debate
[00:20:42.880 --> 00:20:46.320]   about whether they should invest half of their GDP for this.
[00:20:46.320 --> 00:20:47.160]   It would more be like,
[00:20:47.160 --> 00:20:49.600]   well, if any little fraction of the civilization
[00:20:49.600 --> 00:20:51.680]   feels like doing this at any point
[00:20:51.680 --> 00:20:55.960]   during maybe their millions of years of existence,
[00:20:55.960 --> 00:20:59.120]   then there will be millions of simulations.
[00:21:00.200 --> 00:21:05.200]   But certainly, there could be many conceivable reasons
[00:21:05.200 --> 00:21:07.280]   for why there would be this convert,
[00:21:07.280 --> 00:21:10.600]   many possible reasons for not running ancestor simulations
[00:21:10.600 --> 00:21:12.720]   or other computer simulations,
[00:21:12.720 --> 00:21:15.000]   even if you could do so cheaply.
[00:21:15.000 --> 00:21:17.080]   - By the way, what's an ancestor simulation?
[00:21:17.080 --> 00:21:20.720]   - Well, that would be the type of computer simulation
[00:21:20.720 --> 00:21:24.600]   that would contain people like those
[00:21:24.600 --> 00:21:27.560]   we think have lived on our planet in the past
[00:21:27.560 --> 00:21:30.120]   and like ourselves in terms of the types of experiences
[00:21:30.120 --> 00:21:33.520]   that they have and where those simulated people are conscious.
[00:21:33.520 --> 00:21:36.440]   So like not just simulated in the same sense
[00:21:36.440 --> 00:21:41.440]   that a non-player character would be simulated
[00:21:41.440 --> 00:21:42.360]   in the current computer game,
[00:21:42.360 --> 00:21:45.040]   where it's kind of has like an avatar body
[00:21:45.040 --> 00:21:46.680]   and then a very simple mechanism
[00:21:46.680 --> 00:21:49.600]   that moves it forward or backwards,
[00:21:49.600 --> 00:21:54.600]   but something where the simulated being has a brain,
[00:21:54.600 --> 00:21:56.200]   let's say, that's simulated
[00:21:56.200 --> 00:21:57.960]   at a sufficient level of granularity
[00:21:57.960 --> 00:22:02.640]   that it would have the same subjective experiences
[00:22:02.640 --> 00:22:03.480]   as we have.
[00:22:03.480 --> 00:22:06.320]   - So where does consciousness fit into this?
[00:22:06.320 --> 00:22:08.400]   Do you think simulation,
[00:22:08.400 --> 00:22:09.920]   like is there different ways to think about
[00:22:09.920 --> 00:22:11.620]   how this can be simulated,
[00:22:11.620 --> 00:22:13.360]   just like you're talking about now?
[00:22:13.360 --> 00:22:18.220]   Do we have to simulate each brain
[00:22:18.220 --> 00:22:20.960]   within the larger simulation?
[00:22:20.960 --> 00:22:24.660]   Is it enough to simulate just the brain, just the minds,
[00:22:24.660 --> 00:22:27.560]   and not the simulation, not the big universe itself?
[00:22:27.560 --> 00:22:29.920]   Like is there different ways to think about this?
[00:22:29.920 --> 00:22:33.000]   - Yeah, I guess there is a kind of premise
[00:22:33.000 --> 00:22:36.920]   in the simulation argument rolled in
[00:22:36.920 --> 00:22:38.840]   from philosophy of mind.
[00:22:38.840 --> 00:22:41.080]   That is that it would be possible
[00:22:41.080 --> 00:22:44.960]   to create a conscious mind in a computer
[00:22:44.960 --> 00:22:48.280]   and that what determines whether some system
[00:22:48.280 --> 00:22:52.040]   is conscious or not is not like whether it's built
[00:22:52.040 --> 00:22:54.640]   from organic biological neurons,
[00:22:54.640 --> 00:22:56.760]   but maybe something like what the structure
[00:22:56.760 --> 00:22:59.560]   of the computation is that it implements.
[00:22:59.560 --> 00:23:02.400]   So we can discuss that if we want,
[00:23:02.400 --> 00:23:04.480]   but I think it would be,
[00:23:04.480 --> 00:23:08.520]   or as far as my view, that it would be sufficient, say,
[00:23:08.520 --> 00:23:13.520]   if you had a computation that was identical
[00:23:13.520 --> 00:23:15.760]   to the computation in the human brain
[00:23:15.760 --> 00:23:17.400]   down to the level of neurons.
[00:23:17.400 --> 00:23:19.800]   So if you had a simulation with 100 billion neurons
[00:23:19.800 --> 00:23:21.420]   connected in the same way as the human brain,
[00:23:21.420 --> 00:23:23.320]   and you then roll that forward
[00:23:24.400 --> 00:23:27.560]   with the same kind of synaptic weights and so forth,
[00:23:27.560 --> 00:23:30.400]   so you actually had the same behavior coming out of this
[00:23:30.400 --> 00:23:33.720]   as a human with that brain would have done,
[00:23:33.720 --> 00:23:35.840]   then I think that would be conscious.
[00:23:35.840 --> 00:23:38.640]   Now, it's possible you could also generate consciousness
[00:23:38.640 --> 00:23:44.440]   without having that detailed simulation.
[00:23:44.440 --> 00:23:47.000]   There I'm getting more uncertain
[00:23:47.000 --> 00:23:50.840]   exactly how much you could simplify or abstract away.
[00:23:50.840 --> 00:23:51.920]   - Can you look on that?
[00:23:51.920 --> 00:23:52.760]   What do you mean?
[00:23:53.680 --> 00:23:56.680]   I missed where you're placing consciousness in the second.
[00:23:56.680 --> 00:23:59.160]   - Well, so if you are a computationalist,
[00:23:59.160 --> 00:24:01.840]   do you think that what creates consciousness
[00:24:01.840 --> 00:24:04.720]   is the implementation of a computation?
[00:24:04.720 --> 00:24:06.520]   - Some property, emerging property
[00:24:06.520 --> 00:24:08.200]   of the computation itself.
[00:24:08.200 --> 00:24:09.160]   - Yeah. - That's the idea.
[00:24:09.160 --> 00:24:10.520]   - Yeah, you could say that.
[00:24:10.520 --> 00:24:12.240]   But then the question is,
[00:24:12.240 --> 00:24:14.400]   what's the class of computations
[00:24:14.400 --> 00:24:18.100]   such that when they are run, consciousness emerges?
[00:24:18.100 --> 00:24:20.760]   So if you just have something that adds
[00:24:20.760 --> 00:24:22.880]   one plus one plus one plus one,
[00:24:22.880 --> 00:24:24.160]   like a simple computation,
[00:24:24.160 --> 00:24:26.960]   you think maybe that's not gonna have any consciousness.
[00:24:26.960 --> 00:24:31.040]   If on the other hand, the computation is one
[00:24:31.040 --> 00:24:34.040]   like our human brains are performing,
[00:24:34.040 --> 00:24:37.640]   where as part of the computation,
[00:24:37.640 --> 00:24:40.920]   there is a global workspace,
[00:24:40.920 --> 00:24:43.360]   a sophisticated attention mechanism,
[00:24:43.360 --> 00:24:47.880]   there is self-representations of other cognitive processes
[00:24:47.880 --> 00:24:50.320]   and a whole lot of other things,
[00:24:50.320 --> 00:24:52.800]   that possibly would be conscious.
[00:24:52.800 --> 00:24:54.640]   And in fact, if it's exactly like ours,
[00:24:54.640 --> 00:24:56.520]   I think definitely it would.
[00:24:56.520 --> 00:25:01.200]   But exactly how much less than the full computation
[00:25:01.200 --> 00:25:04.560]   that the human brain is performing would be required
[00:25:04.560 --> 00:25:07.340]   is a little bit, I think, of an open question.
[00:25:07.340 --> 00:25:12.560]   You asked another interesting question as well,
[00:25:12.560 --> 00:25:17.560]   which is, would it be sufficient to just have,
[00:25:17.720 --> 00:25:20.760]   say, the brain or would you need the environment
[00:25:20.760 --> 00:25:24.960]   in order to generate the same kind of experiences
[00:25:24.960 --> 00:25:25.840]   that we have?
[00:25:25.840 --> 00:25:29.240]   And there is a bunch of stuff we don't know.
[00:25:29.240 --> 00:25:30.600]   I mean, if you look at, say,
[00:25:30.600 --> 00:25:34.080]   current virtual reality environments,
[00:25:34.080 --> 00:25:37.320]   one thing that's clear is that we don't have to simulate
[00:25:37.320 --> 00:25:39.560]   all details of them all the time
[00:25:39.560 --> 00:25:42.440]   in order for, say, the human player
[00:25:42.440 --> 00:25:46.160]   to have the perception that there is a full reality in there.
[00:25:46.160 --> 00:25:48.200]   You can have, say, procedurally generated,
[00:25:48.200 --> 00:25:49.920]   where you might only render a scene
[00:25:49.920 --> 00:25:53.560]   when it's actually within the view of the player character.
[00:25:53.560 --> 00:26:00.200]   And so similarly, if this environment
[00:26:00.200 --> 00:26:03.800]   that we perceive is simulated,
[00:26:03.800 --> 00:26:08.480]   it might be that all of the parts that come into our view
[00:26:08.480 --> 00:26:10.240]   are rendered at any given time.
[00:26:10.240 --> 00:26:13.600]   And a lot of aspects that never come into view,
[00:26:13.600 --> 00:26:17.320]   say, the details of this microphone I'm talking into,
[00:26:17.320 --> 00:26:21.720]   exactly what each atom is doing at any given point in time,
[00:26:21.720 --> 00:26:24.120]   might not be part of the simulation,
[00:26:24.120 --> 00:26:26.200]   only a more coarse-grained representation.
[00:26:26.200 --> 00:26:28.320]   - So that to me is actually
[00:26:28.320 --> 00:26:29.960]   from an engineering perspective
[00:26:29.960 --> 00:26:31.680]   why the simulation hypothesis
[00:26:31.680 --> 00:26:33.600]   is really interesting to think about,
[00:26:33.600 --> 00:26:38.600]   is how difficult is it to fake
[00:26:38.600 --> 00:26:41.640]   sort of in a virtual reality context,
[00:26:41.640 --> 00:26:42.880]   I don't know if fake is the right word,
[00:26:42.880 --> 00:26:46.960]   but to construct a reality that is sufficiently real to us
[00:26:46.960 --> 00:26:52.120]   to be immersive in the way that the physical world is.
[00:26:52.120 --> 00:26:56.960]   I think that's actually probably an answerable question
[00:26:56.960 --> 00:26:59.440]   of psychology, of computer science,
[00:26:59.440 --> 00:27:04.440]   of where's the line where it becomes so immersive
[00:27:04.440 --> 00:27:08.240]   that you don't wanna leave that world?
[00:27:08.240 --> 00:27:10.960]   - Yeah, or that you don't realize while you're in it
[00:27:10.960 --> 00:27:13.240]   that it is a virtual world.
[00:27:13.240 --> 00:27:15.000]   - Yeah, those are two actually questions.
[00:27:15.000 --> 00:27:17.360]   Yours is the more sort of the good question
[00:27:17.360 --> 00:27:18.800]   about the realism.
[00:27:18.800 --> 00:27:20.600]   But mine, from my perspective,
[00:27:20.600 --> 00:27:23.400]   what's interesting is it doesn't have to be real,
[00:27:23.400 --> 00:27:28.120]   but how can we construct a world
[00:27:28.120 --> 00:27:29.800]   that we wouldn't wanna leave?
[00:27:29.800 --> 00:27:32.520]   - Yeah, I mean, I think that might be too low a bar.
[00:27:32.520 --> 00:27:35.640]   I mean, if you think, say, when people first had Pong
[00:27:35.640 --> 00:27:37.320]   or something like that,
[00:27:37.320 --> 00:27:38.160]   I'm sure there were people
[00:27:38.160 --> 00:27:40.840]   who wanted to keep playing it for a long time
[00:27:40.840 --> 00:27:41.680]   'cause it was fun,
[00:27:41.680 --> 00:27:43.800]   and they wanted to be in this little world.
[00:27:43.800 --> 00:27:46.440]   I'm not sure we would say it's immersive.
[00:27:46.440 --> 00:27:48.160]   I mean, I guess in some sense it is,
[00:27:48.160 --> 00:27:51.240]   but an absorbing activity doesn't even have to be.
[00:27:51.240 --> 00:27:52.960]   - But they left that world though.
[00:27:52.960 --> 00:27:58.880]   So I think that bar is deceivingly high.
[00:27:58.880 --> 00:28:00.880]   So they eventually left.
[00:28:00.880 --> 00:28:04.400]   So you can play Pong or StarCraft
[00:28:04.400 --> 00:28:07.880]   or whatever more sophisticated games for hours,
[00:28:07.880 --> 00:28:09.680]   for months, you know,
[00:28:09.680 --> 00:28:12.280]   while the World of Warcraft could be in a big addiction,
[00:28:12.280 --> 00:28:13.880]   but eventually they escaped that.
[00:28:13.880 --> 00:28:16.920]   - Ah, so you mean when it's absorbing enough
[00:28:16.920 --> 00:28:18.400]   that you would spend your entire,
[00:28:18.400 --> 00:28:21.040]   you would choose to spend your entire life in there.
[00:28:21.040 --> 00:28:24.440]   - And then thereby changing the concept of what reality is
[00:28:24.440 --> 00:28:29.440]   because your reality becomes the game,
[00:28:29.440 --> 00:28:31.240]   not because you're fooled,
[00:28:31.240 --> 00:28:34.160]   but because you've made that choice.
[00:28:34.160 --> 00:28:35.400]   - Yeah, and it may be different.
[00:28:35.400 --> 00:28:38.480]   People might have different preferences regarding that.
[00:28:38.480 --> 00:28:43.320]   Some might, even if you had a perfect virtual reality,
[00:28:43.320 --> 00:28:47.560]   might still prefer not to spend
[00:28:47.560 --> 00:28:49.120]   the rest of their lives there.
[00:28:49.120 --> 00:28:53.080]   I mean, in philosophy, there's this experience machine,
[00:28:53.080 --> 00:28:54.240]   thought experiment.
[00:28:54.240 --> 00:28:55.760]   Have you come across this?
[00:28:55.760 --> 00:28:58.840]   So Robert Nozick had this thought experiment
[00:28:58.840 --> 00:29:03.840]   where you imagine some crazy, super duper neuroscientists
[00:29:03.840 --> 00:29:06.160]   of the future have created a machine
[00:29:06.160 --> 00:29:08.040]   that could give you any experience you want
[00:29:08.040 --> 00:29:09.120]   if you step in there.
[00:29:09.120 --> 00:29:11.120]   And for the rest of your life,
[00:29:11.120 --> 00:29:14.000]   you can kind of pre-programmed it in different ways.
[00:29:14.000 --> 00:29:20.640]   So your fondest dreams could come true.
[00:29:20.640 --> 00:29:22.640]   You could, whatever you dream,
[00:29:22.640 --> 00:29:26.080]   you wanna be a great artist, a great lover,
[00:29:26.080 --> 00:29:29.080]   like have a wonderful life, all of these things.
[00:29:29.080 --> 00:29:31.080]   If you step into the experience machine,
[00:29:31.080 --> 00:29:34.040]   will be your experiences constantly happy.
[00:29:36.240 --> 00:29:38.360]   But you would kind of disconnect from the rest of reality
[00:29:38.360 --> 00:29:40.160]   and you would float there in a tank.
[00:29:40.160 --> 00:29:44.600]   And so Nozick thought that most people
[00:29:44.600 --> 00:29:49.000]   would choose not to enter the experience machine.
[00:29:49.000 --> 00:29:50.920]   I mean, many might wanna go there for a holiday,
[00:29:50.920 --> 00:29:52.480]   but they wouldn't want to sort of check out
[00:29:52.480 --> 00:29:54.360]   of existence permanently.
[00:29:54.360 --> 00:29:56.120]   And so he thought that was an argument
[00:29:56.120 --> 00:29:59.640]   against certain views of value,
[00:29:59.640 --> 00:30:01.880]   according to what we value
[00:30:01.880 --> 00:30:04.480]   is a function of what we experience.
[00:30:04.480 --> 00:30:05.800]   And because in the experience machine,
[00:30:05.800 --> 00:30:07.080]   you could have any experience you want.
[00:30:07.080 --> 00:30:09.200]   And yet many people would think
[00:30:09.200 --> 00:30:11.880]   that would not be much value.
[00:30:11.880 --> 00:30:15.360]   So therefore what we value depends on other things
[00:30:15.360 --> 00:30:18.440]   than what we experience.
[00:30:18.440 --> 00:30:21.280]   So, okay, can you take that argument further?
[00:30:21.280 --> 00:30:23.800]   I mean, what about the fact that maybe what we value
[00:30:23.800 --> 00:30:25.040]   is the up and down of life?
[00:30:25.040 --> 00:30:27.840]   So you could have up and downs in the experience machine.
[00:30:27.840 --> 00:30:31.120]   But what can't you have in the experience machine?
[00:30:31.120 --> 00:30:33.760]   Well, I mean, that then becomes
[00:30:33.760 --> 00:30:35.320]   an interesting question to explore,
[00:30:35.320 --> 00:30:38.920]   but for example, real connection with other people,
[00:30:38.920 --> 00:30:40.920]   if the experience machine is a solar machine
[00:30:40.920 --> 00:30:42.000]   where it's only you,
[00:30:42.000 --> 00:30:44.840]   like that's something you wouldn't have there.
[00:30:44.840 --> 00:30:46.560]   You would have this objective experience
[00:30:46.560 --> 00:30:48.160]   that would be like fake people.
[00:30:48.160 --> 00:30:49.240]   - Yeah.
[00:30:49.240 --> 00:30:51.840]   - But if you gave somebody flowers,
[00:30:51.840 --> 00:30:54.360]   that wouldn't be anybody there who actually got happy.
[00:30:54.360 --> 00:30:58.360]   It would just be a little simulation of somebody smiling,
[00:30:58.360 --> 00:31:00.360]   but the simulation would not be the kind of simulation
[00:31:00.360 --> 00:31:01.920]   I'm talking about in the simulation argument
[00:31:01.920 --> 00:31:04.000]   where the simulated creature is conscious.
[00:31:04.000 --> 00:31:06.680]   It would just be a kind of a smiley face
[00:31:06.680 --> 00:31:08.400]   that would look perfectly real to you.
[00:31:08.400 --> 00:31:10.480]   - So we're now drawing a distinction
[00:31:10.480 --> 00:31:13.640]   between appear to be perfectly real
[00:31:13.640 --> 00:31:14.920]   and actually being real.
[00:31:14.920 --> 00:31:15.760]   - Yeah.
[00:31:15.760 --> 00:31:18.160]   So that could be one thing.
[00:31:18.160 --> 00:31:20.440]   I mean, like a big impact on history,
[00:31:20.440 --> 00:31:22.720]   maybe it's also something you won't have
[00:31:22.720 --> 00:31:25.560]   if you check into this experience machine.
[00:31:25.560 --> 00:31:27.760]   So some people might actually feel
[00:31:27.760 --> 00:31:29.520]   the life I wanna have for me is one
[00:31:29.520 --> 00:31:33.000]   where I have a big positive impact
[00:31:33.000 --> 00:31:35.280]   on how history unfolds.
[00:31:35.280 --> 00:31:38.560]   So you could kind of explore
[00:31:38.560 --> 00:31:43.120]   these different possible explanations for why it is
[00:31:43.120 --> 00:31:45.000]   you wouldn't wanna go into the experience machine
[00:31:45.000 --> 00:31:47.840]   if that's what you feel.
[00:31:47.840 --> 00:31:50.560]   And one interesting observation
[00:31:50.560 --> 00:31:52.680]   regarding this Nozick thought experiment
[00:31:52.680 --> 00:31:54.400]   and the conclusions he wanted to draw from it
[00:31:54.400 --> 00:31:58.720]   is how much is a kind of a status quo effect.
[00:31:58.720 --> 00:32:01.760]   So a lot of people might not wanna get this
[00:32:01.760 --> 00:32:05.840]   on current reality to plug into this dream machine.
[00:32:05.840 --> 00:32:09.920]   But if they instead were told,
[00:32:09.920 --> 00:32:13.040]   well, what you've experienced up to this point
[00:32:13.040 --> 00:32:18.040]   was a dream, now, do you wanna disconnect from this
[00:32:18.040 --> 00:32:20.440]   and enter the real world
[00:32:20.440 --> 00:32:22.920]   when you have no idea maybe what the real world is?
[00:32:22.920 --> 00:32:23.760]   Or maybe you could say,
[00:32:23.760 --> 00:32:26.480]   well, you're actually a farmer in Peru
[00:32:26.480 --> 00:32:30.280]   growing peanuts and you could live
[00:32:30.280 --> 00:32:33.520]   for the rest of your life in this.
[00:32:33.520 --> 00:32:37.840]   Or would you wanna continue your dream life
[00:32:37.840 --> 00:32:40.560]   as Alex Friedman going around the world,
[00:32:40.560 --> 00:32:42.640]   making podcasts and doing research?
[00:32:42.640 --> 00:32:48.200]   If the status quo was that they were actually
[00:32:48.200 --> 00:32:50.840]   in the experience machine,
[00:32:50.840 --> 00:32:52.560]   I think a lot of people might then prefer
[00:32:52.560 --> 00:32:54.960]   to live the life that they are familiar with
[00:32:54.960 --> 00:32:57.000]   rather than sort of bail out into.
[00:32:57.000 --> 00:32:57.880]   So--
[00:32:57.880 --> 00:33:00.240]   - That's interesting, the change itself, the leap.
[00:33:00.240 --> 00:33:01.840]   - Yeah, so it might not be so much
[00:33:01.840 --> 00:33:04.320]   the reality itself that we are after,
[00:33:04.320 --> 00:33:06.640]   but it's more that we are maybe involved
[00:33:06.640 --> 00:33:08.760]   in certain projects and relationships.
[00:33:08.760 --> 00:33:12.480]   And we have a self-identity and these things
[00:33:12.480 --> 00:33:14.120]   that our values are kind of connected
[00:33:14.120 --> 00:33:15.760]   with carrying that forward.
[00:33:15.760 --> 00:33:19.200]   And then whether it's inside a tank
[00:33:19.200 --> 00:33:21.440]   or outside a tank in Peru,
[00:33:21.440 --> 00:33:23.720]   or whether inside a computer, outside a computer,
[00:33:23.720 --> 00:33:26.280]   that's kind of less important
[00:33:26.280 --> 00:33:29.600]   to what we ultimately care about.
[00:33:29.600 --> 00:33:32.520]   - Yeah, but still, so just to linger on it,
[00:33:32.520 --> 00:33:33.800]   it is interesting.
[00:33:33.800 --> 00:33:36.600]   I find maybe people are different,
[00:33:36.600 --> 00:33:39.360]   but I find myself quite willing to take the leap
[00:33:39.360 --> 00:33:42.080]   to the farmer in Peru,
[00:33:42.080 --> 00:33:44.000]   especially as the virtual reality system
[00:33:44.000 --> 00:33:45.400]   become more realistic.
[00:33:45.400 --> 00:33:48.440]   I find that possibility.
[00:33:48.440 --> 00:33:50.680]   And I think more people would take that leap.
[00:33:50.680 --> 00:33:52.120]   - But so in this thought experiment,
[00:33:52.120 --> 00:33:53.200]   just to make sure we are understanding,
[00:33:53.200 --> 00:33:55.200]   so in this case, the farmer in Peru
[00:33:55.200 --> 00:33:57.600]   would not be a virtual reality.
[00:33:57.600 --> 00:33:58.880]   That would be the real--
[00:33:58.880 --> 00:33:59.720]   - The real.
[00:33:59.720 --> 00:34:01.080]   - The real, your life,
[00:34:01.080 --> 00:34:04.440]   like before this whole experience machine started.
[00:34:04.440 --> 00:34:06.920]   - Well, I kind of assumed from that description,
[00:34:06.920 --> 00:34:08.160]   you're being very specific,
[00:34:08.160 --> 00:34:12.720]   but that kind of idea just like washes away
[00:34:12.720 --> 00:34:14.720]   the concept of what's real.
[00:34:14.720 --> 00:34:16.920]   I mean, I'm still a little hesitant
[00:34:16.920 --> 00:34:21.920]   about your kind of distinction between real and illusion.
[00:34:21.920 --> 00:34:26.880]   Because when you can have an illusion that feels,
[00:34:26.880 --> 00:34:28.720]   I mean, that looks real,
[00:34:28.720 --> 00:34:32.640]   I mean, what, I don't know how you can definitively say
[00:34:32.640 --> 00:34:33.760]   something is real or not.
[00:34:33.760 --> 00:34:36.480]   Like what's a good way to prove that something is real
[00:34:36.480 --> 00:34:38.000]   in that context?
[00:34:38.000 --> 00:34:39.920]   - Well, so I guess in this case,
[00:34:39.920 --> 00:34:41.040]   it's more a stipulation.
[00:34:41.040 --> 00:34:43.880]   In one case, you're floating in a tank
[00:34:43.880 --> 00:34:47.400]   with these wires by the super duper neuroscientists
[00:34:47.400 --> 00:34:49.400]   plugging into your head,
[00:34:49.400 --> 00:34:52.080]   giving you like Friedman experiences.
[00:34:52.080 --> 00:34:52.920]   And in the other,
[00:34:52.920 --> 00:34:55.600]   you're actually tilling the soil in Peru,
[00:34:55.600 --> 00:34:56.440]   growing peanuts,
[00:34:56.440 --> 00:34:58.200]   and then those peanuts are being eaten
[00:34:58.200 --> 00:35:01.040]   by other people all around the world who buy the exports.
[00:35:01.040 --> 00:35:01.880]   - That's real.
[00:35:01.880 --> 00:35:03.960]   - So these are two different possible situations
[00:35:03.960 --> 00:35:06.760]   in the one and the same real world
[00:35:06.760 --> 00:35:09.960]   that you could choose to occupy.
[00:35:09.960 --> 00:35:11.040]   - But just to be clear,
[00:35:11.040 --> 00:35:15.160]   when you're in a vat with wires and the neuroscientists,
[00:35:15.160 --> 00:35:18.560]   you can still go farming in Peru, right?
[00:35:18.560 --> 00:35:22.400]   - No, well, you could, if you wanted to,
[00:35:22.400 --> 00:35:24.440]   you could have the experience of farming in Peru,
[00:35:24.440 --> 00:35:27.240]   but that wouldn't actually be any peanuts grown.
[00:35:27.240 --> 00:35:30.000]   - Well, but what makes a peanut?
[00:35:30.000 --> 00:35:33.800]   So a peanut could be grown
[00:35:33.800 --> 00:35:37.280]   and you could feed things with that peanut.
[00:35:37.280 --> 00:35:41.640]   And why can't all of that be done in a simulation?
[00:35:41.640 --> 00:35:42.480]   - I hope, first of all,
[00:35:42.480 --> 00:35:45.200]   that they actually have peanut farms in Peru.
[00:35:45.200 --> 00:35:48.040]   I guess we'll get a lot of comments otherwise from Angry.
[00:35:48.040 --> 00:35:51.680]   - I was with you up to the point
[00:35:51.680 --> 00:35:52.800]   when you started talking about Peru.
[00:35:52.800 --> 00:35:57.800]   - You should know you can't grow peanuts in that climate.
[00:35:57.800 --> 00:36:02.600]   - No, I mean, I think, I mean, in the simulation,
[00:36:02.600 --> 00:36:05.240]   I think there is a sense, the important sense,
[00:36:05.240 --> 00:36:06.760]   in which it would all be real.
[00:36:06.760 --> 00:36:09.840]   Nevertheless, there is a distinction
[00:36:09.840 --> 00:36:12.720]   between inside a simulation and outside a simulation,
[00:36:12.720 --> 00:36:16.160]   or in the case of NOSIG's thought experiment,
[00:36:16.160 --> 00:36:19.400]   whether you're in the vat or outside the vat.
[00:36:19.400 --> 00:36:22.480]   And some of those differences may or may not be important.
[00:36:22.480 --> 00:36:25.480]   I mean, that comes down to your values and preferences.
[00:36:25.480 --> 00:36:29.800]   So if the experience machine
[00:36:29.800 --> 00:36:32.760]   only gives you the experience of growing peanuts,
[00:36:32.760 --> 00:36:35.760]   but you're the only one in the experience machines.
[00:36:35.760 --> 00:36:37.920]   - No, but there's other, you can,
[00:36:37.920 --> 00:36:41.280]   within the experience machine, others can plug in.
[00:36:41.280 --> 00:36:43.760]   - Well, there are versions of the experience machine.
[00:36:43.760 --> 00:36:45.600]   So in fact, you might wanna have
[00:36:45.600 --> 00:36:47.040]   distinguished different thought experiments,
[00:36:47.040 --> 00:36:48.320]   different versions of it.
[00:36:48.320 --> 00:36:50.640]   So in like in the original thought experiment,
[00:36:50.640 --> 00:36:51.640]   maybe it's only you, right?
[00:36:51.640 --> 00:36:52.480]   - Maybe just you.
[00:36:52.480 --> 00:36:54.400]   So, and you think, I wouldn't wanna go in there.
[00:36:54.400 --> 00:36:56.160]   Well, that tells you something interesting
[00:36:56.160 --> 00:36:58.040]   about what you value and what you care about.
[00:36:58.040 --> 00:37:01.080]   Then you could say, well, what if you add the fact
[00:37:01.080 --> 00:37:02.320]   that there would be other people in there
[00:37:02.320 --> 00:37:03.280]   and you would interact with them?
[00:37:03.280 --> 00:37:05.840]   Well, it starts to make it more attractive, right?
[00:37:05.840 --> 00:37:07.720]   Then you could add in, well,
[00:37:07.720 --> 00:37:10.560]   what if you could also have important long-term effects
[00:37:10.560 --> 00:37:11.800]   on human history and the world,
[00:37:11.800 --> 00:37:14.320]   and you could actually do something useful,
[00:37:14.320 --> 00:37:15.200]   even though you were in there,
[00:37:15.200 --> 00:37:17.640]   that makes it maybe even more attractive.
[00:37:17.640 --> 00:37:21.320]   Like you could actually have a life that had a purpose
[00:37:21.320 --> 00:37:22.360]   and consequences.
[00:37:22.360 --> 00:37:25.360]   And so as you sort of add more into it,
[00:37:25.360 --> 00:37:30.360]   it becomes more similar to the baseline reality
[00:37:30.360 --> 00:37:32.840]   that you were comparing it to.
[00:37:32.840 --> 00:37:35.120]   - Yeah, but I just think inside the experience machine
[00:37:35.120 --> 00:37:39.240]   and without taking those steps that you just mentioned,
[00:37:39.240 --> 00:37:43.240]   you still have an impact on long-term history
[00:37:43.240 --> 00:37:47.680]   of the creatures that live inside that,
[00:37:47.680 --> 00:37:50.560]   of the quote unquote fake creatures that live inside
[00:37:50.560 --> 00:37:53.200]   that experience machine.
[00:37:53.200 --> 00:37:55.360]   And that, like at a certain point,
[00:37:55.360 --> 00:37:59.560]   if there's a person waiting for you
[00:37:59.560 --> 00:38:01.080]   inside that experience machine,
[00:38:01.080 --> 00:38:05.760]   maybe your newly found wife, and she dies,
[00:38:05.760 --> 00:38:08.840]   she has fear, she has hopes,
[00:38:08.840 --> 00:38:10.480]   and she exists in that machine.
[00:38:10.480 --> 00:38:13.680]   When you unplug yourself and plug back in,
[00:38:13.680 --> 00:38:16.160]   she's still there going on about her life.
[00:38:16.160 --> 00:38:17.400]   - Oh, well, in that case, yeah,
[00:38:17.400 --> 00:38:20.160]   she starts to have more of an independent existence.
[00:38:20.160 --> 00:38:21.320]   - Independent existence.
[00:38:21.320 --> 00:38:24.240]   But it depends, I think, on how she's implemented
[00:38:24.240 --> 00:38:25.640]   in the experience machine.
[00:38:25.640 --> 00:38:29.360]   Take one limit case where all she is
[00:38:29.360 --> 00:38:33.160]   is a static picture on the wall, a photograph.
[00:38:33.160 --> 00:38:35.880]   So you think, well, I can look at her, right?
[00:38:35.880 --> 00:38:37.960]   But that's it, there's no,
[00:38:37.960 --> 00:38:40.360]   but then you think, well, it doesn't really matter much
[00:38:40.360 --> 00:38:41.280]   what happens to that,
[00:38:41.280 --> 00:38:42.920]   and any more than a normal photograph,
[00:38:42.920 --> 00:38:45.000]   if you tear it up, right?
[00:38:45.000 --> 00:38:46.360]   It means you can't see it anymore,
[00:38:46.360 --> 00:38:48.760]   but you haven't harmed the person
[00:38:48.760 --> 00:38:50.040]   whose picture you tore up.
[00:38:50.040 --> 00:38:51.560]   - That's the good one.
[00:38:51.560 --> 00:38:53.640]   - But if she's actually implemented,
[00:38:53.640 --> 00:38:57.040]   say, at a neural level of detail,
[00:38:57.040 --> 00:39:00.640]   so that she's a fully realized digital mind
[00:39:00.640 --> 00:39:04.760]   with the same behavioral repertoire as you have,
[00:39:04.760 --> 00:39:06.160]   then very plausibly,
[00:39:06.160 --> 00:39:08.840]   she would be a conscious person like you are.
[00:39:08.840 --> 00:39:12.240]   And then you would, what you do in this experience machine
[00:39:12.240 --> 00:39:13.640]   would have real consequences
[00:39:13.640 --> 00:39:16.040]   for how this other mind felt.
[00:39:16.040 --> 00:39:18.880]   So you have to specify
[00:39:18.880 --> 00:39:21.080]   which of these experience machines you're talking about.
[00:39:21.080 --> 00:39:23.520]   I think it's not entirely obvious
[00:39:23.520 --> 00:39:28.000]   that it would be possible to have an experience machine
[00:39:28.000 --> 00:39:31.320]   that gave you a normal set of human experiences,
[00:39:31.320 --> 00:39:35.280]   which include experiences of interacting with other people,
[00:39:35.280 --> 00:39:39.600]   without that also generating consciousnesses
[00:39:39.600 --> 00:39:41.320]   corresponding to those other people.
[00:39:41.320 --> 00:39:44.920]   That is, if you create another entity
[00:39:44.920 --> 00:39:46.880]   that you perceive and interact with
[00:39:46.880 --> 00:39:49.120]   that to you looks entirely realistic,
[00:39:49.120 --> 00:39:51.160]   not just when you say hello, they say hello back,
[00:39:51.160 --> 00:39:52.840]   but you have a rich interaction,
[00:39:52.840 --> 00:39:54.600]   many days, deep conversations.
[00:39:54.600 --> 00:39:57.000]   Like it might be that the only
[00:39:57.000 --> 00:39:59.920]   plausible way of implementing that
[00:39:59.920 --> 00:40:02.120]   would be one that also has a side effect,
[00:40:02.120 --> 00:40:05.240]   instantiated this other person in enough detail
[00:40:05.240 --> 00:40:07.720]   that you would have a second consciousness there.
[00:40:07.720 --> 00:40:11.640]   I think that's to some extent an open question.
[00:40:11.640 --> 00:40:13.160]   - So you don't think it's possible
[00:40:13.160 --> 00:40:14.880]   to fake consciousness and fake intelligence?
[00:40:14.880 --> 00:40:15.920]   - Well, it might be.
[00:40:15.920 --> 00:40:18.240]   I mean, I think you can certainly fake,
[00:40:18.240 --> 00:40:21.200]   if you have a very limited interaction with somebody,
[00:40:21.200 --> 00:40:22.960]   you could certainly fake that.
[00:40:22.960 --> 00:40:25.240]   That is, if all you have to go on
[00:40:25.240 --> 00:40:26.880]   is somebody said hello to you,
[00:40:26.880 --> 00:40:28.600]   that's not enough for you to tell
[00:40:28.600 --> 00:40:30.440]   whether that was a real person there
[00:40:30.440 --> 00:40:32.000]   or a prerecorded message,
[00:40:32.000 --> 00:40:35.400]   or like a very superficial simulation
[00:40:35.400 --> 00:40:36.920]   that has no consciousness.
[00:40:36.920 --> 00:40:39.160]   'Cause that's something easy to fake.
[00:40:39.160 --> 00:40:40.080]   We could already fake it.
[00:40:40.080 --> 00:40:41.880]   Now you can record a voice recording.
[00:40:41.880 --> 00:40:45.600]   But if you have a richer set of interactions
[00:40:45.600 --> 00:40:47.480]   where you're allowed to answer,
[00:40:47.480 --> 00:40:51.320]   ask open-ended questions and probe from different angles,
[00:40:51.320 --> 00:40:52.760]   that couldn't sort of be,
[00:40:52.760 --> 00:40:54.040]   you couldn't give canned answer
[00:40:54.040 --> 00:40:57.480]   to all of the possible ways that you could probe it,
[00:40:57.480 --> 00:40:59.440]   then it starts to become more plausible
[00:40:59.440 --> 00:41:02.240]   that the only way to realize this thing
[00:41:02.240 --> 00:41:04.600]   in such a way that you would get the right answer
[00:41:04.600 --> 00:41:07.000]   from any which angle you probed it
[00:41:07.000 --> 00:41:08.440]   would be a way of instantiating it
[00:41:08.440 --> 00:41:10.640]   where you also instantiated a conscious mind.
[00:41:10.640 --> 00:41:12.360]   - Yeah, I'm with you on the intelligence part,
[00:41:12.360 --> 00:41:13.560]   but there's something about me
[00:41:13.560 --> 00:41:15.800]   that says consciousness is easier to fake.
[00:41:15.800 --> 00:41:19.720]   Like I've recently gotten my hands on a lot of Roombas.
[00:41:19.720 --> 00:41:21.200]   Don't ask me why or how.
[00:41:21.200 --> 00:41:24.360]   And I've made them,
[00:41:24.360 --> 00:41:28.360]   there's just a nice robotic mobile platform for experiments.
[00:41:28.360 --> 00:41:33.040]   And I made them scream or moan in pain and so on
[00:41:33.040 --> 00:41:35.520]   just to see when they're responding to me.
[00:41:35.520 --> 00:41:38.920]   And it's just a sort of psychological experiment on myself.
[00:41:38.920 --> 00:41:41.880]   And I think they appear conscious to me pretty quickly.
[00:41:42.960 --> 00:41:46.000]   To me, at least my brain can be tricked quite easily.
[00:41:46.000 --> 00:41:46.840]   - Right.
[00:41:46.840 --> 00:41:48.760]   - So if I introspect,
[00:41:48.760 --> 00:41:51.480]   it's harder for me to be tricked
[00:41:51.480 --> 00:41:53.640]   that something is intelligent.
[00:41:53.640 --> 00:41:55.040]   So I just have this feeling
[00:41:55.040 --> 00:41:57.800]   that inside this experience machine,
[00:41:57.800 --> 00:41:59.560]   just saying that you're conscious
[00:41:59.560 --> 00:42:03.360]   and having certain qualities of the interaction,
[00:42:03.360 --> 00:42:06.320]   like being able to suffer, like being able to hurt,
[00:42:06.320 --> 00:42:09.560]   like being able to wander about
[00:42:09.560 --> 00:42:11.760]   the essence of your own existence,
[00:42:11.760 --> 00:42:13.720]   not actually, I mean,
[00:42:13.720 --> 00:42:17.480]   creating the illusion that you're wandering about it
[00:42:17.480 --> 00:42:19.640]   is enough to create the feeling of consciousness
[00:42:19.640 --> 00:42:23.040]   and the illusion of consciousness.
[00:42:23.040 --> 00:42:26.080]   And because of that, create a really immersive experience
[00:42:26.080 --> 00:42:28.240]   to where you feel like that is the real world.
[00:42:28.240 --> 00:42:29.280]   - So you think there's a big gap
[00:42:29.280 --> 00:42:33.120]   between appearing conscious and being conscious?
[00:42:33.120 --> 00:42:36.120]   Or is it that you think it's very easy to be conscious?
[00:42:36.120 --> 00:42:37.960]   - I'm not actually sure what it means to be conscious.
[00:42:37.960 --> 00:42:41.640]   All I'm saying is the illusion of consciousness
[00:42:41.640 --> 00:42:47.600]   is enough to create a social interaction
[00:42:47.600 --> 00:42:50.640]   that's as good as if the thing was conscious.
[00:42:50.640 --> 00:42:52.600]   Meaning I'm making it about myself.
[00:42:52.600 --> 00:42:53.440]   - Right, yeah.
[00:42:53.440 --> 00:42:55.000]   I mean, I guess there are a few different things.
[00:42:55.000 --> 00:42:56.440]   One is how good the interaction is,
[00:42:56.440 --> 00:42:59.600]   which might, I mean, if you don't really care about,
[00:42:59.600 --> 00:43:02.280]   like probing hard for whether the thing is conscious,
[00:43:02.280 --> 00:43:05.960]   maybe it would be a satisfactory interaction.
[00:43:07.800 --> 00:43:10.360]   Whether or not you really thought it was conscious.
[00:43:10.360 --> 00:43:15.600]   Now, if you really care about it being conscious
[00:43:15.600 --> 00:43:18.240]   in like inside this experience machine,
[00:43:18.240 --> 00:43:22.240]   how easy would it be to fake it?
[00:43:22.240 --> 00:43:24.640]   And you say it sounds fairly easy.
[00:43:24.640 --> 00:43:26.360]   But then the question is,
[00:43:26.360 --> 00:43:28.440]   would that also mean it's very easy
[00:43:28.440 --> 00:43:30.360]   to instantiate consciousness?
[00:43:30.360 --> 00:43:33.160]   Like it's much more widely spread in the world
[00:43:33.160 --> 00:43:36.080]   and we have thought it doesn't require a big human brain
[00:43:36.080 --> 00:43:37.120]   with a hundred billion neurons.
[00:43:37.120 --> 00:43:39.600]   All you need is some system that exhibits
[00:43:39.600 --> 00:43:41.600]   basic intentionality and can respond
[00:43:41.600 --> 00:43:43.160]   and you already have consciousness.
[00:43:43.160 --> 00:43:46.160]   Like in that case, I guess you still have a close coupling.
[00:43:46.160 --> 00:43:51.160]   That I guess a data case would be where they can come apart,
[00:43:51.160 --> 00:43:54.640]   where you could create the appearance
[00:43:54.640 --> 00:43:56.080]   of there being a conscious mind
[00:43:56.080 --> 00:43:59.240]   with actually not being another conscious mind.
[00:43:59.240 --> 00:44:02.400]   I'm, yeah, I'm somewhat agnostic exactly
[00:44:02.400 --> 00:44:03.360]   where these lines go.
[00:44:03.360 --> 00:44:06.680]   I think one observation that makes it plausible,
[00:44:06.680 --> 00:44:11.080]   that you could have very realistic appearances
[00:44:11.080 --> 00:44:13.440]   relatively simply,
[00:44:13.440 --> 00:44:17.200]   which also is relevant for the simulation argument.
[00:44:17.200 --> 00:44:20.840]   And in terms of thinking about how realistic
[00:44:20.840 --> 00:44:23.680]   would a virtual reality model have to be
[00:44:23.680 --> 00:44:25.560]   in order for the simulated creature
[00:44:25.560 --> 00:44:27.840]   not to notice that anything was awry.
[00:44:27.840 --> 00:44:32.080]   Well, just think of our own humble brains
[00:44:32.080 --> 00:44:35.000]   during the wee hours of the night when we are dreaming.
[00:44:35.000 --> 00:44:38.680]   Many times, well, dreams are very immersive
[00:44:38.680 --> 00:44:41.520]   but often you also don't realize that you're in a dream.
[00:44:41.520 --> 00:44:46.640]   And that's produced by simple, primitive,
[00:44:46.640 --> 00:44:51.080]   three pound lumps of neural matter effortlessly.
[00:44:51.080 --> 00:44:53.560]   So if a simple brain like this can create
[00:44:53.560 --> 00:44:58.480]   the virtual reality that seems pretty real to us,
[00:44:58.480 --> 00:45:00.640]   then how much easier would it be
[00:45:00.640 --> 00:45:02.480]   for a super intelligent civilization
[00:45:02.520 --> 00:45:05.920]   with planetary sized computers optimized over the eons
[00:45:05.920 --> 00:45:10.920]   to create a realistic environment for you to interact with?
[00:45:10.920 --> 00:45:14.360]   - Yeah, by the way, behind that intuition
[00:45:14.360 --> 00:45:17.280]   is that our brain is not that impressive
[00:45:17.280 --> 00:45:19.040]   relative to the possibilities
[00:45:19.040 --> 00:45:21.120]   of what technology could bring.
[00:45:21.120 --> 00:45:24.480]   It's also possible that the brain is the epitome,
[00:45:24.480 --> 00:45:25.680]   is the ceiling.
[00:45:25.680 --> 00:45:28.120]   Like just because-- - The ceiling.
[00:45:28.120 --> 00:45:30.640]   How is that possible?
[00:45:30.640 --> 00:45:34.000]   - Meaning like this is the smartest possible thing
[00:45:34.000 --> 00:45:36.040]   that the universe could create.
[00:45:36.040 --> 00:45:39.000]   - So that seems-- - Unlikely.
[00:45:39.000 --> 00:45:39.960]   - Unlikely to me, yeah.
[00:45:39.960 --> 00:45:43.920]   I mean, for some of these reasons we alluded to earlier
[00:45:43.920 --> 00:45:48.920]   in terms of designs we already have for computers
[00:45:48.920 --> 00:45:53.920]   that would be faster by many orders of magnitude
[00:45:53.920 --> 00:45:55.120]   than the human brain.
[00:45:55.120 --> 00:45:58.000]   - Yeah, but it could be that the constraints,
[00:45:58.000 --> 00:46:00.160]   the cognitive constraints in themselves
[00:46:00.160 --> 00:46:02.280]   is what enables the intelligence.
[00:46:02.280 --> 00:46:05.360]   So the more powerful you make the computer,
[00:46:05.360 --> 00:46:07.920]   the less likely it is to become super intelligent.
[00:46:07.920 --> 00:46:12.080]   This is where I say dumb things to push back on.
[00:46:12.080 --> 00:46:14.160]   - Well, yeah, I'm not sure I follow you.
[00:46:14.160 --> 00:46:15.800]   No, I mean, so there are different dimensions
[00:46:15.800 --> 00:46:16.840]   of intelligence.
[00:46:16.840 --> 00:46:20.000]   A simple one is just speed.
[00:46:20.000 --> 00:46:22.840]   Like if you could solve the same challenge faster
[00:46:22.840 --> 00:46:25.240]   in some sense, you're smarter.
[00:46:25.240 --> 00:46:28.840]   So there, I think we have very strong evidence
[00:46:28.840 --> 00:46:31.680]   for thinking that you could have a computer
[00:46:31.680 --> 00:46:34.680]   in this universe that would be much faster
[00:46:34.680 --> 00:46:37.880]   than the human brain and therefore have speed
[00:46:37.880 --> 00:46:39.840]   super intelligence, like be completely superior,
[00:46:39.840 --> 00:46:41.400]   maybe a million times faster.
[00:46:41.400 --> 00:46:43.960]   Then maybe there are other ways
[00:46:43.960 --> 00:46:45.720]   in which you could be smarter as well,
[00:46:45.720 --> 00:46:48.520]   maybe more qualitative ways, right?
[00:46:48.520 --> 00:46:51.720]   And there, the concepts are a little bit less clear cut.
[00:46:51.720 --> 00:46:56.160]   So it's harder to make a very crisp, neat,
[00:46:56.160 --> 00:47:00.200]   firmly logical argument for why that could be
[00:47:00.200 --> 00:47:02.640]   qualitative super intelligence as opposed to just things
[00:47:02.640 --> 00:47:03.480]   that were faster.
[00:47:03.480 --> 00:47:06.000]   Although I still think it's very plausible
[00:47:06.000 --> 00:47:07.880]   and for various reasons that are less
[00:47:07.880 --> 00:47:09.080]   than watertight arguments.
[00:47:09.080 --> 00:47:10.360]   But I mean, you can sort of, for example,
[00:47:10.360 --> 00:47:12.120]   if you look at animals and-
[00:47:12.120 --> 00:47:12.960]   - Brain cells.
[00:47:12.960 --> 00:47:14.520]   - Yeah, and even within humans,
[00:47:14.520 --> 00:47:18.520]   like there seems to be like Einstein versus random person.
[00:47:18.520 --> 00:47:21.920]   Like it's not just that Einstein was a little bit faster,
[00:47:21.920 --> 00:47:23.960]   but like how long would it take a normal person
[00:47:23.960 --> 00:47:26.480]   to invent general relativity?
[00:47:26.480 --> 00:47:29.200]   It's like, it's not 20% longer than it took Einstein
[00:47:29.200 --> 00:47:30.040]   or something like that.
[00:47:30.040 --> 00:47:31.840]   It's like, I don't know whether they would do it at all
[00:47:31.840 --> 00:47:35.720]   or it would take millions of years or some totally bizarre.
[00:47:35.720 --> 00:47:36.840]   So-
[00:47:36.840 --> 00:47:39.080]   - But your intuition is that the compute size
[00:47:39.080 --> 00:47:43.360]   will get you, increasing the size of the computer
[00:47:43.360 --> 00:47:45.520]   and the speed of the computer
[00:47:45.520 --> 00:47:49.600]   might create some much more powerful levels of intelligence
[00:47:49.600 --> 00:47:51.160]   that would enable some of the things
[00:47:51.160 --> 00:47:53.320]   we've been talking about with like the simulation,
[00:47:53.320 --> 00:47:56.760]   being able to simulate an ultra realistic environment,
[00:47:56.760 --> 00:48:01.280]   ultra realistic perception of reality.
[00:48:01.280 --> 00:48:04.160]   - Yeah, strictly speaking, it would not be necessary
[00:48:04.160 --> 00:48:06.640]   to have super intelligence in order to have say,
[00:48:06.640 --> 00:48:09.160]   the technology to make these simulations,
[00:48:09.160 --> 00:48:11.720]   ancestor simulations or other kinds of simulations.
[00:48:11.720 --> 00:48:19.160]   As a matter of fact, I think if we are in a simulation,
[00:48:19.160 --> 00:48:22.520]   it would most likely be one built by a civilization
[00:48:22.520 --> 00:48:23.960]   that had super intelligence.
[00:48:23.960 --> 00:48:27.640]   It certainly would help a lot.
[00:48:27.640 --> 00:48:28.960]   I mean, it could build more efficient,
[00:48:28.960 --> 00:48:31.360]   larger scale structures if you had super intelligence.
[00:48:31.360 --> 00:48:33.120]   I also think that if you had the technology
[00:48:33.120 --> 00:48:34.160]   to build these simulations,
[00:48:34.160 --> 00:48:35.920]   that's like a very advanced technology.
[00:48:35.920 --> 00:48:38.200]   It seems kind of easier to get technology
[00:48:38.200 --> 00:48:39.360]   to super intelligence.
[00:48:39.360 --> 00:48:42.840]   So I'd expect by the time they could make these
[00:48:42.840 --> 00:48:45.360]   fully realistic simulations of human history
[00:48:45.360 --> 00:48:47.680]   with human brains in there, like before that,
[00:48:47.680 --> 00:48:49.400]   they got to that stage, they would have figured out
[00:48:49.400 --> 00:48:52.520]   how to create machine super intelligence
[00:48:52.520 --> 00:48:56.120]   or maybe biological enhancements of their own brains
[00:48:56.120 --> 00:48:59.040]   if they were biological creatures to start with.
[00:48:59.040 --> 00:49:02.880]   - So we talked about the three parts
[00:49:02.880 --> 00:49:04.360]   of the simulation argument.
[00:49:04.360 --> 00:49:06.480]   One, we destroy ourselves before we ever create
[00:49:06.480 --> 00:49:07.520]   the simulation.
[00:49:07.520 --> 00:49:11.440]   Two, we somehow, everybody somehow loses interest
[00:49:11.440 --> 00:49:12.440]   in creating simulation.
[00:49:12.440 --> 00:49:15.240]   Three, we're living in a simulation.
[00:49:16.080 --> 00:49:20.120]   So you've kind of, I don't know if your thinking
[00:49:20.120 --> 00:49:22.040]   has evolved on this point, but you kind of said
[00:49:22.040 --> 00:49:25.760]   that we know so little that these three cases
[00:49:25.760 --> 00:49:28.160]   might as well be equally probable.
[00:49:28.160 --> 00:49:30.040]   So probabilistically speaking,
[00:49:30.040 --> 00:49:31.720]   where do you stand on this?
[00:49:31.720 --> 00:49:34.880]   - Yeah, I mean, I don't think equal necessarily
[00:49:34.880 --> 00:49:39.880]   would be the most supported probability assignment.
[00:49:39.880 --> 00:49:44.160]   - So how would you, without assigning actual numbers,
[00:49:44.160 --> 00:49:48.000]   what's more or less likely in your view?
[00:49:48.000 --> 00:49:50.120]   - Well, I mean, I've historically tended to punt
[00:49:50.120 --> 00:49:55.120]   on the question of like as between these three.
[00:49:55.120 --> 00:49:59.600]   - So maybe you ask another way is which kind of things
[00:49:59.600 --> 00:50:02.960]   would make each of these more or less likely?
[00:50:02.960 --> 00:50:04.960]   What kind of, yeah, intuition.
[00:50:04.960 --> 00:50:07.480]   - Certainly in general terms, if you take anything
[00:50:07.480 --> 00:50:10.960]   that say increases or reduces the probability
[00:50:10.960 --> 00:50:15.960]   of one of these, we tend to slosh probability around
[00:50:15.960 --> 00:50:17.280]   on the other.
[00:50:17.280 --> 00:50:19.160]   So if one becomes less probable, like the other
[00:50:19.160 --> 00:50:21.520]   would have to, 'cause it's gotta add up to one.
[00:50:21.520 --> 00:50:22.360]   - Yes.
[00:50:22.360 --> 00:50:25.080]   - So if we consider the first hypothesis,
[00:50:25.080 --> 00:50:29.040]   the first alternative that there's this filter
[00:50:29.040 --> 00:50:32.840]   that makes it so that virtually no civilization
[00:50:32.840 --> 00:50:37.840]   reaches technical maturity, in particular,
[00:50:37.840 --> 00:50:39.960]   our own civilization.
[00:50:39.960 --> 00:50:41.880]   And if that's true, then it's like very unlikely
[00:50:41.880 --> 00:50:44.280]   that we would reach technical maturity,
[00:50:44.280 --> 00:50:46.760]   just because if almost no civilization at our stage does it,
[00:50:46.760 --> 00:50:49.040]   then it's unlikely that we do it.
[00:50:49.040 --> 00:50:49.880]   So hence--
[00:50:49.880 --> 00:50:51.240]   - Sorry, can you linger on that for a second?
[00:50:51.240 --> 00:50:53.920]   - Well, if it's the case that almost all civilizations
[00:50:53.920 --> 00:50:58.920]   at our current stage of technological development
[00:50:58.920 --> 00:51:03.640]   fail to reach maturity, that would give us
[00:51:03.640 --> 00:51:06.000]   very strong reason for thinking we will fail
[00:51:06.000 --> 00:51:07.720]   to reach technical maturity.
[00:51:07.760 --> 00:51:10.280]   - And also sort of the flip side of that is the fact
[00:51:10.280 --> 00:51:13.200]   that we've reached it means that many other civilizations
[00:51:13.200 --> 00:51:14.040]   have reached this point.
[00:51:14.040 --> 00:51:15.720]   - Yeah, so that means if we get closer and closer
[00:51:15.720 --> 00:51:18.280]   to actually reaching technical maturity,
[00:51:18.280 --> 00:51:22.600]   there's less and less distance left where we could
[00:51:22.600 --> 00:51:25.040]   go extinct before we are there.
[00:51:25.040 --> 00:51:28.720]   And therefore the probability that we will reach
[00:51:28.720 --> 00:51:30.880]   increases as we get closer.
[00:51:30.880 --> 00:51:32.840]   And that would make it less likely to be true
[00:51:32.840 --> 00:51:35.880]   that almost all civilizations at our current stage
[00:51:35.880 --> 00:51:37.080]   failed to get there.
[00:51:37.080 --> 00:51:41.120]   Like we would have this, the one case we'd started ourselves
[00:51:41.120 --> 00:51:42.680]   would be very close to getting there.
[00:51:42.680 --> 00:51:44.480]   That would be strong evidence that it's not so hard
[00:51:44.480 --> 00:51:46.320]   to get to technical maturity.
[00:51:46.320 --> 00:51:50.520]   So to the extent that we feel we are moving nearer
[00:51:50.520 --> 00:51:53.560]   to technical maturity, that would tend to reduce
[00:51:53.560 --> 00:51:56.280]   the probability of the first alternative
[00:51:56.280 --> 00:51:59.080]   and increase the probability of the other two.
[00:51:59.080 --> 00:52:01.800]   It doesn't need to be a monotonic change.
[00:52:01.800 --> 00:52:05.040]   Like if every once in a while some new threat
[00:52:05.040 --> 00:52:07.640]   comes into view, some bad new thing you could do
[00:52:07.640 --> 00:52:11.160]   with some novel technology, for example,
[00:52:11.160 --> 00:52:14.880]   that could change our probabilities in the other direction.
[00:52:14.880 --> 00:52:17.800]   - But that technology, again, you have to think about
[00:52:17.800 --> 00:52:21.640]   as that technology has to be able to equally
[00:52:21.640 --> 00:52:26.200]   in an even way affect every civilization out there.
[00:52:26.200 --> 00:52:27.920]   - Yeah, pretty much.
[00:52:27.920 --> 00:52:30.720]   I mean, strictly speaking, it's not true.
[00:52:30.720 --> 00:52:33.920]   I mean, that could be two different existential risks
[00:52:33.920 --> 00:52:36.160]   in every civilization, you know,
[00:52:36.160 --> 00:52:38.440]   - As in one of them. - Not from one or the other.
[00:52:38.440 --> 00:52:41.840]   But none of them kills more than 50%.
[00:52:41.840 --> 00:52:42.920]   - Yeah, gotcha.
[00:52:42.920 --> 00:52:47.280]   - Incidentally, so some of my other work,
[00:52:47.280 --> 00:52:48.920]   I mean, on machine superintelligence,
[00:52:48.920 --> 00:52:51.240]   like pointed to some existential risks
[00:52:51.240 --> 00:52:53.800]   related to sort of superintelligent AI
[00:52:53.800 --> 00:52:58.200]   and how we must make sure to handle that wisely
[00:52:58.200 --> 00:52:59.440]   and carefully.
[00:52:59.480 --> 00:53:04.000]   It's not the right kind of existential catastrophe
[00:53:04.000 --> 00:53:09.000]   to make the first alternative true though.
[00:53:09.000 --> 00:53:12.040]   Like it might be bad for us
[00:53:12.040 --> 00:53:13.760]   if the future lost a lot of value
[00:53:13.760 --> 00:53:17.560]   as a result of it being shaped by some process
[00:53:17.560 --> 00:53:20.880]   that optimized for some completely non-human value.
[00:53:20.880 --> 00:53:25.360]   But even if we got killed by machine superintelligence
[00:53:25.360 --> 00:53:27.480]   is that machine superintelligence
[00:53:27.480 --> 00:53:30.000]   might still attain technological maturity.
[00:53:30.000 --> 00:53:30.840]   So-- - Oh, I see.
[00:53:30.840 --> 00:53:33.400]   So you're not human exclusive.
[00:53:33.400 --> 00:53:36.840]   This could be any intelligent species that achieves,
[00:53:36.840 --> 00:53:38.800]   like it's all about the technological maturity.
[00:53:38.800 --> 00:53:42.800]   It's not that the humans have to attain it.
[00:53:42.800 --> 00:53:44.800]   - Right. - So like superintelligence
[00:53:44.800 --> 00:53:46.440]   'cause it replaced us and that's just as well
[00:53:46.440 --> 00:53:47.640]   for the simulation argument. - And that could still,
[00:53:47.640 --> 00:53:48.480]   yeah, yeah, I mean,
[00:53:48.480 --> 00:53:51.640]   it could interact with the second alternative.
[00:53:51.640 --> 00:53:53.240]   Like if the thing that replaced us
[00:53:53.240 --> 00:53:55.760]   was either more likely or less likely,
[00:53:55.760 --> 00:53:57.880]   then we would be to have an interest
[00:53:57.880 --> 00:54:00.000]   in creating ancestor simulations,
[00:54:00.000 --> 00:54:02.680]   you know, that could affect probabilities.
[00:54:02.680 --> 00:54:04.240]   But yeah, to a first order,
[00:54:04.240 --> 00:54:07.840]   like if we all just die, then yeah,
[00:54:07.840 --> 00:54:11.720]   we won't produce any simulations 'cause we are dead.
[00:54:11.720 --> 00:54:14.720]   But if we all die and get replaced
[00:54:14.720 --> 00:54:15.960]   by some other intelligent thing
[00:54:15.960 --> 00:54:18.520]   that then gets to technological maturity,
[00:54:18.520 --> 00:54:19.800]   the question remains, of course,
[00:54:19.800 --> 00:54:22.600]   if might not that thing then use some of its resources
[00:54:22.600 --> 00:54:25.120]   to do this stuff.
[00:54:25.120 --> 00:54:27.560]   - So can you reason about this stuff?
[00:54:27.560 --> 00:54:30.600]   This is given how little we know about the universe.
[00:54:30.600 --> 00:54:35.600]   Is it reasonable to reason about these probabilities?
[00:54:35.600 --> 00:54:41.000]   So like how little, well, maybe you can disagree,
[00:54:41.000 --> 00:54:45.160]   but to me, it's not trivial to figure out
[00:54:45.160 --> 00:54:47.400]   how difficult it is to build a simulation.
[00:54:47.400 --> 00:54:49.520]   We kind of talked about it a little bit.
[00:54:49.520 --> 00:54:54.240]   We've also don't know, like as we try to start building it,
[00:54:54.240 --> 00:54:56.920]   like start creating virtual worlds and so on,
[00:54:56.920 --> 00:54:59.560]   how that changes the fabric of society.
[00:54:59.560 --> 00:55:01.560]   Like there's all these things along the way
[00:55:01.560 --> 00:55:05.040]   that can fundamentally change just so many aspects
[00:55:05.040 --> 00:55:07.320]   of our society about our existence
[00:55:07.320 --> 00:55:09.200]   that we don't know anything about.
[00:55:09.200 --> 00:55:11.720]   Like the kind of things we might discover
[00:55:11.720 --> 00:55:15.800]   when we understand to a greater degree
[00:55:15.800 --> 00:55:19.280]   the fundamental, the physics,
[00:55:19.280 --> 00:55:21.760]   like the theory, if we have a breakthrough,
[00:55:21.760 --> 00:55:23.840]   have a theory and everything, how that changes the stuff,
[00:55:23.840 --> 00:55:27.480]   how that changes deep space exploration and so on.
[00:55:27.480 --> 00:55:31.360]   So like, is it still possible to reason about probabilities
[00:55:31.360 --> 00:55:32.600]   given how little we know?
[00:55:32.600 --> 00:55:37.800]   - Yes, I think though there will be a large residual
[00:55:37.800 --> 00:55:41.800]   of uncertainty that we'll just have to acknowledge.
[00:55:41.800 --> 00:55:46.240]   And I think that's true for most of these big picture
[00:55:46.240 --> 00:55:48.600]   questions that we might wonder about.
[00:55:49.680 --> 00:55:54.680]   It's just, we are small, short-lived, small-brained,
[00:55:54.680 --> 00:55:59.000]   cognitively very limited humans with little evidence.
[00:55:59.000 --> 00:56:03.000]   And it's amazing we can figure out as much as we can
[00:56:03.000 --> 00:56:04.600]   really about the cosmos.
[00:56:04.600 --> 00:56:08.960]   - But, okay, so there's this cognitive trick
[00:56:08.960 --> 00:56:11.840]   that seems to happen when I look at the simulation argument,
[00:56:11.840 --> 00:56:16.440]   which for me, it seems like case one and two feel unlikely.
[00:56:16.440 --> 00:56:19.440]   I wanna say feel unlikely as opposed to
[00:56:19.440 --> 00:56:23.000]   sort of, it's not like I have too much scientific evidence
[00:56:23.000 --> 00:56:26.920]   to say that either one or two are not true.
[00:56:26.920 --> 00:56:30.440]   It just seems unlikely that every single civilization
[00:56:30.440 --> 00:56:32.240]   destroys itself.
[00:56:32.240 --> 00:56:34.920]   And it seems, like feels unlikely
[00:56:34.920 --> 00:56:37.000]   that the civilizations lose interest.
[00:56:37.000 --> 00:56:42.000]   So naturally, without necessarily explicitly doing it,
[00:56:42.000 --> 00:56:45.600]   but the simulation argument basically says,
[00:56:45.600 --> 00:56:48.780]   it's very likely we're living in a simulation.
[00:56:48.780 --> 00:56:51.800]   Like to me, my mind naturally goes there.
[00:56:51.800 --> 00:56:54.720]   I think the mind goes there for a lot of people.
[00:56:54.720 --> 00:56:57.720]   Is that the incorrect place for it to go?
[00:56:57.720 --> 00:56:59.160]   - Well, not necessarily.
[00:56:59.160 --> 00:57:00.860]   I think the second alternative,
[00:57:00.860 --> 00:57:07.560]   which has to do with the motivations and interest
[00:57:07.560 --> 00:57:11.000]   of technological immature civilizations,
[00:57:11.000 --> 00:57:15.600]   I think there is much we don't understand about that.
[00:57:15.600 --> 00:57:18.320]   - Yeah, can you talk about that a little bit?
[00:57:18.320 --> 00:57:19.160]   What do you think?
[00:57:19.160 --> 00:57:20.280]   I mean, this is a question that pops up
[00:57:20.280 --> 00:57:22.480]   when you build an AGI system
[00:57:22.480 --> 00:57:24.200]   or build a general intelligence.
[00:57:24.200 --> 00:57:27.840]   How does that change our motivations?
[00:57:27.840 --> 00:57:31.520]   Do you think it'll fundamentally transform our motivations?
[00:57:31.520 --> 00:57:33.160]   - Well, it doesn't seem that implausible
[00:57:33.160 --> 00:57:38.160]   that once you take this leap to technological maturity,
[00:57:38.160 --> 00:57:41.840]   I mean, I think like it involves creating
[00:57:41.840 --> 00:57:45.040]   machine superintelligence possibly,
[00:57:45.040 --> 00:57:48.080]   that would be sort of on the path for basically
[00:57:48.080 --> 00:57:50.880]   all civilizations maybe before they are able
[00:57:50.880 --> 00:57:53.640]   to create large numbers of ancestry simulations.
[00:57:53.640 --> 00:57:56.520]   That possibly could be one of these things
[00:57:56.520 --> 00:58:00.680]   that quite radically changes the orientation
[00:58:00.680 --> 00:58:04.720]   of what a civilization is in fact optimizing for.
[00:58:04.720 --> 00:58:08.560]   There are other things as well.
[00:58:08.560 --> 00:58:13.560]   So at the moment we have not perfect control
[00:58:16.040 --> 00:58:20.080]   over our own being, our own mental states,
[00:58:20.080 --> 00:58:23.660]   our own experiences are not under our direct control.
[00:58:23.660 --> 00:58:30.140]   So for example, if you want to experience a pleasure
[00:58:30.140 --> 00:58:35.800]   and happiness, you might have to do a whole host of things
[00:58:35.800 --> 00:58:39.260]   in the external world to try to get into the stage,
[00:58:39.260 --> 00:58:42.320]   into the mental state where you experience pleasure.
[00:58:42.320 --> 00:58:43.880]   You're like, like some people get some pleasure
[00:58:43.880 --> 00:58:44.840]   from eating great food.
[00:58:44.840 --> 00:58:47.040]   Well, they can just turn that on.
[00:58:47.040 --> 00:58:50.000]   They have to kind of actually go to a nice restaurant
[00:58:50.000 --> 00:58:51.320]   and then they have to make money.
[00:58:51.320 --> 00:58:53.100]   So there's like all this kind of activity
[00:58:53.100 --> 00:58:58.100]   that maybe arises from the fact that we are trying
[00:58:58.100 --> 00:59:02.000]   to ultimately produce mental states.
[00:59:02.000 --> 00:59:04.280]   But the only way to do that is by a whole host
[00:59:04.280 --> 00:59:06.920]   of complicated activities in the external world.
[00:59:06.920 --> 00:59:09.320]   Now, at some level of technological development,
[00:59:09.320 --> 00:59:11.640]   I think we'll become auto potent in the sense
[00:59:11.640 --> 00:59:15.440]   of gaining direct ability to choose
[00:59:15.440 --> 00:59:19.000]   our own internal configuration and enough knowledge
[00:59:19.000 --> 00:59:21.200]   and insight to be able to actually do that
[00:59:21.200 --> 00:59:22.680]   in a meaningful way.
[00:59:22.680 --> 00:59:24.880]   So then it could turn out that there are a lot
[00:59:24.880 --> 00:59:29.360]   of instrumental goals that would drop out of the picture
[00:59:29.360 --> 00:59:31.480]   and be replaced by other instrumental goals
[00:59:31.480 --> 00:59:35.720]   because we could now serve some of these final goals
[00:59:35.720 --> 00:59:36.960]   in more direct ways.
[00:59:36.960 --> 00:59:39.840]   And who knows how all of that shakes out
[00:59:41.240 --> 00:59:45.640]   after civilizations reflect on that and converge
[00:59:45.640 --> 00:59:48.040]   and different attractors and so on and so forth.
[00:59:48.040 --> 00:59:54.520]   And that could be new instrumental considerations
[00:59:54.520 --> 00:59:57.800]   that come into view as well that we are just oblivious to
[00:59:57.800 --> 01:00:02.800]   that would maybe have a strong shaping effect on actions,
[01:00:02.800 --> 01:00:05.440]   like very strong reasons to do something
[01:00:05.440 --> 01:00:06.440]   or not to do something.
[01:00:06.440 --> 01:00:08.280]   And we just don't realize they are there
[01:00:08.280 --> 01:00:11.000]   because we are so dumb, fumbling through the universe.
[01:00:11.000 --> 01:00:15.920]   But if almost inevitably on route to attaining the ability
[01:00:15.920 --> 01:00:17.480]   to create many answers to simulations,
[01:00:17.480 --> 01:00:20.920]   you do have this cognitive enhancement or advice
[01:00:20.920 --> 01:00:22.960]   from super intelligences or you yourself,
[01:00:22.960 --> 01:00:24.960]   then maybe there's like this additional set
[01:00:24.960 --> 01:00:26.320]   of considerations coming into view.
[01:00:26.320 --> 01:00:28.720]   And you have to realize it's obvious that the thing
[01:00:28.720 --> 01:00:30.680]   that makes sense is to do X.
[01:00:30.680 --> 01:00:32.960]   Whereas right now it seems, hey, you could X, Y or Z
[01:00:32.960 --> 01:00:34.640]   and different people will do different things.
[01:00:34.640 --> 01:00:38.920]   And we are kind of random in that sense.
[01:00:38.920 --> 01:00:42.880]   - Yeah, because at this time with our limited technology,
[01:00:42.880 --> 01:00:45.200]   the impact of our decisions is minor.
[01:00:45.200 --> 01:00:49.360]   I mean, that's starting to change in some ways, but-
[01:00:49.360 --> 01:00:52.360]   - Well, I'm not sure it follows that the impact
[01:00:52.360 --> 01:00:54.440]   of our decisions is minor.
[01:00:54.440 --> 01:00:55.640]   - Well, it's starting to change.
[01:00:55.640 --> 01:00:58.600]   I mean, I suppose a hundred years ago it was minor.
[01:00:58.600 --> 01:01:00.560]   It's starting to-
[01:01:00.560 --> 01:01:02.560]   - So it depends on how you view it.
[01:01:02.560 --> 01:01:05.080]   So what people did a hundred years ago
[01:01:06.000 --> 01:01:09.080]   still have effects on the world today.
[01:01:09.080 --> 01:01:10.440]   - Oh, I see.
[01:01:10.440 --> 01:01:14.360]   As a civilization in the togetherness.
[01:01:14.360 --> 01:01:18.080]   - Yeah, so it might be that the greatest impact
[01:01:18.080 --> 01:01:21.920]   of individuals is not at technological maturity
[01:01:21.920 --> 01:01:22.840]   or very far down.
[01:01:22.840 --> 01:01:25.920]   It might be earlier on when there are different tracks,
[01:01:25.920 --> 01:01:28.000]   civilization could go down.
[01:01:28.000 --> 01:01:30.640]   I mean, maybe the population is smaller.
[01:01:30.640 --> 01:01:32.360]   Things still haven't settled out.
[01:01:32.360 --> 01:01:35.120]   If you count the indirect effects
[01:01:35.120 --> 01:01:40.120]   that those could be bigger than the direct effects
[01:01:40.120 --> 01:01:43.240]   that people have later on.
[01:01:43.240 --> 01:01:46.200]   - So part three of the argument says that,
[01:01:46.200 --> 01:01:50.120]   so that leads us to a place where eventually
[01:01:50.120 --> 01:01:51.820]   somebody creates a simulation.
[01:01:51.820 --> 01:01:55.520]   That I think you had a conversation with Joe Rogan.
[01:01:55.520 --> 01:01:57.320]   I think there's some aspect here
[01:01:57.320 --> 01:01:58.960]   where you got stuck a little bit.
[01:02:01.040 --> 01:02:06.040]   How does that lead to where likely living in a simulation?
[01:02:06.040 --> 01:02:10.360]   So this kind of probability argument,
[01:02:10.360 --> 01:02:12.600]   if somebody eventually creates a simulation,
[01:02:12.600 --> 01:02:15.600]   why does that mean that we're now in a simulation?
[01:02:15.600 --> 01:02:18.920]   - What you get to if you accept alternative three first
[01:02:18.920 --> 01:02:23.320]   is there would be more simulated people
[01:02:23.320 --> 01:02:26.320]   with our kinds of experiences than non simulated ones.
[01:02:26.320 --> 01:02:31.320]   Like if you look at the world as a whole
[01:02:31.320 --> 01:02:34.960]   by the end of time as it were, you just count it up.
[01:02:34.960 --> 01:02:39.440]   That would be more simulated ones than non simulated ones.
[01:02:39.440 --> 01:02:43.120]   Then there is an extra step to get from that.
[01:02:43.120 --> 01:02:45.160]   If you assume that suppose for the sake of the argument
[01:02:45.160 --> 01:02:49.320]   that that's true, how do you get from that
[01:02:49.320 --> 01:02:54.320]   to the statement we are probably in a simulation?
[01:02:55.320 --> 01:02:57.600]   So here you're introducing an indexical statement
[01:02:57.600 --> 01:03:02.600]   like it's that this person right now is in a simulation.
[01:03:02.600 --> 01:03:06.200]   There are all these other people
[01:03:06.200 --> 01:03:08.040]   that are in simulations
[01:03:08.040 --> 01:03:09.840]   and some that are not in a simulation.
[01:03:09.840 --> 01:03:14.240]   But what probability should you have that you yourself
[01:03:14.240 --> 01:03:18.240]   is one of the simulated ones in that setup?
[01:03:18.240 --> 01:03:21.560]   So yeah, so I call it the bland principle of indifference,
[01:03:21.560 --> 01:03:25.920]   which is that in cases like this,
[01:03:25.920 --> 01:03:29.040]   when you have two, I guess, sets of observers,
[01:03:29.040 --> 01:03:33.920]   one of which is much larger than the other.
[01:03:33.920 --> 01:03:37.800]   And you can't from any internal evidence you have
[01:03:37.800 --> 01:03:40.720]   tell which set you belong to.
[01:03:40.720 --> 01:03:45.720]   You should assign a probability that's proportional
[01:03:45.720 --> 01:03:48.160]   to the size of these sets.
[01:03:48.160 --> 01:03:51.440]   So that if there are 10,000 people in a simulation,
[01:03:51.440 --> 01:03:53.680]   10 times more simulated people
[01:03:53.680 --> 01:03:55.200]   with your kinds of experiences,
[01:03:55.200 --> 01:03:58.480]   you would be 10 times more likely to be one of those.
[01:03:58.480 --> 01:04:00.680]   - Is that as intuitive as it sounds?
[01:04:00.680 --> 01:04:03.360]   I mean, that seems kind of,
[01:04:03.360 --> 01:04:04.800]   if you don't have enough information,
[01:04:04.800 --> 01:04:07.840]   you should rationally just assign the same probability
[01:04:07.840 --> 01:04:10.840]   as the size of the set.
[01:04:10.840 --> 01:04:15.720]   - It seems pretty plausible to me.
[01:04:15.720 --> 01:04:17.000]   - Where are the holes in this?
[01:04:17.000 --> 01:04:19.720]   Is it at the very beginning,
[01:04:19.720 --> 01:04:22.360]   the assumption that everything stretches,
[01:04:22.360 --> 01:04:25.360]   sort of you have infinite time, essentially?
[01:04:25.360 --> 01:04:26.880]   - You don't need infinite time.
[01:04:26.880 --> 01:04:29.800]   - You just need, how long does the time--
[01:04:29.800 --> 01:04:31.400]   - Well, however long it takes, I guess,
[01:04:31.400 --> 01:04:35.840]   for a universe to produce an intelligent civilization
[01:04:35.840 --> 01:04:37.160]   that then attains the technology
[01:04:37.160 --> 01:04:38.960]   to run some ancestor simulations.
[01:04:38.960 --> 01:04:39.800]   - Gotcha.
[01:04:39.800 --> 01:04:43.040]   At some point, when the first simulation is created,
[01:04:43.040 --> 01:04:45.560]   that stretch of time, just a little longer
[01:04:45.560 --> 01:04:48.160]   than they'll all start creating simulations.
[01:04:48.160 --> 01:04:49.000]   Kind of like order of magnitude.
[01:04:49.000 --> 01:04:51.120]   - Well, I mean, there might be different,
[01:04:51.120 --> 01:04:53.880]   it might, if you think of there being
[01:04:53.880 --> 01:04:54.920]   a lot of different planets
[01:04:54.920 --> 01:04:57.720]   and some subset of them have life,
[01:04:57.720 --> 01:05:00.960]   and then some subset of those get to intelligent life,
[01:05:00.960 --> 01:05:03.080]   and some of those maybe eventually
[01:05:03.080 --> 01:05:05.000]   start creating simulations,
[01:05:05.000 --> 01:05:07.360]   they might get started at quite different times.
[01:05:07.360 --> 01:05:10.360]   Like maybe on some planet, it takes a billion years longer
[01:05:10.360 --> 01:05:15.360]   before you get monkeys, or before you get even bacteria,
[01:05:15.360 --> 01:05:16.760]   than on another planet.
[01:05:17.560 --> 01:05:21.680]   So this might happen kind of
[01:05:21.680 --> 01:05:24.880]   at different cosmological epochs.
[01:05:24.880 --> 01:05:27.360]   - Is there a connection here to the doomsday argument
[01:05:27.360 --> 01:05:29.320]   and that sampling there?
[01:05:29.320 --> 01:05:32.080]   - Yeah, there is a connection in that
[01:05:32.080 --> 01:05:36.960]   they both involve an application of anthropic reasoning,
[01:05:36.960 --> 01:05:40.960]   that is reasoning about these kind of indexical propositions.
[01:05:40.960 --> 01:05:42.640]   But the assumption you need
[01:05:42.640 --> 01:05:45.880]   in the case of the simulation argument
[01:05:47.000 --> 01:05:50.080]   is much weaker than the assumption you need
[01:05:50.080 --> 01:05:53.560]   to make the doomsday argument go through.
[01:05:53.560 --> 01:05:55.040]   - What is the doomsday argument?
[01:05:55.040 --> 01:05:57.880]   And maybe you can speak to the anthropic reasoning
[01:05:57.880 --> 01:05:58.920]   in more general.
[01:05:58.920 --> 01:06:01.240]   - Yeah, that's a big and interesting topic
[01:06:01.240 --> 01:06:02.960]   in its own right, anthropics.
[01:06:02.960 --> 01:06:07.920]   But the doomsday argument is this really first discovered
[01:06:07.920 --> 01:06:11.160]   by Brandon Carter, who was a theoretical physicist
[01:06:11.160 --> 01:06:15.600]   and then developed by philosopher John Leslie.
[01:06:16.720 --> 01:06:18.240]   I think it might've been discovered initially
[01:06:18.240 --> 01:06:21.080]   in the '70s or '80s, and Leslie wrote this book,
[01:06:21.080 --> 01:06:23.120]   I think, in '96.
[01:06:23.120 --> 01:06:25.640]   And there are some other versions as well
[01:06:25.640 --> 01:06:27.280]   by Richard Gott, who's a physicist,
[01:06:27.280 --> 01:06:29.520]   but let's focus on the Carter-Leslie version,
[01:06:29.520 --> 01:06:33.440]   where it's an argument
[01:06:33.440 --> 01:06:38.400]   that we have systematically underestimated
[01:06:38.400 --> 01:06:42.320]   the probability that humanity will go extinct soon.
[01:06:44.040 --> 01:06:47.720]   Now, I should say most people probably think
[01:06:47.720 --> 01:06:49.120]   at the end of the day, there is something wrong
[01:06:49.120 --> 01:06:52.080]   with this doomsday argument that it doesn't really hold.
[01:06:52.080 --> 01:06:53.480]   It's like there's something wrong with it,
[01:06:53.480 --> 01:06:57.200]   but it's proved hard to say exactly what is wrong with it.
[01:06:57.200 --> 01:06:59.440]   And different people have different accounts.
[01:06:59.440 --> 01:07:03.560]   My own view is it seems inconclusive.
[01:07:03.560 --> 01:07:06.600]   And I can say what the argument is.
[01:07:06.600 --> 01:07:07.640]   - Yeah, that would be good.
[01:07:07.640 --> 01:07:09.960]   - Yeah, so maybe it's easiest to explain
[01:07:09.960 --> 01:07:14.960]   via an analogy to sampling from urns.
[01:07:14.960 --> 01:07:19.000]   So imagine you have a big,
[01:07:19.000 --> 01:07:22.760]   imagine you have two urns in front of you,
[01:07:22.760 --> 01:07:25.160]   and they have balls in them that have numbers.
[01:07:25.160 --> 01:07:27.840]   The two urns look the same,
[01:07:27.840 --> 01:07:29.800]   but inside one, there are 10 balls,
[01:07:29.800 --> 01:07:32.360]   ball number one, two, three, up to ball number 10.
[01:07:32.360 --> 01:07:37.120]   And then in the other urn, you have a million balls
[01:07:37.120 --> 01:07:39.720]   numbered one to a million.
[01:07:40.360 --> 01:07:44.240]   And somebody puts one of these urns in front of you
[01:07:44.240 --> 01:07:49.000]   and ask you to guess what's the chance it's the 10 ball urn.
[01:07:49.000 --> 01:07:52.000]   And you say, well, 50-50, I can't tell which urn it is.
[01:07:52.000 --> 01:07:55.320]   But then you're allowed to reach in
[01:07:55.320 --> 01:07:57.800]   and pick a ball at random from the urn.
[01:07:57.800 --> 01:08:00.560]   And let's suppose you find that it's ball number seven.
[01:08:00.560 --> 01:08:05.400]   So that's strong evidence for the 10 ball hypothesis.
[01:08:05.400 --> 01:08:08.360]   Like it's a lot more likely that you would get
[01:08:08.360 --> 01:08:10.640]   such a low numbered ball
[01:08:10.640 --> 01:08:11.880]   if there are only 10 balls in the urn,
[01:08:11.880 --> 01:08:13.680]   like it's in fact 10% then, right?
[01:08:13.680 --> 01:08:16.560]   Then if there are a million balls,
[01:08:16.560 --> 01:08:19.520]   it would be very unlikely you would get number seven.
[01:08:19.520 --> 01:08:22.520]   So you perform a Bayesian update.
[01:08:22.520 --> 01:08:27.120]   And if your prior was 50-50 that it was the 10 ball urn,
[01:08:27.120 --> 01:08:28.280]   you become virtually certain
[01:08:28.280 --> 01:08:30.800]   after finding the random sample was seven
[01:08:30.800 --> 01:08:33.200]   that it only has 10 balls in it.
[01:08:33.200 --> 01:08:35.200]   So in the case of the urns, this is uncontroversial,
[01:08:35.200 --> 01:08:37.360]   just elementary probability theory.
[01:08:37.360 --> 01:08:40.360]   The Doomsday Argument says that you should reason
[01:08:40.360 --> 01:08:44.040]   in a similar way with respect to different hypotheses
[01:08:44.040 --> 01:08:49.040]   about how many balls there will be in the urn of humanity.
[01:08:49.040 --> 01:08:51.520]   I said, for how many humans there will ever be
[01:08:51.520 --> 01:08:52.880]   by the time we go extinct.
[01:08:52.880 --> 01:08:56.640]   So to simplify, let's suppose we only consider
[01:08:56.640 --> 01:09:01.440]   two hypotheses, either maybe 200 billion humans in total
[01:09:01.440 --> 01:09:04.280]   or 200 trillion humans in total.
[01:09:05.600 --> 01:09:07.320]   You could fill in more hypotheses,
[01:09:07.320 --> 01:09:09.280]   but it doesn't change the principle here.
[01:09:09.280 --> 01:09:12.040]   So it's easiest to see if we just consider these two.
[01:09:12.040 --> 01:09:13.320]   So you start with some prior
[01:09:13.320 --> 01:09:15.880]   based on ordinary empirical ideas
[01:09:15.880 --> 01:09:18.800]   about threats to civilization and so forth.
[01:09:18.800 --> 01:09:22.520]   And maybe you say it's a 5% chance that we will go extinct
[01:09:22.520 --> 01:09:25.440]   by the time there will have been 200 billion only.
[01:09:25.440 --> 01:09:27.120]   You're kind of optimistic, let's say.
[01:09:27.120 --> 01:09:28.800]   You think probably we'll make it through,
[01:09:28.800 --> 01:09:30.000]   colonize the universe.
[01:09:30.000 --> 01:09:34.400]   But then according to this Doomsday Argument,
[01:09:34.400 --> 01:09:37.000]   you should take off your own birth rank
[01:09:37.000 --> 01:09:40.080]   as a random sample.
[01:09:40.080 --> 01:09:43.160]   So your birth rank is your sequence in the position
[01:09:43.160 --> 01:09:47.680]   of all humans that have ever existed.
[01:09:47.680 --> 01:09:51.680]   It turns out you're about a human number of 100 billion,
[01:09:51.680 --> 01:09:52.520]   you know, give or take.
[01:09:52.520 --> 01:09:54.000]   That's like roughly how many people
[01:09:54.000 --> 01:09:55.280]   have been born before you.
[01:09:55.280 --> 01:09:57.440]   - That's fascinating 'cause I probably,
[01:09:57.440 --> 01:09:59.080]   we each have a number.
[01:09:59.080 --> 01:10:01.160]   - We would each have a number in this.
[01:10:01.160 --> 01:10:04.000]   I mean, obviously the exact number would depend
[01:10:04.000 --> 01:10:05.280]   on where you started counting,
[01:10:05.280 --> 01:10:08.920]   like which ancestors was human enough to count as human.
[01:10:08.920 --> 01:10:10.960]   But those are not really important.
[01:10:10.960 --> 01:10:12.880]   They're relatively few of them.
[01:10:12.880 --> 01:10:16.080]   So yeah, so you're roughly 100 billion.
[01:10:16.080 --> 01:10:18.520]   Now, if they're only gonna be 200 billion in total,
[01:10:18.520 --> 01:10:20.960]   that's a perfectly unremarkable number.
[01:10:20.960 --> 01:10:23.160]   You're somewhere in the middle, right?
[01:10:23.160 --> 01:10:27.280]   It's run-of-the-mill human, completely unsurprising.
[01:10:27.280 --> 01:10:28.880]   Now, if they're gonna be 200 trillion,
[01:10:28.880 --> 01:10:31.720]   you would be remarkably early.
[01:10:31.720 --> 01:10:33.880]   Like, what are the chances?
[01:10:33.880 --> 01:10:35.880]   Out of these 200 trillion human,
[01:10:35.880 --> 01:10:39.800]   that you should be human number 100 billion?
[01:10:39.800 --> 01:10:42.240]   That seems it would have a much lower
[01:10:42.240 --> 01:10:43.680]   conditional probability.
[01:10:43.680 --> 01:10:47.560]   And so analogously to how in the urn case,
[01:10:47.560 --> 01:10:51.920]   you thought after finding this low-numbered random sample,
[01:10:51.920 --> 01:10:54.480]   you updated in favor of the urn having few balls.
[01:10:54.480 --> 01:10:56.160]   Similarly, in this case,
[01:10:56.160 --> 01:10:59.320]   you should update in favor of the human species
[01:10:59.320 --> 01:11:02.400]   having a lower total number of members.
[01:11:02.400 --> 01:11:03.560]   That is doom soon.
[01:11:04.160 --> 01:11:05.600]   - You said doom soon?
[01:11:05.600 --> 01:11:06.800]   That's the- - Yeah.
[01:11:06.800 --> 01:11:09.120]   Well, that would be the hypothesis in this case,
[01:11:09.120 --> 01:11:11.680]   that it will end after 100 billion.
[01:11:11.680 --> 01:11:14.200]   - I just like that term for that hypothesis, yeah.
[01:11:14.200 --> 01:11:17.240]   - So what it kind of crucially relies on,
[01:11:17.240 --> 01:11:18.080]   the doomsday argument,
[01:11:18.080 --> 01:11:21.680]   is the idea that you should reason
[01:11:21.680 --> 01:11:23.840]   as if you were a random sample
[01:11:23.840 --> 01:11:27.400]   from the set of all humans that will ever have existed.
[01:11:27.400 --> 01:11:28.400]   If you have that assumption,
[01:11:28.400 --> 01:11:30.960]   then I think the rest kind of follows.
[01:11:30.960 --> 01:11:34.240]   The question then is why should you make that assumption?
[01:11:34.240 --> 01:11:36.000]   In fact, you know you're 100 billion,
[01:11:36.000 --> 01:11:38.560]   so where do you get this prior?
[01:11:38.560 --> 01:11:40.360]   And then there's like a literature on that
[01:11:40.360 --> 01:11:43.680]   with different ways of supporting that assumption.
[01:11:43.680 --> 01:11:48.080]   - That's just one example of a theropic reasoning, right?
[01:11:48.080 --> 01:11:49.880]   That seems to be kind of convenient
[01:11:49.880 --> 01:11:52.520]   when you think about humanity.
[01:11:52.520 --> 01:11:55.840]   When you think about sort of even like existential threats
[01:11:55.840 --> 01:12:00.240]   and so on, as it seems that quite naturally
[01:12:00.240 --> 01:12:03.040]   that you should assume that you're just an average case.
[01:12:03.040 --> 01:12:08.160]   - Yeah, that you're a kind of a typical or randomly sample.
[01:12:08.160 --> 01:12:09.600]   Now in the case of the doomsday argument,
[01:12:09.600 --> 01:12:12.280]   it seems to lead to what intuitively we think
[01:12:12.280 --> 01:12:13.440]   is the wrong conclusion,
[01:12:13.440 --> 01:12:15.920]   or at least many people have this reaction,
[01:12:15.920 --> 01:12:19.040]   that there's gotta be something fishy about this argument,
[01:12:19.040 --> 01:12:21.560]   because from very, very weak premises,
[01:12:21.560 --> 01:12:24.960]   it gets this very striking implication
[01:12:24.960 --> 01:12:27.000]   that we have almost no chance
[01:12:27.000 --> 01:12:30.880]   of reaching size 200 trillion humans in the future.
[01:12:30.880 --> 01:12:33.040]   And how could we possibly get there
[01:12:33.040 --> 01:12:35.440]   just by reflecting on when we were born?
[01:12:35.440 --> 01:12:37.480]   It seems you would need sophisticated arguments
[01:12:37.480 --> 01:12:40.480]   about the impossibility of space colonization, blah, blah.
[01:12:40.480 --> 01:12:43.680]   So one might be tempted to reject this key assumption,
[01:12:43.680 --> 01:12:45.480]   I call it the self-sampling assumption.
[01:12:45.480 --> 01:12:46.520]   The idea that you should reason
[01:12:46.520 --> 01:12:48.840]   as if you're a random sample from all observers
[01:12:48.840 --> 01:12:51.480]   or in some reference class.
[01:12:51.480 --> 01:12:56.600]   However, it turns out that in other domains,
[01:12:56.600 --> 01:12:58.320]   it looks like we need something
[01:12:58.320 --> 01:13:00.120]   like this self-sampling assumption
[01:13:00.120 --> 01:13:04.320]   to make sense of bona fide scientific inferences.
[01:13:04.320 --> 01:13:06.920]   In contemporary cosmology, for example,
[01:13:06.920 --> 01:13:09.120]   you have these multiverse theories.
[01:13:09.120 --> 01:13:10.840]   And according to a lot of those,
[01:13:10.840 --> 01:13:14.920]   all possible human observations are made.
[01:13:14.920 --> 01:13:17.480]   So I mean, if you have a sufficiently large universe,
[01:13:17.480 --> 01:13:18.880]   you will have a lot of people observing
[01:13:18.880 --> 01:13:20.320]   all kinds of different things.
[01:13:20.320 --> 01:13:23.800]   So if you have two competing theories,
[01:13:23.800 --> 01:13:26.560]   say about the value of some constant,
[01:13:26.560 --> 01:13:32.200]   it could be true according to both of these theories
[01:13:32.200 --> 01:13:35.800]   that there will be some observers observing the value
[01:13:35.800 --> 01:13:39.720]   that corresponds to the other theory
[01:13:39.720 --> 01:13:43.080]   because there will be some observers that have hallucinations
[01:13:43.080 --> 01:13:44.960]   or there's a local fluctuation
[01:13:44.960 --> 01:13:47.520]   or a statistically anomalous measurement,
[01:13:47.520 --> 01:13:49.200]   these things will happen.
[01:13:49.200 --> 01:13:52.320]   And if enough observers make enough different observations,
[01:13:52.320 --> 01:13:53.960]   there will be some that sort of by chance
[01:13:53.960 --> 01:13:55.760]   make these different ones.
[01:13:55.760 --> 01:13:57.640]   And so what we would wanna say is,
[01:13:57.640 --> 01:14:02.640]   well, many more observers,
[01:14:02.640 --> 01:14:04.840]   a larger proportion of the observers
[01:14:04.840 --> 01:14:06.800]   will observe as it were the true value.
[01:14:06.800 --> 01:14:10.720]   And a few will observe the wrong value.
[01:14:10.720 --> 01:14:12.600]   If we think of ourselves as a random sample,
[01:14:12.600 --> 01:14:15.040]   we should expect with a probability
[01:14:15.040 --> 01:14:16.000]   to observe the true value.
[01:14:16.000 --> 01:14:19.200]   And that will then allow us to conclude
[01:14:19.200 --> 01:14:20.600]   that the evidence we actually have
[01:14:20.600 --> 01:14:24.520]   is evidence for the theories we think are supported.
[01:14:24.520 --> 01:14:29.200]   It kind of then is a way of making sense
[01:14:29.200 --> 01:14:31.800]   of these inferences that clearly seem correct
[01:14:31.800 --> 01:14:34.840]   that we can make various observations
[01:14:34.840 --> 01:14:39.000]   and infer what the temperature of the cosmic background is
[01:14:39.000 --> 01:14:42.600]   and the fine structure constant and all of this.
[01:14:42.600 --> 01:14:46.640]   But it seems that without rolling in some assumption
[01:14:46.640 --> 01:14:49.520]   similar to the self-sampling assumption,
[01:14:49.520 --> 01:14:51.560]   this inference just doesn't go through.
[01:14:51.560 --> 01:14:53.120]   And there are other examples.
[01:14:53.120 --> 01:14:54.720]   So there are these scientific contexts
[01:14:54.720 --> 01:14:56.760]   where it looks like this kind of anthropic reasoning
[01:14:56.760 --> 01:14:59.080]   is needed and makes perfect sense.
[01:14:59.080 --> 01:15:01.080]   And yet in the case of the Doomsday Argument,
[01:15:01.080 --> 01:15:02.440]   it has this weird consequence
[01:15:02.440 --> 01:15:05.680]   and people might think there's something wrong with it there.
[01:15:05.680 --> 01:15:10.680]   So there's then this project that would consist
[01:15:10.680 --> 01:15:13.200]   in trying to figure out
[01:15:13.200 --> 01:15:15.880]   what are the legitimate ways of reasoning
[01:15:15.880 --> 01:15:18.240]   about these indexical facts
[01:15:18.240 --> 01:15:20.400]   when observer selection effects are in play.
[01:15:20.400 --> 01:15:23.080]   In other words, developing a theory of anthropics.
[01:15:23.080 --> 01:15:25.920]   And there are different views of looking at that.
[01:15:25.920 --> 01:15:29.120]   And it's a difficult methodological area.
[01:15:29.120 --> 01:15:33.960]   But to tie it back to the simulation argument,
[01:15:33.960 --> 01:15:36.440]   the key assumption there,
[01:15:36.440 --> 01:15:39.440]   this bland principle of indifference,
[01:15:39.440 --> 01:15:41.880]   is much weaker than the self-sampling assumption.
[01:15:41.880 --> 01:15:46.320]   So if you think about in the case of the Doomsday Argument,
[01:15:47.400 --> 01:15:49.600]   it says you should reason as if you're a random sample
[01:15:49.600 --> 01:15:51.120]   from all humans that will ever have lived,
[01:15:51.120 --> 01:15:54.000]   even though in fact, you know that you are
[01:15:54.000 --> 01:15:57.360]   about number 100 billionth human
[01:15:57.360 --> 01:15:59.680]   and you're alive in the year 2020.
[01:15:59.680 --> 01:16:01.520]   Whereas in the case of the simulation argument,
[01:16:01.520 --> 01:16:04.560]   it says that, well, if you actually have no way
[01:16:04.560 --> 01:16:05.720]   of telling which one you are,
[01:16:05.720 --> 01:16:10.720]   then you should assign this kind of uniform probability.
[01:16:10.720 --> 01:16:12.840]   - Yeah, yeah, your role as the observer
[01:16:12.840 --> 01:16:15.680]   in the simulation argument is different, it seems like.
[01:16:15.680 --> 01:16:17.360]   Like, who's the observer?
[01:16:17.360 --> 01:16:19.640]   I mean, I keep assigning the individual consciousness.
[01:16:19.640 --> 01:16:22.200]   - Yeah, I mean, well, there are a lot of observers
[01:16:22.200 --> 01:16:24.120]   in the simulation, in the context
[01:16:24.120 --> 01:16:25.160]   of the simulation argument.
[01:16:25.160 --> 01:16:26.000]   - But they're all observers.
[01:16:26.000 --> 01:16:27.840]   - The relevant observers would be, A,
[01:16:27.840 --> 01:16:30.120]   the people in original histories,
[01:16:30.120 --> 01:16:33.320]   and B, the people in simulations.
[01:16:33.320 --> 01:16:35.960]   So this would be the class of observers that we need.
[01:16:35.960 --> 01:16:37.400]   I mean, there are also maybe the simulators,
[01:16:37.400 --> 01:16:40.160]   but we can set those aside for this.
[01:16:40.160 --> 01:16:43.080]   So the question is, given that class of observers,
[01:16:43.080 --> 01:16:46.240]   a small set of original history observers
[01:16:46.240 --> 01:16:48.560]   and a large class of simulated observers,
[01:16:48.560 --> 01:16:51.120]   which one should you think is you?
[01:16:51.120 --> 01:16:53.680]   Where are you amongst this set of observers?
[01:16:53.680 --> 01:16:56.320]   - I'm maybe having a little bit of trouble
[01:16:56.320 --> 01:16:59.440]   wrapping my head around the intricacies
[01:16:59.440 --> 01:17:01.120]   of what it means to be an observer
[01:17:01.120 --> 01:17:06.120]   in the different instantiations
[01:17:06.120 --> 01:17:09.360]   of the anthropic reasoning cases that we mentioned.
[01:17:09.360 --> 01:17:11.400]   I mean, does it have to be--
[01:17:11.400 --> 01:17:12.600]   - It's like the observer, no, I mean,
[01:17:12.600 --> 01:17:14.760]   it may be an easier way of putting it.
[01:17:14.760 --> 01:17:18.160]   It's just like, are you simulated or are you not simulated?
[01:17:18.160 --> 01:17:21.280]   Given this assumption that these two groups of people exist.
[01:17:21.280 --> 01:17:22.400]   - Yeah, in the simulation case,
[01:17:22.400 --> 01:17:24.600]   it seems pretty straightforward.
[01:17:24.600 --> 01:17:28.840]   - Yeah, so the key point is the methodological assumption
[01:17:28.840 --> 01:17:32.480]   you need to make to get the simulation argument
[01:17:32.480 --> 01:17:36.960]   to where it wants to go is much weaker and less problematic
[01:17:36.960 --> 01:17:39.520]   than the methodological assumption you need to make
[01:17:39.520 --> 01:17:42.560]   to get the doomsday argument to its conclusion.
[01:17:42.560 --> 01:17:46.680]   Maybe the doomsday argument is sound or unsound,
[01:17:46.680 --> 01:17:48.000]   but you need to make a much stronger
[01:17:48.000 --> 01:17:52.040]   and more controversial assumption to make it go through.
[01:17:52.040 --> 01:17:54.520]   In the case of the simulation argument,
[01:17:54.520 --> 01:17:57.640]   I guess one maybe way intuition popped
[01:17:57.640 --> 01:18:00.880]   to support this bland principle of indifference
[01:18:00.880 --> 01:18:05.400]   is to consider a sequence of different cases
[01:18:05.400 --> 01:18:08.720]   where the fraction of people who are simulated
[01:18:08.720 --> 01:18:12.480]   to non-simulated approaches one.
[01:18:12.480 --> 01:18:17.120]   So in the limiting case where everybody is simulated,
[01:18:17.120 --> 01:18:22.640]   obviously you can deduce with certainty
[01:18:22.640 --> 01:18:23.840]   that you are simulated.
[01:18:23.840 --> 01:18:28.360]   If everybody with your experiences is simulated
[01:18:28.360 --> 01:18:30.880]   and you know you're gotta be one of those,
[01:18:30.880 --> 01:18:32.400]   you don't need a probability at all.
[01:18:32.400 --> 01:18:35.640]   You just kind of logically conclude it, right?
[01:18:35.640 --> 01:18:36.480]   - Right.
[01:18:36.480 --> 01:18:41.480]   - So then as we move from a case where say 90% of people
[01:18:41.480 --> 01:18:46.480]   90% of everybody is simulated, 99%, 99.9%,
[01:18:46.480 --> 01:18:50.920]   it should seem plausible that the probability assigned
[01:18:50.920 --> 01:18:54.720]   should sort of approach one certainty
[01:18:54.720 --> 01:18:57.600]   as the fraction approaches the case
[01:18:57.600 --> 01:19:00.880]   where everybody is in a simulation.
[01:19:00.880 --> 01:19:01.840]   - Yeah, that's exactly--
[01:19:01.840 --> 01:19:04.760]   - Like you wouldn't expect that to be a discrete.
[01:19:04.760 --> 01:19:06.560]   Well, if there's one non-simulated person,
[01:19:06.560 --> 01:19:08.800]   then it's 50/50, but if we'd move that,
[01:19:08.800 --> 01:19:09.680]   then it's 100%.
[01:19:09.680 --> 01:19:11.720]   It's like it should kind of...
[01:19:11.720 --> 01:19:14.800]   There are other arguments as well one can use
[01:19:14.800 --> 01:19:16.800]   to support this blind principle of indifference,
[01:19:16.800 --> 01:19:19.480]   but that might be enough to--
[01:19:19.480 --> 01:19:22.440]   - But in general, when you start from time equals zero
[01:19:22.440 --> 01:19:26.560]   and go into the future, the fraction of simulated,
[01:19:26.560 --> 01:19:29.160]   if it's possible to create simulated worlds,
[01:19:29.160 --> 01:19:31.560]   the fraction of simulated worlds will go to one.
[01:19:31.560 --> 01:19:33.480]   - Well, it won't--
[01:19:33.480 --> 01:19:35.200]   - I mean, is that an obvious kind of thing?
[01:19:35.200 --> 01:19:37.240]   - Go all the way to one.
[01:19:37.240 --> 01:19:40.720]   - In reality, that would be some ratio,
[01:19:40.720 --> 01:19:43.760]   although maybe a technologically mature civilization
[01:19:43.760 --> 01:19:47.400]   could run a lot of simulations
[01:19:47.400 --> 01:19:50.040]   using a small portion of its resources.
[01:19:50.040 --> 01:19:53.160]   It probably wouldn't be able to run infinitely many.
[01:19:53.160 --> 01:19:56.840]   I mean, if we take, say, the observed,
[01:19:56.840 --> 01:19:58.680]   the physics in the observed universe,
[01:19:58.680 --> 01:20:01.720]   if we assume that that's also the physics
[01:20:01.720 --> 01:20:03.560]   at the level of the simulators,
[01:20:03.560 --> 01:20:05.520]   that would be limits to the amount
[01:20:05.520 --> 01:20:09.360]   of information processing that any one civilization
[01:20:09.360 --> 01:20:12.480]   could perform in its future trajectory.
[01:20:12.480 --> 01:20:15.680]   - Right, I mean--
[01:20:15.680 --> 01:20:17.800]   - Well, first of all, there's a limited amount of matter
[01:20:17.800 --> 01:20:18.880]   you can get your hands off
[01:20:18.880 --> 01:20:22.440]   because with a positive cosmological constant,
[01:20:22.440 --> 01:20:24.440]   the universe is accelerating.
[01:20:24.440 --> 01:20:26.480]   There's a finite sphere of stuff,
[01:20:26.480 --> 01:20:27.960]   even if you travel with the speed of light
[01:20:27.960 --> 01:20:28.800]   that you could ever reach,
[01:20:28.800 --> 01:20:31.440]   you have a finite amount of stuff.
[01:20:31.440 --> 01:20:34.440]   And then if you think there's a lower limit
[01:20:34.440 --> 01:20:37.600]   to the amount of loss you get
[01:20:37.600 --> 01:20:40.240]   when you perform an erasure of a computation,
[01:20:40.240 --> 01:20:41.360]   or if you think, for example,
[01:20:41.360 --> 01:20:45.000]   just matter gradually over cosmological timescales,
[01:20:45.000 --> 01:20:48.120]   decay, maybe protons decay, other things,
[01:20:48.120 --> 01:20:50.480]   and you radiate out gravitational waves,
[01:20:50.480 --> 01:20:54.840]   like there's all kinds of seemingly unavoidable losses
[01:20:54.840 --> 01:20:55.840]   that occur.
[01:20:55.840 --> 01:20:59.240]   So eventually, we'll have something
[01:20:59.240 --> 01:21:02.160]   like a heat death of the universe
[01:21:02.160 --> 01:21:04.320]   or a cold death or whatever, but yeah.
[01:21:04.320 --> 01:21:06.600]   - So it's finite, but of course, we don't know which,
[01:21:06.600 --> 01:21:11.320]   if there's many ancestral simulations,
[01:21:11.320 --> 01:21:13.600]   we don't know which level we are.
[01:21:13.600 --> 01:21:14.680]   So that could be,
[01:21:14.680 --> 01:21:18.200]   couldn't there be like an arbitrary number of simulation
[01:21:18.200 --> 01:21:19.880]   that spawned ours,
[01:21:19.880 --> 01:21:22.680]   and those had more resources
[01:21:22.680 --> 01:21:26.640]   in terms of physical universe to work with?
[01:21:26.640 --> 01:21:28.880]   - Sorry, what do you mean that that could be?
[01:21:28.880 --> 01:21:30.880]   - So sort of, okay, so
[01:21:32.680 --> 01:21:37.440]   if simulations spawn other simulations,
[01:21:37.440 --> 01:21:41.920]   it seems like each new spawn has fewer resources
[01:21:41.920 --> 01:21:42.840]   to work with.
[01:21:42.840 --> 01:21:49.080]   But we don't know at which step along the way we are at.
[01:21:49.080 --> 01:21:54.600]   Any one observer doesn't know whether we're in level 42
[01:21:54.600 --> 01:21:57.840]   or 100 or one,
[01:21:57.840 --> 01:21:59.940]   or does that not matter for the resources?
[01:21:59.940 --> 01:22:02.080]   I mean--
[01:22:02.080 --> 01:22:05.680]   - I mean, it's true that there would be uncertainty as,
[01:22:05.680 --> 01:22:07.640]   you could have stacked simulations.
[01:22:07.640 --> 01:22:08.480]   - Yes, so.
[01:22:08.480 --> 01:22:11.520]   - And that could then be uncertainty
[01:22:11.520 --> 01:22:13.720]   as to which level we are at.
[01:22:13.720 --> 01:22:17.480]   As you remarked also,
[01:22:17.480 --> 01:22:21.280]   all the computations performed
[01:22:21.280 --> 01:22:24.680]   in a simulation within the simulation
[01:22:24.680 --> 01:22:27.760]   also have to be expanded at the level of the simulation.
[01:22:27.760 --> 01:22:28.600]   - Right.
[01:22:28.600 --> 01:22:30.800]   - So the computer in basement reality
[01:22:30.800 --> 01:22:32.400]   where all the simulations within simulations
[01:22:32.400 --> 01:22:33.680]   within simulations are taking place,
[01:22:33.680 --> 01:22:35.320]   like that computer ultimately,
[01:22:35.320 --> 01:22:37.760]   it's CPU or whatever it is,
[01:22:37.760 --> 01:22:39.880]   like that has to power this whole tower, right?
[01:22:39.880 --> 01:22:44.280]   So if there is a finite compute power in basement reality,
[01:22:44.280 --> 01:22:48.120]   that would impose a limit to how tall this tower can be.
[01:22:48.120 --> 01:22:53.040]   And if each level kind of imposes a large extra overhead,
[01:22:53.040 --> 01:22:55.560]   you might think maybe the tower would not be very tall,
[01:22:55.560 --> 01:22:59.480]   that most people would be low down in the tower.
[01:23:00.720 --> 01:23:03.000]   - I love the term basement reality.
[01:23:03.000 --> 01:23:06.560]   Let me ask, one of the popularizers,
[01:23:06.560 --> 01:23:08.600]   you said there's many through this,
[01:23:08.600 --> 01:23:11.820]   when you look at sort of the last few years
[01:23:11.820 --> 01:23:13.420]   of the simulation hypothesis,
[01:23:13.420 --> 01:23:15.640]   just like you said, it comes up every once in a while,
[01:23:15.640 --> 01:23:17.880]   some new community discovers it and so on.
[01:23:17.880 --> 01:23:20.000]   But I would say one of the biggest popularizers
[01:23:20.000 --> 01:23:21.640]   of this idea is Elon Musk.
[01:23:21.640 --> 01:23:24.080]   Do you have any kind of intuition
[01:23:24.080 --> 01:23:26.280]   about what Elon thinks about
[01:23:26.280 --> 01:23:27.760]   when he thinks about simulation?
[01:23:27.760 --> 01:23:29.880]   Why is this of such interest?
[01:23:29.880 --> 01:23:32.000]   Is it all the things we've talked about
[01:23:32.000 --> 01:23:33.840]   or is there some special kind of intuition
[01:23:33.840 --> 01:23:35.440]   about simulation that he has?
[01:23:35.440 --> 01:23:37.480]   - I mean, you might have a better,
[01:23:37.480 --> 01:23:39.220]   I think, I mean, why it's of interest,
[01:23:39.220 --> 01:23:42.120]   I think it's like seems fairly obvious
[01:23:42.120 --> 01:23:45.120]   why to the extent that one think the argument is credible,
[01:23:45.120 --> 01:23:46.120]   why it would be of interest.
[01:23:46.120 --> 01:23:47.720]   It would, if it's correct,
[01:23:47.720 --> 01:23:50.000]   tell us something very important about the world
[01:23:50.000 --> 01:23:50.880]   in one way or the other,
[01:23:50.880 --> 01:23:53.320]   whichever of the three alternatives for a simulation
[01:23:53.320 --> 01:23:55.300]   that seems like arguably one
[01:23:55.300 --> 01:23:58.480]   of the most fundamental discoveries, right?
[01:23:58.480 --> 01:24:00.600]   Now, interestingly, in the case of somebody like Elon,
[01:24:00.600 --> 01:24:02.240]   so there's like the standard arguments
[01:24:02.240 --> 01:24:04.760]   for why you might wanna take the simulation hypothesis
[01:24:04.760 --> 01:24:07.360]   seriously, the simulation argument, right?
[01:24:07.360 --> 01:24:10.320]   In the case that if you are actually Elon Musk, let us say,
[01:24:10.320 --> 01:24:13.400]   there's a kind of an additional reason
[01:24:13.400 --> 01:24:16.000]   in that what are the chances you would be Elon Musk?
[01:24:16.000 --> 01:24:20.680]   Like, it seems like maybe there would be more interest
[01:24:20.680 --> 01:24:24.400]   in simulating the lives of very unusual
[01:24:24.400 --> 01:24:26.160]   and remarkable people.
[01:24:26.160 --> 01:24:29.120]   So if you consider not just a simulations
[01:24:29.120 --> 01:24:31.760]   where all of human history
[01:24:31.760 --> 01:24:34.440]   or the whole of human civilization are simulated,
[01:24:34.440 --> 01:24:36.020]   but also other kinds of simulations,
[01:24:36.020 --> 01:24:39.160]   which only include some subset of people,
[01:24:39.160 --> 01:24:43.680]   like in those simulations that only include a subset,
[01:24:43.680 --> 01:24:45.920]   it might be more likely that that would include subsets
[01:24:45.920 --> 01:24:49.320]   of people with unusually interesting or consequential life.
[01:24:49.320 --> 01:24:50.640]   - So if you're Elon Musk,
[01:24:50.640 --> 01:24:51.480]   - You gotta wonder, right?
[01:24:51.480 --> 01:24:54.120]   - It's more likely that you're a simulation.
[01:24:54.120 --> 01:24:55.440]   - Like if you're Donald Trump
[01:24:55.440 --> 01:24:56.760]   or if you are Bill Gates,
[01:24:56.760 --> 01:25:01.760]   or you're like some particularly like distinctive character,
[01:25:01.760 --> 01:25:04.080]   you might think that that add,
[01:25:04.080 --> 01:25:07.120]   I mean, if you just think of yourself into the shoes, right?
[01:25:07.120 --> 01:25:10.120]   It's gotta be like an extra reason to think,
[01:25:10.120 --> 01:25:11.160]   that's kind of.
[01:25:11.160 --> 01:25:12.320]   - So interesting.
[01:25:12.320 --> 01:25:17.320]   So on a scale of like farmer in Peru to Elon Musk,
[01:25:17.320 --> 01:25:19.160]   the more you get towards the Elon Musk,
[01:25:19.160 --> 01:25:20.600]   the higher the probability.
[01:25:20.600 --> 01:25:23.560]   - You'd imagine that would be some extra boost from that.
[01:25:25.120 --> 01:25:26.080]   - There's an extra boost.
[01:25:26.080 --> 01:25:30.000]   So he also asked the question of what he would ask an AGI
[01:25:30.000 --> 01:25:34.520]   saying the question being, what's outside the simulation?
[01:25:34.520 --> 01:25:37.640]   Do you think about the answer to this question,
[01:25:37.640 --> 01:25:39.320]   if we are living in a simulation,
[01:25:39.320 --> 01:25:41.440]   what is outside the simulation?
[01:25:41.440 --> 01:25:45.440]   So the programmer of the simulation?
[01:25:45.440 --> 01:25:47.640]   - Yeah, I mean, I think it connects to the question
[01:25:47.640 --> 01:25:50.280]   of what's inside the simulation in that,
[01:25:50.280 --> 01:25:53.920]   if you had views about the creatures of the simulation,
[01:25:53.920 --> 01:25:57.040]   it might help you make predictions
[01:25:57.040 --> 01:25:59.160]   about what kind of simulation it is,
[01:25:59.160 --> 01:26:03.480]   what might happen, what happens after the simulation,
[01:26:03.480 --> 01:26:06.600]   if there is some after, but also like the kind of setup.
[01:26:06.600 --> 01:26:10.720]   So these two questions would be quite closely intertwined.
[01:26:10.720 --> 01:26:14.960]   - But do you think it would be very surprising to,
[01:26:14.960 --> 01:26:17.880]   like, is the stuff inside the simulation,
[01:26:17.880 --> 01:26:19.880]   is it possible for it to be fundamentally different
[01:26:19.880 --> 01:26:21.840]   than the stuff outside?
[01:26:21.840 --> 01:26:22.680]   - Yeah.
[01:26:22.680 --> 01:26:25.000]   - Like another way to put it,
[01:26:25.000 --> 01:26:28.000]   can the creatures inside the simulation
[01:26:28.000 --> 01:26:30.200]   be smart enough to even understand
[01:26:30.200 --> 01:26:31.800]   or have the cognitive capabilities
[01:26:31.800 --> 01:26:34.760]   or any kind of information processing capabilities
[01:26:34.760 --> 01:26:39.760]   enough to understand the mechanism that's created them?
[01:26:39.760 --> 01:26:43.080]   - They might understand some aspects of it.
[01:26:43.080 --> 01:26:45.600]   I mean, it's a level of,
[01:26:45.600 --> 01:26:49.000]   it's kind of there are levels of explanation,
[01:26:49.000 --> 01:26:51.040]   like degrees to which you can understand.
[01:26:51.040 --> 01:26:53.960]   So does your dog understand what it is to be human?
[01:26:53.960 --> 01:26:54.920]   Well, it's got some idea,
[01:26:54.920 --> 01:26:57.120]   like humans are these physical objects
[01:26:57.120 --> 01:26:59.760]   that move around and do things.
[01:26:59.760 --> 01:27:03.640]   And like a normal human would have a deeper understanding
[01:27:03.640 --> 01:27:05.600]   of what it is to be a human.
[01:27:05.600 --> 01:27:10.280]   And maybe some very experienced psychologist
[01:27:10.280 --> 01:27:12.520]   or a great novelist might understand a little bit more
[01:27:12.520 --> 01:27:14.080]   about what it is to be human.
[01:27:14.080 --> 01:27:16.360]   And maybe a superintelligence could see
[01:27:16.360 --> 01:27:17.560]   right through your soul.
[01:27:18.640 --> 01:27:23.640]   So similarly, I do think that we are quite limited
[01:27:23.640 --> 01:27:28.560]   in our ability to understand all of the relevant aspects
[01:27:28.560 --> 01:27:31.920]   of the larger context that we exist in.
[01:27:31.920 --> 01:27:33.400]   - But there might be hope for some.
[01:27:33.400 --> 01:27:35.880]   - I think we understand some aspects of it,
[01:27:35.880 --> 01:27:38.080]   but how much good is that?
[01:27:38.080 --> 01:27:41.800]   If there's like one key aspect
[01:27:41.800 --> 01:27:44.560]   that changes the significance of all the other aspects.
[01:27:44.560 --> 01:27:48.880]   So we understand maybe seven out of 10 key insights
[01:27:48.880 --> 01:27:49.720]   that you need,
[01:27:49.720 --> 01:27:55.840]   but the answer actually like varies completely
[01:27:55.840 --> 01:27:58.960]   depending on what like number eight, nine, and 10 insight is.
[01:27:58.960 --> 01:28:01.520]   It's like whether you wanna,
[01:28:01.520 --> 01:28:06.520]   suppose that the big task were to guess
[01:28:06.520 --> 01:28:10.040]   whether a certain number was odd or even,
[01:28:10.040 --> 01:28:12.000]   like a 10 digit number.
[01:28:12.000 --> 01:28:15.520]   And if it's even, the best thing for you to do in life
[01:28:15.520 --> 01:28:16.360]   is to go north.
[01:28:16.360 --> 01:28:19.120]   And if it's odd, the best thing for you is to go south.
[01:28:19.120 --> 01:28:23.640]   Now we are in a situation where maybe through our science
[01:28:23.640 --> 01:28:25.040]   and philosophy, we figured out
[01:28:25.040 --> 01:28:26.960]   what the first seven digits are.
[01:28:26.960 --> 01:28:28.440]   So we have a lot of information, right?
[01:28:28.440 --> 01:28:29.800]   Most of it we figured out,
[01:28:29.800 --> 01:28:34.200]   but we are clueless about what the last three digits are.
[01:28:34.200 --> 01:28:36.520]   So we are still completely clueless
[01:28:36.520 --> 01:28:38.240]   about whether the number is odd or even,
[01:28:38.240 --> 01:28:41.160]   and therefore whether we should go north or go south.
[01:28:41.160 --> 01:28:42.880]   I feel that's an analogy,
[01:28:42.880 --> 01:28:45.720]   but I feel we're somewhat in that predicament.
[01:28:45.720 --> 01:28:48.000]   We know a lot about the universe.
[01:28:48.000 --> 01:28:51.280]   We've come maybe more than half of the way there
[01:28:51.280 --> 01:28:52.560]   to kind of fully understanding it,
[01:28:52.560 --> 01:28:55.320]   but the parts we're missing are plausibly ones
[01:28:55.320 --> 01:28:59.160]   that could completely change the overall upshot
[01:28:59.160 --> 01:29:02.720]   of the thing, and including change our overall view
[01:29:02.720 --> 01:29:05.280]   about what the scheme of priorities should be
[01:29:05.280 --> 01:29:07.760]   or which strategic direction would make sense to pursue.
[01:29:07.760 --> 01:29:11.160]   - Yeah, I think your analogy of us being the dog
[01:29:11.160 --> 01:29:15.720]   trying to understand human beings is an entertaining one,
[01:29:15.720 --> 01:29:17.560]   and probably correct.
[01:29:17.560 --> 01:29:21.840]   The closer the understanding tends from the dog's viewpoint
[01:29:21.840 --> 01:29:24.720]   to us human psychologists' viewpoint,
[01:29:24.720 --> 01:29:26.680]   the steps along the way there
[01:29:26.680 --> 01:29:28.760]   will have completely transformative ideas
[01:29:28.760 --> 01:29:30.800]   of what it means to be human.
[01:29:30.800 --> 01:29:33.760]   So the dog has a very shallow understanding.
[01:29:33.760 --> 01:29:36.120]   It's interesting to think that,
[01:29:36.120 --> 01:29:39.880]   to analogize that a dog's understanding of a human being
[01:29:39.880 --> 01:29:42.320]   is the same as our current understanding
[01:29:42.320 --> 01:29:45.120]   of the fundamental laws of physics in the universe.
[01:29:45.120 --> 01:29:48.440]   Oh man, okay.
[01:29:48.440 --> 01:29:50.320]   We spent an hour and 40 minutes
[01:29:50.320 --> 01:29:51.600]   talking about the simulation.
[01:29:51.600 --> 01:29:52.920]   I like it.
[01:29:52.920 --> 01:29:54.800]   Let's talk about superintelligence,
[01:29:54.800 --> 01:29:56.920]   at least for a little bit.
[01:29:56.920 --> 01:29:58.520]   And let's start at the basics.
[01:29:58.520 --> 01:30:00.560]   What to you is intelligence?
[01:30:00.560 --> 01:30:04.560]   - Yeah, I tend not to get too stuck
[01:30:04.560 --> 01:30:05.960]   with the definitional question.
[01:30:05.960 --> 01:30:08.640]   I mean, the common sense understanding,
[01:30:08.640 --> 01:30:11.040]   like the ability to solve complex problems,
[01:30:11.040 --> 01:30:14.280]   to learn from experience, to plan, to reason,
[01:30:14.280 --> 01:30:18.520]   some combination of things like that.
[01:30:18.520 --> 01:30:21.080]   - Is consciousness mixed up into that or no?
[01:30:21.080 --> 01:30:23.000]   Is consciousness mixed up into that or is it--
[01:30:23.000 --> 01:30:24.120]   - Well, I don't think,
[01:30:24.120 --> 01:30:26.120]   I think it could be fairly intelligent,
[01:30:26.120 --> 01:30:29.200]   at least without being conscious, probably.
[01:30:29.200 --> 01:30:33.400]   - So then what is superintelligence?
[01:30:33.400 --> 01:30:35.880]   - Yeah, that would be like something that was much more,
[01:30:35.880 --> 01:30:37.560]   - Of that.
[01:30:37.560 --> 01:30:40.200]   - Had much more general cognitive capacity
[01:30:40.200 --> 01:30:41.600]   than we humans have.
[01:30:41.600 --> 01:30:45.120]   So if we talk about general superintelligence,
[01:30:45.120 --> 01:30:48.000]   it would be much faster learner,
[01:30:48.000 --> 01:30:49.640]   be able to reason much better,
[01:30:49.640 --> 01:30:53.000]   make plans that are more effective at achieving its goals,
[01:30:53.000 --> 01:30:56.880]   say in a wide range of complex, challenging environments.
[01:30:56.880 --> 01:31:00.040]   - In terms of, as we turn our eye to the idea
[01:31:00.040 --> 01:31:03.920]   of sort of existential threats from superintelligence,
[01:31:03.920 --> 01:31:07.400]   do you think superintelligence has to exist
[01:31:07.400 --> 01:31:10.680]   in the physical world or can it be digital only?
[01:31:10.680 --> 01:31:15.120]   Sort of, we think of our general intelligence as us humans,
[01:31:15.120 --> 01:31:18.520]   as an intelligence that's associated with a body
[01:31:18.520 --> 01:31:20.080]   that's able to interact with the world,
[01:31:20.080 --> 01:31:23.960]   that's able to affect the world directly with physically.
[01:31:23.960 --> 01:31:26.120]   - I mean, digital only is perfectly fine, I think.
[01:31:26.120 --> 01:31:28.920]   I mean, it's physical in the sense that obviously
[01:31:28.920 --> 01:31:32.040]   the computers and the memories are physical.
[01:31:32.040 --> 01:31:34.840]   But its capability to affect the world sort of-
[01:31:34.840 --> 01:31:35.760]   - Could be very strong,
[01:31:35.760 --> 01:31:40.240]   even if it has a limited set of actuators.
[01:31:40.240 --> 01:31:43.560]   If it can type text on the screen or something like that,
[01:31:43.560 --> 01:31:45.720]   that would be, I think, ample.
[01:31:45.720 --> 01:31:50.720]   - So in terms of the concerns of existential threat of AI,
[01:31:50.720 --> 01:31:54.800]   how can an AI system that's in the digital world
[01:31:54.800 --> 01:31:58.200]   have existential risk sort of,
[01:31:58.200 --> 01:32:01.800]   and what are the attack vectors for a digital system?
[01:32:01.800 --> 01:32:04.160]   - Well, I mean, I guess maybe to take one step back,
[01:32:04.160 --> 01:32:07.840]   so I should emphasize that I also think
[01:32:07.840 --> 01:32:10.120]   there's this huge positive potential
[01:32:10.120 --> 01:32:13.280]   from machine intelligence, including superintelligence.
[01:32:13.280 --> 01:32:18.160]   And I wanna stress that because some of my writing
[01:32:18.160 --> 01:32:20.680]   has focused on what can go wrong.
[01:32:20.680 --> 01:32:23.040]   And when I wrote the book "Superintelligence,"
[01:32:23.040 --> 01:32:28.040]   at that point, I felt that there was a kind of neglect
[01:32:28.040 --> 01:32:31.680]   of what would happen if AI succeeds
[01:32:31.680 --> 01:32:33.520]   and in particular, a need to get
[01:32:33.520 --> 01:32:36.240]   a more granular understanding of where the pitfalls are
[01:32:36.240 --> 01:32:37.440]   so we can avoid them.
[01:32:37.440 --> 01:32:43.360]   I think that since the book came out in 2014,
[01:32:43.360 --> 01:32:45.920]   there has been a much wider recognition of that.
[01:32:45.920 --> 01:32:47.840]   And a number of research groups
[01:32:47.840 --> 01:32:50.040]   are now actually working on developing,
[01:32:50.040 --> 01:32:52.640]   say, AI alignment techniques and so on and so forth.
[01:32:52.640 --> 01:32:56.720]   So I'd like, yeah, I think now it's important
[01:32:56.720 --> 01:33:01.280]   to make sure we bring back onto the table
[01:33:01.280 --> 01:33:02.320]   the upside as well.
[01:33:02.320 --> 01:33:05.800]   - And there's a little bit of a neglect now on the upside,
[01:33:05.800 --> 01:33:08.040]   which is, I mean, if you look at,
[01:33:08.040 --> 01:33:08.880]   I was talking to a friend,
[01:33:08.880 --> 01:33:11.680]   if you look at the amount of information there's available
[01:33:11.680 --> 01:33:13.720]   or people talking and people being excited
[01:33:13.720 --> 01:33:16.960]   about the positive possibilities of general intelligence,
[01:33:16.960 --> 01:33:20.400]   that's not, it's far outnumbered
[01:33:20.400 --> 01:33:22.760]   by the negative possibilities
[01:33:22.760 --> 01:33:25.200]   in terms of our public discourse.
[01:33:25.200 --> 01:33:26.840]   - Possibly, yeah.
[01:33:26.840 --> 01:33:28.880]   It's hard to measure, but--
[01:33:28.880 --> 01:33:30.920]   - What are, can you link on that for a little bit?
[01:33:30.920 --> 01:33:35.680]   What are some, to you, possible big positive impacts
[01:33:35.680 --> 01:33:38.080]   of general intelligence, superintelligence?
[01:33:38.080 --> 01:33:39.800]   - Well, I mean, so superintelligence,
[01:33:39.800 --> 01:33:42.800]   because I tend to also wanna distinguish
[01:33:42.800 --> 01:33:45.920]   these two different contexts of thinking about AI
[01:33:45.920 --> 01:33:49.200]   and AI impacts, the kind of near-term and long-term,
[01:33:49.200 --> 01:33:53.000]   if you want, both of which I think are legitimate things
[01:33:53.000 --> 01:33:58.000]   to think about, and people should discuss both of them.
[01:33:58.000 --> 01:33:59.120]   But they are different,
[01:33:59.120 --> 01:34:01.920]   and they often get mixed up,
[01:34:01.920 --> 01:34:05.000]   and then I get, you get confusion.
[01:34:05.000 --> 01:34:06.400]   Like, I think you get simultaneously,
[01:34:06.400 --> 01:34:08.440]   like maybe an overhyping of the near-term
[01:34:08.440 --> 01:34:10.160]   and an underhyping of the long-term.
[01:34:10.160 --> 01:34:12.200]   And so I think as long as we keep them apart,
[01:34:12.200 --> 01:34:15.120]   we can have like two good conversations,
[01:34:15.120 --> 01:34:17.440]   but, or we can mix them together
[01:34:17.440 --> 01:34:18.560]   and have one bad conversation.
[01:34:18.560 --> 01:34:21.640]   - Can you clarify just the two things we were talking about,
[01:34:21.640 --> 01:34:23.080]   the near-term and the long-term?
[01:34:23.080 --> 01:34:24.320]   - Yeah, and-- - What are the distinctions?
[01:34:24.320 --> 01:34:27.960]   - Well, it's a blurry distinction,
[01:34:27.960 --> 01:34:30.120]   but say the things I wrote about in this book,
[01:34:30.120 --> 01:34:33.080]   "Superintelligence," long-term,
[01:34:33.080 --> 01:34:37.440]   things people are worrying about today with,
[01:34:37.440 --> 01:34:39.920]   I don't know, algorithmic discrimination,
[01:34:39.920 --> 01:34:44.360]   or even things, self-driving cars and drones and stuff,
[01:34:44.360 --> 01:34:45.360]   more near-term.
[01:34:45.360 --> 01:34:50.120]   And then, of course, you could imagine some medium-term
[01:34:50.120 --> 01:34:51.200]   where they kind of overlap
[01:34:51.200 --> 01:34:53.160]   and one evolves into the other.
[01:34:53.160 --> 01:34:56.640]   But at any rate, I think both, yeah,
[01:34:56.640 --> 01:34:59.680]   the issues look kind of somewhat different
[01:34:59.680 --> 01:35:01.480]   depending on which of these contexts.
[01:35:01.480 --> 01:35:03.880]   - So I think it would be nice
[01:35:03.880 --> 01:35:05.600]   if we can talk about the long-term
[01:35:05.600 --> 01:35:11.600]   and think about a positive impact
[01:35:11.600 --> 01:35:15.520]   or a better world because of the existence
[01:35:15.520 --> 01:35:17.760]   of the long-term superintelligence.
[01:35:17.760 --> 01:35:19.240]   Do you have views of such a world?
[01:35:19.240 --> 01:35:22.200]   - Yeah, I mean, I guess it's a little hard to articulate
[01:35:22.200 --> 01:35:24.560]   because it seems obvious that the world
[01:35:24.560 --> 01:35:27.760]   has a lot of problems as it currently stands.
[01:35:27.760 --> 01:35:32.320]   And it's hard to think of any one of those
[01:35:32.320 --> 01:35:34.720]   which it wouldn't be useful to have
[01:35:34.720 --> 01:35:39.720]   a friendly aligned superintelligence working on.
[01:35:39.720 --> 01:35:44.920]   - So from health to the economic system
[01:35:44.920 --> 01:35:48.160]   to be able to sort of improve the investment
[01:35:48.160 --> 01:35:50.320]   and trade and foreign policy decisions,
[01:35:50.320 --> 01:35:52.080]   all that kind of stuff.
[01:35:52.080 --> 01:35:54.320]   - All that kind of stuff and a lot more.
[01:35:55.320 --> 01:35:57.960]   - I mean, what's the killer app?
[01:35:57.960 --> 01:35:59.480]   - Well, I don't think there is one.
[01:35:59.480 --> 01:36:04.160]   I think AI, especially artificial general intelligence
[01:36:04.160 --> 01:36:07.640]   is really the ultimate general purpose technology.
[01:36:07.640 --> 01:36:09.600]   So it's not that there is this one problem,
[01:36:09.600 --> 01:36:12.000]   this one area where it will have a big impact,
[01:36:12.000 --> 01:36:14.720]   but if and when it succeeds,
[01:36:14.720 --> 01:36:17.560]   it will really apply across the board
[01:36:17.560 --> 01:36:21.200]   in all fields where human creativity and intelligence
[01:36:21.200 --> 01:36:22.400]   and problem solving is useful,
[01:36:22.400 --> 01:36:24.920]   which is pretty much all fields, right?
[01:36:24.920 --> 01:36:27.920]   The thing that it would do
[01:36:27.920 --> 01:36:30.720]   is give us a lot more control over nature.
[01:36:30.720 --> 01:36:32.920]   It wouldn't automatically solve the problems
[01:36:32.920 --> 01:36:35.200]   that arise from conflict between humans,
[01:36:35.200 --> 01:36:38.080]   fundamentally political problems.
[01:36:38.080 --> 01:36:39.520]   Some subset of those might go away
[01:36:39.520 --> 01:36:42.040]   if we just had more resources and cooler tech,
[01:36:42.040 --> 01:36:46.040]   but some subset would require coordination
[01:36:46.040 --> 01:36:50.040]   that is not automatically achieved
[01:36:50.040 --> 01:36:52.840]   just by having more technological capability.
[01:36:52.840 --> 01:36:54.680]   But anything that's not of that sort,
[01:36:54.680 --> 01:36:56.920]   I think you just get like an enormous boost
[01:36:56.920 --> 01:37:00.840]   with this kind of cognitive technology
[01:37:00.840 --> 01:37:02.720]   once it goes all the way.
[01:37:02.720 --> 01:37:05.120]   Now, again, that doesn't mean I'm like thinking,
[01:37:05.120 --> 01:37:10.120]   oh, people don't recognize what's possible
[01:37:10.120 --> 01:37:11.920]   with current technology
[01:37:11.920 --> 01:37:14.000]   and like sometimes things get overhyped,
[01:37:14.000 --> 01:37:16.840]   but I mean, those are perfectly consistent views to hold
[01:37:16.840 --> 01:37:19.760]   the ultimate potential being enormous.
[01:37:19.760 --> 01:37:21.680]   And then it's a very different question
[01:37:21.680 --> 01:37:23.160]   of how far are we from that
[01:37:23.160 --> 01:37:25.320]   or what can we do with near-term technology?
[01:37:25.320 --> 01:37:26.400]   - Yeah, so what's your intuition
[01:37:26.400 --> 01:37:29.120]   about the idea of intelligence explosion?
[01:37:29.120 --> 01:37:30.160]   So there's this,
[01:37:30.160 --> 01:37:34.040]   when you start to think about that leap
[01:37:34.040 --> 01:37:36.120]   from the near term to the long term,
[01:37:36.120 --> 01:37:38.120]   the natural inclination,
[01:37:38.120 --> 01:37:40.960]   like for me sort of building machine learning systems today,
[01:37:40.960 --> 01:37:43.000]   it seems like it's a lot of work
[01:37:43.000 --> 01:37:44.880]   to get to general intelligence,
[01:37:44.880 --> 01:37:47.120]   but there's some intuition of exponential growth,
[01:37:47.120 --> 01:37:50.680]   of exponential improvement of intelligence explosion.
[01:37:50.680 --> 01:37:55.400]   Can you maybe try to elucidate,
[01:37:55.400 --> 01:37:58.920]   try to talk about what's your intuition
[01:37:58.920 --> 01:38:02.800]   about the possibility of intelligence explosion,
[01:38:02.800 --> 01:38:05.160]   that it won't be this gradual, slow process,
[01:38:05.160 --> 01:38:07.200]   there might be a phase shift?
[01:38:07.200 --> 01:38:10.920]   - Yeah, I think it's,
[01:38:10.920 --> 01:38:13.320]   we don't know how explosive it will be.
[01:38:13.320 --> 01:38:15.360]   I think for what it's worth,
[01:38:16.160 --> 01:38:19.240]   it seems fairly likely to me that at some point
[01:38:19.240 --> 01:38:21.240]   there will be some intelligence explosion,
[01:38:21.240 --> 01:38:24.280]   like some period of time where progress in AI
[01:38:24.280 --> 01:38:25.840]   becomes extremely rapid,
[01:38:25.840 --> 01:38:30.360]   roughly in the area where you might say
[01:38:30.360 --> 01:38:33.520]   it's kind of human-ish equivalent
[01:38:33.520 --> 01:38:37.320]   in core cognitive faculties,
[01:38:37.320 --> 01:38:39.880]   that the concept of human equivalent,
[01:38:39.880 --> 01:38:41.080]   like it starts to break down
[01:38:41.080 --> 01:38:42.960]   when you look too closely at it,
[01:38:42.960 --> 01:38:45.280]   and just how explosive does something have
[01:38:45.280 --> 01:38:48.920]   to be for it to be called an intelligence explosion?
[01:38:48.920 --> 01:38:50.880]   Like, does it have to be like overnight, literally,
[01:38:50.880 --> 01:38:52.320]   or a few years?
[01:38:52.320 --> 01:38:55.960]   But overall, I guess,
[01:38:55.960 --> 01:39:00.000]   if you plotted the opinions of different people in the world,
[01:39:00.000 --> 01:39:02.680]   I guess that would be somewhat more probability
[01:39:02.680 --> 01:39:05.360]   towards the intelligence explosion scenario
[01:39:05.360 --> 01:39:09.480]   than probably the average AI researcher, I guess.
[01:39:09.480 --> 01:39:12.680]   - So, and then the other part of the intelligence explosion,
[01:39:12.680 --> 01:39:15.880]   or just forget explosion, just progress,
[01:39:15.880 --> 01:39:18.320]   is once you achieve that gray area
[01:39:18.320 --> 01:39:20.320]   of human-level intelligence,
[01:39:20.320 --> 01:39:23.040]   is it obvious to you that we should be able
[01:39:23.040 --> 01:39:27.040]   to proceed beyond it to get to superintelligence?
[01:39:27.040 --> 01:39:31.040]   - Yeah, that seems, I mean, as much as any of these things
[01:39:31.040 --> 01:39:34.960]   can be obvious, given we've never had one,
[01:39:34.960 --> 01:39:36.000]   people have different views,
[01:39:36.000 --> 01:39:37.320]   smart people have different views,
[01:39:37.320 --> 01:39:40.520]   it's like there's some degree of uncertainty
[01:39:40.520 --> 01:39:43.440]   that always remains for any big, futuristic,
[01:39:43.440 --> 01:39:46.040]   philosophical, grand question,
[01:39:46.040 --> 01:39:47.880]   that just we realize humans are fallible,
[01:39:47.880 --> 01:39:49.440]   especially about these things.
[01:39:49.440 --> 01:39:52.920]   But it does seem, as far as I'm judging things
[01:39:52.920 --> 01:39:55.120]   based on my own impressions,
[01:39:55.120 --> 01:39:59.400]   it seems very unlikely that there would be a ceiling
[01:39:59.400 --> 01:40:03.960]   at or near human cognitive capacity.
[01:40:03.960 --> 01:40:06.840]   - But, and there's such a, I don't know,
[01:40:06.840 --> 01:40:08.760]   there's such a special moment.
[01:40:08.760 --> 01:40:11.400]   It's both terrifying and exciting
[01:40:11.400 --> 01:40:14.900]   to create a system that's beyond our intelligence.
[01:40:14.900 --> 01:40:18.920]   So maybe you can step back and say,
[01:40:18.920 --> 01:40:21.640]   how does that possibility make you feel,
[01:40:21.640 --> 01:40:24.520]   that we can create something,
[01:40:24.520 --> 01:40:28.280]   it feels like there's a line beyond which it steps,
[01:40:28.280 --> 01:40:31.040]   it'll be able to outsmart you,
[01:40:31.040 --> 01:40:35.520]   and therefore it feels like a step where we lose control.
[01:40:35.520 --> 01:40:39.480]   - Well, I don't think the latter follows,
[01:40:39.480 --> 01:40:41.880]   that is, you could imagine,
[01:40:41.880 --> 01:40:44.200]   and in fact, this is what a number of people
[01:40:44.200 --> 01:40:45.040]   are working towards,
[01:40:45.040 --> 01:40:48.800]   making sure that we could ultimately project
[01:40:48.800 --> 01:40:51.920]   higher levels of problem-solving ability,
[01:40:51.920 --> 01:40:54.800]   while still making sure that they are aligned,
[01:40:54.800 --> 01:40:57.500]   like they are in the service of human values.
[01:40:57.500 --> 01:41:03.640]   I mean, so losing control, I think, is not a given,
[01:41:05.040 --> 01:41:06.280]   that that would happen.
[01:41:06.280 --> 01:41:08.040]   Now you asked how it makes me feel,
[01:41:08.040 --> 01:41:10.640]   I mean, to some extent, I've lived with this for so long,
[01:41:10.640 --> 01:41:14.640]   since as long as I can remember,
[01:41:14.640 --> 01:41:16.440]   being an adult or even a teenager,
[01:41:16.440 --> 01:41:18.240]   it seemed to me obvious that at some point,
[01:41:18.240 --> 01:41:19.640]   AI will succeed.
[01:41:19.640 --> 01:41:24.640]   - And so I actually misspoke, I didn't mean control.
[01:41:24.640 --> 01:41:27.920]   I meant, because the control problem is an interesting thing,
[01:41:27.920 --> 01:41:30.720]   and I think the hope is,
[01:41:30.720 --> 01:41:33.040]   at least we should be able to maintain control
[01:41:33.040 --> 01:41:35.280]   over systems that are smarter than us,
[01:41:35.280 --> 01:41:39.620]   but we do lose our specialness.
[01:41:39.620 --> 01:41:44.480]   It's sort of, we'll lose our place
[01:41:44.480 --> 01:41:48.240]   as the smartest, coolest thing on Earth,
[01:41:48.240 --> 01:41:51.840]   and there's an ego involved with that,
[01:41:51.840 --> 01:41:55.720]   that humans aren't very good at dealing with.
[01:41:55.720 --> 01:41:59.720]   I mean, I value my intelligence as a human being.
[01:41:59.720 --> 01:42:02.280]   It seems like a big transformative step
[01:42:02.280 --> 01:42:04.560]   to realize there's something out there
[01:42:04.560 --> 01:42:05.840]   that's more intelligent.
[01:42:05.840 --> 01:42:09.600]   I mean, you don't see that as such a fundamentally--
[01:42:09.600 --> 01:42:13.160]   - I think yes, a lot, I think it would be small.
[01:42:13.160 --> 01:42:16.360]   I mean, I think there are already a lot of things out there
[01:42:16.360 --> 01:42:18.160]   that are, I mean, certainly if you think
[01:42:18.160 --> 01:42:20.440]   the universe is big, there's gonna be other civilizations
[01:42:20.440 --> 01:42:22.880]   that already have super intelligences,
[01:42:22.880 --> 01:42:26.720]   or that just naturally have brains the size of beach balls,
[01:42:26.720 --> 01:42:30.520]   and are like completely leaving us in the dust.
[01:42:30.520 --> 01:42:33.480]   And we haven't come face to face with that.
[01:42:33.480 --> 01:42:34.680]   - We haven't come face to face,
[01:42:34.680 --> 01:42:36.840]   but I mean, that's an open question,
[01:42:36.840 --> 01:42:41.720]   what would happen in a kind of post-human world,
[01:42:41.720 --> 01:42:46.720]   like how much day to day would these super intelligences
[01:42:46.720 --> 01:42:49.520]   be involved in the lives of ordinary?
[01:42:49.520 --> 01:42:52.520]   I mean, you could imagine some scenario
[01:42:52.520 --> 01:42:54.200]   where it would be more like a background thing
[01:42:54.200 --> 01:42:56.280]   that would help protect against some things,
[01:42:56.280 --> 01:42:58.880]   but you wouldn't, like there wouldn't be
[01:42:58.880 --> 01:43:02.000]   this intrusive kind of like making you feel bad
[01:43:02.000 --> 01:43:04.560]   by like making clever jokes on your expert,
[01:43:04.560 --> 01:43:05.880]   like there's like all sorts of things
[01:43:05.880 --> 01:43:07.920]   that maybe in the human context
[01:43:07.920 --> 01:43:10.600]   would feel awkward about that.
[01:43:10.600 --> 01:43:12.600]   You don't wanna be the dumbest kid in your class,
[01:43:12.600 --> 01:43:15.040]   everybody picks it, like a lot of those things
[01:43:15.040 --> 01:43:18.000]   maybe you need to abstract away from,
[01:43:18.000 --> 01:43:19.440]   if you're thinking about this context
[01:43:19.440 --> 01:43:21.880]   where we have infrastructure that is in some sense
[01:43:21.880 --> 01:43:26.160]   beyond any or all humans.
[01:43:27.240 --> 01:43:29.400]   I mean, it's a little bit like say the scientific community
[01:43:29.400 --> 01:43:32.280]   as a whole, if you think of that as a mind,
[01:43:32.280 --> 01:43:33.280]   it's a little bit of metaphor,
[01:43:33.280 --> 01:43:37.840]   but I mean, obviously it's gotta be like way more capacious
[01:43:37.840 --> 01:43:39.400]   than any individual.
[01:43:39.400 --> 01:43:42.520]   So in some sense, there is this mind like thing
[01:43:42.520 --> 01:43:47.280]   already out there that's just vastly more intelligent
[01:43:47.280 --> 01:43:49.520]   than a new individual is.
[01:43:49.520 --> 01:43:52.640]   And we think, okay, that's,
[01:43:52.640 --> 01:43:55.240]   you just accept that as a fact.
[01:43:55.240 --> 01:43:57.440]   - That's the basic fabric of our existence
[01:43:57.440 --> 01:43:58.880]   is there's a super intelligent.
[01:43:58.880 --> 01:44:00.560]   - Yeah, you get used to a lot of.
[01:44:00.560 --> 01:44:03.840]   - I mean, there's already Google and Twitter and Facebook,
[01:44:03.840 --> 01:44:08.840]   these recommender systems that are the basic fabric of our,
[01:44:08.840 --> 01:44:12.840]   I could see them becoming,
[01:44:12.840 --> 01:44:14.960]   I mean, do you think of the collective intelligence
[01:44:14.960 --> 01:44:17.160]   of these systems as already perhaps
[01:44:17.160 --> 01:44:18.960]   reaching super intelligence level?
[01:44:18.960 --> 01:44:21.840]   - Well, I mean, so here it comes to this,
[01:44:21.840 --> 01:44:24.440]   the concept of intelligence and the scale.
[01:44:25.200 --> 01:44:27.160]   And what human level means,
[01:44:27.160 --> 01:44:32.760]   the kind of vagueness and indeterminacy of those concepts
[01:44:32.760 --> 01:44:37.760]   starts to dominate how you would answer that question.
[01:44:37.760 --> 01:44:41.680]   So like, say the Google search engine
[01:44:41.680 --> 01:44:44.640]   has a very high capacity of a certain kind,
[01:44:44.640 --> 01:44:47.960]   like remembering and retrieving information,
[01:44:50.200 --> 01:44:53.880]   particularly like text or images
[01:44:53.880 --> 01:44:58.680]   that you have a kind of string, a word string key,
[01:44:58.680 --> 01:45:00.320]   like obviously superhuman at that,
[01:45:00.320 --> 01:45:05.320]   but a vast set of other things it can't even do at all,
[01:45:05.320 --> 01:45:07.400]   not just not do well.
[01:45:07.400 --> 01:45:10.880]   So you have these current AI systems
[01:45:10.880 --> 01:45:14.160]   that are superhuman in some limited domain
[01:45:14.160 --> 01:45:19.160]   and then like radically subhuman in all other domains.
[01:45:19.160 --> 01:45:22.320]   Same with a chess, like, or just a simple computer
[01:45:22.320 --> 01:45:24.280]   that can multiply really large numbers, right?
[01:45:24.280 --> 01:45:27.320]   So it's gonna have this like one spike of super intelligence
[01:45:27.320 --> 01:45:30.160]   and then a kind of a zero level of capability
[01:45:30.160 --> 01:45:32.200]   across all other cognitive fields.
[01:45:32.200 --> 01:45:35.440]   - Yeah, I don't necessarily think the generalness,
[01:45:35.440 --> 01:45:36.720]   I mean, I'm not so attached to it,
[01:45:36.720 --> 01:45:40.400]   but I could sort of, it's a gray area and it's a feeling,
[01:45:40.400 --> 01:45:44.240]   but to me, sort of alpha zero
[01:45:44.240 --> 01:45:47.560]   is somehow much more intelligent,
[01:45:47.560 --> 01:45:50.400]   much, much more intelligent than Deep Blue.
[01:45:50.400 --> 01:45:52.920]   And to say which domain,
[01:45:52.920 --> 01:45:55.080]   well, you could say, well, these are both just board games,
[01:45:55.080 --> 01:45:56.680]   they're both just able to play board games,
[01:45:56.680 --> 01:45:59.100]   who cares if they're gonna do better or not?
[01:45:59.100 --> 01:46:01.680]   But there's something about the learning, the self play--
[01:46:01.680 --> 01:46:03.760]   - The learning, yeah. - That makes it,
[01:46:03.760 --> 01:46:07.560]   crosses over into that land of intelligence
[01:46:07.560 --> 01:46:09.680]   that doesn't necessarily need to be general.
[01:46:09.680 --> 01:46:12.120]   In the same way, Google is much closer to Deep Blue
[01:46:12.120 --> 01:46:15.240]   currently in terms of its search engine
[01:46:15.240 --> 01:46:17.840]   than it is to sort of the alpha zero.
[01:46:17.840 --> 01:46:19.560]   And the moment it becomes,
[01:46:19.560 --> 01:46:21.280]   and the moment these recommender systems
[01:46:21.280 --> 01:46:24.320]   really become more like alpha zero,
[01:46:24.320 --> 01:46:27.880]   but being able to learn a lot without the constraints
[01:46:27.880 --> 01:46:31.560]   of being heavily constrained by human interaction,
[01:46:31.560 --> 01:46:34.640]   that seems like a special moment in time.
[01:46:34.640 --> 01:46:37.640]   - I mean, certainly learning ability
[01:46:37.640 --> 01:46:42.640]   seems to be an important facet of general intelligence.
[01:46:42.920 --> 01:46:45.720]   That you can take some new domain
[01:46:45.720 --> 01:46:47.040]   that you haven't seen before,
[01:46:47.040 --> 01:46:49.120]   and you weren't specifically pre-programmed for,
[01:46:49.120 --> 01:46:51.280]   and then figure out what's going on there
[01:46:51.280 --> 01:46:53.360]   and eventually become really good at it.
[01:46:53.360 --> 01:46:56.160]   So that's something alpha zero
[01:46:56.160 --> 01:46:58.880]   has much more of than Deep Blue had.
[01:46:58.880 --> 01:47:03.840]   And in fact, I mean, systems like alpha zero can learn,
[01:47:03.840 --> 01:47:05.800]   not just Go, but other,
[01:47:05.800 --> 01:47:09.200]   in fact, probably beat Deep Blue in chess and so forth.
[01:47:09.200 --> 01:47:10.040]   Right, so-- - Not just--
[01:47:10.040 --> 01:47:11.600]   - So you do see this-- - Destroy Deep Blue.
[01:47:11.600 --> 01:47:13.640]   - General, and so it matches the intuition.
[01:47:13.640 --> 01:47:15.120]   We feel it's more intelligent,
[01:47:15.120 --> 01:47:16.480]   and it also has more of this
[01:47:16.480 --> 01:47:18.440]   general purpose learning ability.
[01:47:18.440 --> 01:47:20.800]   And if we get systems
[01:47:20.800 --> 01:47:22.760]   that have even more general purpose learning ability,
[01:47:22.760 --> 01:47:24.720]   it might also trigger an even stronger intuition
[01:47:24.720 --> 01:47:28.000]   that they are actually starting to get smart.
[01:47:28.000 --> 01:47:29.520]   - So if you were to pick a future,
[01:47:29.520 --> 01:47:33.960]   what do you think a utopia looks like with AGI systems?
[01:47:33.960 --> 01:47:37.840]   Sort of, is it the neural link,
[01:47:37.840 --> 01:47:39.400]   brain computer interface world,
[01:47:39.400 --> 01:47:41.640]   where we're kind of really closely interlinked
[01:47:41.640 --> 01:47:43.600]   with AI systems?
[01:47:43.600 --> 01:47:48.080]   Is it possibly where AGI systems replace us completely
[01:47:48.080 --> 01:47:53.080]   while maintaining the values and the consciousness?
[01:47:53.080 --> 01:47:55.960]   Is it something like it's a completely invisible fabric,
[01:47:55.960 --> 01:47:58.320]   like you mentioned, a society where it's just AIDS
[01:47:58.320 --> 01:48:00.480]   and a lot of stuff that we do,
[01:48:00.480 --> 01:48:02.080]   like curing diseases and so on?
[01:48:02.080 --> 01:48:04.360]   What is utopia, if you get to pick?
[01:48:04.360 --> 01:48:05.920]   - Yeah, I mean, it's a good question,
[01:48:05.920 --> 01:48:09.040]   and a deep and difficult one.
[01:48:09.040 --> 01:48:10.320]   I'm quite interested in it.
[01:48:10.320 --> 01:48:15.000]   I don't have all the answers yet, or might never have,
[01:48:15.000 --> 01:48:18.480]   but I think there are some different observations
[01:48:18.480 --> 01:48:19.320]   one could make.
[01:48:19.320 --> 01:48:23.600]   One is if this scenario actually did come to pass,
[01:48:23.600 --> 01:48:28.600]   it would open up this vast space of possible modes of being.
[01:48:28.600 --> 01:48:33.720]   On one hand, material and resource constraints
[01:48:33.720 --> 01:48:36.120]   would just be expanded dramatically.
[01:48:36.120 --> 01:48:40.240]   So there would be a lot of, a big pie, let's say.
[01:48:40.240 --> 01:48:44.920]   Also, it would enable us to do things,
[01:48:44.920 --> 01:48:48.760]   including to ourselves,
[01:48:48.760 --> 01:48:51.680]   or like that it would just open up
[01:48:51.680 --> 01:48:54.680]   this much larger design space and option space
[01:48:54.680 --> 01:48:58.320]   than we have ever had access to in human history.
[01:48:58.320 --> 01:49:01.200]   So I think two things follow from that.
[01:49:01.200 --> 01:49:04.360]   One is that we probably would need to make
[01:49:04.360 --> 01:49:09.360]   a fairly fundamental rethink of what ultimately we value.
[01:49:09.360 --> 01:49:11.880]   Like think things through more from first principles.
[01:49:11.880 --> 01:49:13.720]   The context would be so different from the familiar
[01:49:13.720 --> 01:49:16.240]   that we could have just take what we've always been doing,
[01:49:16.240 --> 01:49:19.520]   and then like, oh, well, we have this cleaning robot
[01:49:19.520 --> 01:49:22.960]   that cleans the dishes in the sink,
[01:49:22.960 --> 01:49:24.600]   and a few other small things.
[01:49:24.600 --> 01:49:26.920]   Like, I think we would have to go back to first principles.
[01:49:26.920 --> 01:49:29.000]   - So even from the individual level,
[01:49:29.000 --> 01:49:31.640]   go back to the first principles of what is the meaning
[01:49:31.640 --> 01:49:34.120]   of life, what is happiness, what is fulfillment?
[01:49:34.120 --> 01:49:35.440]   - Yeah.
[01:49:35.440 --> 01:49:39.840]   And then also connected to this large space of resources
[01:49:39.840 --> 01:49:43.200]   is that it would be possible,
[01:49:43.200 --> 01:49:48.200]   and I think something we should aim for is to do well
[01:49:48.200 --> 01:49:53.960]   by the lights of more than one value system.
[01:49:53.960 --> 01:50:00.120]   That is, we wouldn't have to choose only one value system.
[01:50:00.120 --> 01:50:05.120]   We wouldn't have to choose only one value criterion
[01:50:05.120 --> 01:50:08.680]   and say, we're gonna do something that scores really high
[01:50:08.680 --> 01:50:13.080]   on the metric of, say, hedonism,
[01:50:13.080 --> 01:50:17.640]   and then is like a zero by other criteria,
[01:50:17.640 --> 01:50:20.280]   like kind of wire headed brain Cinnabat,
[01:50:20.280 --> 01:50:22.840]   and it's like a lot of pleasure, that's good,
[01:50:22.840 --> 01:50:24.920]   but then like no beauty, no achievement,
[01:50:24.920 --> 01:50:27.520]   or pick it up.
[01:50:27.520 --> 01:50:30.960]   I think to some significant, not unlimited sense,
[01:50:30.960 --> 01:50:32.640]   but a significant sense,
[01:50:32.640 --> 01:50:36.160]   it would be possible to do very well by many criteria.
[01:50:36.160 --> 01:50:41.160]   Like maybe you could get like 98% of the best
[01:50:41.160 --> 01:50:43.960]   according to several criteria at the same time,
[01:50:43.960 --> 01:50:48.960]   given this great expansion of the option space.
[01:50:48.960 --> 01:50:50.720]   And so-
[01:50:50.720 --> 01:50:52.600]   - So have competing value systems,
[01:50:52.600 --> 01:50:57.040]   competing criteria as a sort of forever,
[01:50:57.040 --> 01:51:00.160]   just like our Democrat versus Republican,
[01:51:00.160 --> 01:51:02.640]   there seems to be this always multiple parties
[01:51:02.640 --> 01:51:05.520]   that are useful for our progress in society,
[01:51:05.520 --> 01:51:08.120]   even though it might seem dysfunctional inside the moment,
[01:51:08.120 --> 01:51:11.200]   but having the multiple value systems
[01:51:11.200 --> 01:51:16.200]   seems to be beneficial for, I guess, a balance of power.
[01:51:16.200 --> 01:51:19.120]   - So that's, yeah, not exactly what I have in mind,
[01:51:19.120 --> 01:51:21.000]   that it's, well, although it can be,
[01:51:21.000 --> 01:51:22.560]   maybe in an indirect way it is,
[01:51:23.560 --> 01:51:27.920]   but that if you had the chance to do something
[01:51:27.920 --> 01:51:32.720]   that scored well on several different metrics,
[01:51:32.720 --> 01:51:34.640]   our first instinct should be to do that
[01:51:34.640 --> 01:51:38.040]   rather than immediately leap to the thing,
[01:51:38.040 --> 01:51:40.800]   which ones of these value systems are we gonna screw over?
[01:51:40.800 --> 01:51:42.200]   Like I think our first,
[01:51:42.200 --> 01:51:44.440]   let's first try to do very well by all of them.
[01:51:44.440 --> 01:51:47.120]   Then it might be that you can't get 100% of all,
[01:51:47.120 --> 01:51:49.960]   and you would have to then like have the hard conversation
[01:51:49.960 --> 01:51:51.880]   about which one will only get 97%.
[01:51:51.880 --> 01:51:53.360]   - There you go, there's my cynicism
[01:51:53.360 --> 01:51:56.200]   that all of existence is always a trade-off.
[01:51:56.200 --> 01:51:58.840]   But you're saying, maybe it's not such a bad trade-off.
[01:51:58.840 --> 01:52:00.120]   Let's first at least try to--
[01:52:00.120 --> 01:52:02.320]   - Well, this would be a distinctive context
[01:52:02.320 --> 01:52:07.320]   in which at least some of the constraints would be removed.
[01:52:07.320 --> 01:52:08.800]   - I'll leave you there.
[01:52:08.800 --> 01:52:10.440]   - So there's probably still be trade-offs in the end.
[01:52:10.440 --> 01:52:11.960]   It's just that we should first make sure
[01:52:11.960 --> 01:52:15.960]   we at least take advantage of this abundance.
[01:52:15.960 --> 01:52:19.040]   So in terms of thinking about this, like, yeah,
[01:52:19.280 --> 01:52:24.280]   one should think, I think in this kind of frame of mind
[01:52:24.280 --> 01:52:29.640]   of generosity and inclusiveness to different value systems
[01:52:29.640 --> 01:52:33.560]   and see how far one can get there first.
[01:52:33.560 --> 01:52:37.760]   And I think one could do something that would be very good
[01:52:37.760 --> 01:52:41.760]   according to many different criteria.
[01:52:41.760 --> 01:52:46.120]   - We kind of talked about AGI fundamentally transforming
[01:52:46.120 --> 01:52:51.120]   the value system of our existence, the meaning of life.
[01:52:51.120 --> 01:52:55.280]   But today, what do you think is the meaning of life?
[01:52:55.280 --> 01:52:58.560]   The silliest or perhaps the biggest question,
[01:52:58.560 --> 01:52:59.560]   what's the meaning of life?
[01:52:59.560 --> 01:53:01.920]   What's the meaning of existence?
[01:53:01.920 --> 01:53:04.800]   What makes, what gives your life fulfillment,
[01:53:04.800 --> 01:53:07.340]   purpose, happiness, meaning?
[01:53:07.340 --> 01:53:10.600]   - Yeah, I think these are, I guess,
[01:53:10.600 --> 01:53:14.760]   a bunch of different but related questions in there
[01:53:14.760 --> 01:53:15.880]   that one can ask.
[01:53:15.880 --> 01:53:19.280]   - Happiness, meaning, they're all different.
[01:53:19.280 --> 01:53:21.120]   - I mean, like you could imagine somebody
[01:53:21.120 --> 01:53:22.320]   getting a lot of happiness from something
[01:53:22.320 --> 01:53:24.920]   that they didn't think was meaningful.
[01:53:24.920 --> 01:53:29.680]   Like mindless, like watching reruns
[01:53:29.680 --> 01:53:31.440]   of some television series while eating junk food.
[01:53:31.440 --> 01:53:33.320]   Like maybe some people that gives pleasure,
[01:53:33.320 --> 01:53:35.760]   but they wouldn't think it had a lot of meaning.
[01:53:35.760 --> 01:53:38.440]   Whereas conversely, something that might be quite
[01:53:38.440 --> 01:53:41.320]   loaded with meaning might not be very fun always.
[01:53:41.320 --> 01:53:42.920]   Like some difficult achievement
[01:53:42.920 --> 01:53:44.360]   that really helps a lot of people,
[01:53:44.360 --> 01:53:47.720]   maybe requires self-sacrifice and hard work.
[01:53:47.720 --> 01:53:52.080]   And so these things can, I think, come apart,
[01:53:52.080 --> 01:53:57.200]   which is something to bear in mind also
[01:53:57.200 --> 01:54:01.000]   when if you're thinking about these utopia questions
[01:54:01.000 --> 01:54:06.000]   that you might actually start to do
[01:54:06.000 --> 01:54:07.640]   some constructive thinking about that.
[01:54:07.640 --> 01:54:10.920]   You might have to isolate and distinguish
[01:54:10.920 --> 01:54:13.240]   these different kinds of things
[01:54:13.240 --> 01:54:15.360]   that might be valuable in different ways.
[01:54:15.360 --> 01:54:18.680]   Make sure you can sort of clearly perceive each one of them.
[01:54:18.680 --> 01:54:22.040]   And then you can think about how you can combine them.
[01:54:22.040 --> 01:54:24.840]   - And just as you said, hopefully come up with a way
[01:54:24.840 --> 01:54:27.480]   to maximize all of them together.
[01:54:27.480 --> 01:54:29.840]   - Yeah, or at least get, I mean, maximize
[01:54:29.840 --> 01:54:33.760]   or get like a very high score on a wide range of them,
[01:54:33.760 --> 01:54:35.040]   even if not literally all.
[01:54:35.040 --> 01:54:36.440]   You can always come up with values
[01:54:36.440 --> 01:54:39.240]   that are exactly opposed to one another, right?
[01:54:39.240 --> 01:54:43.440]   But I think for many values, they are kind of opposed with,
[01:54:43.440 --> 01:54:47.400]   if you place them within a certain dimensionality
[01:54:47.400 --> 01:54:50.200]   of your space, like there are shapes that are kind of,
[01:54:50.200 --> 01:54:55.280]   you can't untangle like in a given dimensionality,
[01:54:55.280 --> 01:54:56.640]   but if you start adding dimensions,
[01:54:56.640 --> 01:54:59.000]   then it might in many cases just be that they are easy
[01:54:59.000 --> 01:55:02.000]   to pull apart and you could.
[01:55:02.000 --> 01:55:04.200]   So we'll see how much space there is for that.
[01:55:04.200 --> 01:55:06.600]   But I think that there could be a lot
[01:55:06.600 --> 01:55:09.200]   in this context of radical abundance.
[01:55:09.200 --> 01:55:10.640]   If ever we get to that.
[01:55:10.640 --> 01:55:15.320]   - I don't think there's a better way to end it, Nick.
[01:55:15.320 --> 01:55:18.160]   You've influenced a huge number of people
[01:55:18.160 --> 01:55:20.520]   to work on what could very well be
[01:55:20.520 --> 01:55:22.520]   the most important problems of our time.
[01:55:22.520 --> 01:55:23.520]   So it's a huge honor.
[01:55:23.520 --> 01:55:24.360]   Thank you so much for talking to me.
[01:55:24.360 --> 01:55:25.560]   - Well, thank you for coming by, Lex.
[01:55:25.560 --> 01:55:26.440]   That was fun.
[01:55:26.440 --> 01:55:27.960]   Thank you.
[01:55:27.960 --> 01:55:29.520]   - Thanks for listening to this conversation
[01:55:29.520 --> 01:55:30.600]   with Nick Bostrom.
[01:55:30.600 --> 01:55:33.680]   And thank you to our presenting sponsor, Cash App.
[01:55:33.680 --> 01:55:35.440]   Please consider supporting the podcast
[01:55:35.440 --> 01:55:39.960]   by downloading Cash App and using code LexPodcast.
[01:55:39.960 --> 01:55:42.400]   If you enjoy this podcast, subscribe on YouTube,
[01:55:42.400 --> 01:55:44.640]   review it with Five Stars on Apple Podcast,
[01:55:44.640 --> 01:55:45.960]   support it on Patreon,
[01:55:45.960 --> 01:55:49.320]   or simply connect with me on Twitter @LexFriedman.
[01:55:49.320 --> 01:55:53.920]   And now let me leave you with some words from Nick Bostrom.
[01:55:53.920 --> 01:55:57.840]   "Our approach to existential risks
[01:55:57.840 --> 01:56:00.400]   "cannot be one of trial and error.
[01:56:00.400 --> 01:56:03.000]   "There's no opportunity to learn from errors.
[01:56:03.000 --> 01:56:05.720]   "The reactive approach, see what happens,
[01:56:05.720 --> 01:56:10.080]   "limit damages, and learn from experience, is unworkable.
[01:56:10.080 --> 01:56:12.880]   "Rather, we must take a proactive approach.
[01:56:12.880 --> 01:56:16.320]   "This requires foresight to anticipate new types of threats
[01:56:16.320 --> 01:56:19.600]   "and a willingness to take decisive, preventative action
[01:56:19.600 --> 01:56:24.280]   "and to bear the costs, moral and economic, of such actions."
[01:56:24.280 --> 01:56:29.200]   Thank you for listening and hope to see you next time.
[01:56:29.200 --> 01:56:31.800]   (upbeat music)
[01:56:31.800 --> 01:56:34.400]   (upbeat music)
[01:56:34.400 --> 01:56:44.400]   [BLANK_AUDIO]

