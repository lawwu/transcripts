
[00:00:00.000 --> 00:00:06.000]   Welcome back everyone.
[00:00:06.000 --> 00:00:09.160]   This is part five in our series on methods and metrics.
[00:00:09.160 --> 00:00:11.180]   This will be a short, focused,
[00:00:11.180 --> 00:00:15.080]   technical screencast about data organization.
[00:00:15.080 --> 00:00:18.140]   Within the field of NLP and indeed all of AI,
[00:00:18.140 --> 00:00:20.440]   we're all accustomed to having datasets that have
[00:00:20.440 --> 00:00:23.440]   train, dev, and test portions.
[00:00:23.440 --> 00:00:27.480]   This is common in our largest publicly available datasets.
[00:00:27.480 --> 00:00:30.760]   It does presuppose a fairly large dataset,
[00:00:30.760 --> 00:00:32.440]   and that's in virtue of the fact that we
[00:00:32.440 --> 00:00:34.480]   hardly ever get to use the test set.
[00:00:34.480 --> 00:00:37.080]   As I've said repeatedly in the field,
[00:00:37.080 --> 00:00:40.020]   we're all on the honor system to do test set runs
[00:00:40.020 --> 00:00:43.400]   only when all of system development is complete.
[00:00:43.400 --> 00:00:46.480]   That test set is under lock and key most of the time,
[00:00:46.480 --> 00:00:49.320]   and that does mean that it goes hardly ever
[00:00:49.320 --> 00:00:53.320]   used during the course of scientific inquiry.
[00:00:53.320 --> 00:00:57.080]   Having these fixed test sets is good because it ensures
[00:00:57.080 --> 00:00:58.480]   consistent evaluations.
[00:00:58.480 --> 00:01:01.200]   It's much easier to compare two models if they were
[00:01:01.200 --> 00:01:04.240]   evaluated according to exactly the same protocol.
[00:01:04.240 --> 00:01:06.120]   But it does have a downside that
[00:01:06.120 --> 00:01:08.520]   because we always use the same test set,
[00:01:08.520 --> 00:01:12.660]   we get a community-wide hill climbing on that test set as
[00:01:12.660 --> 00:01:16.000]   later papers learn indirect lessons about
[00:01:16.000 --> 00:01:18.840]   the test set from earlier papers in the literature,
[00:01:18.840 --> 00:01:21.320]   and that ends up inflating performance.
[00:01:21.320 --> 00:01:23.040]   But on balance, I think
[00:01:23.040 --> 00:01:26.920]   train-dev-test has been good for the field of NLP.
[00:01:26.920 --> 00:01:30.360]   However, if you're doing work outside of NLP,
[00:01:30.360 --> 00:01:32.040]   you might encounter datasets that
[00:01:32.040 --> 00:01:34.480]   don't have predefined splits.
[00:01:34.480 --> 00:01:36.340]   That could be because they're small or
[00:01:36.340 --> 00:01:38.280]   because they're from a different field.
[00:01:38.280 --> 00:01:40.000]   For example, in psychology,
[00:01:40.000 --> 00:01:43.340]   you hardly ever get this train-dev-test methodology,
[00:01:43.340 --> 00:01:45.200]   and so datasets from that field,
[00:01:45.200 --> 00:01:46.760]   which you might want to make use of,
[00:01:46.760 --> 00:01:50.280]   are unlikely to have the predefined splits.
[00:01:50.280 --> 00:01:53.240]   This poses a challenge for assessment,
[00:01:53.240 --> 00:01:55.900]   because as I said, for robust comparisons,
[00:01:55.900 --> 00:01:58.480]   we really want to have all our models run using
[00:01:58.480 --> 00:02:01.200]   the same assessment regime and that means
[00:02:01.200 --> 00:02:04.960]   using the same splits for all of your experimental runs.
[00:02:04.960 --> 00:02:06.760]   Now, for large datasets,
[00:02:06.760 --> 00:02:08.860]   you could just impose the splits yourself
[00:02:08.860 --> 00:02:11.160]   and then use them for the entire project.
[00:02:11.160 --> 00:02:14.200]   That will simplify your experimental design,
[00:02:14.200 --> 00:02:15.960]   and it will also reduce the amount of
[00:02:15.960 --> 00:02:18.280]   hyperparameter optimization that you need to do.
[00:02:18.280 --> 00:02:19.880]   If you can get away with it,
[00:02:19.880 --> 00:02:22.680]   just impose the splits and maybe bake that into
[00:02:22.680 --> 00:02:26.060]   how people think about the dataset in NLP now.
[00:02:26.060 --> 00:02:28.160]   But for small datasets,
[00:02:28.160 --> 00:02:30.720]   imposing these splits might simply leave you with
[00:02:30.720 --> 00:02:32.380]   too little data and that could lead to
[00:02:32.380 --> 00:02:35.880]   very highly variable system assessments.
[00:02:35.880 --> 00:02:39.640]   Either you're training on too few examples to have a lot of
[00:02:39.640 --> 00:02:42.560]   examples for assessment and that causes some noise,
[00:02:42.560 --> 00:02:46.020]   or you're leaving too few examples to assess on,
[00:02:46.020 --> 00:02:47.900]   and then the resulting assessments
[00:02:47.900 --> 00:02:50.240]   are very noisy and highly variable.
[00:02:50.240 --> 00:02:51.600]   It's hard to get that right.
[00:02:51.600 --> 00:02:53.000]   In these situations,
[00:02:53.000 --> 00:02:56.940]   I think what you should do is think about cross-validation.
[00:02:56.940 --> 00:03:00.240]   In cross-validation, we take a set of examples
[00:03:00.240 --> 00:03:04.260]   and partition them into two or more train test splits.
[00:03:04.260 --> 00:03:08.560]   We run a bunch of system evaluations and then we aggregate over
[00:03:08.560 --> 00:03:11.860]   those scores in some way usually by taking an average and we
[00:03:11.860 --> 00:03:15.880]   report that as a measure of system performance.
[00:03:15.880 --> 00:03:18.740]   There are two broad methods that you
[00:03:18.740 --> 00:03:20.960]   can use for this kind of cross-validation.
[00:03:20.960 --> 00:03:22.560]   The first is very simple.
[00:03:22.560 --> 00:03:24.920]   I've called it random splits here.
[00:03:24.920 --> 00:03:27.160]   The idea is for k splits,
[00:03:27.160 --> 00:03:28.240]   that is k times,
[00:03:28.240 --> 00:03:31.720]   you shuffle your dataset and then you split it into
[00:03:31.720 --> 00:03:34.160]   T percent train and usually one minus
[00:03:34.160 --> 00:03:36.560]   T percent test to use all the data,
[00:03:36.560 --> 00:03:38.800]   and then you conduct an evaluation.
[00:03:38.800 --> 00:03:41.320]   You repeat that k times and you get a vector of
[00:03:41.320 --> 00:03:44.640]   scores and then you aggregate those scores in some way.
[00:03:44.640 --> 00:03:46.680]   Usually, you would take an average,
[00:03:46.680 --> 00:03:48.760]   but you could also think about an average plus
[00:03:48.760 --> 00:03:51.520]   a confidence interval or some kind of stats test that would
[00:03:51.520 --> 00:03:56.020]   tell you about how two systems differ according to this regime.
[00:03:56.020 --> 00:03:58.380]   Usually, but not always,
[00:03:58.380 --> 00:04:01.160]   you want these splits to be stratified in the sense that
[00:04:01.160 --> 00:04:03.840]   the train and test splits have approximately
[00:04:03.840 --> 00:04:06.280]   the same distribution over the classes or
[00:04:06.280 --> 00:04:10.720]   output values to give you consistent evaluations.
[00:04:10.720 --> 00:04:14.680]   Trade-offs. Well, the good part of this is that you can create
[00:04:14.680 --> 00:04:17.880]   as many experiments as you want without having
[00:04:17.880 --> 00:04:21.320]   this impact the ratio of training to testing examples.
[00:04:21.320 --> 00:04:26.600]   The value of k here is separate from the value of T and one minus T.
[00:04:26.600 --> 00:04:30.600]   What that means is that you can run lots of experiments and
[00:04:30.600 --> 00:04:32.200]   independently set the number of
[00:04:32.200 --> 00:04:35.320]   train examples or the number of assessment examples.
[00:04:35.320 --> 00:04:37.240]   That's certainly to the good.
[00:04:37.240 --> 00:04:40.080]   The bad here is that you don't get a guarantee that
[00:04:40.080 --> 00:04:42.320]   every example will be used the same number of
[00:04:42.320 --> 00:04:44.480]   times for training and testing because of
[00:04:44.480 --> 00:04:46.560]   the shuffle stuff that you do here,
[00:04:46.560 --> 00:04:48.720]   introducing a lot of randomness.
[00:04:48.720 --> 00:04:51.440]   Frankly, for reasonably sized datasets,
[00:04:51.440 --> 00:04:53.960]   this bad here is very minimal indeed.
[00:04:53.960 --> 00:04:56.960]   I really like random splits and I would worry about
[00:04:56.960 --> 00:05:01.960]   the bad only in situations in which you have a very small dataset.
[00:05:01.960 --> 00:05:04.640]   Finally, Scikit-learn has lots of
[00:05:04.640 --> 00:05:07.640]   utilities for doing this random split stuff.
[00:05:07.640 --> 00:05:09.320]   I would encourage you to use them.
[00:05:09.320 --> 00:05:10.560]   They've worked them out,
[00:05:10.560 --> 00:05:14.400]   nice reliable code that will help you with these protocols.
[00:05:14.400 --> 00:05:17.440]   Now, in some situations,
[00:05:17.440 --> 00:05:21.200]   you might instead want to do what's called k-fold cross-validation,
[00:05:21.200 --> 00:05:22.640]   and this is somewhat different.
[00:05:22.640 --> 00:05:26.600]   Let's imagine we have a dataset and we have divided it ahead of time into
[00:05:26.600 --> 00:05:30.640]   three folds that is three disjoint parts.
[00:05:30.640 --> 00:05:34.560]   Then we have experiment 1 where we have our test fold is
[00:05:34.560 --> 00:05:38.280]   fold 1 and we train on folds 2 and 3 together.
[00:05:38.280 --> 00:05:45.880]   Experiment 2, we test on fold 2 and train on 1 and 3.
[00:05:45.880 --> 00:05:49.840]   For experiment 3, we test on fold 3 and train on 1 and 2.
[00:05:49.840 --> 00:05:51.880]   We've covered all of the combinations.
[00:05:51.880 --> 00:05:54.920]   Our three folds give us three separate experiments,
[00:05:54.920 --> 00:05:59.520]   and then we aggregate results across all three of the experiments.
[00:05:59.520 --> 00:06:01.480]   Let's think about our trade-offs again.
[00:06:01.480 --> 00:06:05.320]   The good part is that every example appears in a train set
[00:06:05.320 --> 00:06:09.440]   exactly k minus 1 times and in a test set exactly 1.
[00:06:09.440 --> 00:06:13.840]   We get a nice pristine experimental setting in that regard.
[00:06:13.840 --> 00:06:17.700]   The bad though is really bad to my mind.
[00:06:17.700 --> 00:06:22.160]   The size of k determines the size of the train set.
[00:06:22.160 --> 00:06:24.360]   If I do three folds cross-validation,
[00:06:24.360 --> 00:06:28.920]   I get to train on 67 percent of the data and test on 33.
[00:06:28.920 --> 00:06:31.640]   But if I want to do 10 folds cross-validation,
[00:06:31.640 --> 00:06:34.960]   now I have to train on 90 percent and test on 10.
[00:06:34.960 --> 00:06:37.880]   It feels like the number of experiments has gotten
[00:06:37.880 --> 00:06:40.560]   problematically entwined with
[00:06:40.560 --> 00:06:43.080]   the percentage of train and test that I want to have,
[00:06:43.080 --> 00:06:44.320]   and that's really problematic.
[00:06:44.320 --> 00:06:46.680]   You might want to have a lot of folds,
[00:06:46.680 --> 00:06:48.340]   that is a lot of experiments,
[00:06:48.340 --> 00:06:52.680]   but nonetheless train on only 80 percent of the data in each case.
[00:06:52.680 --> 00:06:54.880]   That leads me to prefer
[00:06:54.880 --> 00:06:57.800]   the random splits approach in almost all settings,
[00:06:57.800 --> 00:07:01.400]   because the bad there was relatively small relative to
[00:07:01.400 --> 00:07:03.820]   the confound that this introduces for
[00:07:03.820 --> 00:07:06.080]   k-folds cross-validation.
[00:07:06.080 --> 00:07:09.160]   Finally, I'll just note that Scikit again has you covered.
[00:07:09.160 --> 00:07:11.560]   They have lots of great utilities for doing
[00:07:11.560 --> 00:07:14.720]   this k-folds cross-validation in various ways.
[00:07:14.720 --> 00:07:17.000]   Do make use of them to make sure that
[00:07:17.000 --> 00:07:20.080]   your protocols are the ones that you wanted.
[00:07:20.080 --> 00:07:30.080]   [BLANK_AUDIO]

