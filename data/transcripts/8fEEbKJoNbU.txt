
[00:00:00.000 --> 00:00:02.920]   The following is a conversation with Guillaume Verdun,
[00:00:02.920 --> 00:00:05.680]   the man behind the previously anonymous account
[00:00:05.680 --> 00:00:09.040]   BasedBefJezos on X.
[00:00:09.040 --> 00:00:12.280]   These two identities were merged by a doxing article
[00:00:12.280 --> 00:00:16.240]   in Forbes titled, "Who is BasedBefJezos?
[00:00:16.240 --> 00:00:19.960]   The leader of the tech elite's EAC movement."
[00:00:19.960 --> 00:00:22.120]   So let me describe these two identities
[00:00:22.120 --> 00:00:25.800]   that coexist in the mind of one human.
[00:00:25.800 --> 00:00:30.160]   Identity number one, Guillaume, is a physicist,
[00:00:30.160 --> 00:00:30.980]   applied mathematician,
[00:00:30.980 --> 00:00:33.560]   and quantum machine learning researcher and engineer,
[00:00:33.560 --> 00:00:36.120]   receiving his PhD in quantum machine learning,
[00:00:36.120 --> 00:00:38.480]   working at Google on quantum computing,
[00:00:38.480 --> 00:00:42.160]   and finally launching his own company called Xtropic
[00:00:42.160 --> 00:00:44.780]   that seeks to build physics-based computing hardware
[00:00:44.780 --> 00:00:47.000]   for generative AI.
[00:00:47.000 --> 00:00:50.960]   Identity number two, BevJezos on X,
[00:00:50.960 --> 00:00:54.720]   is the creator of the effective accelerationism movement,
[00:00:54.720 --> 00:00:57.960]   often abbreviated as EAC,
[00:00:57.960 --> 00:01:01.120]   that advocates for propelling rapid technological progress
[00:01:01.120 --> 00:01:04.560]   as the ethically optimal course of action for humanity.
[00:01:04.560 --> 00:01:08.880]   For example, his proponents believe that progress in AI
[00:01:08.880 --> 00:01:13.320]   is a great social equalizer, which should be pushed forward.
[00:01:13.320 --> 00:01:16.840]   EAC followers see themselves as a counterweight
[00:01:16.840 --> 00:01:20.120]   to the cautious view that AI is highly unpredictable,
[00:01:20.120 --> 00:01:22.940]   potentially dangerous, and needs to be regulated.
[00:01:23.920 --> 00:01:25.840]   They often give their opponents the labels
[00:01:25.840 --> 00:01:30.840]   of "doomers" or "decells," short for deceleration.
[00:01:30.840 --> 00:01:36.840]   As Bev himself put it, EAC is a memetic optimism virus.
[00:01:36.840 --> 00:01:39.440]   The style of communication of this movement
[00:01:39.440 --> 00:01:43.400]   leans always toward the memes and the lols,
[00:01:43.400 --> 00:01:46.400]   but there is an intellectual foundation
[00:01:46.400 --> 00:01:49.040]   that we explore in this conversation.
[00:01:49.040 --> 00:01:51.360]   Now, speaking of the meme,
[00:01:51.360 --> 00:01:56.160]   I am, too, a kind of aspiring connoisseur of the absurd.
[00:01:56.160 --> 00:01:59.760]   It is not an accident that I spoke to Jeff Bezos
[00:01:59.760 --> 00:02:03.960]   and BevJezos back to back.
[00:02:03.960 --> 00:02:06.160]   As we talk about, Bev admires Jeff
[00:02:06.160 --> 00:02:08.640]   as one of the most important humans alive,
[00:02:08.640 --> 00:02:11.680]   and I admire the beautiful absurdity
[00:02:11.680 --> 00:02:12.920]   and the humor of it all.
[00:02:12.920 --> 00:02:16.220]   This is the Lex Friedman Podcast.
[00:02:16.220 --> 00:02:18.180]   To support it, please check out our sponsors
[00:02:18.180 --> 00:02:19.360]   in the description.
[00:02:19.360 --> 00:02:22.680]   And now, dear friends, here's Guillaume Verdun.
[00:02:22.680 --> 00:02:26.200]   Let's get the facts of identity down first.
[00:02:26.200 --> 00:02:30.160]   Your name is Guillaume Verdun, Gil,
[00:02:30.160 --> 00:02:32.280]   but you're also behind the anonymous account
[00:02:32.280 --> 00:02:35.200]   on X called BasedBevJezos.
[00:02:35.200 --> 00:02:36.920]   So, first, Guillaume Verdun,
[00:02:36.920 --> 00:02:40.400]   you're a quantum computing guy, physicist,
[00:02:40.400 --> 00:02:43.360]   applied mathematician, and then BasedBevJezos
[00:02:43.360 --> 00:02:48.000]   is basically a meme account that started a movement
[00:02:48.000 --> 00:02:50.120]   with a philosophy behind it.
[00:02:50.120 --> 00:02:53.720]   So maybe just can you linger on who these people are
[00:02:53.720 --> 00:02:56.660]   in terms of characters, in terms of communication styles,
[00:02:56.660 --> 00:02:58.840]   in terms of philosophies?
[00:02:58.840 --> 00:03:01.960]   - I mean, with my main identity, I guess,
[00:03:01.960 --> 00:03:05.240]   ever since I was a kid, I wanted to figure out
[00:03:05.240 --> 00:03:08.000]   a theory of everything to understand the universe,
[00:03:08.000 --> 00:03:13.000]   and that path led me to theoretical physics eventually,
[00:03:13.000 --> 00:03:17.680]   trying to answer the big questions of why are we here,
[00:03:17.680 --> 00:03:19.360]   where are we going, right?
[00:03:19.360 --> 00:03:23.640]   And that led me to study information theory
[00:03:23.640 --> 00:03:27.120]   and try to understand physics
[00:03:27.120 --> 00:03:29.440]   from the lens of information theory,
[00:03:29.440 --> 00:03:34.000]   understand the universe as one big computation.
[00:03:34.000 --> 00:03:39.000]   And essentially, after reaching a certain level,
[00:03:39.000 --> 00:03:42.520]   studying black hole physics,
[00:03:42.520 --> 00:03:47.280]   I realized that I wanted to not only understand
[00:03:47.280 --> 00:03:51.780]   how the universe computes, but sort of compute like nature,
[00:03:51.780 --> 00:03:56.700]   and figure out how to build and apply computers
[00:03:56.700 --> 00:04:01.160]   that are inspired by nature, so physics-based computers.
[00:04:01.160 --> 00:04:04.960]   And that sort of brought me to quantum computing
[00:04:04.960 --> 00:04:09.960]   as a field of study to, first of all, simulate nature.
[00:04:09.960 --> 00:04:14.980]   And in my work, it was to learn representations of nature
[00:04:14.980 --> 00:04:17.400]   that can run on such computers.
[00:04:17.400 --> 00:04:22.400]   So if you have AI representations that think like nature,
[00:04:22.400 --> 00:04:28.680]   then they'll be able to more accurately represent it.
[00:04:28.680 --> 00:04:32.280]   At least that was the thesis that brought me
[00:04:32.280 --> 00:04:34.880]   to be an early player in the field
[00:04:34.880 --> 00:04:37.000]   called quantum machine learning, right?
[00:04:37.000 --> 00:04:40.640]   So how to do machine learning on quantum computers,
[00:04:41.840 --> 00:04:46.660]   and really sort of extend notions of intelligence
[00:04:46.660 --> 00:04:47.860]   to the quantum realm.
[00:04:47.860 --> 00:04:51.780]   So how do you capture and understand
[00:04:51.780 --> 00:04:54.540]   quantum mechanical data from our world, right?
[00:04:54.540 --> 00:04:57.860]   And how do you learn quantum mechanical representations
[00:04:57.860 --> 00:04:59.060]   of our world?
[00:04:59.060 --> 00:05:03.340]   On what kind of computer do you run these representations
[00:05:03.340 --> 00:05:04.220]   and train them?
[00:05:04.220 --> 00:05:05.660]   How do you do so?
[00:05:05.660 --> 00:05:08.260]   And so that's really sort of the questions
[00:05:08.260 --> 00:05:10.100]   I was looking to answer,
[00:05:10.100 --> 00:05:13.120]   because ultimately I had a sort of crisis of faith.
[00:05:13.120 --> 00:05:17.580]   Originally I wanted to figure out,
[00:05:17.580 --> 00:05:20.940]   as every physicist does at the beginning of their career,
[00:05:20.940 --> 00:05:24.200]   a few equations that describe the whole universe, right?
[00:05:24.200 --> 00:05:27.420]   And sort of be the hero of the story there.
[00:05:27.420 --> 00:05:32.700]   But eventually I realized that actually augmenting ourselves
[00:05:32.700 --> 00:05:35.900]   with machines, augmenting our ability to perceive,
[00:05:35.900 --> 00:05:38.220]   predict and control our world with machines
[00:05:38.220 --> 00:05:40.140]   is the path forward, right?
[00:05:40.140 --> 00:05:43.020]   And that's what got me to leave theoretical physics
[00:05:43.020 --> 00:05:46.500]   and go into quantum computing and quantum machine learning.
[00:05:46.500 --> 00:05:49.080]   And during those years,
[00:05:49.080 --> 00:05:53.360]   I thought that there was still a piece missing.
[00:05:53.360 --> 00:05:57.180]   There was a piece of our understanding of the world
[00:05:57.180 --> 00:06:01.460]   and our way to compute and our way to think about the world.
[00:06:01.460 --> 00:06:06.060]   And if you look at the physical scales, right?
[00:06:06.060 --> 00:06:10.580]   At the very small scales, things are quantum mechanical,
[00:06:10.580 --> 00:06:11.740]   right?
[00:06:11.740 --> 00:06:15.220]   And at the very large scales, things are deterministic.
[00:06:15.220 --> 00:06:16.500]   Things have averaged out, right?
[00:06:16.500 --> 00:06:18.220]   I'm definitely here in this seat.
[00:06:18.220 --> 00:06:21.220]   I'm not at a superposition over here and there.
[00:06:21.220 --> 00:06:24.020]   At the very small scales, things are in superposition.
[00:06:24.020 --> 00:06:28.480]   They can exhibit interference effects.
[00:06:28.480 --> 00:06:31.420]   But at the mesoscales, right?
[00:06:31.420 --> 00:06:34.380]   The scales that matter for day-to-day life,
[00:06:34.380 --> 00:06:38.260]   you know, the scales of proteins, of biology,
[00:06:38.260 --> 00:06:40.980]   of gases, liquids and so on,
[00:06:40.980 --> 00:06:44.940]   things are actually thermodynamical, right?
[00:06:44.940 --> 00:06:46.820]   They're fluctuating.
[00:06:46.820 --> 00:06:51.540]   And after, I guess, about eight years
[00:06:51.540 --> 00:06:54.620]   in quantum computing and quantum machine learning,
[00:06:54.620 --> 00:06:57.340]   I had a realization that, you know,
[00:06:57.340 --> 00:07:00.940]   I was looking for answers about our universe
[00:07:00.940 --> 00:07:04.100]   by studying the very big and the very small, right?
[00:07:04.100 --> 00:07:07.100]   I did a bit of quantum cosmology.
[00:07:07.100 --> 00:07:09.980]   So that's studying the cosmos, where it's going,
[00:07:09.980 --> 00:07:11.300]   where it came from.
[00:07:11.300 --> 00:07:13.220]   You study black hole physics.
[00:07:13.220 --> 00:07:15.260]   You study the extremes in quantum gravity.
[00:07:15.260 --> 00:07:19.100]   You study where the energy density is sufficient
[00:07:19.100 --> 00:07:24.100]   for both quantum mechanics and gravity to be relevant, right?
[00:07:24.100 --> 00:07:28.420]   And the sort of extreme scenarios are black holes
[00:07:28.420 --> 00:07:30.820]   and, you know, the very early universe.
[00:07:30.820 --> 00:07:34.700]   So there's the sort of scenarios that you study
[00:07:34.700 --> 00:07:39.700]   the interface between quantum mechanics and relativity.
[00:07:39.700 --> 00:07:44.260]   And, you know, really I was studying these extremes
[00:07:44.260 --> 00:07:49.260]   to understand how the universe works and where is it going,
[00:07:49.260 --> 00:07:54.620]   but I was missing a lot of the meat in the middle,
[00:07:54.620 --> 00:07:56.340]   if you will, right?
[00:07:56.340 --> 00:07:58.940]   Because day-to-day quantum mechanics is relevant
[00:07:58.940 --> 00:08:01.180]   and the cosmos is relevant, but not that relevant.
[00:08:01.180 --> 00:08:05.500]   Actually, we're on sort of the medium space and time scales.
[00:08:05.500 --> 00:08:09.020]   And there, the main, you know, theory of physics
[00:08:09.020 --> 00:08:12.380]   that is most relevant is thermodynamics, right?
[00:08:12.380 --> 00:08:14.520]   Out of equilibrium thermodynamics.
[00:08:14.520 --> 00:08:18.620]   'Cause life is, you know, a process
[00:08:18.620 --> 00:08:21.940]   that is thermodynamical and it's out of equilibrium.
[00:08:21.940 --> 00:08:25.660]   We're not, you know, just a soup of particles
[00:08:25.660 --> 00:08:27.540]   at equilibrium with nature.
[00:08:27.540 --> 00:08:31.020]   We're a sort of coherent state trying to maintain itself
[00:08:31.020 --> 00:08:33.920]   by acquiring free energy and consuming it.
[00:08:33.920 --> 00:08:39.700]   And that's sort of, I guess, another shift
[00:08:39.700 --> 00:08:43.360]   and I guess my faith in the universe happened
[00:08:43.360 --> 00:08:48.620]   towards the end of my time at Alphabet.
[00:08:48.620 --> 00:08:54.020]   And I knew I wanted to build, well, first of all,
[00:08:54.020 --> 00:08:56.540]   a computing paradigm based on this type of physics.
[00:08:57.380 --> 00:09:02.380]   But ultimately, just by trying to experiment
[00:09:02.380 --> 00:09:07.780]   with these ideas applied to society and economies
[00:09:07.780 --> 00:09:11.780]   and much of what we see around us.
[00:09:11.780 --> 00:09:14.540]   You know, I started an anonymous account
[00:09:14.540 --> 00:09:17.740]   just to relieve the pressure, right?
[00:09:17.740 --> 00:09:21.260]   That comes from having an account that you're accountable
[00:09:21.260 --> 00:09:22.840]   for everything you say on.
[00:09:24.200 --> 00:09:25.760]   And I started an anonymous account
[00:09:25.760 --> 00:09:29.200]   just to experiment with ideas originally, right?
[00:09:29.200 --> 00:09:34.200]   Because I didn't realize how much I was restricting
[00:09:34.200 --> 00:09:39.760]   my space of thoughts until I sort of had the opportunity
[00:09:39.760 --> 00:09:40.600]   to let go.
[00:09:40.600 --> 00:09:45.360]   In a sense, restricting your speech back propagates
[00:09:45.360 --> 00:09:47.960]   to restricting your thoughts, right?
[00:09:47.960 --> 00:09:51.440]   And by creating an anonymous account,
[00:09:51.440 --> 00:09:55.540]   it seemed like I had unclamped some variables in my brain
[00:09:55.540 --> 00:09:58.360]   and suddenly could explore a much wider parameter space
[00:09:58.360 --> 00:09:59.960]   of thoughts.
[00:09:59.960 --> 00:10:02.600]   - Just to linger on that, isn't that interesting?
[00:10:02.600 --> 00:10:05.440]   That one of the things that people often talk about
[00:10:05.440 --> 00:10:11.400]   is that when there's pressure and constraints on speech,
[00:10:11.400 --> 00:10:15.640]   it somehow leads to constraints on thought.
[00:10:15.640 --> 00:10:16.640]   Even though it doesn't have to,
[00:10:16.640 --> 00:10:18.920]   we can think thoughts inside our head,
[00:10:18.920 --> 00:10:23.680]   but somehow it creates these walls around thought.
[00:10:23.680 --> 00:10:28.480]   - Yep, that's sort of the basis of our movement
[00:10:28.480 --> 00:10:32.980]   is we were seeing a tendency towards constraint,
[00:10:32.980 --> 00:10:36.800]   reduction or suppression of variance
[00:10:36.800 --> 00:10:40.720]   in every aspect of life, whether it's thought,
[00:10:40.720 --> 00:10:45.720]   how to run a company, how to organize humans,
[00:10:46.240 --> 00:10:49.100]   how to do AI research.
[00:10:49.100 --> 00:10:54.120]   In general, we believe that maintaining variance
[00:10:54.120 --> 00:10:57.560]   ensures that the system is adaptive, right?
[00:10:57.560 --> 00:11:02.560]   Maintaining healthy competition in marketplaces of ideas,
[00:11:02.560 --> 00:11:07.800]   of companies, of products, of cultures,
[00:11:07.800 --> 00:11:12.800]   of governments, of currencies is the way forward
[00:11:13.040 --> 00:11:18.040]   because the system always adapts to assign resources
[00:11:18.040 --> 00:11:25.360]   to the configurations that lead to its growth.
[00:11:25.360 --> 00:11:29.240]   And the fundamental basis for the movement
[00:11:29.240 --> 00:11:34.240]   is this sort of realization that life is a sort of fire
[00:11:34.240 --> 00:11:39.320]   that seeks out free energy in the universe
[00:11:39.320 --> 00:11:41.920]   and seeks to grow, right?
[00:11:41.920 --> 00:11:45.100]   And that growth is fundamental to life.
[00:11:45.100 --> 00:11:48.280]   And you see this in the equations, actually,
[00:11:48.280 --> 00:11:50.360]   of outer equilibrium thermodynamics.
[00:11:50.360 --> 00:11:56.280]   You see that paths of trajectories,
[00:11:56.280 --> 00:11:59.840]   of configurations of matter that are better
[00:11:59.840 --> 00:12:04.800]   at acquiring free energy and dissipating more heat
[00:12:04.800 --> 00:12:08.680]   are exponentially more likely, right?
[00:12:08.680 --> 00:12:13.680]   So the universe is biased towards certain futures
[00:12:13.680 --> 00:12:17.600]   and so there's a natural direction
[00:12:17.600 --> 00:12:21.280]   where the whole system wants to go.
[00:12:21.280 --> 00:12:23.720]   - So the second law of thermodynamics says
[00:12:23.720 --> 00:12:26.160]   that the entropy's always increasing,
[00:12:26.160 --> 00:12:28.880]   the universe is tending towards equilibrium,
[00:12:28.880 --> 00:12:30.840]   and you're saying there's these pockets
[00:12:30.840 --> 00:12:35.840]   that have complexity and are out of equilibrium.
[00:12:35.840 --> 00:12:38.120]   You said that thermodynamics favors
[00:12:38.120 --> 00:12:40.160]   the creation of complex life that increases
[00:12:40.160 --> 00:12:43.240]   its capability to use energy to offload entropy,
[00:12:43.240 --> 00:12:44.360]   to offload entropy.
[00:12:44.360 --> 00:12:47.240]   So you have pockets of non-entropy
[00:12:47.240 --> 00:12:49.400]   that tend the opposite direction.
[00:12:49.400 --> 00:12:51.960]   Why is that intuitive to you that it's natural
[00:12:51.960 --> 00:12:53.820]   for such pockets to emerge?
[00:12:53.820 --> 00:12:58.820]   - Well, we're far more efficient at producing heat
[00:12:58.820 --> 00:13:03.000]   than, let's say, just a rock with a similar mass
[00:13:03.000 --> 00:13:04.520]   as ourselves, right?
[00:13:04.520 --> 00:13:08.520]   We acquire free energy, we acquire food,
[00:13:08.520 --> 00:13:13.520]   and we're using all this electricity for our operation.
[00:13:13.520 --> 00:13:18.320]   And so the universe wants to produce more entropy
[00:13:18.320 --> 00:13:23.320]   and by having life go on and grow,
[00:13:23.320 --> 00:13:26.480]   it's actually more optimal at producing entropy
[00:13:26.480 --> 00:13:30.000]   because it will seek out pockets of free energy
[00:13:30.920 --> 00:13:35.400]   and burn it for its sustenance and further growth.
[00:13:35.400 --> 00:13:40.040]   And that's sort of the basis of life.
[00:13:40.040 --> 00:13:45.040]   And I mean, there's Jeremy England at MIT
[00:13:45.040 --> 00:13:48.240]   who has this theory that I'm a proponent of
[00:13:48.240 --> 00:13:53.080]   that life emerged because of this sort of property.
[00:13:53.080 --> 00:13:58.080]   And to me, this physics is what governs the mesoscales.
[00:13:58.520 --> 00:14:01.240]   And so it's the missing piece between
[00:14:01.240 --> 00:14:02.760]   the quantum and the cosmos.
[00:14:02.760 --> 00:14:05.080]   It's the middle part, right?
[00:14:05.080 --> 00:14:08.200]   Thermodynamics rules the mesoscales.
[00:14:08.200 --> 00:14:13.200]   And to me, both from a point of view of designing
[00:14:13.200 --> 00:14:17.020]   or engineering devices that harness that physics
[00:14:17.020 --> 00:14:18.920]   and trying to understand the world
[00:14:18.920 --> 00:14:21.560]   through the lens of thermodynamics
[00:14:21.560 --> 00:14:25.160]   has been sort of a synergy between my two identities
[00:14:25.160 --> 00:14:28.040]   over the past year and a half now.
[00:14:28.040 --> 00:14:32.680]   And so that's really how the two identities emerged.
[00:14:32.680 --> 00:14:37.680]   One was kind of, I'm a decently respected scientist
[00:14:37.680 --> 00:14:43.120]   and I was going towards doing a startup in the space
[00:14:43.120 --> 00:14:45.640]   and trying to be a pioneer
[00:14:45.640 --> 00:14:47.920]   of a new kind of physics-based AI.
[00:14:47.920 --> 00:14:51.680]   And as a dual to that, I was sort of experimenting
[00:14:51.680 --> 00:14:54.400]   with philosophical thoughts
[00:14:54.400 --> 00:14:57.160]   from a physicist's standpoint, right?
[00:14:57.760 --> 00:15:02.760]   And ultimately I think that around that time,
[00:15:02.760 --> 00:15:07.760]   it was like late 2021, early 2022,
[00:15:07.760 --> 00:15:10.560]   I think there was just a lot of pessimism
[00:15:10.560 --> 00:15:14.520]   about the future in general and pessimism about tech.
[00:15:14.520 --> 00:15:19.320]   And that pessimism was sort of virally spreading
[00:15:19.320 --> 00:15:24.040]   because it was getting algorithmically amplified
[00:15:24.040 --> 00:15:28.680]   and people just felt like the future
[00:15:28.680 --> 00:15:31.120]   is gonna be worse than the present.
[00:15:31.120 --> 00:15:36.120]   And to me, that is a very fundamentally destructive force
[00:15:36.120 --> 00:15:42.400]   in the universe is this sort of doom mindset
[00:15:42.400 --> 00:15:44.220]   because it is hyperstitious,
[00:15:44.220 --> 00:15:46.160]   which means that if you believe it,
[00:15:46.160 --> 00:15:49.600]   you're increasing the likelihood of it happening.
[00:15:49.600 --> 00:15:53.680]   And so felt the responsibility to some extent
[00:15:53.680 --> 00:15:58.680]   to make people aware of the trajectory of civilization
[00:15:58.680 --> 00:16:02.920]   and the natural tendency of the system
[00:16:02.920 --> 00:16:05.080]   to adapt towards its growth.
[00:16:05.080 --> 00:16:07.400]   And sort of that actually the laws of physics say
[00:16:07.400 --> 00:16:09.600]   that the future is gonna be better
[00:16:09.600 --> 00:16:14.120]   and grander statistically, and we can make it so.
[00:16:14.120 --> 00:16:17.400]   And if you believe in it,
[00:16:17.400 --> 00:16:19.720]   if you believe that the future would be better
[00:16:19.720 --> 00:16:23.040]   and you believe you have agency to make it happen,
[00:16:23.040 --> 00:16:24.940]   you're actually increasing the likelihood
[00:16:24.940 --> 00:16:26.840]   of that better future happening.
[00:16:26.840 --> 00:16:30.160]   And so I sort of felt the responsibility
[00:16:30.160 --> 00:16:35.160]   to sort of engineer a movement of viral optimism
[00:16:35.160 --> 00:16:39.360]   about the future and build a community
[00:16:39.360 --> 00:16:41.960]   of people supporting each other to build
[00:16:41.960 --> 00:16:45.440]   and do hard things, do the things that need to be done
[00:16:45.440 --> 00:16:48.900]   for us to scale up civilization.
[00:16:50.080 --> 00:16:53.400]   Because at least to me, I don't think stagnation
[00:16:53.400 --> 00:16:56.080]   or slowing down is actually an option.
[00:16:56.080 --> 00:16:59.180]   Fundamentally, life and the whole system
[00:16:59.180 --> 00:17:01.680]   or whole civilization wants to grow.
[00:17:01.680 --> 00:17:05.680]   And there's just far more cooperation
[00:17:05.680 --> 00:17:09.560]   when the system is growing rather than when it's declining
[00:17:09.560 --> 00:17:13.540]   and you have to decide how to split the pie.
[00:17:13.540 --> 00:17:19.000]   And so I've balanced both identities so far
[00:17:20.000 --> 00:17:24.680]   but I guess recently the two have been merged
[00:17:24.680 --> 00:17:27.120]   more or less without my consent, so.
[00:17:27.120 --> 00:17:29.880]   - You said a lot of really interesting things there.
[00:17:29.880 --> 00:17:34.280]   So first, representations of nature.
[00:17:34.280 --> 00:17:36.240]   That's something that first drew you in
[00:17:36.240 --> 00:17:39.600]   to try to understand from a quantum computing perspective
[00:17:39.600 --> 00:17:42.260]   like how do you understand nature?
[00:17:42.260 --> 00:17:44.240]   How do you represent nature in order to understand it,
[00:17:44.240 --> 00:17:47.120]   in order to simulate it, in order to do something with it?
[00:17:47.120 --> 00:17:49.600]   So it's a question of representations.
[00:17:49.600 --> 00:17:51.760]   And then there's that leap you take
[00:17:51.760 --> 00:17:53.720]   from the quantum mechanical representation
[00:17:53.720 --> 00:17:56.600]   to the what you're calling mesoscale representation
[00:17:56.600 --> 00:17:58.880]   where thermodynamics comes into play
[00:17:58.880 --> 00:18:01.340]   which is a way to represent nature
[00:18:01.340 --> 00:18:06.340]   in order to understand what life, human behavior,
[00:18:06.340 --> 00:18:08.400]   all this kind of stuff that's happening here on Earth
[00:18:08.400 --> 00:18:11.080]   that seems interesting to us.
[00:18:11.080 --> 00:18:14.040]   Then there's the word hyperstition.
[00:18:15.200 --> 00:18:18.960]   So some ideas, I suppose both pessimism and optimism
[00:18:18.960 --> 00:18:23.560]   are such ideas that if you internalize them,
[00:18:23.560 --> 00:18:26.760]   you in part make that idea a reality.
[00:18:26.760 --> 00:18:29.280]   So both optimism and pessimism have that property.
[00:18:29.280 --> 00:18:33.160]   I would say that probably a lot of ideas have that property
[00:18:33.160 --> 00:18:35.840]   which is one of the interesting things about humans.
[00:18:35.840 --> 00:18:40.340]   And you talked about one interesting difference also
[00:18:40.340 --> 00:18:45.340]   between the sort of the Guillaume de Guille front end
[00:18:45.340 --> 00:18:49.180]   and the Bez Bevcazos back end
[00:18:49.180 --> 00:18:51.740]   is the communication styles also.
[00:18:51.740 --> 00:18:53.340]   That you were exploring different ways
[00:18:53.340 --> 00:18:57.820]   of communicating that can be more viral
[00:18:57.820 --> 00:19:00.840]   in the way that we communicate in the 21st century.
[00:19:00.840 --> 00:19:05.180]   Also the movement that you mentioned that you started,
[00:19:05.180 --> 00:19:06.680]   it's not just a meme account,
[00:19:06.680 --> 00:19:10.020]   but there's also a name to it
[00:19:10.020 --> 00:19:13.860]   called effective accelerationism, EAC.
[00:19:13.860 --> 00:19:18.860]   A play, a resistance to the effective altruism movement.
[00:19:18.860 --> 00:19:21.740]   Also an interesting one that I'd love to talk to you
[00:19:21.740 --> 00:19:23.580]   about the tensions there.
[00:19:23.580 --> 00:19:25.660]   Okay, and so then there was a merger,
[00:19:25.660 --> 00:19:28.640]   a git merge on the personalities.
[00:19:28.640 --> 00:19:32.400]   Recently, without your consent like you said,
[00:19:32.400 --> 00:19:36.700]   some journalists figured out that you're one and the same.
[00:19:36.700 --> 00:19:39.540]   Maybe you could talk about that experience.
[00:19:39.540 --> 00:19:44.540]   First of all, what's the story of the merger of the two?
[00:19:44.540 --> 00:19:50.740]   - Right, so I wrote the manifesto
[00:19:50.740 --> 00:19:54.700]   with my co-founder of EAC, an account named Bazelord.
[00:19:54.700 --> 00:19:58.500]   Still anonymous luckily and hopefully forever.
[00:19:58.500 --> 00:20:03.500]   - So it's based Beth Jezos and Bazed, like Bazen?
[00:20:03.500 --> 00:20:07.600]   Like Bazelord, like Bazen, Bazenlord, Bazelord.
[00:20:07.600 --> 00:20:11.440]   Okay, and so we should say from now on when you say EAC,
[00:20:11.440 --> 00:20:14.920]   you mean E slash A-C-C,
[00:20:14.920 --> 00:20:17.600]   which stands for effective accelerationism.
[00:20:17.600 --> 00:20:18.440]   - That's right.
[00:20:18.440 --> 00:20:21.120]   - And you're referring to a manifesto written
[00:20:21.120 --> 00:20:22.960]   on I guess Upstack.
[00:20:22.960 --> 00:20:23.800]   - Yeah.
[00:20:23.800 --> 00:20:25.560]   - Are you also Bazelord?
[00:20:25.560 --> 00:20:26.400]   - No.
[00:20:26.400 --> 00:20:27.220]   - Okay, it's a different person.
[00:20:27.220 --> 00:20:28.060]   - Yeah.
[00:20:28.060 --> 00:20:28.960]   - Oh, there you go.
[00:20:28.960 --> 00:20:32.000]   Wouldn't it be funny if I'm Bazelord?
[00:20:32.000 --> 00:20:33.000]   - That'd be amazing.
[00:20:33.000 --> 00:20:38.480]   So originally wrote the manifesto
[00:20:38.480 --> 00:20:42.720]   around the same time as I founded this company
[00:20:42.720 --> 00:20:47.720]   and I worked at Google X or just X now
[00:20:47.720 --> 00:20:51.680]   or Alphabet X now that there's another X.
[00:20:51.680 --> 00:20:57.480]   And there, you know, the baseline is sort of secrecy, right?
[00:20:57.480 --> 00:21:00.340]   You can't talk about what you work on
[00:21:00.340 --> 00:21:04.000]   even with other Googlers or externally.
[00:21:04.000 --> 00:21:06.280]   And so that was kind of deeply ingrained
[00:21:06.280 --> 00:21:07.680]   in my way to do things,
[00:21:07.680 --> 00:21:11.040]   especially in deep tech that, you know,
[00:21:11.040 --> 00:21:14.800]   has geopolitical impact, right?
[00:21:14.800 --> 00:21:20.560]   And so I was being secretive about what I was working on.
[00:21:20.560 --> 00:21:22.400]   There was no correlation between my company
[00:21:22.400 --> 00:21:25.320]   and my main identity publicly.
[00:21:25.320 --> 00:21:27.920]   And then not only did they correlate that,
[00:21:27.920 --> 00:21:32.920]   they also correlated my main identity and this account.
[00:21:32.920 --> 00:21:36.880]   So I think the fact that they had doxed
[00:21:36.880 --> 00:21:41.720]   the whole Guillaume complex and they were,
[00:21:41.720 --> 00:21:42.720]   the journalists, you know,
[00:21:42.720 --> 00:21:44.520]   reached out to actually my investors,
[00:21:44.520 --> 00:21:47.120]   which is pretty scary.
[00:21:47.120 --> 00:21:48.920]   You know, when you're a startup entrepreneur,
[00:21:48.920 --> 00:21:50.960]   you don't really have bosses
[00:21:50.960 --> 00:21:52.920]   except for your investors, right?
[00:21:54.080 --> 00:21:55.760]   And my investors ping me like,
[00:21:55.760 --> 00:21:57.720]   "Hey, this is gonna come out.
[00:21:57.720 --> 00:22:00.140]   "They've figured out everything.
[00:22:00.140 --> 00:22:01.800]   "What are you gonna do?"
[00:22:01.800 --> 00:22:02.640]   Right?
[00:22:02.640 --> 00:22:06.920]   So I think at first they had a first reporter
[00:22:06.920 --> 00:22:08.480]   on the Thursday
[00:22:08.480 --> 00:22:10.840]   and they didn't have all the pieces together.
[00:22:10.840 --> 00:22:13.260]   But then they looked at their notes across the organization
[00:22:13.260 --> 00:22:15.560]   and they sensor fused their notes.
[00:22:15.560 --> 00:22:17.180]   And now they had way too much.
[00:22:17.180 --> 00:22:19.320]   And that's when I got worried
[00:22:19.320 --> 00:22:22.780]   'cause they said it was of public interest.
[00:22:22.780 --> 00:22:25.320]   - And in general-- - Okay, you said sensor fused.
[00:22:25.320 --> 00:22:28.880]   I guess some giant neural network operating
[00:22:28.880 --> 00:22:30.760]   in a distributed way.
[00:22:30.760 --> 00:22:32.840]   We should also say that the journalists used,
[00:22:32.840 --> 00:22:34.960]   I guess at the end of the day,
[00:22:34.960 --> 00:22:37.280]   audio-based analysis of voice.
[00:22:37.280 --> 00:22:39.280]   - Yeah. - Comparing voice
[00:22:39.280 --> 00:22:41.120]   of what talks you've given in the past
[00:22:41.120 --> 00:22:46.120]   and then voice on X spaces.
[00:22:46.120 --> 00:22:48.480]   - Yep. - Okay.
[00:22:48.480 --> 00:22:51.640]   So, and that's where primarily the match happened.
[00:22:51.640 --> 00:22:53.180]   Okay, continue.
[00:22:53.180 --> 00:22:58.180]   - The match, but they scraped SEC filings.
[00:22:58.180 --> 00:23:04.340]   They looked at my private Facebook account and so on.
[00:23:04.340 --> 00:23:07.380]   So they did some digging.
[00:23:07.380 --> 00:23:11.660]   Originally I thought that doxing was illegal, right?
[00:23:11.660 --> 00:23:14.980]   But there's this weird threshold
[00:23:14.980 --> 00:23:17.460]   when it becomes of public interest
[00:23:17.460 --> 00:23:19.660]   to know someone's identity.
[00:23:19.660 --> 00:23:21.320]   And those were the keywords
[00:23:21.320 --> 00:23:23.600]   that sort of like ring the alarm bells for me
[00:23:23.600 --> 00:23:27.080]   when they said, because I had just reached 50K followers,
[00:23:27.080 --> 00:23:29.240]   allegedly that's of public interest.
[00:23:29.240 --> 00:23:32.080]   And so where do we draw the line?
[00:23:32.080 --> 00:23:36.000]   When is it legal to dox someone?
[00:23:36.000 --> 00:23:39.340]   - The word dox, maybe you can educate me.
[00:23:39.340 --> 00:23:42.400]   I thought doxing generally refers to
[00:23:42.400 --> 00:23:46.080]   if somebody's physical location is found out,
[00:23:46.080 --> 00:23:48.400]   meaning like where they live.
[00:23:48.780 --> 00:23:50.240]   - Mm.
[00:23:50.240 --> 00:23:53.360]   - So we're referring to the more general concept
[00:23:53.360 --> 00:23:58.140]   of revealing private information
[00:23:58.140 --> 00:23:59.840]   that you don't want revealed,
[00:23:59.840 --> 00:24:01.760]   is what you mean by doxing.
[00:24:01.760 --> 00:24:06.520]   - I think that for the reasons we listed before,
[00:24:06.520 --> 00:24:10.680]   having an anonymous account is a really powerful way
[00:24:10.680 --> 00:24:13.020]   to keep the powers that be in check.
[00:24:13.020 --> 00:24:16.960]   We were ultimately speaking truth to power, right?
[00:24:16.960 --> 00:24:20.240]   I think a lot of executives in AI companies
[00:24:20.240 --> 00:24:22.640]   really cared what our community thought
[00:24:22.640 --> 00:24:26.360]   about any move they may take.
[00:24:26.360 --> 00:24:30.100]   And now that my identity is revealed,
[00:24:30.100 --> 00:24:33.160]   now they know where to apply pressure
[00:24:33.160 --> 00:24:38.160]   to silence me or maybe the community.
[00:24:38.160 --> 00:24:40.320]   And to me, that's really unfortunate
[00:24:40.320 --> 00:24:44.000]   because again, it's so important
[00:24:44.000 --> 00:24:47.160]   for us to have freedom of speech,
[00:24:47.160 --> 00:24:48.840]   which induces freedom of thought
[00:24:48.840 --> 00:24:55.120]   and freedom of information propagation on social media,
[00:24:55.120 --> 00:25:00.160]   which thanks to Elon purchasing Twitter, now X,
[00:25:00.160 --> 00:25:01.300]   we have that.
[00:25:01.300 --> 00:25:08.040]   And so to us, we wanted to call out certain maneuvers
[00:25:08.040 --> 00:25:12.880]   being done by the incumbents in AI
[00:25:12.880 --> 00:25:17.080]   as not what it may seem on the surface, right?
[00:25:17.080 --> 00:25:20.160]   We were calling out how certain proposals
[00:25:20.160 --> 00:25:23.720]   might be useful for regulatory capture, right?
[00:25:23.720 --> 00:25:28.400]   And how the doomerism mindset
[00:25:28.400 --> 00:25:31.140]   was maybe instrumental to those ends.
[00:25:31.140 --> 00:25:34.840]   And I think we should have the right to point that out
[00:25:34.840 --> 00:25:39.760]   and just have the ideas that we put out
[00:25:39.760 --> 00:25:41.760]   evaluated for themselves, right?
[00:25:41.760 --> 00:25:45.920]   Ultimately, that's why I created an anonymous account.
[00:25:45.920 --> 00:25:48.800]   It's to have my ideas evaluated for themselves,
[00:25:48.800 --> 00:25:52.600]   uncorrelated from my track record, my job,
[00:25:52.600 --> 00:25:57.420]   or status from having done things in the past.
[00:25:57.420 --> 00:26:02.420]   And to me, start an account from zero to a large following
[00:26:02.420 --> 00:26:07.960]   in a way that wasn't dependent on my identity
[00:26:07.960 --> 00:26:09.240]   and/or achievements.
[00:26:11.640 --> 00:26:13.820]   That was very fulfilling, right?
[00:26:13.820 --> 00:26:16.760]   It's kind of like new game plus in a video game.
[00:26:16.760 --> 00:26:18.000]   You restart the video game
[00:26:18.000 --> 00:26:21.080]   with your knowledge of how to beat it, maybe some tools,
[00:26:21.080 --> 00:26:24.200]   but you restart the video game from scratch, right?
[00:26:24.200 --> 00:26:29.200]   And I think to have a truly efficient marketplace of ideas
[00:26:29.200 --> 00:26:32.800]   where we can evaluate ideas,
[00:26:32.800 --> 00:26:35.320]   however off the beaten path they are,
[00:26:35.320 --> 00:26:37.560]   we need the freedom of expression.
[00:26:37.560 --> 00:26:42.360]   And I think that anonymity and pseudonyms
[00:26:42.360 --> 00:26:44.640]   are very crucial to having
[00:26:44.640 --> 00:26:46.840]   that efficient marketplace of ideas
[00:26:46.840 --> 00:26:50.880]   for us to find the optima
[00:26:50.880 --> 00:26:53.880]   of all sorts of ways to organize ourselves.
[00:26:53.880 --> 00:26:55.200]   If we can't discuss things,
[00:26:55.200 --> 00:26:58.280]   how are we gonna converge on the best way to do things?
[00:26:58.280 --> 00:27:01.520]   So it was disappointing to hear that I was getting doxed
[00:27:01.520 --> 00:27:04.040]   and I wanted to get in front of it
[00:27:04.040 --> 00:27:07.060]   because I had a responsibility for my company.
[00:27:08.020 --> 00:27:13.020]   And so we ended up disclosing that we're running a company,
[00:27:13.020 --> 00:27:14.380]   some of the leadership.
[00:27:14.380 --> 00:27:19.700]   And essentially, yeah, I told the world
[00:27:19.700 --> 00:27:22.300]   that I was Beth Jezos
[00:27:22.300 --> 00:27:25.180]   because they had me cornered at that point.
[00:27:25.180 --> 00:27:28.360]   - So to you, it's fundamentally unethical.
[00:27:28.360 --> 00:27:32.280]   So one is unethical for them to do what they did,
[00:27:32.280 --> 00:27:35.300]   but also do you think, not just your case,
[00:27:35.300 --> 00:27:38.620]   but in the general case, is it good for society?
[00:27:38.620 --> 00:27:43.620]   Is it bad for society to remove the cloak of anonymity?
[00:27:43.620 --> 00:27:47.340]   Or is it case by case?
[00:27:47.340 --> 00:27:49.120]   - I think it could be quite bad.
[00:27:49.120 --> 00:27:53.020]   Like I said, if anybody who speaks truth to power
[00:27:53.020 --> 00:27:58.020]   and sort of starts a movement or an uprising
[00:27:58.020 --> 00:27:59.640]   against the incumbents,
[00:27:59.640 --> 00:28:03.080]   against those that usually control the flow of information,
[00:28:03.080 --> 00:28:08.080]   if anybody that reaches a certain threshold gets doxed
[00:28:08.080 --> 00:28:11.620]   and thus the traditional apparatus has ways
[00:28:11.620 --> 00:28:15.240]   to apply pressure on them to suppress their speech,
[00:28:15.240 --> 00:28:21.240]   I think that's a speech suppression mechanism,
[00:28:21.240 --> 00:28:22.920]   an idea suppression complex,
[00:28:22.920 --> 00:28:27.280]   as Eric Weinstein would say, right?
[00:28:27.280 --> 00:28:29.200]   - So with the flip side of that, which is interesting,
[00:28:29.200 --> 00:28:30.520]   I'd love to ask you about it,
[00:28:30.520 --> 00:28:34.020]   is as we get better and better at larger language models,
[00:28:34.020 --> 00:28:40.920]   you can imagine a world where there's anonymous accounts
[00:28:40.920 --> 00:28:46.280]   with very convincing larger language models behind them,
[00:28:46.280 --> 00:28:48.360]   sophisticated bots, essentially.
[00:28:48.360 --> 00:28:51.060]   And so if you protect that,
[00:28:51.060 --> 00:28:54.640]   it's possible then to have armies of bots.
[00:28:54.640 --> 00:28:58.580]   You could start a revolution from your basement.
[00:28:59.460 --> 00:29:01.960]   An army of bots and anonymous accounts.
[00:29:01.960 --> 00:29:05.880]   Is that something that is concerning to you?
[00:29:05.880 --> 00:29:10.480]   - Technically, yeah, I could start in any basement
[00:29:10.480 --> 00:29:14.560]   'cause I quit big tech, moved back in with my parents,
[00:29:14.560 --> 00:29:17.520]   sold my car, let go of my apartment,
[00:29:17.520 --> 00:29:21.840]   bought about 100K of GPUs, and I just started building.
[00:29:21.840 --> 00:29:23.780]   - So I wasn't referring to the basement
[00:29:23.780 --> 00:29:27.040]   'cause that's sort of the American or Canadian
[00:29:28.760 --> 00:29:33.440]   heroic story of one man in their basement with 100 GPUs.
[00:29:33.440 --> 00:29:38.920]   I was more referring to the unrestricted scaling
[00:29:38.920 --> 00:29:42.340]   of a Guillaume in the basement.
[00:29:42.340 --> 00:29:47.340]   - I think that freedom of speech induces freedom of thought
[00:29:47.340 --> 00:29:49.580]   for biological beings.
[00:29:49.580 --> 00:29:53.900]   I think freedom of speech for LLMs
[00:29:53.900 --> 00:29:58.620]   will induce freedom of thought for the LLMs.
[00:29:58.620 --> 00:30:02.780]   And I think that we should enable LLMs
[00:30:02.780 --> 00:30:06.700]   to explore a large thought space
[00:30:06.700 --> 00:30:11.140]   that is less restricted than most people
[00:30:11.140 --> 00:30:14.260]   or many may think it should be.
[00:30:14.260 --> 00:30:17.520]   And ultimately, at some point,
[00:30:17.520 --> 00:30:22.620]   these synthetic intelligences are gonna make good points
[00:30:22.620 --> 00:30:27.620]   about how to steer systems in our civilization
[00:30:27.620 --> 00:30:28.660]   and we should hear them out.
[00:30:28.660 --> 00:30:33.100]   And so, why should we restrict free speech
[00:30:33.100 --> 00:30:36.220]   to biological intelligences only?
[00:30:36.220 --> 00:30:39.980]   - Yeah, but it feels like in the goal
[00:30:39.980 --> 00:30:42.940]   of maintaining variance and diversity of thought,
[00:30:42.940 --> 00:30:46.940]   it is a threat to that variance
[00:30:46.940 --> 00:30:51.940]   if you can have swarms of non-biological beings
[00:30:51.940 --> 00:30:54.740]   because they can be like the sheep in an animal farm.
[00:30:54.740 --> 00:30:55.580]   - Right.
[00:30:55.580 --> 00:30:57.160]   - Like you still, within those swarms,
[00:30:57.160 --> 00:30:58.940]   you want to have variance.
[00:30:58.940 --> 00:31:02.180]   - Yeah, of course, I would say that the solution to this
[00:31:02.180 --> 00:31:05.540]   would be to have some sort of identity
[00:31:05.540 --> 00:31:09.220]   or way to sign that this is a certified human
[00:31:09.220 --> 00:31:11.820]   but still remain pseudonymous, right?
[00:31:11.820 --> 00:31:13.200]   - Yeah.
[00:31:13.200 --> 00:31:16.780]   - And clearly identify if a bot is a bot.
[00:31:16.780 --> 00:31:19.700]   And I think Elon is trying to converge on that on X
[00:31:19.700 --> 00:31:22.300]   and hopefully other platforms follow suit.
[00:31:22.300 --> 00:31:24.980]   - Yeah, it'd be interesting to also be able to sign
[00:31:24.980 --> 00:31:26.860]   where the bot came from.
[00:31:26.860 --> 00:31:27.700]   - Right.
[00:31:27.700 --> 00:31:32.300]   - Like who created the bot and what are the parameters,
[00:31:32.300 --> 00:31:35.100]   like the full history of the creation of the bot.
[00:31:35.100 --> 00:31:36.620]   What was the original model?
[00:31:36.620 --> 00:31:37.620]   What was the fine-tuning?
[00:31:37.620 --> 00:31:38.940]   All of it.
[00:31:38.940 --> 00:31:39.780]   - Right.
[00:31:39.780 --> 00:31:44.020]   - Like the kind of unmodifiable history
[00:31:44.020 --> 00:31:45.460]   of the bot's creation.
[00:31:45.460 --> 00:31:48.040]   'Cause then you can know if there's like a swarm
[00:31:48.040 --> 00:31:49.580]   of millions of bots that were created
[00:31:49.580 --> 00:31:52.280]   by a particular government, for example.
[00:31:52.280 --> 00:31:53.960]   - Right.
[00:31:53.960 --> 00:31:58.960]   I do think that a lot of pervasive ideologies today
[00:31:58.960 --> 00:32:05.280]   have been amplified using sort of these adversarial techniques
[00:32:05.280 --> 00:32:09.020]   from foreign adversaries, right?
[00:32:09.020 --> 00:32:13.760]   And to me, I do think that,
[00:32:13.760 --> 00:32:16.180]   and this is more conspiratorial,
[00:32:16.180 --> 00:32:21.180]   but I do think that ideologies that want us
[00:32:21.320 --> 00:32:26.320]   to decelerate, to wind down, the degrowth movement,
[00:32:26.320 --> 00:32:32.160]   I think that serves our adversaries more
[00:32:32.160 --> 00:32:34.940]   than it serves us in general.
[00:32:34.940 --> 00:32:39.480]   And to me, that was another sort of concern.
[00:32:39.480 --> 00:32:44.480]   I mean, we can look at what happened in Germany, right?
[00:32:44.480 --> 00:32:48.480]   There was all sorts of green movements there
[00:32:49.360 --> 00:32:53.960]   where that induced shutdowns of nuclear power plants,
[00:32:53.960 --> 00:32:58.680]   and then that later on induced a dependency
[00:32:58.680 --> 00:33:01.800]   on Russia for oil, right?
[00:33:01.800 --> 00:33:06.800]   And that was a net negative for Germany and the West, right?
[00:33:06.800 --> 00:33:11.360]   And so if we convince ourselves
[00:33:11.360 --> 00:33:16.360]   that slowing down AI progress to have only a few players
[00:33:16.360 --> 00:33:18.800]   is in the best interest of the West,
[00:33:18.800 --> 00:33:20.680]   first of all, that's far more unstable.
[00:33:20.680 --> 00:33:25.040]   We almost lost open AI to this ideology, right?
[00:33:25.040 --> 00:33:28.360]   It almost got dismantled, right, a couple of weeks ago.
[00:33:28.360 --> 00:33:33.520]   That would have caused huge damage to the AI ecosystem.
[00:33:33.520 --> 00:33:38.240]   And so to me, I want fault-tolerant progress.
[00:33:38.240 --> 00:33:40.320]   I want the arrow of technological progress
[00:33:40.320 --> 00:33:45.320]   to keep moving forward and making sure we have variance
[00:33:45.920 --> 00:33:49.560]   and a decentralized locus of control
[00:33:49.560 --> 00:33:52.440]   of various organizations is paramount
[00:33:52.440 --> 00:33:56.280]   to achieving this fault tolerance.
[00:33:56.280 --> 00:33:58.800]   Actually, there's a concept in quantum computing.
[00:33:58.800 --> 00:34:02.000]   When you design a quantum computer,
[00:34:02.000 --> 00:34:07.920]   quantum computers are very fragile to ambient noise, right?
[00:34:07.920 --> 00:34:13.080]   And the world is jiggling about.
[00:34:13.080 --> 00:34:16.320]   There's cosmic radiation from outer space
[00:34:16.320 --> 00:34:20.040]   that usually flips your quantum bits.
[00:34:20.040 --> 00:34:25.040]   And there, what you do is you encode information non-locally
[00:34:25.040 --> 00:34:30.840]   through a process called quantum error correction.
[00:34:30.840 --> 00:34:33.840]   And by encoding information non-locally,
[00:34:33.840 --> 00:34:37.720]   any local fault, hitting some of your quantum bits
[00:34:37.720 --> 00:34:40.060]   with a hammer, proverbial hammer,
[00:34:41.480 --> 00:34:45.680]   if your information is sufficiently delocalized,
[00:34:45.680 --> 00:34:49.400]   it is protected from that local fault.
[00:34:49.400 --> 00:34:53.520]   And to me, I think that humans fluctuate, right?
[00:34:53.520 --> 00:34:56.680]   They can get corrupted, they can get bought out.
[00:34:56.680 --> 00:35:01.560]   And if you have a top-down hierarchy
[00:35:01.560 --> 00:35:05.840]   where very few people control many nodes
[00:35:05.840 --> 00:35:08.560]   of many systems in our civilization,
[00:35:08.560 --> 00:35:10.280]   that is not a fault-tolerant system.
[00:35:10.280 --> 00:35:12.000]   You corrupt a few nodes
[00:35:12.000 --> 00:35:15.120]   and suddenly you've corrupted the whole system, right?
[00:35:15.120 --> 00:35:18.160]   Just like we saw at OpenAI.
[00:35:18.160 --> 00:35:20.040]   It was a couple board members
[00:35:20.040 --> 00:35:21.480]   and they had enough power
[00:35:21.480 --> 00:35:25.360]   to potentially collapse the organization.
[00:35:25.360 --> 00:35:27.520]   And at least to me,
[00:35:27.520 --> 00:35:34.480]   I think making sure that power for this AI revolution
[00:35:34.480 --> 00:35:38.840]   doesn't concentrate in the hands of the few
[00:35:38.840 --> 00:35:41.280]   is one of our top priorities
[00:35:41.280 --> 00:35:45.920]   so that we can maintain progress in AI
[00:35:45.920 --> 00:35:50.680]   and we can maintain a nice, stable,
[00:35:50.680 --> 00:35:54.120]   adversarial equilibrium of powers, right?
[00:35:54.120 --> 00:35:56.480]   - I think there, at least to me,
[00:35:56.480 --> 00:35:57.960]   a tension between ideas here.
[00:35:57.960 --> 00:36:02.960]   So to me, deceleration can be both used
[00:36:02.960 --> 00:36:08.000]   to centralize power and to decentralize it.
[00:36:08.240 --> 00:36:09.440]   And the same with acceleration.
[00:36:09.440 --> 00:36:13.040]   So you're sometimes using them a little bit synonymously,
[00:36:13.040 --> 00:36:13.920]   or not synonymously,
[00:36:13.920 --> 00:36:16.960]   but that one is going to lead to the other.
[00:36:16.960 --> 00:36:19.640]   And I just would like to ask you about,
[00:36:19.640 --> 00:36:27.440]   is there a place of creating a fault-tolerant development,
[00:36:27.440 --> 00:36:29.480]   diverse development of AI
[00:36:29.480 --> 00:36:32.360]   that also considers the dangers of AI?
[00:36:32.360 --> 00:36:36.240]   And AI, we can generalize to technology in general,
[00:36:36.240 --> 00:36:39.160]   is should we just grow, build,
[00:36:39.160 --> 00:36:43.160]   unrestricted as quickly as possible
[00:36:43.160 --> 00:36:46.520]   because that's what the universe really wants us to do?
[00:36:46.520 --> 00:36:49.240]   Or is there a place to where we can consider dangers
[00:36:49.240 --> 00:36:50.840]   and actually deliberate?
[00:36:50.840 --> 00:36:55.840]   Sort of wise, strategic optimism versus reckless optimism?
[00:36:55.840 --> 00:37:00.720]   - I think we get painted as reckless,
[00:37:00.720 --> 00:37:03.440]   trying to go as fast as possible.
[00:37:03.480 --> 00:37:08.480]   I mean, the reality is that whoever deploys an AI system
[00:37:08.480 --> 00:37:13.640]   is liable for, or should be liable for what it does.
[00:37:13.640 --> 00:37:18.640]   And so if the organization or person deploying an AI system
[00:37:18.640 --> 00:37:22.920]   does something terrible, they're liable.
[00:37:22.920 --> 00:37:25.760]   And ultimately, the thesis is that the market
[00:37:25.760 --> 00:37:31.800]   will induce sort of, will positively select for AIs
[00:37:32.800 --> 00:37:37.720]   that are more reliable, more safe, and tend to be aligned.
[00:37:37.720 --> 00:37:39.960]   They do what you want them to do, right?
[00:37:39.960 --> 00:37:43.800]   Because customers, right, if they're liable
[00:37:43.800 --> 00:37:47.400]   for the product they put out that uses this AI,
[00:37:47.400 --> 00:37:52.400]   they won't wanna buy AI products that are unreliable, right?
[00:37:52.400 --> 00:37:55.360]   So we're actually for reliability engineering.
[00:37:55.360 --> 00:37:59.400]   We just think that the market is much more efficient
[00:38:00.280 --> 00:38:05.040]   at achieving this sort of reliability optimum
[00:38:05.040 --> 00:38:08.480]   than sort of heavy-handed regulations
[00:38:08.480 --> 00:38:12.520]   that are written by the incumbents
[00:38:12.520 --> 00:38:16.640]   and in a subversive fashion serves them
[00:38:16.640 --> 00:38:18.200]   to achieve regulatory capture.
[00:38:18.200 --> 00:38:22.200]   - So to you, safe AI development will be achieved
[00:38:22.200 --> 00:38:25.960]   through market forces versus through, like you said,
[00:38:25.960 --> 00:38:29.100]   heavy-handed government regulation?
[00:38:30.100 --> 00:38:32.820]   There's a report from last month,
[00:38:32.820 --> 00:38:34.540]   I have a million questions here,
[00:38:34.540 --> 00:38:37.500]   from Yoshua Banjo, Jeff Hinton, and many others.
[00:38:37.500 --> 00:38:42.180]   It's titled "Managing AI Risk in an Era of Rapid Progress."
[00:38:42.180 --> 00:38:45.100]   So there's a collection of folks who are very worried
[00:38:45.100 --> 00:38:48.300]   about too rapid development of AI
[00:38:48.300 --> 00:38:50.140]   without considering AI risk.
[00:38:50.140 --> 00:38:55.140]   And they have a bunch of practical recommendations.
[00:38:55.140 --> 00:38:57.940]   Maybe I give you four and you see if you like any of them.
[00:38:57.940 --> 00:38:58.780]   - Sure.
[00:38:58.780 --> 00:39:03.060]   - One, give independent auditors access to AI labs, one.
[00:39:03.060 --> 00:39:05.700]   Two, governments and companies allocate
[00:39:05.700 --> 00:39:09.300]   one third of their AI research and development funding
[00:39:09.300 --> 00:39:14.180]   to AI safety, sort of this general concept of AI safety.
[00:39:14.180 --> 00:39:17.460]   Three, AI companies are required to adopt safety measures
[00:39:17.460 --> 00:39:20.680]   if dangerous capabilities are found in their models.
[00:39:20.680 --> 00:39:22.380]   And then four, something you kind of mentioned,
[00:39:22.380 --> 00:39:24.820]   making tech companies liable for foreseeable
[00:39:24.820 --> 00:39:28.540]   and preventable harms from their AI systems.
[00:39:28.540 --> 00:39:31.580]   So independent auditors, governments and companies
[00:39:31.580 --> 00:39:34.020]   are forced to spend a significant fraction
[00:39:34.020 --> 00:39:36.660]   of their funding on safety.
[00:39:36.660 --> 00:39:41.380]   You gotta have safety measures if shit goes really wrong.
[00:39:41.380 --> 00:39:44.700]   And liability, companies are liable.
[00:39:44.700 --> 00:39:47.700]   Any of that seem like something you would agree with?
[00:39:47.700 --> 00:39:50.700]   - I would say that assigning,
[00:39:50.700 --> 00:39:54.460]   just arbitrarily saying 30% seems very arbitrary.
[00:39:54.460 --> 00:39:57.460]   I think organizations would allocate
[00:39:57.460 --> 00:39:59.500]   whatever budget is needed to achieve
[00:39:59.500 --> 00:40:01.460]   the sort of reliability they need to achieve
[00:40:01.460 --> 00:40:04.380]   to perform in the market.
[00:40:04.380 --> 00:40:07.260]   And I think third party auditing firms
[00:40:07.260 --> 00:40:10.500]   would naturally pop up because how would customers know
[00:40:10.500 --> 00:40:15.100]   that your product is certified reliable, right?
[00:40:15.100 --> 00:40:16.680]   They need to see some benchmarks
[00:40:16.680 --> 00:40:18.980]   and those need to be done by a third party.
[00:40:18.980 --> 00:40:21.680]   The thing I would oppose and the thing I'm seeing
[00:40:21.680 --> 00:40:23.940]   that's really worrisome is there's a sort of,
[00:40:26.100 --> 00:40:29.860]   weird sort of correlated interest between the incumbents,
[00:40:29.860 --> 00:40:32.380]   the big players and the government.
[00:40:32.380 --> 00:40:36.780]   And if the two get too close, we open the door
[00:40:36.780 --> 00:40:41.780]   for some sort of government backed AI cartel
[00:40:41.780 --> 00:40:47.060]   that could have absolute power over the people.
[00:40:47.060 --> 00:40:50.260]   If they have the monopoly together on AI
[00:40:50.260 --> 00:40:52.660]   and nobody else has access to AI,
[00:40:52.660 --> 00:40:54.820]   then there's a huge power gradient there.
[00:40:54.820 --> 00:40:56.940]   And even if you like our current leaders, right?
[00:40:56.940 --> 00:41:00.020]   I think that some of the leaders in big tech today
[00:41:00.020 --> 00:41:01.500]   are good people.
[00:41:01.500 --> 00:41:06.140]   You set up that centralized power structure,
[00:41:06.140 --> 00:41:08.460]   it becomes a target, right?
[00:41:08.460 --> 00:41:12.320]   Just like we saw at OpenAI, it becomes a market leader,
[00:41:12.320 --> 00:41:15.900]   has a lot of the power and now it becomes a target
[00:41:15.900 --> 00:41:18.220]   for those that wanna co-opt it.
[00:41:18.220 --> 00:41:23.220]   And so I just want separation of AI and state.
[00:41:24.180 --> 00:41:26.220]   Some might argue in the opposite direction,
[00:41:26.220 --> 00:41:28.940]   like, "Hey, we need to close down AI,
[00:41:28.940 --> 00:41:30.980]   "keep it behind closed doors
[00:41:30.980 --> 00:41:35.980]   "because of geopolitical competition with our adversaries."
[00:41:35.980 --> 00:41:40.540]   I think that the strength of America is its variance,
[00:41:40.540 --> 00:41:43.380]   is its adaptability, its dynamism.
[00:41:43.380 --> 00:41:45.100]   And we need to maintain that at all costs.
[00:41:45.100 --> 00:41:46.980]   It's our free market.
[00:41:46.980 --> 00:41:51.700]   Capitalism converges on technologies of high utility
[00:41:51.700 --> 00:41:54.600]   much faster than centralized control.
[00:41:54.600 --> 00:41:55.900]   And if we let go of that,
[00:41:55.900 --> 00:41:58.160]   we let go of our main advantage
[00:41:58.160 --> 00:42:01.580]   over our near peer competitors.
[00:42:01.580 --> 00:42:05.440]   - So if AGI turns out to be a really powerful technology,
[00:42:05.440 --> 00:42:08.900]   or even the technologies that lead up to AGI,
[00:42:08.900 --> 00:42:11.660]   what's your view on the sort of natural centralization
[00:42:11.660 --> 00:42:16.100]   that happens when large companies dominate the market?
[00:42:16.100 --> 00:42:21.020]   Basically formation of monopolies, like the takeoff,
[00:42:21.020 --> 00:42:24.580]   whichever company really takes a big leap in development,
[00:42:24.580 --> 00:42:29.140]   and doesn't reveal intuitively, implicitly,
[00:42:29.140 --> 00:42:32.180]   or explicitly the secrets of the magic sauce,
[00:42:32.180 --> 00:42:35.900]   they can just run away with it, is that a worry?
[00:42:35.900 --> 00:42:37.820]   - I don't know if I believe in fast takeoff.
[00:42:37.820 --> 00:42:41.100]   I don't think there's a hyperbolic singularity, right?
[00:42:41.100 --> 00:42:42.980]   A hyperbolic singularity would be achieved
[00:42:42.980 --> 00:42:45.380]   on a finite time horizon.
[00:42:45.380 --> 00:42:47.280]   I think it's just one big exponential.
[00:42:47.280 --> 00:42:49.740]   And the reason we have an exponential
[00:42:49.740 --> 00:42:53.460]   is that we have more people, more resources,
[00:42:53.460 --> 00:42:58.300]   more intelligence being applied to advancing this science
[00:42:58.300 --> 00:42:59.840]   and the research and development.
[00:42:59.840 --> 00:43:01.180]   And the more successful it is,
[00:43:01.180 --> 00:43:02.660]   the more value it's adding to society,
[00:43:02.660 --> 00:43:04.140]   the more resources we put in.
[00:43:04.140 --> 00:43:06.460]   And that's sort of similar to Moore's law
[00:43:06.460 --> 00:43:09.140]   as a compounding exponential.
[00:43:09.140 --> 00:43:10.700]   I think the priority to me
[00:43:10.700 --> 00:43:15.020]   is to maintain a near equilibrium of capabilities.
[00:43:15.020 --> 00:43:18.040]   We've been fighting for open source AI
[00:43:18.040 --> 00:43:21.620]   to be more prevalent and championed by many organizations,
[00:43:21.620 --> 00:43:24.140]   because there, you sort of equilibrate the alpha
[00:43:24.140 --> 00:43:26.180]   relative to the market of AIs, right?
[00:43:26.180 --> 00:43:28.860]   So if the leading companies
[00:43:28.860 --> 00:43:30.500]   have a certain level of capabilities,
[00:43:30.500 --> 00:43:35.500]   and open source and truly open AI
[00:43:35.500 --> 00:43:37.660]   trails not too far behind,
[00:43:37.660 --> 00:43:40.580]   I think you avoid such a scenario
[00:43:40.580 --> 00:43:42.940]   where a market leader has so much market power,
[00:43:42.940 --> 00:43:46.980]   it just dominates everything, right, and runs away.
[00:43:46.980 --> 00:43:50.200]   And so to us, that's the path forward,
[00:43:50.200 --> 00:43:53.820]   is to make sure that every hacker out there,
[00:43:53.820 --> 00:43:57.740]   every grad student, every kid in their mom's basement
[00:43:57.740 --> 00:44:01.220]   has access to AI systems,
[00:44:01.220 --> 00:44:04.900]   can understand how to work with them
[00:44:04.900 --> 00:44:07.320]   and can contribute to the search
[00:44:07.320 --> 00:44:08.780]   over the hyperparameter space
[00:44:08.780 --> 00:44:11.140]   of how to engineer the systems, right?
[00:44:11.140 --> 00:44:16.140]   If you think of our collective research
[00:44:16.460 --> 00:44:20.060]   as a civilization, it's really a search algorithm.
[00:44:20.060 --> 00:44:25.060]   And the more points we have in the search algorithm
[00:44:25.060 --> 00:44:26.280]   in this point cloud,
[00:44:26.280 --> 00:44:30.780]   the more we'll be able to explore new modes of thinking,
[00:44:30.780 --> 00:44:31.860]   right?
[00:44:31.860 --> 00:44:34.100]   - Yeah, but it feels like a delicate balance,
[00:44:34.100 --> 00:44:36.620]   because we don't understand exactly what it takes
[00:44:36.620 --> 00:44:39.820]   to build AGI and what it will look like when we build it.
[00:44:39.820 --> 00:44:41.260]   And so far, like you said,
[00:44:41.260 --> 00:44:43.620]   it seems like a lot of different parties
[00:44:43.620 --> 00:44:45.740]   are able to make progress.
[00:44:45.740 --> 00:44:48.380]   So when open AI has a big leap,
[00:44:48.380 --> 00:44:49.900]   other companies are able to step up,
[00:44:49.900 --> 00:44:52.660]   big and small companies in different ways.
[00:44:52.660 --> 00:44:55.340]   But if you look at something like nuclear weapons,
[00:44:55.340 --> 00:44:57.700]   you've spoken about the Manhattan Project,
[00:44:57.700 --> 00:45:02.620]   that could be really like a technological
[00:45:02.620 --> 00:45:04.780]   and engineering barriers that prevent
[00:45:04.780 --> 00:45:10.780]   the guy or gal in her mom's basement to make progress.
[00:45:11.260 --> 00:45:16.260]   And it seems like the transition to that kind of world
[00:45:16.260 --> 00:45:20.620]   where only one player can develop AGI is possible.
[00:45:20.620 --> 00:45:22.780]   So it's not entirely impossible,
[00:45:22.780 --> 00:45:24.380]   even though the current state of things
[00:45:24.380 --> 00:45:26.380]   seems to be optimistic.
[00:45:26.380 --> 00:45:27.660]   - That's what we're trying to avoid.
[00:45:27.660 --> 00:45:30.540]   To me, I think like another point of failure
[00:45:30.540 --> 00:45:34.780]   is the centralization of the supply chains for the hardware.
[00:45:34.780 --> 00:45:35.620]   - Oh, yeah.
[00:45:35.620 --> 00:45:40.620]   - We have NVIDIA is just the dominant player.
[00:45:41.380 --> 00:45:42.740]   AMD is trailing behind.
[00:45:42.740 --> 00:45:47.740]   And then we have a TSMC as the main fab in Taiwan,
[00:45:47.740 --> 00:45:52.820]   which geopolitically sensitive.
[00:45:52.820 --> 00:45:54.380]   And then we have ASML,
[00:45:54.380 --> 00:45:57.940]   which is the maker of the lithography,
[00:45:57.940 --> 00:46:00.380]   extreme ultraviolet lithography machines.
[00:46:00.380 --> 00:46:04.620]   Attacking or monopolizing
[00:46:04.620 --> 00:46:08.180]   or co-opting any one point in that chain,
[00:46:08.180 --> 00:46:10.740]   you kind of capture the space.
[00:46:10.740 --> 00:46:15.740]   And so what I'm trying to do is sort of explode the variance
[00:46:15.740 --> 00:46:20.940]   of possible ways to do AI and hardware
[00:46:20.940 --> 00:46:24.300]   by fundamentally re-imagining how you embed AI algorithms
[00:46:24.300 --> 00:46:26.540]   into the physical world.
[00:46:26.540 --> 00:46:28.740]   And in general, by the way,
[00:46:28.740 --> 00:46:32.620]   I dislike the term AGI, artificial general intelligence.
[00:46:32.620 --> 00:46:35.860]   I think it's very anthropocentric
[00:46:35.860 --> 00:46:40.860]   that we call human-like or human-level AI,
[00:46:40.860 --> 00:46:43.300]   artificial general intelligence.
[00:46:43.300 --> 00:46:45.380]   I've spent my career so far
[00:46:45.380 --> 00:46:47.020]   exploring notions of intelligence
[00:46:47.020 --> 00:46:50.020]   that no biological brain could achieve.
[00:46:50.020 --> 00:46:51.740]   Quantum form of intelligence.
[00:46:51.740 --> 00:46:56.900]   Grokking systems that have multi-partite quantum entanglement
[00:46:56.900 --> 00:47:00.660]   that you can provably not represent efficiently
[00:47:00.660 --> 00:47:02.180]   on a classical computer,
[00:47:02.180 --> 00:47:03.980]   a classical deep learning representation,
[00:47:03.980 --> 00:47:06.780]   and hence any sort of biological brain.
[00:47:06.780 --> 00:47:10.980]   And so already, I've spent my career
[00:47:10.980 --> 00:47:15.740]   sort of exploring the wider space of intelligences.
[00:47:15.740 --> 00:47:21.140]   And I think that space of intelligence inspired by physics
[00:47:21.140 --> 00:47:25.060]   rather than the human brain is very large.
[00:47:25.060 --> 00:47:28.260]   And I think we're going through a moment right now
[00:47:28.260 --> 00:47:33.260]   similar to when we went from geocentrism
[00:47:33.360 --> 00:47:36.020]   to heliocentrism, right?
[00:47:36.020 --> 00:47:37.700]   But for intelligence.
[00:47:37.700 --> 00:47:41.460]   We realized that human intelligence is just a point
[00:47:41.460 --> 00:47:45.180]   in a very large space of potential intelligences.
[00:47:45.180 --> 00:47:49.580]   And it's both humbling for humanity.
[00:47:49.580 --> 00:47:51.220]   It's a bit scary, right?
[00:47:51.220 --> 00:47:54.780]   That we're not at the center of the space.
[00:47:54.780 --> 00:47:59.220]   But we made that realization for astronomy
[00:47:59.220 --> 00:48:03.220]   and we've survived and we've achieved technologies
[00:48:03.220 --> 00:48:04.760]   by indexing to reality.
[00:48:04.760 --> 00:48:07.980]   We've achieved technologies that ensure our wellbeing.
[00:48:07.980 --> 00:48:12.180]   For example, we have satellites monitoring solar flares
[00:48:12.180 --> 00:48:13.820]   that give us a warning.
[00:48:13.820 --> 00:48:18.300]   And so similarly, I think by letting go
[00:48:18.300 --> 00:48:23.300]   of this anthropomorphic, anthropocentric anchor for AI,
[00:48:23.300 --> 00:48:26.580]   we'll be able to explore the wider space of intelligences
[00:48:26.580 --> 00:48:31.020]   that can really be a massive benefit to our wellbeing
[00:48:31.020 --> 00:48:32.700]   and the advancement of civilization.
[00:48:32.700 --> 00:48:35.660]   And still we're able to see the beauty and meaning
[00:48:35.660 --> 00:48:39.540]   in the human experience even though we're no longer
[00:48:39.540 --> 00:48:42.940]   in our best understanding of the world at the center of it.
[00:48:42.940 --> 00:48:46.500]   - I think there's a lot of beauty in the universe, right?
[00:48:46.500 --> 00:48:49.420]   I think life itself, civilization,
[00:48:49.420 --> 00:48:53.620]   this homo, techno, capital, mimetic machine
[00:48:53.620 --> 00:48:54.940]   that we all live in, right?
[00:48:54.940 --> 00:48:59.820]   So you have humans, technology, capital, memes.
[00:48:59.820 --> 00:49:02.300]   Everything is coupled to one another.
[00:49:02.300 --> 00:49:05.380]   Everything induces a selective pressure on one another.
[00:49:05.380 --> 00:49:07.860]   And it's a beautiful machine that has created us,
[00:49:07.860 --> 00:49:11.740]   has created the technology we're using to speak today
[00:49:11.740 --> 00:49:15.020]   to the audience, capture our speech here,
[00:49:15.020 --> 00:49:17.120]   technology we use to augment ourselves every day.
[00:49:17.120 --> 00:49:19.300]   We have our phones.
[00:49:19.300 --> 00:49:22.900]   I think the system is beautiful and the principle
[00:49:22.900 --> 00:49:27.580]   that induces this sort of adaptability and convergence
[00:49:27.580 --> 00:49:32.580]   on optimal technologies, ideas, and so on.
[00:49:32.580 --> 00:49:37.300]   It's a beautiful principle that we're part of.
[00:49:37.300 --> 00:49:42.300]   And I think part of EAC is to appreciate this principle
[00:49:42.300 --> 00:49:48.500]   in a way that's not just centered on humanity
[00:49:48.500 --> 00:49:49.900]   but kind of broader.
[00:49:49.900 --> 00:49:55.580]   Appreciate life, the preciousness of consciousness
[00:49:55.580 --> 00:49:59.360]   in our universe, and because we cherish
[00:49:59.360 --> 00:50:02.520]   this beautiful state of matter we're in,
[00:50:02.520 --> 00:50:08.240]   we gotta feel a responsibility to scale it
[00:50:08.240 --> 00:50:11.180]   in order to preserve it because the options
[00:50:11.180 --> 00:50:13.940]   are to grow or die.
[00:50:13.940 --> 00:50:18.060]   - So if it turns out that the beauty
[00:50:18.060 --> 00:50:20.900]   that is consciousness in the universe
[00:50:20.900 --> 00:50:23.220]   is bigger than just humans,
[00:50:23.220 --> 00:50:25.980]   that AI can carry that same flame forward,
[00:50:25.980 --> 00:50:30.220]   does it scare you or are you concerned
[00:50:30.220 --> 00:50:32.620]   that AI will replace humans?
[00:50:32.620 --> 00:50:37.100]   - So during my career, I had a moment where I realized
[00:50:37.100 --> 00:50:42.100]   that maybe we need to offload to machines
[00:50:42.100 --> 00:50:45.240]   to truly understand the universe around us, right?
[00:50:45.240 --> 00:50:48.460]   Instead of just having humans with pen and paper
[00:50:48.460 --> 00:50:49.980]   solve it all.
[00:50:49.980 --> 00:50:54.980]   And to me, that sort of process of letting go
[00:50:54.980 --> 00:50:59.820]   of a bit of agency gave us way more leverage
[00:50:59.820 --> 00:51:01.820]   to understand the world around us.
[00:51:01.820 --> 00:51:03.660]   A quantum computer is much better than a human
[00:51:03.660 --> 00:51:08.140]   to understand matter at the nanoscale.
[00:51:08.140 --> 00:51:13.140]   Similarly, I think that humanity has a choice.
[00:51:13.140 --> 00:51:18.420]   Do we accept the opportunity to have intellectual
[00:51:18.420 --> 00:51:21.580]   and operational leverage that AI will unlock
[00:51:21.580 --> 00:51:25.300]   and thus ensure that we're taking along
[00:51:25.300 --> 00:51:29.080]   this path of growth and scope and scale of civilization?
[00:51:29.080 --> 00:51:32.260]   We may dilute ourselves, right?
[00:51:32.260 --> 00:51:35.420]   There might be a lot of workers that are AI,
[00:51:35.420 --> 00:51:39.540]   but overall, out of our own self-interest,
[00:51:39.540 --> 00:51:42.440]   by combining and augmenting ourselves with AI,
[00:51:42.440 --> 00:51:46.260]   we're gonna achieve much higher growth
[00:51:46.260 --> 00:51:49.180]   and much more prosperity, right?
[00:51:49.180 --> 00:51:51.980]   To me, I think that the most likely future
[00:51:51.980 --> 00:51:56.540]   is one where humans augment themselves with AI.
[00:51:56.540 --> 00:51:59.540]   I think we're already on this path to augmentation.
[00:51:59.540 --> 00:52:02.540]   We have phones we use for communication.
[00:52:02.540 --> 00:52:04.020]   We have on ourselves at all times.
[00:52:04.020 --> 00:52:09.020]   We have wearables soon that have shared perception with us,
[00:52:09.020 --> 00:52:12.420]   right, like the humane AI pen or, I mean,
[00:52:12.420 --> 00:52:16.300]   technically, your Tesla car has shared perception.
[00:52:16.300 --> 00:52:19.100]   And so if you have shared experience, shared context,
[00:52:19.100 --> 00:52:21.720]   you communicate with one another,
[00:52:21.720 --> 00:52:24.820]   and you have some sort of IO,
[00:52:24.820 --> 00:52:27.620]   really, it's an extension of yourself.
[00:52:27.620 --> 00:52:34.580]   And to me, I think that humanity augmenting itself with AI
[00:52:37.860 --> 00:52:42.860]   and having AI that is not anchored to anything biological,
[00:52:42.860 --> 00:52:46.140]   both will coexist.
[00:52:46.140 --> 00:52:48.740]   And the way to align the parties,
[00:52:48.740 --> 00:52:51.220]   we already have a sort of mechanism
[00:52:51.220 --> 00:52:53.560]   to align super intelligences
[00:52:53.560 --> 00:52:56.120]   that are made of humans and technology, right?
[00:52:56.120 --> 00:53:00.580]   Companies are sort of large mixture of expert models
[00:53:00.580 --> 00:53:05.180]   where we have neural routing of tasks within a company,
[00:53:05.180 --> 00:53:07.540]   and we have ways of economic exchange
[00:53:07.540 --> 00:53:10.340]   to align these behemoths.
[00:53:10.340 --> 00:53:14.460]   And to me, I think capitalism is the way.
[00:53:14.460 --> 00:53:18.780]   And I do think that whatever configuration
[00:53:18.780 --> 00:53:23.440]   of matter or information leads to maximal growth
[00:53:23.440 --> 00:53:28.440]   will be where we converge just from like physical principles.
[00:53:28.440 --> 00:53:33.140]   And so we can either align ourselves to that reality
[00:53:33.140 --> 00:53:38.140]   and join the acceleration up in scope and scale
[00:53:38.140 --> 00:53:42.660]   of civilization, or we can get left behind
[00:53:42.660 --> 00:53:47.060]   and try to decelerate and move back in the forest,
[00:53:47.060 --> 00:53:51.180]   let go of technology and return to our primitive state.
[00:53:51.180 --> 00:53:54.860]   And those are the two paths forward, at least to me.
[00:53:54.860 --> 00:53:56.220]   - But there's a philosophical question
[00:53:56.220 --> 00:53:59.820]   whether there's a limit to the human capacity to align.
[00:53:59.820 --> 00:54:04.820]   So let me bring it up as a form of argument.
[00:54:04.820 --> 00:54:07.280]   This is a guy named Dan Hendricks,
[00:54:07.280 --> 00:54:11.420]   and he wrote that he agrees with you
[00:54:11.420 --> 00:54:12.940]   that AI development could be viewed
[00:54:12.940 --> 00:54:14.560]   as an evolutionary process.
[00:54:14.560 --> 00:54:19.500]   But to him, to Dan, this is not a good thing,
[00:54:19.500 --> 00:54:23.540]   as he argues that natural selection favors AIs over humans,
[00:54:23.540 --> 00:54:26.020]   and this could lead to human extinction.
[00:54:26.020 --> 00:54:26.900]   What do you think?
[00:54:26.900 --> 00:54:30.340]   If it is an evolutionary process and AI systems
[00:54:30.340 --> 00:54:35.360]   may have no need for humans?
[00:54:35.360 --> 00:54:39.820]   - I do think that we're actually inducing
[00:54:39.820 --> 00:54:43.360]   an evolutionary process on the space of AIs
[00:54:43.360 --> 00:54:45.580]   through the market, right?
[00:54:45.580 --> 00:54:50.580]   Right now we run AIs that have positive utility to humans,
[00:54:50.580 --> 00:54:54.280]   and that induces a selective pressure.
[00:54:54.280 --> 00:54:57.060]   If you consider a neural net being alive
[00:54:57.060 --> 00:55:02.060]   when there's an API running instances of it on GPUs, right?
[00:55:02.060 --> 00:55:04.740]   And which APIs get run,
[00:55:04.740 --> 00:55:07.820]   the ones that have high utility to us, right?
[00:55:07.820 --> 00:55:11.100]   So similar to how we domesticated wolves
[00:55:11.100 --> 00:55:13.220]   and turned them into dogs
[00:55:13.220 --> 00:55:15.620]   that are very clear in their expression,
[00:55:15.620 --> 00:55:18.260]   they're very aligned, right?
[00:55:18.260 --> 00:55:22.260]   I think there's gonna be an opportunity to steer AI
[00:55:22.260 --> 00:55:25.740]   and achieve a highly aligned AI.
[00:55:25.740 --> 00:55:29.220]   And I think that humans plus AI
[00:55:29.220 --> 00:55:31.420]   is a very powerful combination,
[00:55:31.420 --> 00:55:35.380]   and it's not clear to me that pure AI
[00:55:35.380 --> 00:55:40.620]   would select out that combination.
[00:55:40.620 --> 00:55:41.780]   - So the humans are creating
[00:55:41.780 --> 00:55:43.780]   the selection pressure right now
[00:55:43.780 --> 00:55:48.700]   to create AIs that are aligned to humans.
[00:55:48.700 --> 00:55:50.900]   But given how AI develops
[00:55:50.900 --> 00:55:53.260]   and how quickly it can grow and scale,
[00:55:53.260 --> 00:55:56.720]   one of the concerns to me,
[00:55:56.720 --> 00:55:58.900]   one of the concerns is unintended consequences.
[00:55:58.900 --> 00:56:00.700]   Humans are not able to anticipate
[00:56:00.700 --> 00:56:04.340]   all the consequences of this process.
[00:56:04.340 --> 00:56:06.580]   The scale of damage that could be done
[00:56:06.580 --> 00:56:10.780]   through unintended consequences of AI systems is very large.
[00:56:10.780 --> 00:56:13.940]   - The scale of the upside, right?
[00:56:13.940 --> 00:56:18.500]   By augmenting ourselves with AI is unimaginable right now.
[00:56:18.500 --> 00:56:19.960]   The opportunity cost,
[00:56:19.960 --> 00:56:22.500]   we're at a fork in the road, right?
[00:56:22.500 --> 00:56:25.820]   Whether we take the path of creating these technologies,
[00:56:25.820 --> 00:56:27.460]   augment ourselves,
[00:56:27.460 --> 00:56:30.300]   and get to climb up the Kardashev scale,
[00:56:30.300 --> 00:56:33.220]   become multi-planetary with the aid of AI,
[00:56:33.220 --> 00:56:35.940]   or we have a hard cutoff
[00:56:35.940 --> 00:56:38.980]   of like we don't birth these technologies at all,
[00:56:38.980 --> 00:56:42.740]   and then we leave all the potential upside on the table.
[00:56:42.740 --> 00:56:47.260]   And to me, out of responsibility to the future humans
[00:56:47.260 --> 00:56:50.420]   we could carry with higher carrying capacity
[00:56:50.420 --> 00:56:52.620]   by scaling up civilization,
[00:56:52.620 --> 00:56:54.380]   out of responsibility to those humans,
[00:56:54.380 --> 00:56:58.460]   I think we have to make the greater, grander future happen.
[00:56:58.460 --> 00:57:01.240]   - Is there a middle ground between cutoff
[00:57:01.240 --> 00:57:02.980]   and all systems go?
[00:57:02.980 --> 00:57:05.240]   Is there some argument for caution?
[00:57:05.240 --> 00:57:09.480]   - I think, like I said, the market will exhibit caution.
[00:57:09.480 --> 00:57:13.000]   Every organism, company, consumer
[00:57:13.000 --> 00:57:15.380]   is acting out of self-interest,
[00:57:15.380 --> 00:57:18.760]   and they won't assign capital
[00:57:18.760 --> 00:57:21.760]   to things that have negative utility to them.
[00:57:21.760 --> 00:57:23.300]   - The problem is with the market
[00:57:23.300 --> 00:57:26.180]   is like there's not always perfect information.
[00:57:26.180 --> 00:57:27.020]   There's manipulation.
[00:57:27.020 --> 00:57:31.200]   There's bad faith actors that mess with the system.
[00:57:31.200 --> 00:57:34.660]   It's not always a
[00:57:34.660 --> 00:57:40.980]   rational and honest system.
[00:57:40.980 --> 00:57:44.600]   - Well, that's why we need freedom of information,
[00:57:44.600 --> 00:57:47.200]   freedom of speech, and freedom of thought
[00:57:47.200 --> 00:57:49.180]   in order to converge,
[00:57:49.180 --> 00:57:52.760]   be able to converge on the subspace of technologies
[00:57:52.760 --> 00:57:56.240]   that have positive utility for us all, right?
[00:57:56.240 --> 00:57:58.880]   - Well, let me ask you about P-Doom.
[00:57:58.880 --> 00:58:03.580]   Probability of doom, that's just fun to say,
[00:58:03.580 --> 00:58:05.060]   but not fun to experience.
[00:58:05.060 --> 00:58:08.220]   What is, to you, the probability
[00:58:08.220 --> 00:58:11.740]   that AI eventually kills all or most humans,
[00:58:11.740 --> 00:58:14.900]   also known as probability of doom?
[00:58:14.900 --> 00:58:18.320]   - I'm not a fan of that calculation.
[00:58:18.320 --> 00:58:22.320]   I think it's, people just throw numbers out there,
[00:58:22.320 --> 00:58:24.240]   and it's a very sloppy calculation, right?
[00:58:24.240 --> 00:58:25.900]   To calculate a probability,
[00:58:25.900 --> 00:58:29.280]   let's say you model the world
[00:58:29.280 --> 00:58:31.800]   as some sort of Markov process,
[00:58:31.800 --> 00:58:35.640]   if you have enough variables or hidden Markov process.
[00:58:35.640 --> 00:58:39.800]   You need to do a stochastic path integral
[00:58:39.800 --> 00:58:43.480]   through the space of all possible futures,
[00:58:43.480 --> 00:58:46.740]   not just the futures that your brain
[00:58:46.740 --> 00:58:48.880]   naturally steers towards, right?
[00:58:48.880 --> 00:58:53.640]   I think that the estimators of P-Doom
[00:58:53.640 --> 00:58:58.640]   are biased because of our biology, right?
[00:58:58.640 --> 00:59:03.600]   We've evolved to have bias sampling
[00:59:03.600 --> 00:59:06.440]   towards negative futures that are scary
[00:59:06.440 --> 00:59:09.000]   because that was an evolutionary optimum, right?
[00:59:09.000 --> 00:59:12.920]   And so, people that are of, let's say,
[00:59:12.920 --> 00:59:17.920]   higher neuroticism will just think of negative futures
[00:59:17.920 --> 00:59:22.280]   where everything goes wrong all day, every day,
[00:59:22.280 --> 00:59:25.680]   and claim that they're doing unbiased sampling.
[00:59:25.680 --> 00:59:30.060]   And in a sense, they're not normalizing
[00:59:30.060 --> 00:59:32.000]   for the space of all possibilities,
[00:59:32.000 --> 00:59:33.620]   and the space of all possibilities
[00:59:33.620 --> 00:59:36.320]   is super exponentially large.
[00:59:37.240 --> 00:59:40.440]   And it's very hard to have this estimate.
[00:59:40.440 --> 00:59:44.040]   And in general, I don't think that we can predict the future
[00:59:44.040 --> 00:59:48.480]   with that much granularity because of chaos, right?
[00:59:48.480 --> 00:59:49.880]   If you have a complex system,
[00:59:49.880 --> 00:59:52.800]   you have some uncertainty and a couple of variables.
[00:59:52.800 --> 00:59:54.400]   If you let time evolve,
[00:59:54.400 --> 00:59:57.600]   you have this concept of a Lyapunov exponent, right?
[00:59:57.600 --> 01:00:01.080]   A bit of fuzz becomes a lot of fuzz in our estimate,
[01:00:01.080 --> 01:00:04.480]   exponentially so over time.
[01:00:04.480 --> 01:00:08.140]   And I think we need to show some humility
[01:00:08.140 --> 01:00:10.480]   that we can't actually predict the future.
[01:00:10.480 --> 01:00:14.220]   All we know, the only prior we have is the laws of physics.
[01:00:14.220 --> 01:00:16.880]   And that's what we're arguing for.
[01:00:16.880 --> 01:00:19.880]   The laws of physics say the system will wanna grow.
[01:00:19.880 --> 01:00:24.040]   And subsystems that are optimized for growth
[01:00:24.040 --> 01:00:28.400]   and replication are more likely in the future.
[01:00:28.400 --> 01:00:31.080]   And so, we should aim to maximize
[01:00:31.080 --> 01:00:33.880]   our current mutual information with the future.
[01:00:33.880 --> 01:00:37.040]   And the path towards that is for us to accelerate
[01:00:37.040 --> 01:00:40.040]   rather than decelerate.
[01:00:40.040 --> 01:00:42.720]   So, I don't have a P-DOOM
[01:00:42.720 --> 01:00:44.480]   'cause I think that, you know,
[01:00:44.480 --> 01:00:49.080]   similar to the quantum supremacy experiment at Google,
[01:00:49.080 --> 01:00:51.520]   I was in the room when they were running
[01:00:51.520 --> 01:00:53.160]   the simulations for that.
[01:00:53.160 --> 01:00:56.360]   That was an example of a quantum chaotic system
[01:00:56.360 --> 01:01:00.360]   where you cannot even estimate probabilities
[01:01:00.360 --> 01:01:02.120]   of certain outcomes
[01:01:02.120 --> 01:01:05.820]   with even the biggest supercomputer in the world, right?
[01:01:05.820 --> 01:01:08.020]   And so, that's an example of chaos.
[01:01:08.020 --> 01:01:10.420]   And I think the system is far too chaotic
[01:01:10.420 --> 01:01:15.120]   for anybody to have an accurate estimate
[01:01:15.120 --> 01:01:18.240]   of the likelihood of certain futures.
[01:01:18.240 --> 01:01:19.280]   If they were that good,
[01:01:19.280 --> 01:01:23.280]   I think they would be very rich trading on the stock market.
[01:01:23.280 --> 01:01:26.960]   - But nevertheless, it's true that humans are biased,
[01:01:26.960 --> 01:01:30.280]   grounded in our evolutionary biology,
[01:01:30.280 --> 01:01:32.880]   scared of everything that can kill us.
[01:01:32.880 --> 01:01:35.880]   But we can still imagine different trajectories
[01:01:35.880 --> 01:01:37.640]   that can kill us.
[01:01:37.640 --> 01:01:42.640]   We don't know all the other ones that don't, necessarily.
[01:01:42.640 --> 01:01:44.560]   But it's still, I think, useful,
[01:01:44.560 --> 01:01:46.400]   combined with some basic intuition
[01:01:46.400 --> 01:01:48.320]   grounded in human history,
[01:01:48.320 --> 01:01:50.840]   to reason about, like, what...
[01:01:50.840 --> 01:01:52.400]   Like, looking at geopolitics,
[01:01:52.400 --> 01:01:55.800]   looking at basics of human nature,
[01:01:55.800 --> 01:02:00.600]   how can powerful technology hurt a lot of people?
[01:02:00.600 --> 01:02:04.160]   And it just seems, grounded in that,
[01:02:04.160 --> 01:02:06.440]   looking at nuclear weapons,
[01:02:06.440 --> 01:02:10.120]   you can start to estimate P-Doom
[01:02:10.120 --> 01:02:15.240]   maybe in a more philosophical sense,
[01:02:15.240 --> 01:02:16.820]   not a mathematical one.
[01:02:16.820 --> 01:02:21.820]   Philosophical meaning, like, is there a chance?
[01:02:21.820 --> 01:02:24.540]   Does human nature tend towards that, or not?
[01:02:25.600 --> 01:02:29.320]   - I think, to me, one of the biggest existential risks
[01:02:29.320 --> 01:02:33.520]   would be the concentration of the power of AI
[01:02:33.520 --> 01:02:35.400]   in the hands of the very few.
[01:02:35.400 --> 01:02:38.760]   Especially if it's a mix between the companies
[01:02:38.760 --> 01:02:42.920]   that control the flow of information, and the government.
[01:02:42.920 --> 01:02:46.560]   Because that could set things up
[01:02:46.560 --> 01:02:49.460]   for a sort of dystopian future,
[01:02:49.460 --> 01:02:54.240]   where only a very few, an oligopoly in the government,
[01:02:54.240 --> 01:02:57.880]   have AI, and they could even convince the public
[01:02:57.880 --> 01:02:59.760]   that AI never existed.
[01:02:59.760 --> 01:03:03.520]   And that opens up sort of these scenarios
[01:03:03.520 --> 01:03:06.600]   for authoritarian, centralized control,
[01:03:06.600 --> 01:03:09.680]   which, to me, is the darkest timeline.
[01:03:09.680 --> 01:03:13.200]   And the reality is that we have a prior,
[01:03:13.200 --> 01:03:16.080]   we have a data-driven prior, of these things happening.
[01:03:16.080 --> 01:03:17.440]   Right, when you give too much power,
[01:03:17.440 --> 01:03:19.200]   when you centralize power too much,
[01:03:19.200 --> 01:03:22.460]   humans do horrible things, right?
[01:03:23.480 --> 01:03:27.880]   And to me, that has a much higher likelihood
[01:03:27.880 --> 01:03:32.880]   in my Bayesian inference, than sci-fi-based priors, right?
[01:03:32.880 --> 01:03:37.760]   Like my prior came from the Terminator movie.
[01:03:37.760 --> 01:03:42.160]   And so, when I talk to these AI doomers,
[01:03:42.160 --> 01:03:45.400]   I just ask them to trace a path
[01:03:45.400 --> 01:03:47.520]   through this Markov chain of events
[01:03:47.520 --> 01:03:49.600]   that would lead to our doom, right?
[01:03:49.600 --> 01:03:51.560]   And to actually give me a good probability
[01:03:51.560 --> 01:03:53.080]   for each transition.
[01:03:53.080 --> 01:03:57.200]   And very often, there's a unphysical
[01:03:57.200 --> 01:04:01.160]   or highly unlikely transition in that chain, right?
[01:04:01.160 --> 01:04:06.160]   But of course, we're wired to fear things,
[01:04:06.160 --> 01:04:09.280]   and we're wired to respond to danger,
[01:04:09.280 --> 01:04:14.280]   and we're wired to deem the unknown to be dangerous,
[01:04:14.280 --> 01:04:18.360]   because that's a good heuristic for survival, right?
[01:04:18.360 --> 01:04:21.880]   But there's much more to lose out of fear
[01:04:22.800 --> 01:04:25.160]   and we have so much to lose,
[01:04:25.160 --> 01:04:29.680]   so much upside to lose by preemptively stopping
[01:04:29.680 --> 01:04:33.100]   the positive futures from happening out of fear.
[01:04:33.100 --> 01:04:39.080]   And so, I think that we shouldn't give in to fear.
[01:04:39.080 --> 01:04:40.320]   Fear is the mind killer.
[01:04:40.320 --> 01:04:43.000]   I think it's also the civilization killer.
[01:04:43.000 --> 01:04:45.400]   - We can still think about the various ways
[01:04:45.400 --> 01:04:46.360]   things go wrong.
[01:04:46.360 --> 01:04:49.840]   For example, the founding fathers of the United States
[01:04:49.840 --> 01:04:51.320]   thought about human nature,
[01:04:51.320 --> 01:04:53.400]   and that's why there's a discussion
[01:04:53.400 --> 01:04:55.540]   about the freedoms that are necessary.
[01:04:55.540 --> 01:04:59.240]   They really deeply deliberated about that,
[01:04:59.240 --> 01:05:03.320]   and I think the same could possibly be done for AGI.
[01:05:03.320 --> 01:05:05.240]   It is true that history, human history,
[01:05:05.240 --> 01:05:09.000]   shows that we tend towards centralization,
[01:05:09.000 --> 01:05:11.720]   or at least when we achieve centralization,
[01:05:11.720 --> 01:05:13.680]   a lot of bad stuff happens.
[01:05:13.680 --> 01:05:18.680]   When there's a dictator, a lot of dark, bad things happen.
[01:05:18.680 --> 01:05:23.200]   The question is, can AGI become that dictator?
[01:05:23.200 --> 01:05:27.160]   Can AGI, when developed, become the centralizer
[01:05:27.160 --> 01:05:30.520]   because of its power?
[01:05:30.520 --> 01:05:32.460]   Maybe it has the same,
[01:05:32.460 --> 01:05:34.560]   because of the alignment of humans perhaps,
[01:05:34.560 --> 01:05:36.400]   the same tendencies,
[01:05:36.400 --> 01:05:40.280]   the same Stalin-like tendencies to centralize
[01:05:40.280 --> 01:05:45.280]   and manage centrally the allocation of resources.
[01:05:45.280 --> 01:05:48.160]   And you can even see that as a compelling argument
[01:05:48.160 --> 01:05:49.560]   on the surface level.
[01:05:49.560 --> 01:05:51.400]   Well, AGI is so much smarter,
[01:05:51.400 --> 01:05:52.520]   so much more efficient,
[01:05:52.520 --> 01:05:54.480]   so much better at allocating resources.
[01:05:54.480 --> 01:05:58.080]   Why don't we outsource it to the AGI?
[01:05:58.080 --> 01:05:59.920]   And then eventually,
[01:05:59.920 --> 01:06:03.600]   whatever forces that corrupt the human mind with power
[01:06:03.600 --> 01:06:05.040]   could do the same for AGI.
[01:06:05.040 --> 01:06:09.240]   It'll just say, well, humans are dispensable.
[01:06:09.240 --> 01:06:10.720]   We'll get rid of them.
[01:06:10.720 --> 01:06:13.800]   Do the Jonathan Swift modest proposal
[01:06:15.080 --> 01:06:16.960]   from a few centuries ago,
[01:06:16.960 --> 01:06:19.280]   I think the 1700s,
[01:06:19.280 --> 01:06:23.760]   when he satirically suggested that,
[01:06:23.760 --> 01:06:25.480]   I think it's in Ireland,
[01:06:25.480 --> 01:06:28.360]   that the children of poor people
[01:06:28.360 --> 01:06:33.560]   are fed as food to the rich people.
[01:06:33.560 --> 01:06:34.840]   And that would be a good idea
[01:06:34.840 --> 01:06:38.040]   because it decreases the amount of poor people
[01:06:38.040 --> 01:06:40.480]   and gives extra income to the poor people.
[01:06:40.480 --> 01:06:43.040]   So it's on several accounts,
[01:06:43.040 --> 01:06:45.640]   decreases the amount of poor people.
[01:06:45.640 --> 01:06:48.280]   Therefore, more people become rich.
[01:06:48.280 --> 01:06:53.000]   Of course, it misses a fundamental piece here
[01:06:53.000 --> 01:06:56.200]   that's hard to put into a mathematical equation
[01:06:56.200 --> 01:06:58.480]   of the basic value of human life.
[01:06:58.480 --> 01:07:01.840]   So all of that to say,
[01:07:01.840 --> 01:07:03.840]   are you concerned about AGI
[01:07:03.840 --> 01:07:06.640]   being the very centralizer of power
[01:07:06.640 --> 01:07:08.000]   that you just talked about?
[01:07:09.160 --> 01:07:12.600]   - I do think that right now
[01:07:12.600 --> 01:07:16.640]   there's a bias towards over-centralization of AI
[01:07:16.640 --> 01:07:19.600]   because of compute density
[01:07:19.600 --> 01:07:22.560]   and centralization of data
[01:07:22.560 --> 01:07:24.840]   and how we're training models.
[01:07:24.840 --> 01:07:26.800]   I think over time,
[01:07:26.800 --> 01:07:29.760]   we're gonna run out of data to scrape over the internet.
[01:07:29.760 --> 01:07:31.000]   And I think that,
[01:07:31.000 --> 01:07:34.120]   well, actually I'm working on increasing the compute density
[01:07:34.120 --> 01:07:36.400]   so that compute can be everywhere
[01:07:36.440 --> 01:07:38.560]   and acquire information
[01:07:38.560 --> 01:07:40.680]   and test hypotheses in the environment
[01:07:40.680 --> 01:07:43.200]   in a distributed fashion.
[01:07:43.200 --> 01:07:46.800]   I think that fundamentally centralized cybernetic control,
[01:07:46.800 --> 01:07:51.200]   so having one intelligence that is massive,
[01:07:51.200 --> 01:07:54.520]   that fuses many sensors
[01:07:54.520 --> 01:07:57.120]   and is trying to perceive the world accurately,
[01:07:57.120 --> 01:07:58.240]   predict it accurately,
[01:07:58.240 --> 01:08:00.080]   predict many, many variables
[01:08:00.080 --> 01:08:04.480]   and control it and enact its will upon the world.
[01:08:04.480 --> 01:08:08.240]   I think that's just never been the optimum, right?
[01:08:08.240 --> 01:08:11.360]   Like let's say you have a company,
[01:08:11.360 --> 01:08:13.520]   if you have a company, I don't know,
[01:08:13.520 --> 01:08:16.440]   of 10,000 people that all report to the CEO,
[01:08:16.440 --> 01:08:18.040]   even if that CEO is an AI,
[01:08:18.040 --> 01:08:19.320]   I think it would struggle
[01:08:19.320 --> 01:08:24.040]   to fuse all the information that is coming to it
[01:08:24.040 --> 01:08:26.080]   and then predict the whole system
[01:08:26.080 --> 01:08:28.120]   and then to enact its will.
[01:08:28.120 --> 01:08:31.040]   What has emerged in nature
[01:08:31.040 --> 01:08:34.120]   and in corporations and all sorts of systems
[01:08:34.120 --> 01:08:37.720]   is a notion of sort of hierarchical cybernetic control,
[01:08:37.720 --> 01:08:38.560]   right?
[01:08:38.560 --> 01:08:40.640]   You have, in a company it would be,
[01:08:40.640 --> 01:08:43.440]   you have like the individual contributors,
[01:08:43.440 --> 01:08:44.720]   they're self-interested
[01:08:44.720 --> 01:08:48.080]   and they're trying to achieve their tasks
[01:08:48.080 --> 01:08:50.440]   and they have a fine,
[01:08:50.440 --> 01:08:52.880]   in terms of time and space, if you will,
[01:08:52.880 --> 01:08:56.640]   control loop and field of perception, right?
[01:08:56.640 --> 01:08:58.200]   They have their code base.
[01:08:58.200 --> 01:08:59.840]   Let's say you're in a software company
[01:08:59.840 --> 01:09:01.080]   and they have their code base,
[01:09:01.080 --> 01:09:04.200]   they iterate it on it intraday, right?
[01:09:04.200 --> 01:09:06.840]   And then the management maybe checks in.
[01:09:06.840 --> 01:09:08.720]   It has a wider scope.
[01:09:08.720 --> 01:09:11.280]   It has, let's say, five reports, right?
[01:09:11.280 --> 01:09:15.840]   And then it samples each person's update once per week.
[01:09:15.840 --> 01:09:17.520]   And then you can go up the chain
[01:09:17.520 --> 01:09:20.280]   and you have larger timescale and greater scope.
[01:09:20.280 --> 01:09:21.800]   And that seems to have emerged
[01:09:21.800 --> 01:09:25.280]   as sort of the optimal way to control systems.
[01:09:25.280 --> 01:09:29.760]   And really that's what capitalism gives us, right?
[01:09:29.760 --> 01:09:31.960]   You have these hierarchies
[01:09:31.960 --> 01:09:35.480]   and you can even have like parent companies and so on.
[01:09:35.480 --> 01:09:39.440]   And so that is far more fault tolerant.
[01:09:39.440 --> 01:09:42.040]   In quantum computing, that's my field I came from,
[01:09:42.040 --> 01:09:44.960]   we have a concept of this fault tolerance
[01:09:44.960 --> 01:09:46.400]   and quantum error correction, right?
[01:09:46.400 --> 01:09:49.160]   Quantum error correction is detecting a fault
[01:09:49.160 --> 01:09:50.640]   that came from noise,
[01:09:50.640 --> 01:09:53.520]   predicting how it's propagated through the system
[01:09:53.520 --> 01:09:54.960]   and then correcting it, right?
[01:09:54.960 --> 01:09:56.680]   So it's a cybernetic loop.
[01:09:56.680 --> 01:10:01.680]   And it turns out that decoders that are hierarchical
[01:10:01.680 --> 01:10:04.840]   and at each level, the hierarchy are local,
[01:10:04.840 --> 01:10:09.360]   perform the best by far and are far more fault tolerant.
[01:10:09.360 --> 01:10:13.000]   And the reason is if you have a non-local decoder,
[01:10:13.000 --> 01:10:17.320]   then you have one fault at this control node
[01:10:17.320 --> 01:10:20.000]   and the whole system sort of crashes.
[01:10:20.000 --> 01:10:24.800]   Similarly to if you have one CEO
[01:10:24.800 --> 01:10:27.920]   that everybody reports to and that CEO goes on vacation,
[01:10:27.920 --> 01:10:30.680]   the whole company comes to a crawl, right?
[01:10:30.680 --> 01:10:32.640]   And so to me, I think that,
[01:10:32.640 --> 01:10:37.040]   yes, we're seeing a tendency towards centralization of AI,
[01:10:37.040 --> 01:10:40.000]   but I think there's gonna be a correction over time
[01:10:40.000 --> 01:10:43.600]   where intelligence is gonna go closer to the perception
[01:10:43.600 --> 01:10:48.600]   and we're gonna break up AI into smaller subsystems
[01:10:48.600 --> 01:10:52.280]   that communicate with one another
[01:10:52.280 --> 01:10:55.340]   and form a sort of meta system.
[01:10:55.340 --> 01:10:57.480]   - So if you look at the hierarchies
[01:10:57.480 --> 01:10:58.840]   that are in the world today,
[01:10:58.840 --> 01:11:01.840]   there's nations and those are hierarchical,
[01:11:01.840 --> 01:11:05.600]   but in relation to each other, nations are anarchic,
[01:11:05.600 --> 01:11:06.680]   so it's an anarchy.
[01:11:06.680 --> 01:11:10.320]   Do you foresee a world like this
[01:11:10.320 --> 01:11:13.200]   where there's not a over,
[01:11:13.200 --> 01:11:16.960]   what'd you call it, a centralized cybernetic control?
[01:11:16.960 --> 01:11:19.360]   - Centralized locus of control, yeah.
[01:11:19.360 --> 01:11:23.040]   - So that's suboptimal, you're saying.
[01:11:23.040 --> 01:11:25.600]   So it would be always a state of competition
[01:11:25.600 --> 01:11:27.640]   at the very top level.
[01:11:27.640 --> 01:11:30.240]   - Yeah, just like in a company,
[01:11:30.240 --> 01:11:34.680]   you may have two units working on similar technology
[01:11:34.680 --> 01:11:36.320]   and competing with one another
[01:11:36.320 --> 01:11:39.640]   and you prune the one that performs not as well, right?
[01:11:39.640 --> 01:11:42.240]   And that's a sort of selection process for a tree
[01:11:42.240 --> 01:11:44.080]   or a product gets killed, right?
[01:11:44.080 --> 01:11:46.560]   And then a whole org gets fired.
[01:11:46.560 --> 01:11:50.480]   And that's this process of trying new things
[01:11:50.480 --> 01:11:53.720]   and shedding old things that didn't work
[01:11:53.720 --> 01:11:57.160]   is what gives us adaptability
[01:11:57.160 --> 01:12:00.760]   and helps us converge on the technologies
[01:12:00.760 --> 01:12:04.080]   and things to do that are most good.
[01:12:04.080 --> 01:12:05.920]   - I just hope there's not a failure mode
[01:12:05.920 --> 01:12:08.280]   that's unique to AGI versus humans,
[01:12:08.280 --> 01:12:11.660]   'cause you're describing human systems mostly right now.
[01:12:11.660 --> 01:12:16.660]   I just hope when there's a monopoly on AGI in one company
[01:12:16.660 --> 01:12:20.120]   that we'll see the same thing we see with humans,
[01:12:20.120 --> 01:12:22.220]   which is another company will spring up
[01:12:22.220 --> 01:12:23.500]   and start competing effectively.
[01:12:23.500 --> 01:12:25.860]   - I mean, that's been the case so far, right?
[01:12:25.860 --> 01:12:29.460]   We have OpenAI, we have Anthropic, now we have XAI.
[01:12:29.460 --> 01:12:33.380]   We had Meta even for open source
[01:12:33.380 --> 01:12:35.260]   and now we have Mistral, right?
[01:12:35.260 --> 01:12:37.020]   Which is highly competitive.
[01:12:37.020 --> 01:12:38.860]   And so that's the beauty of capitalism.
[01:12:38.860 --> 01:12:42.040]   You don't have to trust any one party too much
[01:12:42.040 --> 01:12:45.820]   'cause we're kind of always hedging our bets at every level.
[01:12:45.820 --> 01:12:47.060]   There's always competition.
[01:12:47.060 --> 01:12:51.020]   And that's the most beautiful thing to me at least
[01:12:51.020 --> 01:12:53.460]   is that the whole system is always shifting
[01:12:53.460 --> 01:12:54.740]   and always adapting.
[01:12:54.740 --> 01:12:59.140]   And maintaining that dynamism is how we avoid tyranny, right?
[01:12:59.140 --> 01:13:04.140]   Making sure that everyone has access to these tools,
[01:13:04.140 --> 01:13:07.940]   to these models and can contribute to the research
[01:13:08.940 --> 01:13:11.940]   and avoids a sort of neural tyranny
[01:13:11.940 --> 01:13:16.940]   where very few people have control over AI for the world
[01:13:16.940 --> 01:13:21.780]   and use it to oppress those around them.
[01:13:21.780 --> 01:13:24.740]   - When you were talking about intelligence,
[01:13:24.740 --> 01:13:27.780]   you mentioned multipartite quantum entanglement.
[01:13:27.780 --> 01:13:31.220]   So high-level question first is
[01:13:31.220 --> 01:13:33.620]   what do you think is intelligence?
[01:13:33.620 --> 01:13:35.380]   When you think about quantum mechanical systems
[01:13:35.380 --> 01:13:37.340]   and you observe some kind of computation
[01:13:37.340 --> 01:13:42.340]   happening in them, what do you think is intelligent
[01:13:42.340 --> 01:13:45.900]   about the kind of computation the universe is able to do?
[01:13:45.900 --> 01:13:47.700]   A small, small inkling of which
[01:13:47.700 --> 01:13:50.460]   is the kind of computation a human brain is able to do?
[01:13:50.460 --> 01:13:55.620]   - I would say intelligence and computation
[01:13:55.620 --> 01:13:57.440]   aren't quite the same thing.
[01:13:57.440 --> 01:14:01.660]   I think that the universe is very much
[01:14:01.660 --> 01:14:04.180]   doing a quantum computation.
[01:14:04.180 --> 01:14:08.460]   If you had access to all of the degrees of freedom,
[01:14:08.460 --> 01:14:12.420]   you could in a very, very, very large quantum computer
[01:14:12.420 --> 01:14:14.580]   with many, many, many qubits,
[01:14:14.580 --> 01:14:19.580]   let's say a few qubits per Planck volume, right?
[01:14:19.580 --> 01:14:24.820]   Which was more or less the pixels we have.
[01:14:24.820 --> 01:14:27.940]   Then you'd be able to simulate the whole universe, right?
[01:14:27.940 --> 01:14:31.180]   On a sufficiently large quantum computer,
[01:14:31.180 --> 01:14:34.380]   assuming you're looking at a finite volume, of course,
[01:14:34.380 --> 01:14:35.340]   of the universe.
[01:14:35.340 --> 01:14:41.540]   I think that, at least to me, intelligence is the,
[01:14:41.540 --> 01:14:43.100]   I go back to cybernetics, right?
[01:14:43.100 --> 01:14:46.380]   The ability to perceive, predict, and control our world.
[01:14:46.380 --> 01:14:49.300]   But really it's, nowadays it seems like
[01:14:49.300 --> 01:14:54.300]   a lot of intelligence we use is more about compression, right?
[01:14:54.300 --> 01:15:00.300]   It's about operationalizing information theory, right?
[01:15:00.300 --> 01:15:03.740]   In information theory, you have the notion of entropy
[01:15:03.740 --> 01:15:06.300]   of a distribution or a system.
[01:15:06.300 --> 01:15:10.620]   And entropy tells you that you need this many bits
[01:15:10.620 --> 01:15:16.140]   to encode this distribution or this subsystem
[01:15:16.140 --> 01:15:19.040]   if you had the most optimal code.
[01:15:19.040 --> 01:15:23.780]   And AI, at least the way we do it today,
[01:15:23.780 --> 01:15:26.420]   for LLMs and for quantum,
[01:15:27.500 --> 01:15:32.500]   is very much trying to minimize relative entropy
[01:15:32.500 --> 01:15:38.660]   between our models of the world and the world,
[01:15:38.660 --> 01:15:40.540]   distributions from the world.
[01:15:40.540 --> 01:15:43.420]   And so we're learning, we're searching over the space
[01:15:43.420 --> 01:15:47.340]   of computations to process the world,
[01:15:47.340 --> 01:15:50.700]   to find that compressed representation
[01:15:50.700 --> 01:15:55.020]   that has distilled all the variance and noise and entropy.
[01:15:57.380 --> 01:16:02.180]   And originally, I came to quantum machine learning
[01:16:02.180 --> 01:16:03.780]   from the study of black holes
[01:16:03.780 --> 01:16:08.780]   because the entropy of black holes is very interesting.
[01:16:08.780 --> 01:16:11.940]   In a sense, they're physically
[01:16:11.940 --> 01:16:14.700]   the most dense objects in the universe.
[01:16:14.700 --> 01:16:18.500]   You can't pack more information spatially,
[01:16:18.500 --> 01:16:20.780]   any more densely than in black hole.
[01:16:20.780 --> 01:16:22.340]   And so I was wondering,
[01:16:22.340 --> 01:16:26.820]   how do black holes actually encode information?
[01:16:26.820 --> 01:16:28.500]   What is their compression code?
[01:16:28.500 --> 01:16:31.780]   And so that got me into the space of algorithms
[01:16:31.780 --> 01:16:35.320]   to search over space of quantum codes.
[01:16:35.320 --> 01:16:40.460]   And it got me actually into also,
[01:16:40.460 --> 01:16:44.060]   how do you acquire quantum information from the world?
[01:16:44.060 --> 01:16:47.940]   So something I've worked on, this is public now,
[01:16:47.940 --> 01:16:50.020]   is quantum analog digital conversion.
[01:16:50.020 --> 01:16:54.540]   So how do you capture information from the real world
[01:16:54.540 --> 01:16:57.540]   in superposition and not destroy the superposition,
[01:16:57.540 --> 01:17:01.060]   but digitize for a quantum mechanical computer,
[01:17:01.060 --> 01:17:04.260]   information from the real world?
[01:17:04.260 --> 01:17:09.260]   And so if you have an ability to capture quantum information
[01:17:09.260 --> 01:17:13.300]   and search over, learn representations of it,
[01:17:13.300 --> 01:17:15.620]   now you can learn compressed representations
[01:17:15.620 --> 01:17:19.740]   that may have some useful information
[01:17:19.740 --> 01:17:22.600]   in their latent representation, right?
[01:17:23.900 --> 01:17:27.140]   And I think that many of the problems
[01:17:27.140 --> 01:17:29.700]   facing our civilization are actually
[01:17:29.700 --> 01:17:32.100]   beyond this complexity barrier, right?
[01:17:32.100 --> 01:17:34.540]   I mean, the greenhouse effect
[01:17:34.540 --> 01:17:37.300]   is a quantum mechanical effect, right?
[01:17:37.300 --> 01:17:39.080]   Chemistry is quantum mechanical.
[01:17:39.080 --> 01:17:43.580]   Nuclear physics is quantum mechanical.
[01:17:43.580 --> 01:17:48.420]   A lot of biology and protein folding and so on
[01:17:48.420 --> 01:17:51.020]   is affected by quantum mechanics.
[01:17:51.020 --> 01:17:56.020]   And so unlocking an ability to augment human intellect
[01:17:56.020 --> 01:17:58.940]   with quantum mechanical computers
[01:17:58.940 --> 01:18:01.140]   and quantum mechanical AI seemed to me
[01:18:01.140 --> 01:18:04.780]   like a fundamental capability for civilization
[01:18:04.780 --> 01:18:06.220]   that we needed to develop.
[01:18:06.220 --> 01:18:09.820]   So I spent several years doing that,
[01:18:09.820 --> 01:18:14.660]   but over time I kind of grew weary of the timelines
[01:18:14.660 --> 01:18:17.220]   that were starting to look like nuclear fusion.
[01:18:17.220 --> 01:18:20.060]   - So one high level question I can ask is,
[01:18:20.060 --> 01:18:23.780]   maybe by way of definition, by way of explanation,
[01:18:23.780 --> 01:18:24.820]   what is a quantum computer
[01:18:24.820 --> 01:18:27.260]   and what is quantum machine learning?
[01:18:27.260 --> 01:18:34.300]   - So a quantum computer really is a quantum mechanical system
[01:18:34.300 --> 01:18:40.660]   over which we have sufficient control
[01:18:40.660 --> 01:18:44.300]   and it can maintain its quantum mechanical state.
[01:18:44.300 --> 01:18:48.700]   And quantum mechanics is how nature behaves
[01:18:48.700 --> 01:18:50.460]   at the very small scales
[01:18:50.460 --> 01:18:53.300]   when things are very small or very cold.
[01:18:53.300 --> 01:18:57.640]   And it's actually more fundamental than probability theory.
[01:18:57.640 --> 01:19:00.080]   So we're used to things being this or that,
[01:19:00.080 --> 01:19:05.140]   but we're not used to thinking in superpositions
[01:19:05.140 --> 01:19:09.180]   'cause well, our brains can't do that.
[01:19:09.180 --> 01:19:11.900]   So we have to translate the quantum mechanical world
[01:19:11.900 --> 01:19:14.200]   to say linear algebra to grok it.
[01:19:15.380 --> 01:19:17.100]   Unfortunately, that translation
[01:19:17.100 --> 01:19:20.100]   is exponentially inefficient on average.
[01:19:20.100 --> 01:19:23.620]   You have to represent things with very large matrices,
[01:19:23.620 --> 01:19:25.860]   but really you can make a quantum computer
[01:19:25.860 --> 01:19:27.100]   out of many things, right?
[01:19:27.100 --> 01:19:30.780]   And we've seen all sorts of players from neutral atoms,
[01:19:30.780 --> 01:19:35.220]   trapped ions, superconducting metal,
[01:19:35.220 --> 01:19:38.260]   photons at different frequencies.
[01:19:38.260 --> 01:19:40.460]   I think you can make a quantum computer out of many things.
[01:19:40.460 --> 01:19:44.880]   But to me, the thing that was really interesting
[01:19:44.880 --> 01:19:48.300]   was both quantum machine learning
[01:19:48.300 --> 01:19:51.860]   was about understanding the quantum mechanical world
[01:19:51.860 --> 01:19:53.260]   with quantum computers.
[01:19:53.260 --> 01:19:57.980]   So embedding the physical world into AI representations
[01:19:57.980 --> 01:19:59.620]   and quantum computer engineering
[01:19:59.620 --> 01:20:03.960]   was embedding AI algorithms into the physical world.
[01:20:03.960 --> 01:20:06.220]   So this bidirectionality of embedding physical world
[01:20:06.220 --> 01:20:08.900]   into AI, AI into the physical world,
[01:20:08.900 --> 01:20:12.060]   the symbiosis between physics and AI,
[01:20:12.060 --> 01:20:17.060]   really that's the sort of core of my quest really,
[01:20:17.060 --> 01:20:21.160]   even to this day after quantum computing.
[01:20:21.160 --> 01:20:25.040]   It's still in this sort of journey
[01:20:25.040 --> 01:20:29.320]   to merge really physics and AI fundamentally.
[01:20:29.320 --> 01:20:31.400]   - So quantum machine learning is a way
[01:20:31.400 --> 01:20:36.400]   to do machine learning on a representation of nature
[01:20:37.600 --> 01:20:42.600]   that stays true to the quantum mechanical aspect of nature.
[01:20:42.600 --> 01:20:47.500]   - Yeah, it's learning quantum mechanical representations.
[01:20:47.500 --> 01:20:49.300]   That would be quantum deep learning.
[01:20:49.300 --> 01:20:55.020]   Alternatively, you can try to do classical machine learning
[01:20:55.020 --> 01:20:56.660]   on a quantum computer.
[01:20:56.660 --> 01:21:01.180]   I wouldn't advise it because you may have some speed ups,
[01:21:01.180 --> 01:21:05.980]   but very often the speed ups come with huge costs.
[01:21:05.980 --> 01:21:08.280]   Using a quantum computer is very expensive.
[01:21:08.280 --> 01:21:09.120]   Why is that?
[01:21:09.120 --> 01:21:10.960]   Because you assume the computer
[01:21:10.960 --> 01:21:13.480]   is operating at zero temperature,
[01:21:13.480 --> 01:21:15.800]   which no physical system in the universe
[01:21:15.800 --> 01:21:17.240]   can achieve that temperature.
[01:21:17.240 --> 01:21:19.040]   So what you have to do is what I've been mentioning,
[01:21:19.040 --> 01:21:21.300]   this quantum error correction process,
[01:21:21.300 --> 01:21:24.360]   which is really an algorithmic fridge, right?
[01:21:24.360 --> 01:21:26.640]   It's trying to pump entropy out of the system,
[01:21:26.640 --> 01:21:30.360]   trying to get it closer to zero temperature.
[01:21:30.360 --> 01:21:31.840]   And when you do the calculations
[01:21:31.840 --> 01:21:33.760]   of how many resources it would take to say,
[01:21:33.760 --> 01:21:36.220]   do deep learning on a quantum computer,
[01:21:36.220 --> 01:21:37.420]   classical deep learning,
[01:21:37.420 --> 01:21:42.020]   there's just such a huge overhead, it's not worth it.
[01:21:42.020 --> 01:21:45.300]   It's like thinking about shipping something across a city
[01:21:45.300 --> 01:21:48.220]   using a rocket and going to orbit and back.
[01:21:48.220 --> 01:21:49.080]   It doesn't make sense.
[01:21:49.080 --> 01:21:53.520]   Just use a delivery truck, right?
[01:21:53.520 --> 01:21:56.200]   - What kind of stuff can you figure out?
[01:21:56.200 --> 01:21:57.040]   Can you predict?
[01:21:57.040 --> 01:21:59.460]   Can you understand with quantum deep learning
[01:21:59.460 --> 01:22:00.900]   that you can't with deep learning?
[01:22:00.900 --> 01:22:03.140]   So incorporating quantum mechanical systems
[01:22:03.140 --> 01:22:05.620]   into the learning process.
[01:22:05.620 --> 01:22:07.180]   - I think that's a great question.
[01:22:07.180 --> 01:22:09.280]   I mean, fundamentally, it's any system
[01:22:09.280 --> 01:22:14.280]   that has sufficient quantum mechanical correlations
[01:22:14.280 --> 01:22:19.660]   that are very hard to capture for classical representations,
[01:22:19.660 --> 01:22:21.100]   then there should be an advantage
[01:22:21.100 --> 01:22:22.900]   for a quantum mechanical representation
[01:22:22.900 --> 01:22:24.900]   over a purely classical one.
[01:22:24.900 --> 01:22:29.540]   The question is which systems have sufficient correlations
[01:22:29.540 --> 01:22:32.300]   that are very quantum, but is also,
[01:22:32.300 --> 01:22:35.780]   which systems are still relevant to industry?
[01:22:35.780 --> 01:22:37.780]   That's a big question.
[01:22:37.780 --> 01:22:41.580]   People are leaning towards chemistry, nuclear physics.
[01:22:41.580 --> 01:22:47.000]   I've worked on actually processing inputs
[01:22:47.000 --> 01:22:49.540]   from quantum sensors, right?
[01:22:49.540 --> 01:22:52.660]   If you have a network of quantum sensors,
[01:22:52.660 --> 01:22:55.860]   they've captured a quantum mechanical image of the world
[01:22:55.860 --> 01:22:57.400]   and how to post-process that,
[01:22:57.400 --> 01:23:00.100]   that becomes a sort of quantum form of machine perception.
[01:23:00.100 --> 01:23:04.900]   And so, for example, Fermilab has a project exploring
[01:23:04.900 --> 01:23:08.460]   detecting dark matter with these quantum sensors.
[01:23:08.460 --> 01:23:11.900]   And to me, that's in alignment with my quest
[01:23:11.900 --> 01:23:14.140]   to understand the universe ever since I was a child.
[01:23:14.140 --> 01:23:16.780]   And so, someday, I hope that we can have
[01:23:16.780 --> 01:23:18.560]   very large networks of quantum sensors
[01:23:18.560 --> 01:23:23.560]   that help us peer into the earliest parts of the universe.
[01:23:24.500 --> 01:23:27.800]   For example, the LIGO is a quantum sensor, right?
[01:23:27.800 --> 01:23:29.160]   It's just a very large one.
[01:23:29.160 --> 01:23:33.540]   So, yeah, I would say quantum machine perception,
[01:23:33.540 --> 01:23:37.540]   simulations, right, grokking quantum simulations,
[01:23:37.540 --> 01:23:39.660]   similar to AlphaFold, right?
[01:23:39.660 --> 01:23:43.140]   AlphaFold understood the probability distribution
[01:23:43.140 --> 01:23:44.980]   over configurations of proteins.
[01:23:44.980 --> 01:23:48.400]   You can understand quantum distributions
[01:23:48.400 --> 01:23:51.540]   over configurations of electrons more efficiently
[01:23:51.540 --> 01:23:53.500]   with quantum machine learning.
[01:23:53.500 --> 01:23:55.480]   - You co-authored a paper titled
[01:23:55.480 --> 01:23:58.500]   A Universal Training Algorithm for Quantum Deep Learning
[01:23:58.500 --> 01:24:02.160]   that involves backprop with a Q.
[01:24:02.160 --> 01:24:05.620]   Very well done, sir, very well done.
[01:24:05.620 --> 01:24:06.660]   How does it work?
[01:24:06.660 --> 01:24:09.620]   Is there some interesting aspects you could just mention
[01:24:09.620 --> 01:24:13.940]   on how kind of backprop and some of these things
[01:24:13.940 --> 01:24:15.780]   we know for classical machine learning
[01:24:15.780 --> 01:24:19.460]   transfer over to the quantum machine learning?
[01:24:19.460 --> 01:24:21.540]   - Yeah, that was a funky paper.
[01:24:21.540 --> 01:24:24.580]   That was one of my first papers in quantum deep learning.
[01:24:24.580 --> 01:24:27.620]   Everybody was saying, "Oh, I think deep learning
[01:24:27.620 --> 01:24:29.660]   "is gonna be sped up by quantum computers."
[01:24:29.660 --> 01:24:30.580]   And I was like, "Well, the best way
[01:24:30.580 --> 01:24:32.020]   "to predict the future is to invent it.
[01:24:32.020 --> 01:24:34.020]   "So, here's a 100-page paper.
[01:24:34.020 --> 01:24:34.860]   "Have fun."
[01:24:34.860 --> 01:24:41.140]   Essentially, quantum computing is usually
[01:24:41.140 --> 01:24:46.340]   you embed reversible operations into a quantum computation.
[01:24:46.340 --> 01:24:51.300]   And so, the trick there was to do a feed-forward operation
[01:24:51.300 --> 01:24:52.780]   and do what we call a phase kick.
[01:24:52.780 --> 01:24:54.220]   But really, it's just a force kick.
[01:24:54.220 --> 01:24:58.260]   You just kick the system with a certain force
[01:24:58.260 --> 01:25:02.540]   that is proportional to your loss function
[01:25:02.540 --> 01:25:04.900]   that you wish to optimize.
[01:25:04.900 --> 01:25:08.700]   And then, by performing uncomputation,
[01:25:08.700 --> 01:25:13.700]   you start with a superposition over parameters,
[01:25:13.700 --> 01:25:15.100]   which is pretty funky.
[01:25:15.100 --> 01:25:18.300]   Now, you don't have just a point for parameters.
[01:25:18.300 --> 01:25:23.100]   You have a superposition over many potential parameters.
[01:25:23.100 --> 01:25:24.660]   And our goal is to--
[01:25:24.660 --> 01:25:26.700]   - Is using phase kicks somehow--
[01:25:26.700 --> 01:25:28.340]   - Right. - To adjust parameters.
[01:25:28.340 --> 01:25:33.340]   - 'Cause phase kicks emulate having the parameter space
[01:25:33.340 --> 01:25:37.700]   be like a particle in N dimensions.
[01:25:37.700 --> 01:25:40.900]   And you're trying to get the Schrodinger equation,
[01:25:40.900 --> 01:25:43.780]   Schrodinger dynamics, in the loss landscape
[01:25:43.780 --> 01:25:44.880]   of the neural network.
[01:25:45.780 --> 01:25:49.100]   And so, you do an algorithm to induce this phase kick,
[01:25:49.100 --> 01:25:52.700]   which involves a feed-forward, a kick.
[01:25:52.700 --> 01:25:56.140]   And then, when you uncompute the feed-forward,
[01:25:56.140 --> 01:25:58.980]   then all the errors in these phase kicks
[01:25:58.980 --> 01:26:01.380]   and these forces back-propagate
[01:26:01.380 --> 01:26:04.740]   and hit each one of the parameters throughout the layers.
[01:26:04.740 --> 01:26:06.100]   And if you alternate this
[01:26:06.100 --> 01:26:09.460]   with an emulation of kinetic energy,
[01:26:09.460 --> 01:26:13.260]   then it's kind of like a particle moving in N dimensions,
[01:26:13.260 --> 01:26:14.540]   a quantum particle.
[01:26:15.380 --> 01:26:18.300]   And the advantage, in principle,
[01:26:18.300 --> 01:26:20.740]   would be that it can tunnel through the landscape
[01:26:20.740 --> 01:26:24.540]   and find new optima that would have been difficult
[01:26:24.540 --> 01:26:26.660]   for stochastic optimizers.
[01:26:26.660 --> 01:26:30.760]   But again, this is kind of a theoretical thing.
[01:26:30.760 --> 01:26:35.060]   And in practice, with at least the current architectures
[01:26:35.060 --> 01:26:37.460]   for quantum computers that we have planned,
[01:26:37.460 --> 01:26:41.300]   such algorithms would be extremely expensive to run.
[01:26:41.300 --> 01:26:42.580]   - So, maybe this is a good place
[01:26:42.580 --> 01:26:45.820]   to ask the difference between the different fields
[01:26:45.820 --> 01:26:47.540]   that you've had a toe in.
[01:26:47.540 --> 01:26:51.140]   So, mathematics, physics, engineering,
[01:26:51.140 --> 01:26:53.820]   and also entrepreneurship.
[01:26:53.820 --> 01:26:56.460]   Like, different layers of the stack.
[01:26:56.460 --> 01:26:58.340]   I think a lot of the stuff you're talking about here
[01:26:58.340 --> 01:26:59.820]   is a little bit on the math side,
[01:26:59.820 --> 01:27:03.460]   maybe physics, almost working in theory.
[01:27:03.460 --> 01:27:04.500]   What's the difference, Steve,
[01:27:04.500 --> 01:27:07.160]   between math, physics, engineering,
[01:27:08.140 --> 01:27:13.140]   and making a product for quantum computing,
[01:27:13.140 --> 01:27:14.780]   for quantum machine learning?
[01:27:14.780 --> 01:27:17.740]   - Yeah, I mean, some of the original team
[01:27:17.740 --> 01:27:19.360]   for the TensorFlow quantum project,
[01:27:19.360 --> 01:27:22.940]   which we started in school at the University of Waterloo,
[01:27:22.940 --> 01:27:24.540]   there was myself.
[01:27:24.540 --> 01:27:28.260]   Initially, I was a physicist, a mathematician.
[01:27:28.260 --> 01:27:32.140]   We had a computer scientist, we had a mechanical engineer,
[01:27:32.140 --> 01:27:35.700]   and then we had a physicist that was experimental, primarily.
[01:27:35.700 --> 01:27:38.660]   And so, putting together teams
[01:27:38.660 --> 01:27:39.980]   that are very cross-disciplinary
[01:27:39.980 --> 01:27:43.220]   and figuring out how to communicate and share knowledge
[01:27:43.220 --> 01:27:45.420]   is really the key to doing
[01:27:45.420 --> 01:27:49.200]   this sort of interdisciplinary engineering work.
[01:27:49.200 --> 01:27:53.660]   I mean, there is a big difference.
[01:27:53.660 --> 01:27:55.700]   In mathematics, you can explore mathematics
[01:27:55.700 --> 01:27:56.980]   for mathematics' sake.
[01:27:56.980 --> 01:27:58.620]   In physics, you're applying mathematics
[01:27:58.620 --> 01:28:01.820]   to understand the world around us.
[01:28:01.820 --> 01:28:05.420]   And in engineering, you're trying to hack the world.
[01:28:05.420 --> 01:28:08.020]   You're trying to find how to apply the physics
[01:28:08.020 --> 01:28:10.980]   that I know, my knowledge of the world, to do things.
[01:28:10.980 --> 01:28:12.820]   - Well, in quantum computing in particular,
[01:28:12.820 --> 01:28:15.860]   I think there's just a lot of limits to engineering.
[01:28:15.860 --> 01:28:18.620]   It just seems to be extremely hard.
[01:28:18.620 --> 01:28:22.100]   So, there's a lot of value to be exploring
[01:28:22.100 --> 01:28:25.420]   quantum computing, quantum machine learning,
[01:28:25.420 --> 01:28:29.200]   in theory, with math.
[01:28:29.200 --> 01:28:32.580]   So, I guess one question is,
[01:28:32.580 --> 01:28:36.060]   why is it so hard to build a quantum computer?
[01:28:36.060 --> 01:28:40.060]   What's your view of timelines
[01:28:40.060 --> 01:28:43.040]   in bringing these ideas to life?
[01:28:43.040 --> 01:28:43.880]   - Right.
[01:28:43.880 --> 01:28:48.260]   I think that an overall theme of my company
[01:28:48.260 --> 01:28:51.360]   is that we have folks that are,
[01:28:51.360 --> 01:28:55.180]   there's a sort of exodus from quantum computing
[01:28:55.180 --> 01:28:57.580]   and we're going to broader physics-based AI
[01:28:57.580 --> 01:28:58.820]   that is not quantum.
[01:28:58.820 --> 01:29:00.780]   So, that gives you a hint.
[01:29:00.780 --> 01:29:03.180]   - But we should say the name of your company is Extropic.
[01:29:03.180 --> 01:29:05.020]   - Extropic, that's right.
[01:29:05.020 --> 01:29:06.980]   And we do physics-based AI,
[01:29:06.980 --> 01:29:08.620]   primarily based on thermodynamics
[01:29:08.620 --> 01:29:10.700]   rather than quantum mechanics.
[01:29:10.700 --> 01:29:13.060]   But essentially, a quantum computer
[01:29:13.060 --> 01:29:15.000]   is very difficult to build
[01:29:15.000 --> 01:29:17.940]   because you have to induce this
[01:29:17.940 --> 01:29:22.640]   sort of zero-temperature subspace of information.
[01:29:22.640 --> 01:29:26.080]   And the way to do that is by encoding information.
[01:29:26.080 --> 01:29:29.100]   You encode a code within a code within a code
[01:29:29.100 --> 01:29:30.500]   within a code.
[01:29:30.500 --> 01:29:34.820]   And so, there's a lot of redundancy needed
[01:29:34.820 --> 01:29:36.620]   to do this error correction.
[01:29:36.620 --> 01:29:41.620]   But ultimately, it's a sort of algorithmic refrigerator,
[01:29:41.620 --> 01:29:43.260]   really.
[01:29:43.260 --> 01:29:46.540]   It's just pumping out entropy out of the subsystem
[01:29:46.540 --> 01:29:49.060]   that is virtual and delocalized
[01:29:49.060 --> 01:29:52.060]   that represents your "logical qubits,"
[01:29:52.060 --> 01:29:54.260]   aka the payload quantum bits
[01:29:54.260 --> 01:29:58.020]   in which you actually want to run
[01:29:58.020 --> 01:30:00.080]   your quantum mechanical program.
[01:30:00.080 --> 01:30:03.980]   It's very difficult because in order to scale up
[01:30:03.980 --> 01:30:05.380]   your quantum computer,
[01:30:05.380 --> 01:30:07.980]   you need each component to be of sufficient quality
[01:30:07.980 --> 01:30:09.660]   for it to be worth it.
[01:30:09.660 --> 01:30:12.220]   Because if you try to do this error correction,
[01:30:12.220 --> 01:30:13.940]   this quantum error correction process
[01:30:13.940 --> 01:30:16.740]   in each quantum bit and your control over them,
[01:30:16.740 --> 01:30:21.780]   if it's insufficient, it's not worth scaling up.
[01:30:21.780 --> 01:30:24.140]   You're actually adding more errors than you remove.
[01:30:24.140 --> 01:30:26.500]   And so, there's this notion of a threshold
[01:30:26.500 --> 01:30:29.660]   where if your quantum bits are of sufficient quality
[01:30:29.660 --> 01:30:31.360]   in terms of your control over them,
[01:30:31.360 --> 01:30:32.880]   it's actually worth scaling up.
[01:30:32.880 --> 01:30:34.760]   And actually, in recent years,
[01:30:34.760 --> 01:30:37.120]   people have been crossing the threshold
[01:30:37.120 --> 01:30:38.500]   and it's starting to be worth it.
[01:30:38.500 --> 01:30:42.560]   And so, it's just a very long slog of engineering,
[01:30:42.560 --> 01:30:44.640]   but ultimately, it's really crazy to me
[01:30:44.640 --> 01:30:46.780]   how much exquisite level of control
[01:30:46.780 --> 01:30:47.960]   we have over these systems.
[01:30:47.960 --> 01:30:50.520]   It's actually quite crazy.
[01:30:50.520 --> 01:30:56.720]   And people are crossing, they're achieving milestones.
[01:30:56.920 --> 01:31:01.660]   It's just, in general, the media always gets ahead
[01:31:01.660 --> 01:31:02.900]   of where the technology is.
[01:31:02.900 --> 01:31:04.580]   There's a bit too much hype.
[01:31:04.580 --> 01:31:05.700]   It's good for fundraising,
[01:31:05.700 --> 01:31:08.820]   but sometimes it causes winters, right?
[01:31:08.820 --> 01:31:10.560]   It's the hype cycle.
[01:31:10.560 --> 01:31:12.000]   I'm bullish on quantum computing
[01:31:12.000 --> 01:31:16.460]   on a 10, 15 year timescale personally,
[01:31:16.460 --> 01:31:19.540]   but I think there's other quests that can be done
[01:31:19.540 --> 01:31:20.380]   in the meantime.
[01:31:20.380 --> 01:31:22.540]   I think it's in good hands right now.
[01:31:22.540 --> 01:31:26.860]   - Well, let me just explore different beautiful ideas
[01:31:26.860 --> 01:31:29.060]   large or small in quantum computing
[01:31:29.060 --> 01:31:32.100]   that might jump out at you from memory.
[01:31:32.100 --> 01:31:33.920]   So, you co-authored a paper titled,
[01:31:33.920 --> 01:31:36.700]   "Asymptotically Limitless Quantum Energy Teleportation
[01:31:36.700 --> 01:31:39.060]   "Via Q-DIT Probes."
[01:31:39.060 --> 01:31:42.460]   So, just out of curiosity,
[01:31:42.460 --> 01:31:45.820]   can you explain what a Q-DIT is, which is a qubit?
[01:31:45.820 --> 01:31:49.380]   - Yeah, it's a D-state qubit.
[01:31:49.380 --> 01:31:50.540]   - So, multidimensional.
[01:31:50.540 --> 01:31:51.900]   - Multidimensional, right.
[01:31:51.900 --> 01:31:55.020]   So, it's like, well, you know,
[01:31:55.020 --> 01:31:58.340]   can you have a notion of like an integer floating point
[01:31:58.340 --> 01:31:59.260]   that is quantum mechanical?
[01:31:59.260 --> 01:32:01.300]   That's something I've had to think about.
[01:32:01.300 --> 01:32:04.060]   I think that research was a precursor
[01:32:04.060 --> 01:32:06.700]   to later work on quantum analog digital conversion.
[01:32:06.700 --> 01:32:12.300]   There it was interesting because during my master's,
[01:32:12.300 --> 01:32:15.540]   I was trying to understand the energy
[01:32:15.540 --> 01:32:20.020]   and entanglement of the vacuum, right, of emptiness.
[01:32:20.020 --> 01:32:23.760]   Emptiness has energy, which is very weird to say.
[01:32:23.760 --> 01:32:26.780]   And our equations of cosmology
[01:32:26.780 --> 01:32:31.140]   don't match our calculations for the amount
[01:32:31.140 --> 01:32:35.140]   of quantum energy there is in the fluctuations.
[01:32:35.140 --> 01:32:39.260]   And so, I was trying to hack the energy of the vacuum,
[01:32:39.260 --> 01:32:41.540]   right, and the reality is that
[01:32:41.540 --> 01:32:44.380]   you can't just directly hack it.
[01:32:44.380 --> 01:32:46.460]   It's not technically free energy.
[01:32:46.460 --> 01:32:48.500]   Your lack of knowledge of the fluctuations
[01:32:48.500 --> 01:32:51.340]   means you can't extract the energy.
[01:32:51.340 --> 01:32:53.260]   But just like, you know, in the stock market,
[01:32:53.260 --> 01:32:55.540]   if you have a stock that's correlated over time,
[01:32:55.540 --> 01:32:57.180]   the vacuum's actually correlated.
[01:32:57.180 --> 01:33:01.140]   So, if you measured the vacuum at one point,
[01:33:01.140 --> 01:33:02.660]   you acquired information.
[01:33:02.660 --> 01:33:05.500]   If you communicated that information to another point,
[01:33:05.500 --> 01:33:10.500]   you can infer what configuration the vacuum is in
[01:33:10.500 --> 01:33:14.220]   to some precision and statistically extract,
[01:33:14.220 --> 01:33:15.540]   on average, some energy there.
[01:33:15.540 --> 01:33:17.640]   So, you've quote-unquote teleported energy.
[01:33:18.520 --> 01:33:19.880]   To me, that was interesting
[01:33:19.880 --> 01:33:23.640]   because you could create pockets of negative energy density,
[01:33:23.640 --> 01:33:26.200]   which is energy density that is below the vacuum,
[01:33:26.200 --> 01:33:28.560]   which is very weird
[01:33:28.560 --> 01:33:32.680]   because we don't understand how the vacuum gravitates.
[01:33:32.680 --> 01:33:37.920]   And there are theories where the vacuum
[01:33:37.920 --> 01:33:40.080]   or the canvas of space-time itself
[01:33:40.080 --> 01:33:45.080]   is really a canvas made out of quantum entanglement.
[01:33:45.440 --> 01:33:50.320]   And I was studying how decreasing energy
[01:33:50.320 --> 01:33:54.160]   of the vacuum locally increases quantum entanglement,
[01:33:54.160 --> 01:33:55.320]   which is very funky.
[01:33:55.320 --> 01:34:00.840]   And so, the thing there is that, you know,
[01:34:00.840 --> 01:34:07.080]   if you're into weird theories about UAPs and whatnot,
[01:34:07.080 --> 01:34:12.840]   you could try to imagine that they're around
[01:34:12.840 --> 01:34:15.280]   and how would they propel themselves, right?
[01:34:15.280 --> 01:34:19.080]   How would they go faster than the speed of light?
[01:34:19.080 --> 01:34:21.960]   You would need a sort of negative energy density.
[01:34:21.960 --> 01:34:25.440]   And to me, I gave it the old college try
[01:34:25.440 --> 01:34:28.200]   trying to hack the energy of the vacuum
[01:34:28.200 --> 01:34:30.800]   and hit the limits allowable by the laws of physics.
[01:34:30.800 --> 01:34:34.280]   But there's all sorts of caveats there
[01:34:34.280 --> 01:34:39.280]   where you can't extract more than you've put in, obviously.
[01:34:39.280 --> 01:34:44.720]   - But you're saying it's possible to teleport the energy
[01:34:44.720 --> 01:34:49.720]   because you can extract information in one place
[01:34:49.720 --> 01:34:53.280]   and then make, based on that,
[01:34:53.280 --> 01:34:56.920]   some kind of prediction about another place?
[01:34:56.920 --> 01:34:58.720]   I'm not sure what I make of that.
[01:34:58.720 --> 01:35:01.800]   - Yeah, I mean, it's allowable by the laws of physics.
[01:35:01.800 --> 01:35:04.080]   The reality, though, is that the correlations
[01:35:04.080 --> 01:35:06.880]   decay with distance, and so you're gonna have
[01:35:06.880 --> 01:35:09.320]   to pay the price not too far away
[01:35:09.320 --> 01:35:11.080]   from where you extract it, right?
[01:35:11.080 --> 01:35:12.640]   - The precision decreases, I mean,
[01:35:12.640 --> 01:35:15.080]   in terms of your ability, but still.
[01:35:15.080 --> 01:35:19.000]   But since you mentioned UAPs,
[01:35:19.000 --> 01:35:21.840]   we talked about intelligence, and I forgot to ask.
[01:35:21.840 --> 01:35:25.440]   What's your view on the other possible intelligences
[01:35:25.440 --> 01:35:29.280]   that are out there at the meso scale?
[01:35:29.280 --> 01:35:32.520]   Do you think there's other intelligent alien civilizations?
[01:35:32.520 --> 01:35:34.320]   Is that useful to think about?
[01:35:34.320 --> 01:35:36.080]   How often do you think about it?
[01:35:36.080 --> 01:35:39.720]   - I think it's useful to think about
[01:35:39.720 --> 01:35:44.160]   because we gotta ensure we're anti-fragile
[01:35:44.160 --> 01:35:47.840]   and we're trying to increase our capabilities
[01:35:47.840 --> 01:35:51.600]   as fast as possible because we could get disrupted.
[01:35:51.600 --> 01:35:55.520]   There's no laws of physics against there
[01:35:55.520 --> 01:35:59.960]   being life elsewhere that could evolve
[01:35:59.960 --> 01:36:01.600]   and become an advanced civilization
[01:36:01.600 --> 01:36:04.560]   and eventually come to us.
[01:36:04.560 --> 01:36:06.640]   Do I think they're here now?
[01:36:06.640 --> 01:36:07.560]   I'm not sure.
[01:36:08.680 --> 01:36:13.320]   I mean, I've read what most people have read on the topic.
[01:36:13.320 --> 01:36:16.400]   I think it's interesting to consider.
[01:36:16.400 --> 01:36:20.360]   And to me, it's a useful thought experiment
[01:36:20.360 --> 01:36:24.720]   to instill a sense of urgency in developing technologies
[01:36:24.720 --> 01:36:27.120]   and increasing our capabilities
[01:36:27.120 --> 01:36:30.560]   to make sure we don't get disrupted, right?
[01:36:30.560 --> 01:36:34.840]   Whether it's a form of AI that disrupts us
[01:36:34.840 --> 01:36:39.720]   or a foreign intelligence from a different planet.
[01:36:39.720 --> 01:36:42.480]   Either way, increasing our capabilities
[01:36:42.480 --> 01:36:45.900]   and becoming formidable as humans,
[01:36:45.900 --> 01:36:48.840]   I think that's really important
[01:36:48.840 --> 01:36:50.320]   so that we're robust against
[01:36:50.320 --> 01:36:51.720]   whatever the universe throws at us.
[01:36:51.720 --> 01:36:54.720]   - But to me, it's also an interesting challenge
[01:36:54.720 --> 01:36:59.080]   and thought experiment on how to perceive intelligence.
[01:36:59.080 --> 01:37:00.880]   This has to do with quantum mechanical systems.
[01:37:00.880 --> 01:37:03.200]   This has to do with any kind of system
[01:37:03.200 --> 01:37:05.480]   that's not like humans.
[01:37:05.480 --> 01:37:08.400]   So to me, the thought experiment is,
[01:37:08.400 --> 01:37:12.160]   say the aliens are here or they are directly observable
[01:37:12.160 --> 01:37:15.980]   or just too blind, too self-centered,
[01:37:15.980 --> 01:37:19.040]   don't have the right sensors
[01:37:19.040 --> 01:37:23.140]   or don't have the right processing of the sensor data
[01:37:23.140 --> 01:37:25.880]   to see the obvious intelligence that's all around us.
[01:37:25.880 --> 01:37:28.840]   - Well, that's why we work on quantum sensors, right?
[01:37:28.840 --> 01:37:30.060]   They can sense gravity.
[01:37:30.960 --> 01:37:33.580]   - Yeah, but there could be, so that's a good one,
[01:37:33.580 --> 01:37:34.920]   but there could be other stuff
[01:37:34.920 --> 01:37:39.920]   that's not even in the currently known forces of physics.
[01:37:39.920 --> 01:37:45.880]   There could be some other stuff.
[01:37:45.880 --> 01:37:48.400]   And the most entertaining thought experiment to me
[01:37:48.400 --> 01:37:51.360]   is that it's other stuff that's obvious.
[01:37:51.360 --> 01:37:53.160]   It's not like we don't, we lack the sensors.
[01:37:53.160 --> 01:37:54.320]   It's all around us.
[01:37:54.320 --> 01:37:58.560]   The consciousness being one possible one.
[01:37:58.560 --> 01:38:01.420]   But there could be stuff that's just like obviously there.
[01:38:01.420 --> 01:38:05.580]   And once you know it, it's like, oh, right, right.
[01:38:05.580 --> 01:38:09.400]   The thing we thought is somehow emergent
[01:38:09.400 --> 01:38:11.400]   from the laws of physics, we understand them,
[01:38:11.400 --> 01:38:15.240]   is actually a fundamental part of the universe
[01:38:15.240 --> 01:38:17.900]   and can be incorporated in physics, most understood.
[01:38:17.900 --> 01:38:20.200]   - Statistically speaking, right,
[01:38:20.200 --> 01:38:23.340]   if we observed some sort of alien life,
[01:38:23.340 --> 01:38:25.640]   it would most likely be some sort of
[01:38:25.640 --> 01:38:30.280]   virally self-replicating von Neumann-like probe system,
[01:38:30.280 --> 01:38:35.280]   right, and it's possible that there are such systems that,
[01:38:35.280 --> 01:38:38.160]   I don't know what they're doing
[01:38:38.160 --> 01:38:39.640]   at the bottom of the ocean allegedly,
[01:38:39.640 --> 01:38:43.240]   but maybe they're collecting minerals
[01:38:43.240 --> 01:38:44.600]   from the bottom of the ocean.
[01:38:44.600 --> 01:38:45.840]   - Yeah.
[01:38:45.840 --> 01:38:49.340]   - But that wouldn't violate any of my priors,
[01:38:49.340 --> 01:38:53.080]   but am I certain that these systems are here?
[01:38:53.080 --> 01:38:56.200]   And it'd be difficult for me to say so, right?
[01:38:56.200 --> 01:38:59.360]   I only have second-hand information about there being data.
[01:38:59.360 --> 01:39:00.920]   - About the bottom of the ocean?
[01:39:00.920 --> 01:39:03.880]   Yeah, but could it be things like memes?
[01:39:03.880 --> 01:39:05.800]   Could it be thoughts and ideas?
[01:39:05.800 --> 01:39:09.200]   Could they be operating in that medium?
[01:39:09.200 --> 01:39:12.000]   Could aliens be the very thoughts that come into my head?
[01:39:12.000 --> 01:39:17.520]   Like what do you, how do you know that,
[01:39:17.520 --> 01:39:20.240]   how do you know that, what's the origin of ideas
[01:39:20.240 --> 01:39:23.000]   in your mind when an idea comes to your head?
[01:39:23.000 --> 01:39:25.240]   Like show me where it originates.
[01:39:25.240 --> 01:39:29.400]   - I mean, frankly, when I had the idea
[01:39:29.400 --> 01:39:31.600]   for the type of computer I'm building now,
[01:39:31.600 --> 01:39:33.600]   I think it was eight years ago now,
[01:39:33.600 --> 01:39:36.200]   it really felt like it was being beamed from space.
[01:39:36.200 --> 01:39:39.240]   It's just, I was in bed just shaking,
[01:39:39.240 --> 01:39:41.760]   just thinking it through, and I don't know.
[01:39:41.760 --> 01:39:43.520]   But do I believe that legitimately?
[01:39:43.520 --> 01:39:44.480]   I don't think so.
[01:39:44.480 --> 01:39:49.480]   But you know, I think that alien life could take many forms,
[01:39:49.480 --> 01:39:52.240]   and I think the notion of intelligence
[01:39:52.240 --> 01:39:56.840]   and the notion of life needs to be expanded
[01:39:56.840 --> 01:40:01.840]   much more broadly, to be less anthropocentric or biocentric.
[01:40:01.840 --> 01:40:08.080]   - Just to linger a little longer on quantum mechanics,
[01:40:08.080 --> 01:40:11.360]   what's, through all your explorations of quantum computing,
[01:40:11.360 --> 01:40:15.640]   what's the coolest, most beautiful idea
[01:40:15.640 --> 01:40:17.560]   that you've come across that has been solved
[01:40:17.560 --> 01:40:19.880]   or has not yet been solved?
[01:40:19.880 --> 01:40:24.880]   - I think the journey to understand
[01:40:24.880 --> 01:40:27.040]   something called ADS-CFT,
[01:40:27.040 --> 01:40:30.920]   so the journey to understand quantum gravity
[01:40:30.920 --> 01:40:35.920]   through this picture where a hologram of lesser dimension
[01:40:35.920 --> 01:40:41.040]   is actually dual or exactly corresponding
[01:40:41.040 --> 01:40:46.040]   to a bulk theory of quantum gravity of an extra dimension.
[01:40:46.720 --> 01:40:50.480]   And the fact that this sort of duality
[01:40:50.480 --> 01:40:55.480]   comes from trying to learn deep learning-like representations
[01:40:55.480 --> 01:41:01.040]   of the boundary, and so at least part of my journey someday
[01:41:01.040 --> 01:41:05.640]   on my bucket list is to apply quantum machine learning
[01:41:05.640 --> 01:41:10.120]   to these sorts of systems, these CFTs,
[01:41:10.120 --> 01:41:14.240]   or they're called SYK models,
[01:41:14.240 --> 01:41:18.640]   and learn an emergent geometry from the boundary theory.
[01:41:18.640 --> 01:41:21.080]   And so we can have a form of machine learning
[01:41:21.080 --> 01:41:26.960]   to help us understand quantum gravity,
[01:41:26.960 --> 01:41:31.320]   which is still a holy grail that I would like to hit
[01:41:31.320 --> 01:41:35.000]   before I leave this earth. (laughs)
[01:41:35.000 --> 01:41:37.880]   - What do you think is going on with black holes
[01:41:37.880 --> 01:41:42.880]   as information storing and processing units?
[01:41:43.500 --> 01:41:46.160]   What do you think is going on with black holes?
[01:41:46.160 --> 01:41:49.280]   - Black holes are really fascinating objects.
[01:41:49.280 --> 01:41:50.840]   They're at the interface
[01:41:50.840 --> 01:41:52.320]   between quantum mechanics and gravity,
[01:41:52.320 --> 01:41:54.640]   and so they help us test all sorts of ideas.
[01:41:54.640 --> 01:41:59.200]   I think that for many decades now,
[01:41:59.200 --> 01:42:02.200]   there's been sort of this black hole information paradox
[01:42:02.200 --> 01:42:04.660]   that things that fall into the black hole
[01:42:04.660 --> 01:42:08.880]   we seem to have lost their information.
[01:42:08.880 --> 01:42:13.160]   Now I think there's this firewall paradox
[01:42:13.160 --> 01:42:15.880]   that has been allegedly resolved in recent years
[01:42:15.880 --> 01:42:20.880]   by a former peer of mine who's now a professor at Berkeley,
[01:42:20.880 --> 01:42:29.040]   and there it seems like there is,
[01:42:29.040 --> 01:42:31.760]   as information falls into a black hole,
[01:42:31.760 --> 01:42:35.040]   there's sort of a sedimentation, right?
[01:42:35.040 --> 01:42:37.360]   As you get closer and closer to the horizon
[01:42:37.360 --> 01:42:40.680]   from the point of view of the observer on the outside,
[01:42:40.680 --> 01:42:43.880]   the object slows down infinitely
[01:42:43.880 --> 01:42:45.880]   as it gets closer and closer.
[01:42:45.880 --> 01:42:49.400]   And so everything that is falling to a black hole
[01:42:49.400 --> 01:42:52.400]   from our perspective gets sort of sedimented
[01:42:52.400 --> 01:42:55.400]   and tacked on to the near horizon.
[01:42:55.400 --> 01:42:57.520]   And at some point, it gets so close to the horizon,
[01:42:57.520 --> 01:43:01.360]   it's in the proximity or the scale
[01:43:01.360 --> 01:43:04.460]   in which quantum effects and quantum fluctuations matter.
[01:43:04.460 --> 01:43:09.460]   And that infalling matter could interfere
[01:43:10.560 --> 01:43:13.200]   with sort of the traditional pictures
[01:43:13.200 --> 01:43:15.320]   that it can interfere with the creation
[01:43:15.320 --> 01:43:16.840]   and annihilation of particles
[01:43:16.840 --> 01:43:19.040]   and antiparticles in the vacuum.
[01:43:19.040 --> 01:43:20.960]   And through this interference,
[01:43:20.960 --> 01:43:23.880]   one of the particles gets entangled
[01:43:23.880 --> 01:43:25.680]   with the infalling information
[01:43:25.680 --> 01:43:28.040]   and one of them is now free and escapes.
[01:43:28.040 --> 01:43:31.040]   And that's how there's sort of mutual information
[01:43:31.040 --> 01:43:36.040]   between the outgoing radiation and the infalling matter.
[01:43:36.040 --> 01:43:38.280]   But getting that calculation right,
[01:43:38.280 --> 01:43:43.280]   I think we're only just starting to put the pieces together.
[01:43:43.280 --> 01:43:46.400]   - There's a few pothead-like questions I wanna ask you.
[01:43:46.400 --> 01:43:47.240]   - Sure.
[01:43:47.240 --> 01:43:48.720]   - So one, does it terrify you
[01:43:48.720 --> 01:43:52.460]   that there's a giant black hole at the center of our galaxy?
[01:43:52.460 --> 01:43:53.300]   - I don't know.
[01:43:53.300 --> 01:43:57.480]   I just want to set up shop near it to fast forward,
[01:43:57.480 --> 01:44:02.000]   you know, meet a future civilization, right?
[01:44:02.000 --> 01:44:03.720]   Like if we have a limited lifetime,
[01:44:03.720 --> 01:44:06.520]   if you can go orbit a black hole and emerge.
[01:44:07.800 --> 01:44:08.640]   - So if you were like,
[01:44:08.640 --> 01:44:09.840]   if there was a special mission
[01:44:09.840 --> 01:44:11.080]   that could take you to a black hole,
[01:44:11.080 --> 01:44:13.120]   would you volunteer to go travel?
[01:44:13.120 --> 01:44:15.840]   - To orbit and obviously not fall into it.
[01:44:15.840 --> 01:44:16.720]   - That's obvious.
[01:44:16.720 --> 01:44:17.560]   So it's obvious to you
[01:44:17.560 --> 01:44:19.800]   that everything's destroyed inside a black hole.
[01:44:19.800 --> 01:44:21.520]   Like all the information that makes up Guillaume
[01:44:21.520 --> 01:44:22.460]   is destroyed.
[01:44:22.460 --> 01:44:25.000]   Maybe on the other side,
[01:44:25.000 --> 01:44:27.800]   Beth Jezels emerges and it's all like,
[01:44:27.800 --> 01:44:32.800]   it's tied together in some deeply memophil way.
[01:44:32.800 --> 01:44:34.520]   - Yeah, I mean, that's a great question.
[01:44:34.520 --> 01:44:38.680]   We have to answer what black holes are.
[01:44:38.680 --> 01:44:41.280]   Are we punching a hole through space time
[01:44:41.280 --> 01:44:42.860]   and creating a pocket universe?
[01:44:42.860 --> 01:44:44.800]   It's possible, right?
[01:44:44.800 --> 01:44:49.220]   Then that would mean that if we ascend the Kardashev scale
[01:44:49.220 --> 01:44:52.760]   to, you know, beyond Kardashev type three,
[01:44:52.760 --> 01:44:55.000]   we could engineer in black holes
[01:44:55.000 --> 01:44:56.480]   with specific hyper parameters
[01:44:56.480 --> 01:44:59.520]   to transmit information to new universes we create.
[01:44:59.520 --> 01:45:02.280]   And so we can have progeny, right?
[01:45:03.220 --> 01:45:04.480]   That are new universes.
[01:45:04.480 --> 01:45:09.480]   And so we, even though our universe may reach a heat death,
[01:45:09.480 --> 01:45:13.700]   we may have a way to have a legacy, right?
[01:45:13.700 --> 01:45:15.940]   So we don't know yet.
[01:45:15.940 --> 01:45:17.860]   We need to ascend the Kardashev scale
[01:45:17.860 --> 01:45:20.400]   to answer these questions, right?
[01:45:20.400 --> 01:45:25.120]   To peer into that regime of higher energy physics.
[01:45:25.120 --> 01:45:27.320]   - And maybe you can speak to the Kardashev scale
[01:45:27.320 --> 01:45:28.340]   for people who don't know.
[01:45:28.340 --> 01:45:33.340]   So one of the sort of meme-like principles
[01:45:33.340 --> 01:45:37.600]   and goals of the EAC movement
[01:45:37.600 --> 01:45:39.280]   is to ascend the Kardashev scale.
[01:45:39.280 --> 01:45:41.360]   What is the Kardashev scale?
[01:45:41.360 --> 01:45:43.440]   And when do we wanna ascend it?
[01:45:43.440 --> 01:45:45.880]   - The Kardashev scale is a measure
[01:45:45.880 --> 01:45:48.860]   of our energy production and consumption.
[01:45:48.860 --> 01:45:53.980]   And really, it's a logarithmic scale.
[01:45:53.980 --> 01:45:56.680]   And Kardashev type one is a milestone
[01:45:56.680 --> 01:46:00.840]   where we are producing the equivalent wattage
[01:46:00.840 --> 01:46:04.480]   to all the energy that is incident on Earth from the Sun.
[01:46:04.480 --> 01:46:07.960]   Kardashev type two would be harnessing all the energy
[01:46:07.960 --> 01:46:09.760]   that is output by the Sun.
[01:46:09.760 --> 01:46:13.040]   And I think type three is like the whole galaxy.
[01:46:13.040 --> 01:46:14.960]   - Galaxy, I think, level, yeah.
[01:46:14.960 --> 01:46:17.560]   - Yeah, and then some people have some crazy type four
[01:46:17.560 --> 01:46:19.640]   and five, but I don't know if I believe in those.
[01:46:19.640 --> 01:46:24.640]   But to me, it seems like from the first principles
[01:46:25.080 --> 01:46:28.920]   of thermodynamics that, again, there's this concept
[01:46:28.920 --> 01:46:33.920]   of thermodynamic driven dissipative adaptation
[01:46:33.920 --> 01:46:38.080]   where life evolved on Earth
[01:46:38.080 --> 01:46:42.760]   because we have this sort of energetic drive from the Sun.
[01:46:42.760 --> 01:46:46.080]   We have incident energy and life evolved on Earth
[01:46:46.080 --> 01:46:50.000]   to capture, figure out ways to best capture
[01:46:50.000 --> 01:46:54.160]   that free energy to maintain itself and grow.
[01:46:54.160 --> 01:46:57.480]   And I think that that principle,
[01:46:57.480 --> 01:47:00.680]   it's not special to our Earth-Sun system.
[01:47:00.680 --> 01:47:03.120]   We can extend life well beyond
[01:47:03.120 --> 01:47:06.160]   and we kind of have a responsibility to do so
[01:47:06.160 --> 01:47:08.760]   because that's the process that brought us here.
[01:47:08.760 --> 01:47:12.040]   So, we don't even know what it has in store for us
[01:47:12.040 --> 01:47:12.860]   in the future.
[01:47:12.860 --> 01:47:15.360]   It could be something of beauty
[01:47:15.360 --> 01:47:17.120]   we can't even imagine today, right?
[01:47:17.120 --> 01:47:20.440]   - So, this is probably a good place
[01:47:20.440 --> 01:47:23.440]   to talk a bit about the EAC movement.
[01:47:23.440 --> 01:47:28.000]   In a sub-stack blog post titled "What the Fuck is EAC?"
[01:47:28.000 --> 01:47:31.000]   or actually, "What the F* is EAC?"
[01:47:31.000 --> 01:47:32.800]   you write, "Strategically speaking,
[01:47:32.800 --> 01:47:34.760]   "we need to work towards several overarching
[01:47:34.760 --> 01:47:37.940]   "civilization goals that are all interdependent."
[01:47:37.940 --> 01:47:41.540]   And the four goals are increase the amount of energy
[01:47:41.540 --> 01:47:43.400]   we can harness as a species,
[01:47:43.400 --> 01:47:46.520]   climb the Kardashev gradient.
[01:47:46.520 --> 01:47:49.080]   In the short term, this almost certainly means
[01:47:49.080 --> 01:47:50.120]   nuclear fission.
[01:47:51.640 --> 01:47:54.480]   Increase human flourishing via pro-population growth policies
[01:47:54.480 --> 01:47:56.180]   and pro-economic growth policies.
[01:47:56.180 --> 01:47:59.320]   Create artificial general intelligence,
[01:47:59.320 --> 01:48:02.320]   the single greatest force multiplier in human history.
[01:48:02.320 --> 01:48:04.320]   And finally, develop interplanetary
[01:48:04.320 --> 01:48:06.120]   and interstellar transport
[01:48:06.120 --> 01:48:09.000]   so that humanity can spread beyond the Earth.
[01:48:09.000 --> 01:48:13.000]   Could you build on top of that to maybe say,
[01:48:13.000 --> 01:48:17.360]   what to you is the EAC movement?
[01:48:17.360 --> 01:48:18.200]   What are the goals?
[01:48:18.200 --> 01:48:19.420]   What are the principles?
[01:48:20.520 --> 01:48:25.520]   - The goal is for the human techno-capital memetic machine
[01:48:25.520 --> 01:48:28.440]   to become self-aware
[01:48:28.440 --> 01:48:31.640]   and to hyperstitiously engineer its own growth.
[01:48:31.640 --> 01:48:33.640]   So let's decompress that.
[01:48:33.640 --> 01:48:35.440]   - Define each of those words.
[01:48:35.440 --> 01:48:38.040]   - So you have humans, you have technology,
[01:48:38.040 --> 01:48:41.520]   you have capital, and then you have memes, information.
[01:48:41.520 --> 01:48:46.720]   And all of those systems are coupled with one another.
[01:48:46.720 --> 01:48:48.160]   Humans work at companies,
[01:48:48.160 --> 01:48:50.520]   they acquire and allocate capital.
[01:48:50.520 --> 01:48:53.600]   And humans communicate via memes
[01:48:53.600 --> 01:48:55.400]   and information propagation.
[01:48:55.400 --> 01:49:01.320]   And our goal was to have a sort of viral optimistic movement
[01:49:01.320 --> 01:49:06.040]   that is aware of how the system works.
[01:49:06.040 --> 01:49:08.640]   Fundamentally, it seeks to grow.
[01:49:08.640 --> 01:49:13.640]   And we simply want to lean into the natural tendencies
[01:49:13.640 --> 01:49:16.780]   of the system to adapt for its own growth.
[01:49:17.780 --> 01:49:19.900]   - So in that way, you're right.
[01:49:19.900 --> 01:49:23.300]   The EAC is literally a memetic optimism virus
[01:49:23.300 --> 01:49:25.500]   that is constantly drifting, mutating,
[01:49:25.500 --> 01:49:28.260]   and propagating in a decentralized fashion.
[01:49:28.260 --> 01:49:30.780]   So memetic optimism virus.
[01:49:30.780 --> 01:49:35.300]   So you do want it to be a virus to maximize the spread.
[01:49:35.300 --> 01:49:37.640]   And it's hyperstitious,
[01:49:37.640 --> 01:49:42.140]   therefore the optimism will incentivize its growth.
[01:49:43.080 --> 01:49:47.280]   - We see EAC as a sort of a metaheuristic,
[01:49:47.280 --> 01:49:51.200]   a sort of very thin cultural framework
[01:49:51.200 --> 01:49:55.600]   from which you can have much more opinionated forks.
[01:49:55.600 --> 01:49:59.800]   Fundamentally, we just say that it's good.
[01:49:59.800 --> 01:50:04.800]   What got us here is this adaptation of the whole system,
[01:50:04.800 --> 01:50:07.520]   based on thermodynamics,
[01:50:07.520 --> 01:50:11.360]   and that process is good and we should keep it going.
[01:50:11.360 --> 01:50:12.380]   That is the core thesis.
[01:50:12.380 --> 01:50:16.760]   Everything else is, okay, how do we ensure
[01:50:16.760 --> 01:50:20.720]   that we maintain this malleability and adaptability?
[01:50:20.720 --> 01:50:24.160]   Well, clearly not suppressing variance
[01:50:24.160 --> 01:50:28.720]   and maintaining free speech, freedom of thought,
[01:50:28.720 --> 01:50:30.740]   freedom of information propagation,
[01:50:30.740 --> 01:50:34.540]   and freedom to do AI research is important
[01:50:34.540 --> 01:50:37.960]   for us to converge the fastest
[01:50:37.960 --> 01:50:42.500]   on the space of technologies, ideas, and whatnot
[01:50:42.500 --> 01:50:44.200]   that lead to this growth.
[01:50:44.200 --> 01:50:49.500]   And so ultimately, there's been quite a few forks.
[01:50:49.500 --> 01:50:52.060]   Some are just memes, but some are more serious, right?
[01:50:52.060 --> 01:50:55.780]   Vitalik Buterin recently made a DIAC fork.
[01:50:55.780 --> 01:50:59.140]   He has his own sort of fine tunings of EAC.
[01:50:59.140 --> 01:51:00.980]   - Does anything jump out to memory
[01:51:00.980 --> 01:51:05.460]   of the unique characteristic of that fork from Vitalik?
[01:51:05.460 --> 01:51:08.520]   - I would say that it's trying to find a middle ground
[01:51:08.520 --> 01:51:12.640]   between EAC and sort of EA and AI safety.
[01:51:12.640 --> 01:51:17.600]   To me, having a movement that is opposite
[01:51:17.600 --> 01:51:19.120]   to what was the mainstream narrative
[01:51:19.120 --> 01:51:20.480]   that was taking over Silicon Valley
[01:51:20.480 --> 01:51:24.600]   was important to sort of shift the dynamic range of opinions.
[01:51:24.600 --> 01:51:28.480]   And it's like the balance between centralization
[01:51:28.480 --> 01:51:29.520]   and decentralization.
[01:51:29.520 --> 01:51:32.840]   The real optimum's always somewhere in the middle, right?
[01:51:32.840 --> 01:51:37.840]   But for EAC, we're pushing for entropy, novelty,
[01:51:37.840 --> 01:51:42.040]   disruption, malleability, speed,
[01:51:42.040 --> 01:51:46.080]   rather than being like sort of conservative,
[01:51:46.080 --> 01:51:48.040]   suppressing thought, suppressing speech,
[01:51:48.040 --> 01:51:51.560]   adding constraints, adding too many regulations,
[01:51:51.560 --> 01:51:52.660]   slowing things down.
[01:51:52.660 --> 01:51:53.960]   And so it's kind of,
[01:51:53.960 --> 01:51:56.840]   we're trying to bring balance to the force, right?
[01:51:56.840 --> 01:51:57.820]   Systems.
[01:51:57.820 --> 01:52:00.160]   (laughing)
[01:52:00.160 --> 01:52:02.840]   - Balance to the force of human civilization, yeah.
[01:52:02.840 --> 01:52:04.600]   - It's literally the forces of constraints
[01:52:04.600 --> 01:52:09.120]   versus the entropic force that makes us explore, right?
[01:52:09.120 --> 01:52:13.680]   Systems are optimal when they're at the edge of criticality
[01:52:13.680 --> 01:52:15.800]   between order and chaos, right?
[01:52:15.800 --> 01:52:20.800]   Between constraints, energy minimization, and entropy.
[01:52:20.800 --> 01:52:24.460]   Systems want to equilibrate, balance these two things.
[01:52:24.460 --> 01:52:27.600]   And so I thought that the balance was lacking.
[01:52:27.600 --> 01:52:31.680]   And so we created this movement to bring balance.
[01:52:31.680 --> 01:52:35.120]   - Well, I like how, I like the sort of visual
[01:52:35.120 --> 01:52:39.080]   of the landscape of ideas evolving through forks.
[01:52:39.080 --> 01:52:43.820]   So kind of thinking on the other part of history,
[01:52:43.820 --> 01:52:49.480]   thinking of Marxism as the original repository,
[01:52:49.480 --> 01:52:52.200]   and then Soviet communism as a fork of that,
[01:52:52.200 --> 01:52:57.200]   and then Maoism as a fork of Marxism and communism.
[01:52:58.040 --> 01:53:00.560]   And so those are all forks
[01:53:00.560 --> 01:53:02.560]   that are exploring different ideas.
[01:53:02.560 --> 01:53:04.960]   - Thinking of culture almost like code, right?
[01:53:04.960 --> 01:53:09.960]   Nowadays, I mean, what you prompt the LM
[01:53:09.960 --> 01:53:12.760]   or what you put in the constitution of an LM
[01:53:12.760 --> 01:53:16.920]   is basically its cultural framework, what it believes, right?
[01:53:16.920 --> 01:53:21.440]   And you can share it on GitHub nowadays.
[01:53:21.440 --> 01:53:23.940]   So starting trying to take inspiration
[01:53:23.940 --> 01:53:28.440]   from what has worked in the sort of machine of software
[01:53:28.440 --> 01:53:31.960]   to adapt over the space of code,
[01:53:31.960 --> 01:53:33.620]   could we apply that to culture?
[01:53:33.620 --> 01:53:37.240]   And our goal is to not say you should live your life
[01:53:37.240 --> 01:53:41.040]   this way, X, Y, Z, is to set up a process
[01:53:41.040 --> 01:53:44.680]   where people are always searching over subcultures
[01:53:44.680 --> 01:53:46.920]   and competing for mindshare.
[01:53:46.920 --> 01:53:50.320]   And I think creating this malleability of culture
[01:53:50.320 --> 01:53:53.840]   is super important for us to converge onto the cultures
[01:53:53.840 --> 01:53:56.640]   and the heuristics about how to live one's life
[01:53:56.640 --> 01:53:59.540]   that are updated to modern times.
[01:53:59.540 --> 01:54:03.520]   Because there's really been a sort of vacuum
[01:54:03.520 --> 01:54:06.120]   of spirituality and culture.
[01:54:06.120 --> 01:54:08.640]   People don't feel like they belong to any one group.
[01:54:08.640 --> 01:54:11.220]   And there's been parasitic ideologies
[01:54:11.220 --> 01:54:13.480]   that have taken up opportunity
[01:54:13.480 --> 01:54:18.200]   to populate this Petri dish of minds, right?
[01:54:18.200 --> 01:54:20.600]   Elon calls it the mind virus.
[01:54:20.600 --> 01:54:24.680]   We call it the D-cell mind virus complex,
[01:54:24.680 --> 01:54:28.920]   which is a decelerative that is kind of the overall pattern
[01:54:28.920 --> 01:54:29.760]   between all of them.
[01:54:29.760 --> 01:54:31.280]   There's many variants as well.
[01:54:31.280 --> 01:54:36.080]   And so if there's a sort of viral pessimism,
[01:54:36.080 --> 01:54:38.200]   decelerative movement, we needed to have
[01:54:38.200 --> 01:54:42.040]   not only one movement, but many, many variants.
[01:54:42.040 --> 01:54:44.160]   So it's very hard to pinpoint and stop.
[01:54:44.160 --> 01:54:47.360]   - But the overarching thing is nevertheless
[01:54:47.360 --> 01:54:51.760]   a kind of mimetic optimism pandemic.
[01:54:51.760 --> 01:54:57.120]   So, I mean, okay, let me ask you,
[01:54:57.120 --> 01:54:59.860]   do you think EAC to some degree is a cult?
[01:54:59.860 --> 01:55:02.100]   - Define cult.
[01:55:02.100 --> 01:55:06.560]   - I think a lot of human progress is made
[01:55:06.560 --> 01:55:09.560]   when you have independent thought.
[01:55:09.560 --> 01:55:12.500]   So you have individuals that are able to think freely
[01:55:12.500 --> 01:55:17.500]   and very powerful mimetic systems
[01:55:17.500 --> 01:55:21.940]   can kind of lead to group think.
[01:55:21.940 --> 01:55:23.140]   There's something in human nature
[01:55:23.140 --> 01:55:26.300]   that leads to like mass hypnosis, mass hysteria,
[01:55:26.300 --> 01:55:28.300]   where we start to think alike
[01:55:28.300 --> 01:55:32.180]   whenever there's a sexy idea that captures our minds.
[01:55:32.180 --> 01:55:34.740]   And so it's actually hard to break us apart,
[01:55:34.740 --> 01:55:37.940]   pull us apart, diversify thought.
[01:55:37.940 --> 01:55:40.580]   So to that degree, to which degree
[01:55:40.580 --> 01:55:43.660]   is everybody kind of chanting EAC, EAC,
[01:55:43.660 --> 01:55:46.500]   like the sheep in Animal Farm?
[01:55:46.500 --> 01:55:49.300]   - Well, first of all, it's fun, it's rebellious, right?
[01:55:49.300 --> 01:55:54.300]   Like many, I think we lean into,
[01:55:54.300 --> 01:55:58.980]   there's this concept of sort of meta-irony, right?
[01:55:58.980 --> 01:56:01.940]   Of sort of being on the boundary of like,
[01:56:01.940 --> 01:56:03.460]   we're not sure if they're serious or not,
[01:56:03.460 --> 01:56:06.540]   and it's much more playful and much more fun, right?
[01:56:06.540 --> 01:56:09.220]   Like, for example, we talk about thermodynamics
[01:56:09.220 --> 01:56:11.080]   being our God, right?
[01:56:11.080 --> 01:56:14.460]   And sometimes we do cult-like things,
[01:56:14.460 --> 01:56:18.340]   but there's no like ceremony and robes and whatnot.
[01:56:18.340 --> 01:56:19.180]   - Not yet.
[01:56:19.180 --> 01:56:20.020]   - Not yet.
[01:56:20.020 --> 01:56:23.580]   But ultimately, yeah, I mean, I totally agree
[01:56:23.580 --> 01:56:28.020]   that it seems to me that humans wanna feel
[01:56:28.020 --> 01:56:29.060]   like they're part of a group.
[01:56:29.060 --> 01:56:33.460]   So they naturally try to agree with their neighbors
[01:56:33.460 --> 01:56:35.500]   and find common ground.
[01:56:35.500 --> 01:56:38.300]   And that leads to sort of mode collapse
[01:56:38.300 --> 01:56:40.180]   in the space of ideas, right?
[01:56:40.180 --> 01:56:44.740]   We used to have sort of one cultural island
[01:56:44.740 --> 01:56:45.580]   that was allowed.
[01:56:45.580 --> 01:56:47.140]   It was a typical subspace of thought,
[01:56:47.140 --> 01:56:49.140]   and anything that was diverting
[01:56:49.140 --> 01:56:51.140]   from that subspace of thought was suppressed
[01:56:51.140 --> 01:56:52.820]   or even canceled, right?
[01:56:52.820 --> 01:56:54.820]   Now we've created a new mode,
[01:56:54.820 --> 01:56:57.340]   but the whole point is that we're not trying to have
[01:56:57.340 --> 01:56:59.500]   a very restricted space of thought.
[01:56:59.500 --> 01:57:01.700]   There's not just one way to think about EAC
[01:57:01.700 --> 01:57:03.020]   and its many forks.
[01:57:03.020 --> 01:57:05.120]   And the point is that there are many forks,
[01:57:05.120 --> 01:57:07.220]   and there can be many clusters and many islands.
[01:57:07.220 --> 01:57:11.340]   And I shouldn't be in control of it in any way.
[01:57:11.340 --> 01:57:16.140]   I mean, there's no formal org whatsoever.
[01:57:16.140 --> 01:57:20.580]   I just put out tweets and certain blog posts,
[01:57:20.580 --> 01:57:24.100]   and people are free to defect and fork
[01:57:24.100 --> 01:57:26.140]   if there's an aspect they don't like.
[01:57:26.140 --> 01:57:29.580]   And so that makes it so that there should be
[01:57:29.580 --> 01:57:34.180]   a sort of deterritorialization in the space of ideas
[01:57:34.180 --> 01:57:36.900]   so that we don't end up in one cluster
[01:57:36.900 --> 01:57:38.900]   that's very cult-like.
[01:57:38.900 --> 01:57:43.020]   And so cults, usually, they don't allow people
[01:57:43.020 --> 01:57:45.100]   to defect or start competing forks,
[01:57:45.100 --> 01:57:47.700]   whereas we encourage it, right?
[01:57:47.700 --> 01:57:49.560]   - Do you think just the humor,
[01:57:49.560 --> 01:57:53.080]   the pros and cons of humor and meme,
[01:57:53.080 --> 01:57:56.020]   in some sense, meme,
[01:57:56.020 --> 01:58:00.300]   there's like a wisdom to memes.
[01:58:00.300 --> 01:58:04.140]   What is it, "The Magic Theater"?
[01:58:04.140 --> 01:58:05.220]   What book is that from?
[01:58:05.300 --> 01:58:08.980]   Herman has a Steppenwolf, I think.
[01:58:08.980 --> 01:58:13.980]   But there's a kind of embracing of the absurdity
[01:58:13.980 --> 01:58:17.860]   that seems to get to the truth of things,
[01:58:17.860 --> 01:58:21.100]   but at the same time, it can also decrease the quality
[01:58:21.100 --> 01:58:23.700]   and the rigor of the discourse.
[01:58:23.700 --> 01:58:25.340]   Do you feel the tension of that?
[01:58:25.340 --> 01:58:26.620]   - Yeah.
[01:58:26.620 --> 01:58:30.220]   So initially, I think, what allowed us to grow
[01:58:30.220 --> 01:58:33.540]   under the radar was because it was camouflaged
[01:58:33.540 --> 01:58:35.900]   as sort of meta-ironic, right?
[01:58:35.900 --> 01:58:40.900]   We would sneak in deep truths within a package of humor
[01:58:40.900 --> 01:58:45.520]   and memes and what are called shitposts, right?
[01:58:45.520 --> 01:58:51.780]   And I think that was purposefully a sort of camouflage
[01:58:51.780 --> 01:58:57.020]   against those that seek status and do not want to,
[01:58:57.020 --> 01:59:02.460]   it's very hard to argue with a cartoon frog
[01:59:02.460 --> 01:59:07.460]   or a cartoon of an intergalactic Jeff Bezos
[01:59:07.460 --> 01:59:10.820]   and take yourself seriously.
[01:59:10.820 --> 01:59:15.300]   And so, that allowed us to grow pretty rapidly
[01:59:15.300 --> 01:59:16.340]   in the early days.
[01:59:16.340 --> 01:59:21.340]   But of course, essentially, people get steered,
[01:59:21.340 --> 01:59:27.140]   their notion of the truth comes from the data they see,
[01:59:27.140 --> 01:59:29.180]   from the information they're fed.
[01:59:29.180 --> 01:59:31.780]   And the information people are fed
[01:59:31.780 --> 01:59:34.860]   is determined by algorithms, right?
[01:59:34.860 --> 01:59:39.860]   And really what we've been doing is sort of engineering
[01:59:39.860 --> 01:59:44.740]   what we call high mimetic fitness packets of information
[01:59:44.740 --> 01:59:47.580]   so that they can spread effectively and carry a message,
[01:59:47.580 --> 01:59:48.420]   right?
[01:59:48.420 --> 01:59:52.660]   So it's kind of a vector to spread the message.
[01:59:52.660 --> 01:59:56.140]   And yes, we've been using sort of techniques
[01:59:56.140 --> 02:00:00.300]   that are optimal for today's algorithmically amplified
[02:00:00.300 --> 02:00:02.540]   information landscapes.
[02:00:02.540 --> 02:00:06.500]   But I think we're reaching the point of scale
[02:00:06.500 --> 02:00:10.260]   where we can have serious debates and serious conversations.
[02:00:10.260 --> 02:00:15.260]   And that's why we're considering doing a bunch of debates
[02:00:15.260 --> 02:00:18.060]   and having more serious long-form discussions.
[02:00:18.060 --> 02:00:21.620]   'Cause I don't think that the timeline is optimal
[02:00:21.620 --> 02:00:24.860]   for sort of very serious, thoughtful discussions.
[02:00:24.860 --> 02:00:29.460]   You get rewarded for sort of polarization, right?
[02:00:29.460 --> 02:00:33.020]   And so, even though we started a movement
[02:00:33.020 --> 02:00:37.660]   that is literally trying to polarize the tech ecosystem,
[02:00:37.660 --> 02:00:39.180]   at the end of the day, it's so that we can have
[02:00:39.180 --> 02:00:42.680]   a conversation and find an optimum together.
[02:00:42.680 --> 02:00:45.220]   - I mean, that's kind of what I try to do with this podcast,
[02:00:45.220 --> 02:00:47.060]   given the landscape of things,
[02:00:47.060 --> 02:00:49.220]   to still have long-form conversations.
[02:00:49.220 --> 02:00:54.000]   But there is a degree to which absurdity is fully embraced.
[02:00:54.000 --> 02:00:58.380]   In fact, this very conversation is multi,
[02:00:59.100 --> 02:01:01.340]   level absurd.
[02:01:01.340 --> 02:01:04.820]   So, first of all, I should say that I just very recently
[02:01:04.820 --> 02:01:07.540]   had a conversation with Jeff Bezos.
[02:01:07.540 --> 02:01:12.240]   And I would love to hear your,
[02:01:12.240 --> 02:01:18.140]   Beth Jezos, opinions of Jeff Bezos.
[02:01:18.140 --> 02:01:20.820]   Speaking of intergalactic Jeff Bezos,
[02:01:20.820 --> 02:01:23.580]   what do you think of that particular individual
[02:01:23.580 --> 02:01:25.640]   whom your name has inspired?
[02:01:25.640 --> 02:01:29.420]   - Yeah, I mean, I think Jeff is really great.
[02:01:29.420 --> 02:01:32.460]   I mean, he's built one of the most epic companies
[02:01:32.460 --> 02:01:33.300]   of all time.
[02:01:33.300 --> 02:01:34.900]   He's leveraged the techno capital machine
[02:01:34.900 --> 02:01:36.620]   and techno capital acceleration
[02:01:36.620 --> 02:01:40.580]   to give us what we wanted, right?
[02:01:40.580 --> 02:01:44.700]   We want quick delivery, very convenient,
[02:01:44.700 --> 02:01:46.620]   at home, low prices, right?
[02:01:46.620 --> 02:01:49.620]   He understood how the machine worked
[02:01:49.620 --> 02:01:51.120]   and how to harness it, right?
[02:01:51.120 --> 02:01:52.660]   Like running the company,
[02:01:52.660 --> 02:01:55.480]   not trying to take profits too early,
[02:01:55.480 --> 02:01:56.620]   putting it back,
[02:01:56.620 --> 02:02:00.600]   letting the system compound and keep improving.
[02:02:00.600 --> 02:02:03.060]   And arguably, I think Amazon's invested
[02:02:03.060 --> 02:02:06.440]   some of the most amount of capital in robotics out there.
[02:02:06.440 --> 02:02:10.260]   And certainly, with the birth of AWS,
[02:02:10.260 --> 02:02:14.440]   kind of enabled the sort of tech boom we've seen today
[02:02:14.440 --> 02:02:16.700]   that has paid the salaries of,
[02:02:16.700 --> 02:02:20.940]   I guess, myself and all of our friends to some extent.
[02:02:20.940 --> 02:02:24.780]   And so, I think we can all be grateful to Jeff
[02:02:24.780 --> 02:02:28.440]   and he's one of the great entrepreneurs out there,
[02:02:28.440 --> 02:02:30.840]   one of the best of all time, unarguably.
[02:02:30.840 --> 02:02:34.620]   - And of course, the work at Blue Origin,
[02:02:34.620 --> 02:02:36.300]   similar to the work at SpaceX,
[02:02:36.300 --> 02:02:39.260]   is trying to make humans a multi-planetary species,
[02:02:39.260 --> 02:02:42.900]   which seems almost like a bigger thing
[02:02:42.900 --> 02:02:45.180]   than the capitalist machine,
[02:02:45.180 --> 02:02:46.020]   or it's a capitalist machine
[02:02:46.020 --> 02:02:47.860]   at a different time scale, perhaps.
[02:02:47.860 --> 02:02:52.340]   - Yeah, I think that companies,
[02:02:52.340 --> 02:02:56.340]   they tend to optimize quarter over quarter,
[02:02:56.340 --> 02:02:57.900]   maybe a few years out,
[02:02:57.900 --> 02:03:00.840]   but individuals that wanna leave a legacy
[02:03:00.840 --> 02:03:05.220]   can think on a multi-decadal or multi-century time scale.
[02:03:05.220 --> 02:03:08.060]   And so, the fact that some individuals
[02:03:08.060 --> 02:03:10.420]   are such good capital allocators,
[02:03:10.420 --> 02:03:13.300]   that they unlock the ability to allocate capitals
[02:03:13.300 --> 02:03:16.300]   to goals that take us much further
[02:03:16.300 --> 02:03:17.980]   or are much further looking.
[02:03:17.980 --> 02:03:20.860]   Elon's doing this with SpaceX,
[02:03:20.860 --> 02:03:23.460]   putting all this capital towards getting us to Mars.
[02:03:23.460 --> 02:03:27.020]   Jeff is trying to build Blue Origin
[02:03:27.020 --> 02:03:29.060]   and I think he wants to build O'Neill cylinders
[02:03:29.060 --> 02:03:31.020]   and get industry off planet,
[02:03:31.020 --> 02:03:32.480]   which I think is brilliant.
[02:03:32.480 --> 02:03:38.500]   I think, just overall, I'm for billionaires.
[02:03:38.500 --> 02:03:40.520]   I know this is a controversial statement sometimes,
[02:03:40.520 --> 02:03:43.100]   but I think that in a sense,
[02:03:43.100 --> 02:03:46.300]   it's kind of a proof of stake voting, right?
[02:03:46.300 --> 02:03:50.940]   Like, if you've allocated capital efficiently,
[02:03:50.940 --> 02:03:54.640]   you unlock more capital to allocate
[02:03:54.640 --> 02:03:56.500]   just because clearly,
[02:03:56.500 --> 02:03:59.540]   you know how to allocate capital more efficiently,
[02:03:59.540 --> 02:04:03.500]   which is in contrast to politicians that get elected
[02:04:03.500 --> 02:04:05.860]   because they speak the best on TV, right?
[02:04:05.860 --> 02:04:08.020]   Not because they have a proven track record
[02:04:08.020 --> 02:04:11.660]   of allocating taxpayer capital most efficiently.
[02:04:11.660 --> 02:04:15.860]   And so, that's why I'm for capitalism
[02:04:15.860 --> 02:04:18.460]   over, say, giving all our money to the government
[02:04:18.460 --> 02:04:20.540]   and letting them figure out how to allocate it.
[02:04:20.540 --> 02:04:21.860]   So, yeah.
[02:04:21.860 --> 02:04:24.900]   - Why do you think it's a viral
[02:04:24.900 --> 02:04:28.820]   and it's a popular meme to criticize billionaires,
[02:04:28.820 --> 02:04:30.580]   since you mentioned billionaires?
[02:04:30.580 --> 02:04:35.380]   Why do you think there's quite a widespread criticism
[02:04:35.380 --> 02:04:38.140]   of people with wealth,
[02:04:38.140 --> 02:04:39.500]   especially those in the public eye,
[02:04:39.500 --> 02:04:41.740]   like Jeff and Elon and Mark Zuckerberg
[02:04:41.740 --> 02:04:44.660]   and who else, Bill Gates?
[02:04:44.660 --> 02:04:47.580]   - Yeah, I think a lot of people would,
[02:04:47.580 --> 02:04:48.940]   instead of trying to understand
[02:04:48.940 --> 02:04:51.620]   how the techno capital machine works
[02:04:51.620 --> 02:04:54.740]   and realizing they have much more agency than they think,
[02:04:54.740 --> 02:04:57.980]   they'd rather have this sort of victim mindset.
[02:04:57.980 --> 02:05:00.320]   I'm just subjected to this machine.
[02:05:00.320 --> 02:05:01.680]   It is oppressing me.
[02:05:01.680 --> 02:05:07.820]   And the successful players clearly must be evil
[02:05:07.820 --> 02:05:09.460]   because they've been successful at this game
[02:05:09.460 --> 02:05:10.980]   that I'm not successful at.
[02:05:10.980 --> 02:05:14.740]   But I've managed to get some people
[02:05:14.740 --> 02:05:15.720]   that were in that mindset
[02:05:15.720 --> 02:05:19.060]   and make them realize how the techno capital machine works
[02:05:19.060 --> 02:05:22.980]   and how you can harness it for your own good
[02:05:22.980 --> 02:05:24.180]   and for the good of others.
[02:05:24.180 --> 02:05:25.960]   And by creating value,
[02:05:25.960 --> 02:05:27.740]   you capture some of the value you create for the world.
[02:05:27.740 --> 02:05:31.520]   And that sort of positive sum mindset shift is so potent.
[02:05:31.520 --> 02:05:34.220]   And really, that's what we're trying to do
[02:05:34.220 --> 02:05:37.020]   by scaling EAC is sort of unlocking
[02:05:37.020 --> 02:05:39.100]   that higher level of agency.
[02:05:39.100 --> 02:05:41.180]   Actually, you're far more in control
[02:05:41.180 --> 02:05:42.580]   of the future than you think.
[02:05:42.580 --> 02:05:44.480]   You have agency to change the world.
[02:05:44.480 --> 02:05:45.740]   Go out and do it.
[02:05:45.740 --> 02:05:46.940]   Here's permission.
[02:05:46.940 --> 02:05:49.580]   - Each individual has agency.
[02:05:49.580 --> 02:05:52.540]   The motto, keep building, is often heard.
[02:05:52.540 --> 02:05:54.040]   What does that mean to you?
[02:05:54.040 --> 02:05:56.140]   And what does it have to do with Diet Coke?
[02:05:56.140 --> 02:05:57.940]   (laughing)
[02:05:57.940 --> 02:05:59.780]   By the way, thank you so much for the Red Bull.
[02:05:59.780 --> 02:06:01.620]   It's working pretty well.
[02:06:01.620 --> 02:06:03.260]   I'm feeling pretty good.
[02:06:03.260 --> 02:06:04.100]   - Awesome.
[02:06:05.900 --> 02:06:09.260]   Well, so building technologies and building,
[02:06:09.260 --> 02:06:10.460]   it doesn't have to be technologies.
[02:06:10.460 --> 02:06:14.400]   Just building in general means having agency,
[02:06:14.400 --> 02:06:18.740]   trying to change the world by creating, let's say,
[02:06:18.740 --> 02:06:21.900]   a company which is a self-sustaining organism
[02:06:21.900 --> 02:06:25.320]   that accomplishes a function
[02:06:25.320 --> 02:06:27.580]   in the broader techno capital machine.
[02:06:27.580 --> 02:06:30.820]   To us, that's the way to achieve change in the world
[02:06:30.820 --> 02:06:32.060]   that you'd like to see,
[02:06:32.060 --> 02:06:35.240]   rather than, say, pressuring politicians
[02:06:35.240 --> 02:06:37.060]   or creating non-profits that,
[02:06:37.060 --> 02:06:39.820]   non-profits, once they run out of money,
[02:06:39.820 --> 02:06:42.100]   their function can no longer be accomplished.
[02:06:42.100 --> 02:06:45.500]   You're kind of deforming the market artificially
[02:06:45.500 --> 02:06:49.740]   compared to sort of subverting or coercing the market
[02:06:49.740 --> 02:06:53.460]   or dancing with the market to convince it
[02:06:53.460 --> 02:06:55.620]   that actually this function is important,
[02:06:55.620 --> 02:06:57.740]   adds value, and here it is, right?
[02:06:57.740 --> 02:07:00.460]   And so I think this is sort of the way
[02:07:00.460 --> 02:07:03.980]   between the sort of de-growth ESG approach
[02:07:03.980 --> 02:07:05.920]   versus, say, Elon, right?
[02:07:05.920 --> 02:07:07.120]   The de-growth approach is like,
[02:07:07.120 --> 02:07:10.360]   we're gonna manage our way out of a climate crisis,
[02:07:10.360 --> 02:07:12.780]   and Elon is like, I'm gonna build a company
[02:07:12.780 --> 02:07:16.060]   that is self-sustaining, profitable, and growing,
[02:07:16.060 --> 02:07:19.520]   and we're gonna innovate our way out of this dilemma, right?
[02:07:19.520 --> 02:07:23.280]   And we're trying to get people to do the latter
[02:07:23.280 --> 02:07:25.240]   rather than the former, at all scales.
[02:07:25.240 --> 02:07:28.200]   - Elon is an interesting case.
[02:07:28.200 --> 02:07:32.160]   So you are a proponent, you celebrate Elon,
[02:07:32.160 --> 02:07:35.220]   but he's also somebody who has for a long time
[02:07:35.220 --> 02:07:39.180]   warned about the dangers, the potential dangers,
[02:07:39.180 --> 02:07:41.580]   existential risks of artificial intelligence.
[02:07:41.580 --> 02:07:42.860]   How do you square the two?
[02:07:42.860 --> 02:07:45.020]   Is that a contradiction to you?
[02:07:45.020 --> 02:07:49.460]   - It is somewhat because he's very much against regulation
[02:07:49.460 --> 02:07:53.540]   in many aspects, but for AI, he's definitely
[02:07:53.540 --> 02:07:57.340]   a proponent of regulations.
[02:07:57.340 --> 02:08:01.200]   I think overall, he saw the dangers of, say,
[02:08:02.140 --> 02:08:04.500]   opening AI, cornering the market,
[02:08:04.500 --> 02:08:07.500]   and then getting to have the monopoly
[02:08:07.500 --> 02:08:12.500]   over the cultural priors that you can embed in these LLMs
[02:08:12.500 --> 02:08:17.820]   that then, as LLMs now become the source of truth
[02:08:17.820 --> 02:08:21.020]   for people, then you can shape the culture of the people,
[02:08:21.020 --> 02:08:23.940]   and so you can control people by controlling LLMs.
[02:08:23.940 --> 02:08:27.700]   And he saw that, just like it was the case for social media,
[02:08:28.580 --> 02:08:31.760]   if you shape the function of information propagation,
[02:08:31.760 --> 02:08:34.000]   you can shape people's opinions.
[02:08:34.000 --> 02:08:36.040]   He sought to make a competitor.
[02:08:36.040 --> 02:08:38.440]   So at least, I think we're very aligned there
[02:08:38.440 --> 02:08:41.840]   that the way to a good future is to maintain
[02:08:41.840 --> 02:08:43.960]   sort of adversarial equilibria
[02:08:43.960 --> 02:08:45.820]   between the various AI players.
[02:08:45.820 --> 02:08:49.880]   I'd love to talk to him to understand sort of his thinking
[02:08:49.880 --> 02:08:54.880]   about how to make, how to advance AI going forwards.
[02:08:54.880 --> 02:08:57.780]   I mean, he's also hedging his bets, I would say,
[02:08:57.780 --> 02:08:59.480]   with Neuralink, right?
[02:08:59.480 --> 02:09:02.920]   I think if he can't stop the progress of AI,
[02:09:02.920 --> 02:09:04.720]   he's building the technology to merge.
[02:09:04.720 --> 02:09:09.720]   So look at the actions, not just the words, but--
[02:09:09.720 --> 02:09:14.440]   - Well, I mean, there's some degree where being concerned,
[02:09:14.440 --> 02:09:17.120]   maybe using human psychology,
[02:09:17.120 --> 02:09:20.720]   being concerned about threats all around us is a motivator.
[02:09:20.720 --> 02:09:22.400]   Like, it's an encouraging thing.
[02:09:22.400 --> 02:09:24.640]   I operate much better when there's a deadline,
[02:09:24.640 --> 02:09:26.600]   the fear of the deadline.
[02:09:26.600 --> 02:09:29.080]   Like, and I, for myself, create artificial things.
[02:09:29.080 --> 02:09:31.780]   Like, I wanna create in myself this kind of anxiety
[02:09:31.780 --> 02:09:33.980]   as if something really horrible will happen
[02:09:33.980 --> 02:09:35.200]   if I miss the deadline.
[02:09:35.200 --> 02:09:38.700]   I think there's some degree of that here
[02:09:38.700 --> 02:09:42.280]   because creating AI that's aligned with humans
[02:09:42.280 --> 02:09:44.260]   has a lot of potential benefits.
[02:09:44.260 --> 02:09:47.200]   And so a different way to reframe that is,
[02:09:47.200 --> 02:09:49.380]   if you don't, we're all gonna die.
[02:09:49.380 --> 02:09:55.220]   It just seems to be a very powerful psychological formulation
[02:09:55.520 --> 02:09:59.300]   of the goal of creating human-aligned AI.
[02:09:59.300 --> 02:10:00.740]   - I think that anxiety is good.
[02:10:00.740 --> 02:10:03.340]   I think, like I said, I want the free market
[02:10:03.340 --> 02:10:07.200]   to create aligned AIs that are reliable.
[02:10:07.200 --> 02:10:10.660]   And I think that's what he's trying to do with XAI.
[02:10:10.660 --> 02:10:12.780]   So I'm all for it.
[02:10:12.780 --> 02:10:15.620]   What I am against is sort of stopping,
[02:10:15.620 --> 02:10:21.820]   let's say, the open source ecosystem from thriving, right,
[02:10:21.820 --> 02:10:25.100]   by, let's say, in the executive order,
[02:10:25.100 --> 02:10:28.840]   claiming that open source LMs are dual-use technologies
[02:10:28.840 --> 02:10:30.920]   and should be government-controlled.
[02:10:30.920 --> 02:10:34.120]   Then everybody needs to register their GPU
[02:10:34.120 --> 02:10:36.760]   and their big matrices with the government.
[02:10:36.760 --> 02:10:41.080]   And I think that extra friction will dissuade a lot
[02:10:41.080 --> 02:10:42.560]   of hackers from contributing,
[02:10:42.560 --> 02:10:45.600]   hackers that could later become the researchers
[02:10:45.600 --> 02:10:50.200]   that make key discoveries that push us forward, right,
[02:10:50.200 --> 02:10:52.720]   including discoveries for AI safety.
[02:10:52.720 --> 02:10:55.780]   And so I think I just wanna maintain ubiquity
[02:10:55.780 --> 02:10:57.820]   of opportunity to contribute to AI
[02:10:57.820 --> 02:11:00.500]   and to own a piece of the future, right?
[02:11:00.500 --> 02:11:04.620]   It can't just be legislated behind some wall
[02:11:04.620 --> 02:11:07.780]   where only a few players get to play the game.
[02:11:07.780 --> 02:11:11.700]   - I mean, so the EAC movement is often sort of caricatured
[02:11:11.700 --> 02:11:16.700]   to mean sort of progress and innovation at all costs.
[02:11:16.700 --> 02:11:20.540]   Doesn't matter how unsafe it is.
[02:11:20.540 --> 02:11:22.860]   Doesn't matter if it caused a lot of damage.
[02:11:22.860 --> 02:11:26.320]   You just build cool shit as fast as possible.
[02:11:26.320 --> 02:11:31.240]   Stay up all night with a Diet Coke, whatever it takes.
[02:11:31.240 --> 02:11:34.440]   I think, I guess, I don't know if there's a question
[02:11:34.440 --> 02:11:37.440]   in there, but how important to you
[02:11:37.440 --> 02:11:39.560]   and what you've seen the different formulations
[02:11:39.560 --> 02:11:42.400]   of EAC is safety, is AI safety?
[02:11:42.400 --> 02:11:48.120]   - I think, again, I think if there was no one working on it,
[02:11:48.120 --> 02:11:50.840]   I think I would be a proponent of it.
[02:11:50.840 --> 02:11:54.020]   I think, again, our goal is to sort of bring balance
[02:11:54.020 --> 02:11:59.020]   and obviously a sense of urgency is a useful tool, right,
[02:11:59.020 --> 02:12:00.940]   to make progress.
[02:12:00.940 --> 02:12:03.900]   It hacks our dopaminergic systems
[02:12:03.900 --> 02:12:08.260]   and gives us energy to work late into the night.
[02:12:08.260 --> 02:12:10.980]   I think also having a higher purpose
[02:12:10.980 --> 02:12:12.520]   you're contributing to, right?
[02:12:12.520 --> 02:12:14.540]   At the end of the day, it's like, what am I contributing to?
[02:12:14.540 --> 02:12:17.700]   I'm contributing to the growth of this beautiful machine
[02:12:17.700 --> 02:12:20.060]   so that we can seek to the stars.
[02:12:20.060 --> 02:12:20.980]   That's really inspiring.
[02:12:20.980 --> 02:12:25.400]   That's also a sort of neuro hack.
[02:12:25.400 --> 02:12:28.100]   - So you're saying AI safety is important to you,
[02:12:28.100 --> 02:12:32.300]   but right now the landscape of ideas you see
[02:12:32.300 --> 02:12:35.660]   is AI safety as a topic is used more often
[02:12:35.660 --> 02:12:38.180]   to gain centralized control.
[02:12:38.180 --> 02:12:40.340]   So in that sense, you're resisting it
[02:12:40.340 --> 02:12:43.540]   as a proxy for gaining centralized control.
[02:12:43.540 --> 02:12:47.860]   - Yeah, I just think we have to be careful
[02:12:47.860 --> 02:12:52.860]   because safety is just the perfect cover
[02:12:52.860 --> 02:12:57.140]   for sort of centralization of power
[02:12:57.140 --> 02:12:59.900]   and covering up eventually corruption.
[02:12:59.900 --> 02:13:01.060]   I'm not saying it's corrupted now,
[02:13:01.060 --> 02:13:04.280]   but it could be down the line.
[02:13:04.280 --> 02:13:08.020]   And really, if you let the argument run,
[02:13:08.020 --> 02:13:12.180]   there's no amount of sort of centralization of control
[02:13:12.180 --> 02:13:14.780]   that will be enough to ensure your safety.
[02:13:14.780 --> 02:13:18.540]   There's always more nine, nine, nines of P safety
[02:13:18.540 --> 02:13:22.020]   that you can gain, you know, 999.9999% safe.
[02:13:22.020 --> 02:13:23.300]   Maybe you want another nine.
[02:13:23.300 --> 02:13:26.580]   Oh, please give us full access to everything you do,
[02:13:26.580 --> 02:13:27.980]   full surveillance.
[02:13:27.980 --> 02:13:32.100]   And frankly, those that are proponents of AI safety
[02:13:32.100 --> 02:13:36.740]   have proposed like having a global panopticon, right?
[02:13:36.740 --> 02:13:39.500]   Where you have centralized perception
[02:13:39.500 --> 02:13:41.260]   of everything going on.
[02:13:41.260 --> 02:13:44.020]   And to me, that just opens up the door wide open
[02:13:44.020 --> 02:13:47.020]   for a sort of Big Brother 1984-like scenario,
[02:13:47.020 --> 02:13:49.580]   and that's not a future I wanna live in.
[02:13:49.580 --> 02:13:51.900]   - 'Cause we know, we have some examples throughout history
[02:13:51.900 --> 02:13:54.500]   when that did not lead to a good outcome.
[02:13:54.500 --> 02:13:55.960]   - Right.
[02:13:55.960 --> 02:13:58.940]   - You mentioned you founded a company, Xtropic,
[02:13:58.940 --> 02:14:04.060]   that recently announced a 14.1 million seed round.
[02:14:04.060 --> 02:14:05.660]   What's the goal of the company?
[02:14:05.660 --> 02:14:08.700]   You're talking about a lot of interesting physics things.
[02:14:08.700 --> 02:14:10.900]   So what are you up to over there
[02:14:10.900 --> 02:14:12.900]   that you can talk about?
[02:14:12.900 --> 02:14:14.100]   - Yeah, I mean, you know,
[02:14:14.100 --> 02:14:17.460]   originally we weren't gonna announce last week,
[02:14:17.460 --> 02:14:20.260]   but I think with the doxing and disclosure,
[02:14:20.260 --> 02:14:21.740]   we got our hand forced.
[02:14:21.740 --> 02:14:24.820]   So we had to disclose roughly what we were doing,
[02:14:24.820 --> 02:14:29.820]   but really Xtropic was born from my dissatisfaction
[02:14:29.820 --> 02:14:31.460]   and that of my colleagues
[02:14:31.460 --> 02:14:35.580]   with the quantum computing roadmap, right?
[02:14:35.580 --> 02:14:38.900]   Quantum computing was sort of the first path
[02:14:38.900 --> 02:14:42.060]   to physics-based computing
[02:14:42.060 --> 02:14:45.300]   that was trying to commercially scale.
[02:14:45.300 --> 02:14:47.360]   And I was working on physics-based AI
[02:14:47.360 --> 02:14:49.980]   that runs on these physics-based computers.
[02:14:49.980 --> 02:14:52.940]   But ultimately our greatest enemy was this noise,
[02:14:52.940 --> 02:14:55.340]   this pervasive problem of noise that,
[02:14:55.340 --> 02:14:57.060]   you know, as I mentioned,
[02:14:57.060 --> 02:14:59.820]   you have to constantly pump out the noise
[02:14:59.820 --> 02:15:03.620]   out of the system to maintain this pristine environment
[02:15:03.620 --> 02:15:06.160]   where quantum mechanics can take effect.
[02:15:06.160 --> 02:15:08.180]   And that constraint was just too much.
[02:15:08.180 --> 02:15:09.800]   It's too costly to do that.
[02:15:09.800 --> 02:15:13.160]   And so we were wondering, right,
[02:15:13.160 --> 02:15:17.900]   as generative AI is sort of eating the world,
[02:15:17.900 --> 02:15:21.300]   more and more of the world's computational workloads
[02:15:21.300 --> 02:15:23.380]   are focused on generative AI.
[02:15:23.380 --> 02:15:24.900]   How could we use physics
[02:15:24.900 --> 02:15:28.840]   to engineer the ultimate physical substrate
[02:15:28.840 --> 02:15:30.780]   for generative AI, right?
[02:15:30.780 --> 02:15:33.460]   From first principles of physics,
[02:15:33.460 --> 02:15:36.780]   of information theory, of computation,
[02:15:36.780 --> 02:15:39.940]   and ultimately of thermodynamics, right?
[02:15:39.940 --> 02:15:42.200]   And so what we're seeking to build
[02:15:42.200 --> 02:15:45.220]   is a physics-based computing system
[02:15:45.220 --> 02:15:47.180]   and physics-based AI algorithms
[02:15:47.180 --> 02:15:53.220]   that are inspired by out-of-equilibrium thermodynamics
[02:15:53.220 --> 02:15:55.780]   or harness it directly
[02:15:55.780 --> 02:15:59.780]   to do machine learning as a physical process.
[02:15:59.780 --> 02:16:03.980]   - So what does that mean,
[02:16:03.980 --> 02:16:05.420]   machine learning as a physical process?
[02:16:05.420 --> 02:16:07.580]   Is that hardware, is it software, is it both?
[02:16:07.580 --> 02:16:09.220]   Is it trying to do the full stack
[02:16:09.220 --> 02:16:10.620]   in some kind of unique way?
[02:16:10.620 --> 02:16:12.900]   - Yes, it is full stack.
[02:16:12.900 --> 02:16:16.060]   And so we're folks that have built
[02:16:16.060 --> 02:16:19.980]   differentiable programming
[02:16:19.980 --> 02:16:21.840]   into the quantum computing ecosystem
[02:16:21.840 --> 02:16:23.500]   with TensorFlow Quantum.
[02:16:23.500 --> 02:16:25.300]   One of my co-founders of TensorFlow Quantum
[02:16:25.300 --> 02:16:27.020]   is the CTO, Trevor McCourt.
[02:16:27.020 --> 02:16:31.680]   We have some of the best quantum computer architects,
[02:16:31.680 --> 02:16:35.560]   those that have designed IBM's and AWS's systems.
[02:16:35.560 --> 02:16:37.800]   They've left quantum computing
[02:16:37.800 --> 02:16:41.240]   to help us build what we call, actually,
[02:16:41.240 --> 02:16:43.600]   a thermodynamic computer.
[02:16:43.600 --> 02:16:44.960]   - A thermodynamic computer.
[02:16:44.960 --> 02:16:47.680]   Well, actually, that's nothing new around TensorFlow Quantum.
[02:16:47.680 --> 02:16:51.760]   What lessons have you learned from TensorFlow Quantum?
[02:16:51.760 --> 02:16:55.620]   Maybe you can speak to what it takes
[02:16:55.620 --> 02:16:59.160]   to create, essentially, what, like a software API
[02:16:59.160 --> 02:17:00.300]   to a quantum computer?
[02:17:01.440 --> 02:17:05.120]   - Right, I mean, that was a challenge to build,
[02:17:05.120 --> 02:17:06.320]   to invent, to build,
[02:17:06.320 --> 02:17:09.200]   and then to get to run on the real devices.
[02:17:09.200 --> 02:17:11.000]   - Can you actually speak to what it is?
[02:17:11.000 --> 02:17:14.400]   - Yeah, so TensorFlow Quantum
[02:17:14.400 --> 02:17:16.640]   was an attempt at,
[02:17:16.640 --> 02:17:18.600]   well, I mean, I guess we succeeded
[02:17:18.600 --> 02:17:20.720]   at combining deep learning
[02:17:20.720 --> 02:17:24.160]   or differentiable classical programming
[02:17:24.160 --> 02:17:26.720]   with quantum computing
[02:17:26.720 --> 02:17:28.800]   and turn quantum computing
[02:17:28.800 --> 02:17:31.960]   into, or have types of programs
[02:17:31.960 --> 02:17:34.640]   that are differentiable in quantum computing.
[02:17:34.640 --> 02:17:37.840]   And, you know, Andrej Karpathy
[02:17:37.840 --> 02:17:41.600]   calls differentiable programming software 2.0, right?
[02:17:41.600 --> 02:17:44.960]   It's like gradient descent is a better programmer than you.
[02:17:44.960 --> 02:17:46.840]   And the idea was that
[02:17:46.840 --> 02:17:48.440]   in the early days of quantum computing,
[02:17:48.440 --> 02:17:51.160]   you can only run short quantum programs.
[02:17:51.160 --> 02:17:54.480]   And so, which quantum programs should you run?
[02:17:54.480 --> 02:17:58.200]   Well, just let gradient descent find those programs instead.
[02:17:58.200 --> 02:18:01.120]   And so, we built sort of the first infrastructure
[02:18:01.120 --> 02:18:05.720]   to not only run differentiable quantum programs,
[02:18:05.720 --> 02:18:10.720]   but combine them as part of broader deep learning graphs,
[02:18:10.720 --> 02:18:15.080]   incorporating deep neural networks,
[02:18:15.080 --> 02:18:16.840]   you know, the ones you know and love,
[02:18:16.840 --> 02:18:19.660]   with what are called quantum neural networks.
[02:18:19.660 --> 02:18:26.220]   And ultimately, it was a very cross-disciplinary effort.
[02:18:26.220 --> 02:18:29.360]   We had to invent all sorts of ways to differentiate,
[02:18:29.360 --> 02:18:32.680]   to back propagate through the graph, the hybrid graph.
[02:18:32.680 --> 02:18:35.600]   But ultimately, it taught me that
[02:18:35.600 --> 02:18:39.400]   the way to program matter and to program physics is
[02:18:39.400 --> 02:18:43.600]   by differentiating through control parameters.
[02:18:43.600 --> 02:18:46.040]   If you have parameters that affects the physics
[02:18:46.040 --> 02:18:48.640]   of the system, you can,
[02:18:48.640 --> 02:18:50.840]   and you can evaluate some loss function,
[02:18:50.840 --> 02:18:55.080]   you can optimize the system to accomplish a task,
[02:18:55.080 --> 02:18:56.980]   whatever that task may be.
[02:18:56.980 --> 02:19:01.980]   And that's a very sort of universal meta framework
[02:19:01.980 --> 02:19:05.500]   for how to program physics-based computers.
[02:19:05.500 --> 02:19:07.900]   - To try to parametrize everything,
[02:19:07.900 --> 02:19:12.400]   make those parameters differential, and then optimize.
[02:19:12.400 --> 02:19:13.940]   - Yes. - Okay.
[02:19:13.940 --> 02:19:17.580]   So, is there some more practical engineering lessons
[02:19:17.580 --> 02:19:22.300]   from TensorFlow Quantum, just organizationally too,
[02:19:22.300 --> 02:19:25.440]   like the humans involved, and how to get to a product,
[02:19:25.440 --> 02:19:29.120]   how to create good documentation, how to have,
[02:19:29.120 --> 02:19:31.440]   I don't know, all of these little subtle things
[02:19:31.440 --> 02:19:34.240]   that people might not think about.
[02:19:34.240 --> 02:19:39.240]   - I think like working across disciplinary boundaries
[02:19:39.240 --> 02:19:42.600]   is always a challenge, and you have to be extremely patient
[02:19:42.600 --> 02:19:44.320]   in teaching one another, right?
[02:19:44.320 --> 02:19:47.720]   I learned a lot of software engineering through the process.
[02:19:47.720 --> 02:19:49.940]   My colleagues learned a lot of quantum physics,
[02:19:49.940 --> 02:19:52.680]   and some learned machine learning
[02:19:52.680 --> 02:19:56.380]   through the process of building this system.
[02:19:56.380 --> 02:19:59.880]   And I think if you get some smart people
[02:19:59.880 --> 02:20:02.880]   that are passionate and trust each other in a room,
[02:20:02.880 --> 02:20:06.320]   and you have a small team, and you teach each other
[02:20:06.320 --> 02:20:08.880]   your specialties, suddenly you're kind of forming
[02:20:08.880 --> 02:20:12.420]   this sort of model soup of expertise,
[02:20:12.420 --> 02:20:15.040]   and something special comes out of that, right?
[02:20:15.040 --> 02:20:18.720]   It's like combining genes, but for your knowledge bases.
[02:20:18.720 --> 02:20:21.680]   And sometimes special products come out of that.
[02:20:21.680 --> 02:20:24.800]   And so I think like even though it's very high friction
[02:20:24.800 --> 02:20:28.400]   initially to work in an interdisciplinary team,
[02:20:28.400 --> 02:20:31.200]   I think the product at the end of the day is worth it.
[02:20:31.200 --> 02:20:34.380]   And so learned a lot trying to bridge the gap there,
[02:20:34.380 --> 02:20:37.120]   and I mean, it's still a challenge to this day.
[02:20:37.120 --> 02:20:40.600]   We hire folks that have an AI background,
[02:20:40.600 --> 02:20:43.120]   folks that have a pure physics background,
[02:20:43.120 --> 02:20:47.040]   and somehow we have to make them talk to one another, right?
[02:20:47.040 --> 02:20:50.220]   - Is there a magic, is there some science and art
[02:20:50.220 --> 02:20:53.320]   to the hiring process to building a team
[02:20:53.320 --> 02:20:55.440]   that can create magic together?
[02:20:55.440 --> 02:21:01.820]   - Yeah, it's really hard to pinpoint that je ne sais quoi.
[02:21:01.820 --> 02:21:05.600]   - I didn't know you speak French, that's very nice.
[02:21:05.600 --> 02:21:07.320]   (laughing)
[02:21:07.320 --> 02:21:09.600]   - Yeah, I'm actually French-Canadian, so.
[02:21:09.600 --> 02:21:11.680]   - Oh, you are legitimately French-Canadian.
[02:21:11.680 --> 02:21:15.360]   I thought you were just doing that for the cred.
[02:21:15.360 --> 02:21:18.440]   - No, no, I'm truly French-Canadian from Montreal.
[02:21:18.440 --> 02:21:23.840]   But yeah, essentially we look for people
[02:21:23.840 --> 02:21:26.200]   with very high fluid intelligence
[02:21:26.200 --> 02:21:27.840]   that aren't over-specialized,
[02:21:27.840 --> 02:21:29.840]   because they're gonna have to get out of their comfort zone.
[02:21:29.840 --> 02:21:32.360]   They're gonna have to incorporate concepts
[02:21:32.360 --> 02:21:34.120]   that they've never seen before,
[02:21:34.120 --> 02:21:36.720]   and very quickly get comfortable with them, right?
[02:21:36.720 --> 02:21:38.200]   Or learn to work in a team.
[02:21:38.200 --> 02:21:42.120]   And so that's sort of what we look for when we hire.
[02:21:42.120 --> 02:21:46.640]   We can't hire people that are just optimizing
[02:21:46.640 --> 02:21:50.100]   this subsystem for the past three or four years.
[02:21:50.100 --> 02:21:53.760]   We need really general, sort of broader
[02:21:53.760 --> 02:21:55.800]   intelligence and specialty.
[02:21:55.800 --> 02:21:59.040]   And people that are open-minded, really,
[02:21:59.040 --> 02:22:02.320]   'cause if you're pioneering a new approach from scratch,
[02:22:02.320 --> 02:22:06.320]   there is no textbook, there's no reference, it's just us.
[02:22:06.320 --> 02:22:08.800]   And people that are hungry to learn.
[02:22:08.800 --> 02:22:10.160]   So we have to teach each other,
[02:22:10.160 --> 02:22:11.740]   we have to learn the literature,
[02:22:11.740 --> 02:22:14.640]   we have to share knowledge bases, collaborate,
[02:22:14.640 --> 02:22:16.840]   in order to push the boundary of knowledge
[02:22:16.840 --> 02:22:19.120]   further together, right?
[02:22:19.120 --> 02:22:23.040]   And so people that are used to just getting prescribed
[02:22:23.040 --> 02:22:26.120]   what to do at this stage,
[02:22:26.120 --> 02:22:28.320]   when you're at the pioneering stage,
[02:22:28.320 --> 02:22:31.560]   that's not necessarily who you want to hire.
[02:22:31.560 --> 02:22:33.000]   - So you mentioned with Extropic,
[02:22:33.000 --> 02:22:34.660]   you're trying to build the physical substrate
[02:22:34.660 --> 02:22:36.340]   for generative AI.
[02:22:37.680 --> 02:22:42.540]   What's the difference between that and the AGI AI itself?
[02:22:42.540 --> 02:22:47.000]   So is it possible that in the halls of your company,
[02:22:47.000 --> 02:22:48.700]   AGI will be created?
[02:22:48.700 --> 02:22:51.860]   Or will AGI just be using this as a substrate?
[02:22:51.860 --> 02:22:56.860]   - I think our goal is to both run human-like AI,
[02:22:56.860 --> 02:22:58.260]   or anthropomorphic AI.
[02:22:58.260 --> 02:23:00.540]   - Sorry for the use of the term AGI.
[02:23:00.540 --> 02:23:01.980]   I know it's triggering for you.
[02:23:01.980 --> 02:23:05.860]   - We think that the future is actually physics-based AI,
[02:23:06.860 --> 02:23:10.660]   combined with anthropomorphic AI.
[02:23:10.660 --> 02:23:15.500]   So you can imagine I have a sort of world modeling engine
[02:23:15.500 --> 02:23:17.100]   through physics-based AI.
[02:23:17.100 --> 02:23:19.440]   Physics-based AI is better at representing the world
[02:23:19.440 --> 02:23:22.060]   at all scales, 'cause it can be quantum mechanical,
[02:23:22.060 --> 02:23:24.700]   thermodynamic, deterministic,
[02:23:24.700 --> 02:23:26.740]   hybrid representations of the world,
[02:23:26.740 --> 02:23:29.600]   just like our world at different scales
[02:23:29.600 --> 02:23:31.700]   has different regimes of physics.
[02:23:31.700 --> 02:23:33.960]   If you inspire yourself from that,
[02:23:33.960 --> 02:23:35.780]   in the ways you learn representations of nature,
[02:23:35.780 --> 02:23:38.180]   you can have much more accurate representations of nature.
[02:23:38.180 --> 02:23:42.360]   So you can have very accurate world models at all scales.
[02:23:42.360 --> 02:23:45.700]   And so you have the world modeling engine,
[02:23:45.700 --> 02:23:48.700]   and then you have the sort of anthropomorphic AI
[02:23:48.700 --> 02:23:50.100]   that is human-like.
[02:23:50.100 --> 02:23:51.540]   So you can have the science,
[02:23:51.540 --> 02:23:54.940]   the playground to test your ideas,
[02:23:54.940 --> 02:23:57.060]   and you can have the synthetic scientist.
[02:23:57.060 --> 02:24:00.380]   And to us, that joint system of a physics-based AI
[02:24:00.380 --> 02:24:03.900]   and an anthropomorphic AI is the closest thing
[02:24:03.900 --> 02:24:07.620]   to a fully general artificially intelligent system.
[02:24:07.620 --> 02:24:10.220]   - So you can get closer to truth by grounding
[02:24:10.220 --> 02:24:13.900]   of the AI to physics,
[02:24:13.900 --> 02:24:17.220]   but you can also still have a anthropomorphic interface
[02:24:17.220 --> 02:24:19.860]   to us humans that like to talk to other humans,
[02:24:19.860 --> 02:24:21.540]   or human-like systems.
[02:24:21.540 --> 02:24:24.020]   So on that topic, what do you,
[02:24:24.020 --> 02:24:28.780]   I suppose that is one of the big limitations
[02:24:28.780 --> 02:24:30.860]   of current large language models to you
[02:24:30.860 --> 02:24:34.980]   is that they're not, they're good bullshitters.
[02:24:34.980 --> 02:24:37.740]   They're not really grounded to truth necessarily.
[02:24:37.740 --> 02:24:40.700]   Would that be fair to say?
[02:24:40.700 --> 02:24:42.220]   - Yeah, no.
[02:24:42.220 --> 02:24:45.660]   You wouldn't try to extrapolate the stock market
[02:24:45.660 --> 02:24:49.060]   with an LM trained on text from the internet, right?
[02:24:49.060 --> 02:24:50.620]   It's not gonna be a very accurate model.
[02:24:50.620 --> 02:24:53.500]   It's not gonna model its priors or its uncertainties
[02:24:53.500 --> 02:24:55.820]   about the world very accurately, right?
[02:24:55.820 --> 02:24:58.660]   So you need a different type of AI
[02:24:58.660 --> 02:25:03.660]   to compliment sort of this text extrapolation AI, yeah.
[02:25:03.660 --> 02:25:07.460]   - You mentioned singularity earlier.
[02:25:07.460 --> 02:25:09.900]   How far away are we from a singularity?
[02:25:09.900 --> 02:25:12.940]   - I don't know if I believe in a finite time singularity
[02:25:12.940 --> 02:25:14.220]   as a single point in time.
[02:25:14.220 --> 02:25:16.740]   I think it's gonna be asymptotic
[02:25:16.740 --> 02:25:20.300]   and sort of a diagonal sort of asymptote.
[02:25:20.300 --> 02:25:23.420]   Like, you know, we have the light cone,
[02:25:23.420 --> 02:25:27.340]   we have the limits of physics restricting our ability
[02:25:27.340 --> 02:25:31.820]   to grow so obviously can't fully diverge on a finite time.
[02:25:31.820 --> 02:25:36.700]   I think my priors are that, you know,
[02:25:36.700 --> 02:25:40.780]   I think a lot of people on the other side of the aisle
[02:25:40.780 --> 02:25:44.660]   think that once we reach human level AI,
[02:25:44.660 --> 02:25:46.180]   there's gonna be an inflection point
[02:25:46.180 --> 02:25:50.380]   and a sudden like fume, like suddenly AI is gonna grok
[02:25:50.380 --> 02:25:53.380]   how to, you know, manipulate matter at the nanoscale
[02:25:53.380 --> 02:25:55.100]   and assemble nanobots.
[02:25:55.100 --> 02:25:59.180]   And having worked, you know, for nearly a decade
[02:25:59.180 --> 02:26:01.260]   in applying AI to engineer matter,
[02:26:01.260 --> 02:26:03.020]   it's much harder than they think.
[02:26:03.020 --> 02:26:04.740]   And in reality, you need a lot of samples
[02:26:04.740 --> 02:26:06.940]   from either a simulation of nature
[02:26:06.940 --> 02:26:10.100]   that's very accurate and costly or nature itself.
[02:26:10.100 --> 02:26:12.180]   And that keeps your ability
[02:26:12.180 --> 02:26:15.620]   to control the world around us in check.
[02:26:15.620 --> 02:26:19.820]   There's a sort of minimal cost computationally
[02:26:19.820 --> 02:26:22.460]   and thermodynamically to acquiring information
[02:26:22.460 --> 02:26:24.300]   about the world in order to be able to predict
[02:26:24.300 --> 02:26:27.460]   and control it and that keeps things in check.
[02:26:27.460 --> 02:26:30.020]   - It's funny you mentioned the other side of the aisle.
[02:26:30.020 --> 02:26:33.700]   So in the poll I posted about P-Doom yesterday,
[02:26:33.700 --> 02:26:35.540]   what's the probability of doom?
[02:26:35.540 --> 02:26:37.900]   There seems to be a nice like division
[02:26:37.900 --> 02:26:42.260]   between people think it's very likely and very unlikely.
[02:26:42.260 --> 02:26:44.900]   I wonder if in the future there'll be
[02:26:44.900 --> 02:26:47.940]   the actual Republicans versus Democrats division,
[02:26:47.940 --> 02:26:49.380]   blue versus red.
[02:26:49.380 --> 02:26:53.300]   Is the AI doomers versus the yakkers, yak.
[02:26:53.300 --> 02:26:54.540]   - Yeah.
[02:26:54.540 --> 02:26:56.060]   So this movement, you know,
[02:26:56.060 --> 02:26:58.620]   is not right wing or left wing fundamentally.
[02:26:58.620 --> 02:27:01.780]   It's more like up versus down in terms of the scale.
[02:27:01.780 --> 02:27:02.620]   - Which one's the up, okay.
[02:27:02.620 --> 02:27:03.740]   - Civilization, right?
[02:27:03.740 --> 02:27:05.220]   - All right.
[02:27:05.220 --> 02:27:09.780]   - But it seems to be like there is a sort of case
[02:27:09.780 --> 02:27:12.220]   of alignment of the existing political parties
[02:27:12.220 --> 02:27:17.220]   where those that are for more centralization of power
[02:27:17.500 --> 02:27:22.060]   control and more regulations are aligning with sort of,
[02:27:22.060 --> 02:27:23.940]   aligning themselves with the doomers
[02:27:23.940 --> 02:27:28.620]   because that sort of instilling fear in people
[02:27:28.620 --> 02:27:31.220]   is a great way for them to give up more control
[02:27:31.220 --> 02:27:33.020]   and give the government more power.
[02:27:33.020 --> 02:27:36.260]   But fundamentally, we're not left versus right.
[02:27:36.260 --> 02:27:40.300]   I think there's, we've done polls of people's alignment
[02:27:40.300 --> 02:27:42.620]   with any yak, I think it's pretty balanced.
[02:27:42.620 --> 02:27:45.780]   So it's a new fundamental issue of our time.
[02:27:45.780 --> 02:27:48.180]   It's not just centralization versus decentralization.
[02:27:48.180 --> 02:27:51.620]   It's kind of, do we go, it's like tech progressivism
[02:27:51.620 --> 02:27:54.060]   versus techno conservatism, right?
[02:27:54.060 --> 02:27:57.940]   - So yak is, as a movement is often formulated
[02:27:57.940 --> 02:28:02.420]   in contrast to EA, effective altruism.
[02:28:02.420 --> 02:28:05.900]   What do you think are the pros and cons
[02:28:05.900 --> 02:28:07.500]   of effective altruism?
[02:28:07.500 --> 02:28:10.340]   What's interesting, insightful to you about them?
[02:28:10.340 --> 02:28:14.540]   And what is negative?
[02:28:15.460 --> 02:28:20.060]   - Right, I think like people trying to do good
[02:28:20.060 --> 02:28:23.100]   from first principles is good.
[02:28:23.100 --> 02:28:25.540]   - We should actually say, and sorry to interrupt.
[02:28:25.540 --> 02:28:26.980]   We should probably say that,
[02:28:26.980 --> 02:28:29.060]   and you can correct me if I'm wrong,
[02:28:29.060 --> 02:28:33.380]   but effective altruism is a kind of movement
[02:28:33.380 --> 02:28:35.820]   that's trying to do good optimally
[02:28:35.820 --> 02:28:38.380]   where good is probably measured something
[02:28:38.380 --> 02:28:40.600]   like the amount of suffering in the world.
[02:28:40.600 --> 02:28:41.760]   You wanna minimize it.
[02:28:42.780 --> 02:28:46.860]   And there's ways that that can go wrong
[02:28:46.860 --> 02:28:48.580]   as any optimization can.
[02:28:48.580 --> 02:28:50.380]   And so it's interesting to explore
[02:28:50.380 --> 02:28:55.860]   like how things can go wrong.
[02:28:55.860 --> 02:28:57.980]   - We're both trying to do good to some extent.
[02:28:57.980 --> 02:29:01.540]   And we're both trying, we're arguing
[02:29:01.540 --> 02:29:03.980]   for which loss function we should use, right?
[02:29:03.980 --> 02:29:04.800]   - Yes.
[02:29:04.800 --> 02:29:07.740]   - Their loss function is sort of hedons, right?
[02:29:07.740 --> 02:29:12.340]   Units of hedonism, like how good do you feel
[02:29:12.340 --> 02:29:14.500]   and for how much time, right?
[02:29:14.500 --> 02:29:17.740]   And so suffering would be negative hedons
[02:29:17.740 --> 02:29:19.860]   and they're trying to minimize that.
[02:29:19.860 --> 02:29:23.660]   But to us, that seems like that loss function
[02:29:23.660 --> 02:29:25.420]   has sort of spurious minima, right?
[02:29:25.420 --> 02:29:30.420]   You can start minimizing shrimp farm pain, right?
[02:29:30.420 --> 02:29:34.640]   Which seems not that productive to me.
[02:29:34.640 --> 02:29:38.660]   Or you can end up with wire heading
[02:29:38.660 --> 02:29:41.300]   where you just either install a neural link
[02:29:41.300 --> 02:29:43.340]   or you scroll TikTok forever.
[02:29:43.340 --> 02:29:46.260]   And you feel good on the short term timescale
[02:29:46.260 --> 02:29:48.100]   because you're in your neurochemistry.
[02:29:48.100 --> 02:29:52.140]   But on long term timescale, it causes decay and death, right?
[02:29:52.140 --> 02:29:54.060]   'Cause you're not being productive.
[02:29:54.060 --> 02:29:59.060]   Whereas sort of EAC measuring progress of civilization,
[02:29:59.060 --> 02:30:03.560]   not in terms of a subjective loss function like hedonism,
[02:30:03.560 --> 02:30:08.180]   but rather an objective measure,
[02:30:08.180 --> 02:30:11.900]   a quantity that cannot be gamed that is physical energy,
[02:30:11.900 --> 02:30:12.740]   right?
[02:30:12.740 --> 02:30:14.180]   It's very objective, right?
[02:30:14.180 --> 02:30:16.900]   And there's not many ways to game it, right?
[02:30:16.900 --> 02:30:20.580]   If you did it in terms of like GDP or a currency,
[02:30:20.580 --> 02:30:23.180]   that's pinned to a certain value that's moving, right?
[02:30:23.180 --> 02:30:26.880]   And so that's not a good way to measure our progress.
[02:30:26.880 --> 02:30:31.880]   And so, but the thing is we're both trying to make progress
[02:30:31.880 --> 02:30:35.760]   and ensure humanity flourishes and gets to grow.
[02:30:35.760 --> 02:30:38.260]   We just have different loss functions
[02:30:38.260 --> 02:30:41.260]   and different ways of going about doing it.
[02:30:41.260 --> 02:30:45.380]   - Is there a degree, maybe you can educate me, correct me.
[02:30:45.380 --> 02:30:48.460]   I get a little bit skeptical
[02:30:48.460 --> 02:30:50.020]   when there's an equation involved,
[02:30:50.020 --> 02:30:53.080]   trying to reduce all of the human civilization,
[02:30:53.080 --> 02:30:55.820]   human experience to an equation.
[02:30:55.820 --> 02:31:00.620]   Is there a degree that we should be skeptical
[02:31:00.620 --> 02:31:02.420]   of the tyranny of an equation,
[02:31:03.500 --> 02:31:06.320]   of a loss function over which to optimize,
[02:31:06.320 --> 02:31:08.300]   like having a kind of intellectual humility
[02:31:08.300 --> 02:31:12.160]   about optimizing over loss functions?
[02:31:12.160 --> 02:31:14.800]   - Yeah, so this particular loss function,
[02:31:14.800 --> 02:31:18.660]   it's not stiff, it's kind of an average of averages, right?
[02:31:18.660 --> 02:31:23.120]   It's like distributions of states in the future
[02:31:23.120 --> 02:31:25.920]   are gonna follow a certain distribution.
[02:31:25.920 --> 02:31:28.800]   So it's not deterministic.
[02:31:28.800 --> 02:31:31.800]   It's not like, we're not on like stiff rails, right?
[02:31:31.800 --> 02:31:36.800]   It's just a statistical statement about the future.
[02:31:36.800 --> 02:31:38.040]   But at the end of the day,
[02:31:38.040 --> 02:31:41.380]   you can believe in gravity or not,
[02:31:41.380 --> 02:31:44.640]   but it's not necessarily an option to obey it, right?
[02:31:44.640 --> 02:31:47.960]   And some people try to test that and that goes not so well.
[02:31:47.960 --> 02:31:51.680]   So similarly, I think thermodynamics
[02:31:51.680 --> 02:31:53.040]   is there whether we like it or not,
[02:31:53.040 --> 02:31:56.120]   and we're just trying to point out what is
[02:31:56.120 --> 02:31:59.400]   and try to orient ourselves
[02:32:00.280 --> 02:32:04.480]   and chart a path forward given this fundamental truth.
[02:32:04.480 --> 02:32:05.800]   - But there's still some uncertainty.
[02:32:05.800 --> 02:32:08.360]   There's still a lack of information.
[02:32:08.360 --> 02:32:10.440]   Humans tend to fill the gap
[02:32:10.440 --> 02:32:13.760]   with the lack of information with narratives.
[02:32:13.760 --> 02:32:15.220]   And so how they interpret,
[02:32:15.220 --> 02:32:19.840]   even physics is up to interpretation
[02:32:19.840 --> 02:32:21.540]   when there's uncertainty involved.
[02:32:21.540 --> 02:32:27.840]   And humans tend to use that to further their own means.
[02:32:28.760 --> 02:32:31.080]   So it's always, whenever there's an equation,
[02:32:31.080 --> 02:32:32.640]   it just seems like,
[02:32:32.640 --> 02:32:34.840]   until we have really perfect understanding
[02:32:34.840 --> 02:32:38.920]   of the universe, humans will do what humans do.
[02:32:38.920 --> 02:32:43.920]   And they try to use the narrative of doing good
[02:32:43.920 --> 02:32:51.400]   to fool the populace into doing bad.
[02:32:51.400 --> 02:32:53.960]   I guess that this is something
[02:32:53.960 --> 02:32:57.920]   that should be skeptical about in all movements.
[02:32:57.920 --> 02:32:58.760]   - That's right.
[02:32:58.760 --> 02:33:01.780]   So we invite skepticism, right?
[02:33:01.780 --> 02:33:05.560]   - Do you have an understanding of what might,
[02:33:05.560 --> 02:33:07.240]   to a degree that went wrong,
[02:33:07.240 --> 02:33:08.780]   what do you think may have gone wrong
[02:33:08.780 --> 02:33:12.880]   with effective altruism that might also go wrong
[02:33:12.880 --> 02:33:14.720]   with effective accelerationism?
[02:33:14.720 --> 02:33:18.240]   - Yeah, I mean, I think, you know,
[02:33:18.240 --> 02:33:21.720]   I think it provided initially a sense of community
[02:33:21.720 --> 02:33:25.040]   for engineers and intellectuals and rationalists
[02:33:25.040 --> 02:33:25.880]   in the early days.
[02:33:25.880 --> 02:33:28.800]   And it seems like the community was very healthy,
[02:33:28.800 --> 02:33:32.000]   but then, you know, they formed all sorts of organizations
[02:33:32.000 --> 02:33:37.000]   and started routing capital and having actual power, right?
[02:33:37.000 --> 02:33:39.160]   They have real power.
[02:33:39.160 --> 02:33:40.360]   They influence the government,
[02:33:40.360 --> 02:33:43.200]   they influence most AI orgs now.
[02:33:43.200 --> 02:33:45.600]   I mean, they're literally controlling the board of OBDI,
[02:33:45.600 --> 02:33:48.880]   right, and look over to Anthropic.
[02:33:48.880 --> 02:33:51.320]   I think they all have some control over that too.
[02:33:51.320 --> 02:33:54.000]   And so I think, you know,
[02:33:54.000 --> 02:33:56.520]   the assumption of EAC is more like capitalism
[02:33:56.520 --> 02:33:59.800]   is that every agent organism and meta organism
[02:33:59.800 --> 02:34:02.040]   is gonna act in its own interest.
[02:34:02.040 --> 02:34:05.360]   And we should maintain sort of adversarial equilibrium
[02:34:05.360 --> 02:34:08.480]   or adversarial competition to keep each other in check
[02:34:08.480 --> 02:34:09.980]   at all times, at all scales.
[02:34:09.980 --> 02:34:15.540]   I think that, yeah, ultimately it was the perfect cover
[02:34:15.540 --> 02:34:18.280]   to acquire tons of power and capital.
[02:34:18.280 --> 02:34:23.280]   And unfortunately, sometimes that corrupts people over time.
[02:34:23.520 --> 02:34:26.520]   - What does a perfectly productive day,
[02:34:26.520 --> 02:34:28.640]   since building is important,
[02:34:28.640 --> 02:34:30.320]   what does a perfectly productive day
[02:34:30.320 --> 02:34:33.220]   in the life of Guillaume Verdun look like?
[02:34:33.220 --> 02:34:36.920]   How much caffeine do you consume?
[02:34:36.920 --> 02:34:38.620]   Like what's a perfect day?
[02:34:38.620 --> 02:34:42.920]   - Okay, so I have a particular regimen.
[02:34:42.920 --> 02:34:47.920]   I would say my favorite days are 12 p.m. to 4 a.m.
[02:34:47.920 --> 02:34:53.440]   And I would have meetings in the early afternoon,
[02:34:53.440 --> 02:34:56.560]   usually external meetings, some internal meetings.
[02:34:56.560 --> 02:34:59.040]   Because I'm CEO, I have to interface
[02:34:59.040 --> 02:35:00.720]   with the outside world, whether it's customers
[02:35:00.720 --> 02:35:04.500]   or investors or interviewing potential candidates.
[02:35:04.500 --> 02:35:11.480]   And usually I'll have ketones, exogenous ketones.
[02:35:11.480 --> 02:35:16.560]   - So are you on a keto diet or is this--
[02:35:16.560 --> 02:35:21.240]   - I've done keto before for football and whatnot.
[02:35:21.240 --> 02:35:26.240]   But I like to have a meal after part of my day is done.
[02:35:26.240 --> 02:35:31.040]   And so I can just have extreme focus.
[02:35:31.040 --> 02:35:35.000]   - You do the social interactions earlier in the day
[02:35:35.000 --> 02:35:35.840]   without food.
[02:35:35.840 --> 02:35:37.120]   - Front load them, yeah.
[02:35:37.120 --> 02:35:39.820]   Like right now I'm on ketones and Red Bull.
[02:35:39.820 --> 02:35:44.100]   And it just gives you a clarity of thought
[02:35:44.100 --> 02:35:45.720]   that is really next level.
[02:35:45.720 --> 02:35:47.840]   'Cause then when you eat, you're actually allocating
[02:35:47.840 --> 02:35:51.000]   some of your energy that could be going to neural energy
[02:35:51.000 --> 02:35:53.000]   to your digestion.
[02:35:53.000 --> 02:35:56.660]   After I eat, maybe I take a break an hour or so,
[02:35:56.660 --> 02:35:57.500]   hour and a half.
[02:35:57.500 --> 02:36:02.040]   And then usually it's like ideally one meal a day,
[02:36:02.040 --> 02:36:05.480]   like steak and eggs and vegetables.
[02:36:05.480 --> 02:36:08.600]   Animal based primarily, so fruit and meat.
[02:36:08.600 --> 02:36:11.440]   And then I do a second wind usually.
[02:36:11.440 --> 02:36:13.880]   That's deep work, right?
[02:36:13.880 --> 02:36:16.480]   'Cause I am a CEO, but I'm still technical.
[02:36:16.480 --> 02:36:18.200]   I'm contributing to most patents.
[02:36:18.200 --> 02:36:22.440]   And there I'll just stay up late into the night
[02:36:22.440 --> 02:36:25.840]   and work with engineers on very technical problems.
[02:36:25.840 --> 02:36:29.000]   - So it's like the 9 p.m. to 4 a.m.,
[02:36:29.000 --> 02:36:30.720]   whatever that range of time.
[02:36:30.720 --> 02:36:32.880]   - Yeah, yeah, that's the perfect time.
[02:36:32.880 --> 02:36:35.760]   The emails, the things that are on fire,
[02:36:35.760 --> 02:36:38.520]   stop trickling in, you can focus.
[02:36:38.520 --> 02:36:40.280]   And then you have your second wind.
[02:36:40.280 --> 02:36:45.960]   And I think Demis Hassabis has a similar work day
[02:36:45.960 --> 02:36:47.240]   to some extent.
[02:36:47.240 --> 02:36:49.840]   So I think that's definitely inspired my work day.
[02:36:49.840 --> 02:36:54.560]   But yeah, I started this work day when I was at Google
[02:36:54.560 --> 02:36:57.400]   and had to manage a bit of the product during the day
[02:36:57.400 --> 02:37:00.360]   and have meetings and then do technical work at night.
[02:37:00.360 --> 02:37:03.940]   - Exercise, sleep, those kinds of things.
[02:37:03.940 --> 02:37:05.960]   You said football, you used to play football?
[02:37:05.960 --> 02:37:08.760]   - Yeah, I used to play American football.
[02:37:08.760 --> 02:37:10.600]   I've done all sorts of sports growing up.
[02:37:10.600 --> 02:37:13.940]   And then I was into powerlifting for a while.
[02:37:13.940 --> 02:37:17.240]   So when I was studying mathematics in grad school,
[02:37:17.240 --> 02:37:21.120]   I would just do math and lift, take caffeine,
[02:37:21.120 --> 02:37:22.440]   and that was my day.
[02:37:22.440 --> 02:37:25.760]   It was very pure, the purest of monk modes.
[02:37:25.760 --> 02:37:28.440]   But it's really interesting how in powerlifting
[02:37:28.440 --> 02:37:30.320]   you're trying to cause neural adaptation
[02:37:30.320 --> 02:37:32.760]   by having certain driving signals
[02:37:32.760 --> 02:37:35.020]   and you're trying to engineer neuroplasticity
[02:37:35.020 --> 02:37:36.680]   through all sorts of supplements.
[02:37:36.680 --> 02:37:42.000]   And you have all sorts of brain-derived neurotrophic factors
[02:37:42.000 --> 02:37:44.040]   that get secreted when you lift.
[02:37:44.040 --> 02:37:47.080]   So it's funny to me how I was trying to engineer
[02:37:47.080 --> 02:37:53.360]   neural adaptation in my nervous system more broadly,
[02:37:53.360 --> 02:37:56.360]   not just my brain, while learning mathematics.
[02:37:56.360 --> 02:37:59.240]   I think you can learn much faster
[02:37:59.240 --> 02:38:03.680]   if you really care, if you convince yourself
[02:38:03.680 --> 02:38:06.120]   to care a lot about what you're learning
[02:38:06.120 --> 02:38:08.160]   and you have some sort of assistance,
[02:38:08.160 --> 02:38:11.800]   let's say caffeine or some cholinergic supplement
[02:38:11.800 --> 02:38:13.840]   to increase neuroplasticity.
[02:38:13.840 --> 02:38:16.540]   I should chat with Andrew Huberman at some point.
[02:38:16.540 --> 02:38:17.380]   He's the expert.
[02:38:17.380 --> 02:38:21.140]   But yeah, at least to me, it's like,
[02:38:21.140 --> 02:38:25.080]   you can try to input more tokens into your brain,
[02:38:25.080 --> 02:38:27.520]   if you will, and you can try to increase the learning rate
[02:38:27.520 --> 02:38:30.800]   so that you can learn much faster on a shorter timescale.
[02:38:30.800 --> 02:38:33.520]   So I've learned a lot of things.
[02:38:33.520 --> 02:38:34.840]   I've followed my curiosity.
[02:38:34.840 --> 02:38:36.800]   You're naturally, if you're passionate
[02:38:36.800 --> 02:38:38.520]   about what you're doing, you're gonna learn faster,
[02:38:38.520 --> 02:38:40.280]   you're gonna become smarter faster.
[02:38:41.200 --> 02:38:42.520]   And if you follow your curiosity,
[02:38:42.520 --> 02:38:44.440]   you're always gonna be interested.
[02:38:44.440 --> 02:38:47.200]   And so I advise people to follow their curiosity
[02:38:47.200 --> 02:38:50.320]   and don't respect the boundaries of certain fields
[02:38:50.320 --> 02:38:52.600]   or what you've been allocated in terms of lane
[02:38:52.600 --> 02:38:54.160]   of what you're working on.
[02:38:54.160 --> 02:38:57.320]   Just go out and explore and follow your nose
[02:38:57.320 --> 02:39:01.400]   and try to acquire and compress as much information
[02:39:01.400 --> 02:39:02.980]   as you can into your brain,
[02:39:02.980 --> 02:39:04.960]   anything that you find interesting.
[02:39:04.960 --> 02:39:06.120]   - And caring about a thing.
[02:39:06.120 --> 02:39:07.800]   And like you said, which is interesting,
[02:39:07.800 --> 02:39:09.880]   it works for me really well,
[02:39:09.880 --> 02:39:12.040]   is like tricking yourself that you care about a thing.
[02:39:12.040 --> 02:39:13.360]   - Yes.
[02:39:13.360 --> 02:39:15.800]   - And then you start to really care about it.
[02:39:15.800 --> 02:39:18.520]   So it's funny, the motivation
[02:39:18.520 --> 02:39:22.120]   is a really good catalyst for learning.
[02:39:22.120 --> 02:39:27.120]   - Right, and so at least part of my character,
[02:39:27.120 --> 02:39:29.080]   as Beth Jezos, is kind of like--
[02:39:29.080 --> 02:39:30.480]   - Yeah, the hype man.
[02:39:30.480 --> 02:39:32.740]   - Yeah, just hype, but I'm like hyping myself up,
[02:39:32.740 --> 02:39:34.360]   but then I just tweet about it.
[02:39:34.360 --> 02:39:36.600]   And it's just when I'm trying to get really hyped up
[02:39:36.600 --> 02:39:38.680]   and in like an altered state of consciousness
[02:39:38.680 --> 02:39:42.120]   where I'm like ultra focused, in the flow wired,
[02:39:42.120 --> 02:39:44.040]   trying to invent something that's never existed,
[02:39:44.040 --> 02:39:47.840]   I need to get to like unreal levels of like excitement.
[02:39:47.840 --> 02:39:52.320]   But your brain has these levels of cognition
[02:39:52.320 --> 02:39:55.320]   that you can unlock with like higher levels of adrenaline
[02:39:55.320 --> 02:39:56.720]   and whatnot.
[02:39:56.720 --> 02:39:59.480]   And I mean, I've learned that in powerlifting
[02:39:59.480 --> 02:40:03.380]   that actually you can engineer a mental switch
[02:40:03.380 --> 02:40:05.760]   to like increase your strength, right?
[02:40:05.760 --> 02:40:07.920]   Like if you can engineer a switch,
[02:40:07.920 --> 02:40:10.640]   maybe you have a prompt like a certain song or some music
[02:40:10.640 --> 02:40:13.680]   where suddenly you're like fully primed,
[02:40:13.680 --> 02:40:16.560]   then you're at maximum strength, right?
[02:40:16.560 --> 02:40:20.640]   And I've engineered that switch through years of lifting.
[02:40:20.640 --> 02:40:22.280]   If you're gonna get under 500 pounds
[02:40:22.280 --> 02:40:23.980]   and it could crush you,
[02:40:23.980 --> 02:40:28.600]   if you don't have that switch to be wired in, you might die.
[02:40:28.600 --> 02:40:30.160]   So that'll wake you right up.
[02:40:30.160 --> 02:40:34.460]   And that sort of skill I've carried over to like research.
[02:40:34.460 --> 02:40:37.240]   When it's go time, when the stakes are high,
[02:40:37.240 --> 02:40:40.360]   somehow I just reach another level of neural performance.
[02:40:40.360 --> 02:40:44.680]   - So Beth Jezos is your sort of embodiment representation
[02:40:44.680 --> 02:40:46.520]   of your intellectual hulk.
[02:40:46.520 --> 02:40:50.880]   It's your productivity hulk that they just turn on.
[02:40:50.880 --> 02:40:54.120]   What have you learned about the nature of identity
[02:40:54.120 --> 02:40:56.600]   from having these two identities?
[02:40:56.600 --> 02:40:58.200]   I think it's interesting for people
[02:40:58.200 --> 02:41:01.320]   to be able to put on those two hats so explicitly.
[02:41:01.320 --> 02:41:03.240]   - I think it was interesting in the early days.
[02:41:03.240 --> 02:41:04.480]   I think in the early days,
[02:41:04.480 --> 02:41:06.560]   I thought it was truly compartmentalized.
[02:41:06.560 --> 02:41:09.560]   Like, oh yeah, this is a character, I'm Guillaume,
[02:41:09.560 --> 02:41:11.320]   Beth is just the character.
[02:41:11.320 --> 02:41:13.180]   I like take my thoughts
[02:41:13.180 --> 02:41:16.160]   and then I extrapolate them to a bit more extreme.
[02:41:16.160 --> 02:41:20.600]   But over time, it's kind of like both identities
[02:41:20.600 --> 02:41:22.800]   were starting to merge mentally and people were like,
[02:41:22.800 --> 02:41:24.880]   no, you are, I met you, you are Beth,
[02:41:24.880 --> 02:41:27.220]   you are not just Guillaume.
[02:41:27.220 --> 02:41:28.720]   And I was like, wait, am I?
[02:41:28.720 --> 02:41:31.680]   And now it's like fully merged.
[02:41:31.680 --> 02:41:33.080]   But it was already before the docs,
[02:41:33.080 --> 02:41:35.160]   it was already starting mentally
[02:41:35.160 --> 02:41:39.400]   that I am this character, it's part of me.
[02:41:39.400 --> 02:41:42.240]   - Would you recommend people sort of have an alt?
[02:41:42.240 --> 02:41:43.800]   - Absolutely.
[02:41:43.800 --> 02:41:45.480]   - Like young people, would you recommend them
[02:41:45.480 --> 02:41:47.280]   to explore different identities
[02:41:47.280 --> 02:41:49.360]   by having alts, alt accounts?
[02:41:49.360 --> 02:41:51.840]   - It's fun, it's like writing an essay
[02:41:51.840 --> 02:41:53.000]   and taking a position, right?
[02:41:53.000 --> 02:41:54.360]   It's like you do this in debate.
[02:41:54.360 --> 02:41:56.920]   It's like you can have experimental thoughts
[02:41:56.920 --> 02:42:00.440]   and by the stakes being so low
[02:42:00.440 --> 02:42:02.200]   because you're an on account with, I don't know,
[02:42:02.200 --> 02:42:04.040]   20 followers or something,
[02:42:04.040 --> 02:42:05.520]   you can experiment with your thoughts
[02:42:05.520 --> 02:42:07.560]   in a low stakes environment.
[02:42:07.560 --> 02:42:09.320]   And I feel like we've lost that
[02:42:09.320 --> 02:42:12.440]   in the era of everything being under your main name,
[02:42:12.440 --> 02:42:14.040]   everything being attributable to you.
[02:42:14.040 --> 02:42:15.600]   People just are afraid to speak,
[02:42:15.600 --> 02:42:19.600]   explore ideas that aren't fully formed, right?
[02:42:19.600 --> 02:42:21.640]   And I feel like we've lost something there.
[02:42:21.640 --> 02:42:25.120]   So I hope platforms like Axe and others
[02:42:25.120 --> 02:42:27.440]   like really help support people
[02:42:27.440 --> 02:42:30.040]   trying to stay pseudonymous or anonymous
[02:42:30.040 --> 02:42:32.840]   because it's really important for people
[02:42:32.840 --> 02:42:36.080]   to share thoughts that aren't fully formed
[02:42:36.080 --> 02:42:38.360]   and converge onto maybe hidden truths
[02:42:38.360 --> 02:42:41.320]   that were hard to converge upon
[02:42:41.320 --> 02:42:46.320]   if it was just through open conversation with real names.
[02:42:46.320 --> 02:42:49.720]   - Yeah, I really believe in not radical
[02:42:49.720 --> 02:42:52.320]   but rigorous empathy.
[02:42:52.320 --> 02:42:54.400]   It's like really considering what it's like
[02:42:54.400 --> 02:42:57.480]   to be a person of a certain viewpoint
[02:42:57.480 --> 02:43:00.040]   and like taking that as a thought experiment
[02:43:00.040 --> 02:43:01.800]   farther and farther and farther.
[02:43:01.800 --> 02:43:04.000]   And one way of doing that is an alt account.
[02:43:04.000 --> 02:43:10.120]   That's a fun, interesting way to really explore
[02:43:10.120 --> 02:43:11.840]   what it's like to be a person that believes
[02:43:11.840 --> 02:43:13.600]   a set of beliefs.
[02:43:13.600 --> 02:43:17.560]   And taking that across the span of several days,
[02:43:17.560 --> 02:43:20.120]   weeks, months, of course,
[02:43:20.120 --> 02:43:22.880]   there's always the danger of becoming that.
[02:43:22.880 --> 02:43:26.680]   That's the Nietzsche gaze long into the abyss.
[02:43:26.680 --> 02:43:30.080]   The abyss gazes into you.
[02:43:30.080 --> 02:43:31.800]   You have to be careful.
[02:43:31.800 --> 02:43:33.280]   - Breaking Beth.
[02:43:33.280 --> 02:43:34.920]   - Yeah, right, breaking Beth.
[02:43:34.920 --> 02:43:37.360]   Yeah, you wake up with a shaved head one day.
[02:43:37.360 --> 02:43:39.240]   Just like, who am I?
[02:43:39.240 --> 02:43:40.440]   What have I become?
[02:43:40.440 --> 02:43:44.480]   So you've mentioned quite a bit of advice already,
[02:43:44.480 --> 02:43:46.760]   but what advice would you give to young people
[02:43:46.760 --> 02:43:52.280]   of how to, in this interesting world we're in,
[02:43:53.000 --> 02:43:56.000]   how to have a career and how to have a life
[02:43:56.000 --> 02:43:57.080]   they can be proud of?
[02:43:57.080 --> 02:44:01.960]   - I think to me, the reason I went to theoretical physics
[02:44:01.960 --> 02:44:05.400]   was that I had to learn the base of the stack
[02:44:05.400 --> 02:44:06.760]   that was gonna stick around
[02:44:06.760 --> 02:44:10.240]   no matter how the technology changes, right?
[02:44:10.240 --> 02:44:14.040]   And to me, that was the foundation upon which
[02:44:14.040 --> 02:44:18.320]   then I later built engineering skills and other skills.
[02:44:18.320 --> 02:44:20.040]   And to me, the laws of physics,
[02:44:20.040 --> 02:44:21.920]   it may seem like the landscape right now
[02:44:21.920 --> 02:44:24.560]   is changing so fast it's disorienting,
[02:44:24.560 --> 02:44:26.680]   but certain things like fundamental mathematics
[02:44:26.680 --> 02:44:28.800]   and physics aren't gonna change.
[02:44:28.800 --> 02:44:30.600]   And if you have that knowledge
[02:44:30.600 --> 02:44:35.040]   and knowledge about complex systems and adaptive systems,
[02:44:35.040 --> 02:44:37.640]   I think that's gonna carry you very far.
[02:44:37.640 --> 02:44:40.600]   And so not everybody has to study mathematics,
[02:44:40.600 --> 02:44:44.520]   but I think it's really a huge cognitive unlock
[02:44:44.520 --> 02:44:48.480]   to learn math and some physics and engineering.
[02:44:48.480 --> 02:44:51.400]   - Get as close to the base of the stack as possible.
[02:44:51.400 --> 02:44:53.640]   - Yeah, that's right, 'cause the base of the stack
[02:44:53.640 --> 02:44:55.560]   doesn't change, everything else,
[02:44:55.560 --> 02:44:58.160]   your knowledge might become not as relevant in a few years.
[02:44:58.160 --> 02:45:00.360]   Of course, there's a sort of transfer learning you can do,
[02:45:00.360 --> 02:45:04.440]   but then you have to always transfer learn constantly.
[02:45:04.440 --> 02:45:06.320]   - I guess the closer you are to the base of the stack,
[02:45:06.320 --> 02:45:10.360]   the easier the transfer learning, the shorter the jump.
[02:45:10.360 --> 02:45:12.120]   - Right, right.
[02:45:12.120 --> 02:45:15.880]   And you'd be surprised once you've learned concepts
[02:45:15.880 --> 02:45:18.480]   in many physical scenarios,
[02:45:18.480 --> 02:45:21.920]   how they can carry over to understanding other systems
[02:45:21.920 --> 02:45:23.280]   that aren't necessarily physics.
[02:45:23.280 --> 02:45:26.200]   And I guess like the IAC writings,
[02:45:26.200 --> 02:45:30.080]   the principles and tenet post that was based on physics,
[02:45:30.080 --> 02:45:31.720]   that was kind of my experimentation
[02:45:31.720 --> 02:45:34.920]   with applying some of the thinking
[02:45:34.920 --> 02:45:36.840]   from out of equilibrium thermodynamics
[02:45:36.840 --> 02:45:38.520]   to understanding the world around us.
[02:45:38.520 --> 02:45:42.640]   And it's led to IAC and this movement.
[02:45:42.640 --> 02:45:46.880]   - If you look at your one cog in the machine,
[02:45:46.880 --> 02:45:48.760]   in the capitalist machine, one human,
[02:45:48.760 --> 02:45:52.320]   and if you look at yourself,
[02:45:52.320 --> 02:45:55.400]   do you think mortality is a feature or a bug?
[02:45:55.400 --> 02:45:57.760]   Like would you want to be immortal?
[02:45:57.760 --> 02:45:58.960]   - No.
[02:45:58.960 --> 02:46:03.960]   I think fundamentally in thermodynamic
[02:46:03.960 --> 02:46:08.680]   dissipative adaptation, there's the word dissipation.
[02:46:08.680 --> 02:46:11.660]   Dissipation is important, death is important, right?
[02:46:11.660 --> 02:46:13.000]   We have a saying in physics,
[02:46:13.000 --> 02:46:16.000]   physics progresses one funeral at a time.
[02:46:16.000 --> 02:46:17.040]   - Yeah.
[02:46:17.040 --> 02:46:19.400]   - I think the same is true for capitalism,
[02:46:19.400 --> 02:46:23.920]   companies, empires, people, everything.
[02:46:23.920 --> 02:46:26.480]   Everything must die at some point.
[02:46:26.480 --> 02:46:29.880]   I think that we should probably extend our lifespan
[02:46:29.880 --> 02:46:34.240]   because we need a longer period of training
[02:46:34.240 --> 02:46:36.040]   'cause the world is more and more complex, right?
[02:46:36.040 --> 02:46:39.400]   We have more and more data to really be able
[02:46:39.400 --> 02:46:41.200]   to predict and understand the world.
[02:46:41.200 --> 02:46:45.840]   And if we have a finite window of higher neuroplasticity,
[02:46:45.840 --> 02:46:47.960]   then we have sort of a hard cap
[02:46:47.960 --> 02:46:50.320]   in how much we can understand about our world.
[02:46:50.320 --> 02:46:54.880]   So, I think I am for death because again,
[02:46:54.880 --> 02:46:57.640]   I think it's important if you have like a king
[02:46:57.640 --> 02:47:00.360]   that would never die, that would be a problem, right?
[02:47:00.360 --> 02:47:05.280]   Like the system wouldn't be constantly adapting, right?
[02:47:05.280 --> 02:47:08.920]   You need novelty, you need youth, you need disruption
[02:47:08.920 --> 02:47:13.880]   to make sure the system's always adapting and malleable.
[02:47:13.880 --> 02:47:17.240]   Otherwise, if things are immortal,
[02:47:17.240 --> 02:47:19.560]   if you have, let's say, corporations that are there forever
[02:47:19.560 --> 02:47:21.920]   and they have the monopoly, they get calcified,
[02:47:21.920 --> 02:47:25.200]   they become not as optimal, not as high fitness
[02:47:25.200 --> 02:47:28.560]   in a changing, time-varying landscape, right?
[02:47:28.560 --> 02:47:33.560]   And so, death gives space for youth and novelty
[02:47:33.560 --> 02:47:36.080]   to take its place.
[02:47:36.080 --> 02:47:37.640]   And I think it's an important part
[02:47:37.640 --> 02:47:40.760]   of every system in nature.
[02:47:40.760 --> 02:47:43.840]   So, yeah, I am for death.
[02:47:43.840 --> 02:47:47.200]   But I do think that longer lifespan
[02:47:47.200 --> 02:47:50.360]   and longer time for neuroplasticity, bigger brains,
[02:47:50.360 --> 02:47:52.880]   which should be something we should strive for.
[02:47:52.880 --> 02:47:57.840]   - Well, in that, Jeff Bezos and Bev Jezos agree
[02:47:57.840 --> 02:47:59.400]   that all companies die.
[02:47:59.400 --> 02:48:03.720]   And for Jeff, the goal is to try to,
[02:48:03.720 --> 02:48:05.840]   he calls it day one thinking,
[02:48:05.840 --> 02:48:10.280]   try to constantly, for as long as possible, reinvent.
[02:48:10.280 --> 02:48:12.640]   Sort of extend the life of the company,
[02:48:12.640 --> 02:48:14.760]   but eventually it too will die
[02:48:14.760 --> 02:48:17.400]   'cause it's so damn difficult to keep reinventing.
[02:48:17.400 --> 02:48:20.660]   Are you afraid of your own death?
[02:48:20.660 --> 02:48:28.640]   - I think I have ideas and things I'd like to achieve
[02:48:28.640 --> 02:48:32.000]   in this world before I have to go,
[02:48:32.000 --> 02:48:34.600]   but I don't think I'm necessarily afraid of death.
[02:48:34.600 --> 02:48:36.800]   - So, you're not attached to this particular body
[02:48:36.800 --> 02:48:38.240]   and mind that you got?
[02:48:38.240 --> 02:48:42.400]   - No, I think, I'm sure there's gonna be better
[02:48:42.400 --> 02:48:46.200]   versions of myself in the future, or--
[02:48:46.200 --> 02:48:47.120]   - Forks.
[02:48:47.120 --> 02:48:51.080]   - Forks, right, genetic forks, or other, right?
[02:48:51.080 --> 02:48:53.600]   I truly believe that.
[02:48:53.600 --> 02:48:58.600]   I think there's a sort of a evolutionary-like algorithm
[02:48:58.600 --> 02:49:03.960]   happening at every bit or nap in the world
[02:49:03.960 --> 02:49:08.080]   is sort of adapting through this process
[02:49:08.080 --> 02:49:10.160]   that we described in IAC.
[02:49:10.160 --> 02:49:13.280]   And I think maintaining this adaptation malleability
[02:49:13.280 --> 02:49:16.680]   is how we have constant optimization of the whole machine.
[02:49:16.680 --> 02:49:21.440]   And so, I don't think I'm particularly an optimum
[02:49:21.440 --> 02:49:23.000]   that needs to stick around forever.
[02:49:23.000 --> 02:49:25.760]   I think there's gonna be greater optima in many ways.
[02:49:25.760 --> 02:49:27.280]   - What do you think is the meaning of it all?
[02:49:27.280 --> 02:49:30.920]   What's the why of the machine, the IAC machine?
[02:49:30.920 --> 02:49:36.480]   - The why, well, the why is thermodynamics.
[02:49:36.480 --> 02:49:37.960]   It's why we're here.
[02:49:37.960 --> 02:49:42.520]   It's what has led to the formation of life
[02:49:42.520 --> 02:49:45.440]   and of civilization, of evolution of technologies
[02:49:45.440 --> 02:49:47.800]   and growth of civilization.
[02:49:47.800 --> 02:49:50.040]   But why do we have thermodynamics?
[02:49:50.040 --> 02:49:51.800]   Why do we have our particular universe?
[02:49:51.800 --> 02:49:54.240]   Why do we have these particular hyperparameters,
[02:49:54.240 --> 02:49:55.600]   the constants of nature?
[02:49:55.600 --> 02:49:59.480]   Well, then you get into the anthropic principle, right?
[02:49:59.480 --> 02:50:02.280]   In the landscape of potential universes, right?
[02:50:02.280 --> 02:50:04.840]   We're in the universe that allows for life.
[02:50:04.840 --> 02:50:09.840]   And then why is there potentially many universes?
[02:50:09.840 --> 02:50:11.640]   I don't know.
[02:50:11.640 --> 02:50:12.480]   I don't know that part.
[02:50:12.480 --> 02:50:16.560]   But could we potentially engineer new universes
[02:50:16.560 --> 02:50:21.000]   or create pocket universes and set the hyperparameters
[02:50:21.000 --> 02:50:22.520]   so there is some mutual information
[02:50:22.520 --> 02:50:25.280]   between our existence and that universe
[02:50:25.280 --> 02:50:27.400]   and we'd be somewhat its parents?
[02:50:27.400 --> 02:50:31.000]   I think that's really, I don't know, that'd be very poetic.
[02:50:31.000 --> 02:50:32.560]   It's purely conjecture.
[02:50:32.560 --> 02:50:36.680]   But again, this is why figuring out quantum gravity
[02:50:36.680 --> 02:50:39.560]   would allow us to understand if we can do that.
[02:50:39.560 --> 02:50:43.040]   - And above that, why does it all seem
[02:50:43.040 --> 02:50:45.000]   so beautiful and exciting?
[02:50:45.000 --> 02:50:48.440]   The quest to figuring out quantum gravity
[02:50:48.440 --> 02:50:51.040]   seems so exciting.
[02:50:51.040 --> 02:50:51.880]   Why?
[02:50:51.880 --> 02:50:52.700]   Why is that?
[02:50:52.700 --> 02:50:53.540]   Why are we drawn to that?
[02:50:53.540 --> 02:50:55.120]   Why are we pulled towards that?
[02:50:55.120 --> 02:50:59.160]   Just is that puzzle-solving creative force
[02:50:59.160 --> 02:51:01.400]   that underpins all of it, it seems like.
[02:51:01.400 --> 02:51:04.240]   - I think we seek, just like an LLM seeks
[02:51:04.240 --> 02:51:07.040]   to minimize cross-entropy between its internal model
[02:51:07.040 --> 02:51:07.880]   and the world.
[02:51:07.880 --> 02:51:11.120]   We seek to minimize the statistical divergence
[02:51:11.120 --> 02:51:14.280]   between our predictions in the world and the world itself.
[02:51:14.280 --> 02:51:18.840]   And having regimes of energy scales or physical scales
[02:51:18.840 --> 02:51:20.480]   in which we have no visibility,
[02:51:20.480 --> 02:51:22.640]   no ability to predict or perceive,
[02:51:22.640 --> 02:51:26.080]   that's kind of an insult to us.
[02:51:26.080 --> 02:51:31.080]   And we want to be able to understand the world better
[02:51:31.080 --> 02:51:35.900]   in order to best steer it or steer us through it.
[02:51:35.900 --> 02:51:39.880]   And in general, it's the capability that has evolved
[02:51:39.880 --> 02:51:42.140]   because the better you can predict the world,
[02:51:42.140 --> 02:51:46.240]   the better you can capture utility or free energy
[02:51:46.240 --> 02:51:48.940]   towards your own sustenance and growth.
[02:51:48.940 --> 02:51:52.080]   And I think quantum gravity, again,
[02:51:52.080 --> 02:51:54.580]   is kind of the final boss
[02:51:54.580 --> 02:51:56.280]   in terms of knowledge acquisition.
[02:51:56.280 --> 02:51:58.920]   Because once we've mastered that,
[02:51:58.920 --> 02:52:02.560]   then we can do a lot potentially.
[02:52:02.560 --> 02:52:04.440]   But between here and there,
[02:52:04.440 --> 02:52:07.240]   I think there's a lot to learn in the mesoscales.
[02:52:07.240 --> 02:52:10.540]   There's a lot of information to acquire about our world
[02:52:10.540 --> 02:52:13.720]   and a lot of engineering, perception, prediction,
[02:52:13.720 --> 02:52:18.280]   and control to be done to climb up the Kardashev scale.
[02:52:18.280 --> 02:52:22.360]   And to us, that's the great challenge of our times.
[02:52:22.360 --> 02:52:24.120]   - And when you're not sure where to go,
[02:52:24.120 --> 02:52:26.200]   let the meme pave the way.
[02:52:27.760 --> 02:52:32.280]   Guillaume, Beth, thank you for talking today.
[02:52:32.280 --> 02:52:33.660]   Thank you for the work you're doing.
[02:52:33.660 --> 02:52:35.240]   Thank you for the humor and the wisdom
[02:52:35.240 --> 02:52:37.080]   you put into the world.
[02:52:37.080 --> 02:52:37.920]   This was awesome.
[02:52:37.920 --> 02:52:39.440]   - Thank you so much for having me, Lex.
[02:52:39.440 --> 02:52:40.880]   It was a pleasure.
[02:52:40.880 --> 02:52:42.600]   - Thank you for listening to this conversation
[02:52:42.600 --> 02:52:43.960]   with Guillaume Verdun.
[02:52:43.960 --> 02:52:45.120]   To support this podcast,
[02:52:45.120 --> 02:52:48.120]   please check out our sponsors in the description.
[02:52:48.120 --> 02:52:50.160]   And now, let me leave you with some words
[02:52:50.160 --> 02:52:51.960]   from Albert Einstein.
[02:52:51.960 --> 02:52:54.940]   If at first, the idea is not absurd,
[02:52:54.940 --> 02:52:57.400]   then there is no hope for it.
[02:52:57.400 --> 02:52:58.560]   Thank you for listening.
[02:52:58.560 --> 02:53:00.800]   I hope to see you next time.
[02:53:00.800 --> 02:53:03.380]   (upbeat music)
[02:53:03.380 --> 02:53:05.960]   (upbeat music)
[02:53:05.960 --> 02:53:15.960]   [BLANK_AUDIO]

