
[00:00:00.000 --> 00:00:03.880]   Kids don't get scored.
[00:00:03.880 --> 00:00:06.200]   There's no reward, no feedback most of the time.
[00:00:06.200 --> 00:00:08.280]   They just play.
[00:00:08.280 --> 00:00:11.360]   And can our agents or our robots do the same thing?
[00:00:11.360 --> 00:00:13.460]   Just play around in the environment?
[00:00:13.460 --> 00:00:17.680]   And essentially that is the reinforcement learning equivalent of pre-training, right?
[00:00:17.680 --> 00:00:22.480]   And we know in computer vision, natural language processing, unsupervised pre-training is what
[00:00:22.480 --> 00:00:25.220]   powers all the latest and greatest models.
[00:00:25.220 --> 00:00:26.680]   So can we do the same thing in RL?
[00:00:26.680 --> 00:00:29.480]   Well, that means it has to be reward free.
[00:00:29.480 --> 00:00:33.860]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:33.860 --> 00:00:36.220]   And I'm your host, Lukas Biewald.
[00:00:36.220 --> 00:00:40.260]   I was really looking forward to talking to Peter Abbeel.
[00:00:40.260 --> 00:00:43.620]   He's one of the most well-known people in machine learning today and one of the most
[00:00:43.620 --> 00:00:45.480]   cited professors.
[00:00:45.480 --> 00:00:50.280]   He's the director of the Berkeley Robot Learning Lab and always has really interesting insights
[00:00:50.280 --> 00:00:53.560]   on the state of the art of reinforcement learning and robotics.
[00:00:53.560 --> 00:00:57.320]   Two things that are obviously some of my favorite topics.
[00:00:57.320 --> 00:00:59.120]   He's also a very successful entrepreneur.
[00:00:59.120 --> 00:01:05.040]   He started several companies, including Gradescope, which sold to Turnitin a few years ago, and
[00:01:05.040 --> 00:01:11.320]   Covariant, which is a company trying to build a universal AI for robotic manipulation.
[00:01:11.320 --> 00:01:12.320]   So much to talk about.
[00:01:12.320 --> 00:01:15.400]   This is a really fun conversation.
[00:01:15.400 --> 00:01:19.280]   I guess the place I was interested in starting, if you're up for it, is you've been doing
[00:01:19.280 --> 00:01:26.340]   robotics for a long time and it sort of feels like robotics is one of those fields that
[00:01:26.340 --> 00:01:32.520]   I think it felt like kind of harder or maybe it's unexpectedly hard, it seems to me, to
[00:01:32.520 --> 00:01:37.040]   get robots to kind of navigate our world.
[00:01:37.040 --> 00:01:43.520]   Has it been surprising to you, the challenge of kind of making robots manipulate the world?
[00:01:43.520 --> 00:01:47.840]   Or was it kind of obvious to you when you started on it that it was going to be a huge
[00:01:47.840 --> 00:01:48.840]   challenge?
[00:01:48.840 --> 00:01:54.360]   Yeah, I think you kind of go through phases as you work on problems, right?
[00:01:54.360 --> 00:01:56.660]   In the beginning, maybe it's like, okay, we can do this.
[00:01:56.660 --> 00:01:59.980]   And a few months later, you're like, wow, this could be harder than expected.
[00:01:59.980 --> 00:02:01.940]   And you kind of go back and forth for a while.
[00:02:01.940 --> 00:02:10.740]   But maybe the way I'm seeing it is that our world has so much variation in it.
[00:02:10.740 --> 00:02:13.580]   There's always something new.
[00:02:13.580 --> 00:02:18.300]   And whether it's for driving or for robotic manipulation, driving, it's new.
[00:02:18.300 --> 00:02:23.020]   Driving scenes, new things you encounter, especially in cities when you're driving.
[00:02:23.020 --> 00:02:28.100]   For manipulation, it's just new objects or objects in different configurations surrounded
[00:02:28.100 --> 00:02:29.940]   by other objects.
[00:02:29.940 --> 00:02:37.980]   And so I think it's really intriguing how essentially, in some sense, once you measure
[00:02:37.980 --> 00:02:42.580]   the amount of variability you need to be able to deal with, you essentially know how hard
[00:02:42.580 --> 00:02:44.760]   a problem is going to be.
[00:02:44.760 --> 00:02:51.900]   And so when you look at robotics where it's successful, you can think of car manufacturing.
[00:02:51.900 --> 00:02:54.060]   It's very successful.
[00:02:54.060 --> 00:02:58.460]   I don't think we could buy cars at current prices if we didn't have robots helping out
[00:02:58.460 --> 00:03:01.180]   and the same consistency and so forth.
[00:03:01.180 --> 00:03:05.380]   And you look at the assembly line and you walk in there and you're just stunned.
[00:03:05.380 --> 00:03:06.660]   You're like, wow, what's going on here?
[00:03:06.660 --> 00:03:08.260]   The robots are building the entire car.
[00:03:08.260 --> 00:03:14.220]   But then when you look carefully, you see like, oh, these robots are amazing, but they're
[00:03:14.220 --> 00:03:19.300]   doing a very precisely orchestrated motion over and over and over.
[00:03:19.300 --> 00:03:22.100]   And that's what they're so good at.
[00:03:22.100 --> 00:03:27.420]   But then when you want to take it beyond that, all of a sudden you need a lot of progress
[00:03:27.420 --> 00:03:32.100]   in AI to actually do it because the robot needs to see, understand what it's looking
[00:03:32.100 --> 00:03:37.100]   at, make sense in terms of what decision to make, react to things quickly, and all of
[00:03:37.100 --> 00:03:39.540]   a sudden it becomes so much harder.
[00:03:39.540 --> 00:03:45.120]   And so I think it's like going from robots that do repeated motion to robots that actually
[00:03:45.120 --> 00:03:47.740]   understand the world and have to react to it.
[00:03:47.740 --> 00:03:49.340]   It's a very large gap.
[00:03:49.340 --> 00:03:50.820]   That's pretty clear at this point.
[00:03:50.820 --> 00:04:00.780]   And so sitting here in August, 2021, how would you describe the state of the art in robotics?
[00:04:00.780 --> 00:04:03.820]   I feel like I hear such conflicting messages.
[00:04:03.820 --> 00:04:07.980]   People talk about how laundry folding is like this impossible challenge and yet you can
[00:04:07.980 --> 00:04:13.060]   see videos of robots folding laundry super well.
[00:04:13.060 --> 00:04:17.380]   I feel like OpenAI had the thing where they manipulated a Rubik's cube, but it was funny
[00:04:17.380 --> 00:04:20.780]   showing that to my wife who is kind of like, of all the different OpenAI things they've
[00:04:20.780 --> 00:04:24.420]   put out, in a way this is the least impressive.
[00:04:24.420 --> 00:04:26.820]   A child could do that too.
[00:04:26.820 --> 00:04:30.900]   It's interesting that she was kind of reflecting back to me, why are you so excited about watching
[00:04:30.900 --> 00:04:33.180]   a robot manipulate a Rubik's cube?
[00:04:33.180 --> 00:04:37.060]   So what's your sense on what's possible and not possible today?
[00:04:37.060 --> 00:04:38.060]   Yeah.
[00:04:38.060 --> 00:04:43.260]   I think the reason, I like what you're saying there, Lukas, because I think the reason it's
[00:04:43.260 --> 00:04:50.540]   so confusing of what's possible and not possible is because a lot of the challenges are in
[00:04:50.540 --> 00:04:53.820]   achieving consistency.
[00:04:53.820 --> 00:04:55.500]   And it's like that in many parts of life.
[00:04:55.500 --> 00:04:59.580]   I mean, imagine you like playing sports and you make one free throw, you might be like,
[00:04:59.580 --> 00:05:01.020]   okay, I can make a free throw.
[00:05:01.020 --> 00:05:04.100]   You make a video of that one free throw you made and that's the only video you ever play
[00:05:04.100 --> 00:05:05.100]   on repeat.
[00:05:05.100 --> 00:05:07.100]   Now it looks like you're making every free throw, right?
[00:05:07.100 --> 00:05:09.540]   But actually being consistent is what's hard.
[00:05:09.540 --> 00:05:17.820]   And same for robotics applications, consistently being successful is really, really hard.
[00:05:17.820 --> 00:05:24.060]   And so I think, I mean, part of it is of course how we all communicate our early successes,
[00:05:24.060 --> 00:05:28.220]   but once we have an early success, of course we're excited.
[00:05:28.220 --> 00:05:33.460]   First time a robot folds laundry, that example of course is from my lab and it's close to
[00:05:33.460 --> 00:05:37.140]   my heart because I love robots folding laundry.
[00:05:37.140 --> 00:05:40.100]   And you watch the video and it's very impressive.
[00:05:40.100 --> 00:05:45.660]   But then once you look at the details, it's like, okay, but it's always towels of a limited
[00:05:45.660 --> 00:05:48.180]   range in size.
[00:05:48.180 --> 00:05:53.980]   And it's in a very specific lab setup where there's a nice table for the robot to fold
[00:05:53.980 --> 00:05:57.060]   on and it starts with a pile not too far away.
[00:05:57.060 --> 00:06:02.740]   And you start realizing there's just a lot of specifics in place that make it not as
[00:06:02.740 --> 00:06:04.020]   general as it might seem.
[00:06:04.020 --> 00:06:07.100]   And I think that's where it's always tricky.
[00:06:07.100 --> 00:06:12.380]   In some sense, when you see a robot do something, it's impressive, like solving a Rubik's cube,
[00:06:12.380 --> 00:06:17.380]   folding a towel, maybe picking something from a bin, which is actually a really important
[00:06:17.380 --> 00:06:19.020]   problem.
[00:06:19.020 --> 00:06:27.660]   You see it and I think most people will generalize and will say, "Oh, hey, if a person can do
[00:06:27.660 --> 00:06:32.460]   that, that means they can do all these other things also."
[00:06:32.460 --> 00:06:34.460]   And that's actually not the case.
[00:06:34.460 --> 00:06:35.460]   And we kind of know that.
[00:06:35.460 --> 00:06:36.460]   That's the funny thing.
[00:06:36.460 --> 00:06:43.420]   We know that because nobody takes a self-driving car to our DMV driving test and is like, "Oh,
[00:06:43.420 --> 00:06:45.740]   wow, you solved the written perfectly.
[00:06:45.740 --> 00:06:46.740]   That's impressive.
[00:06:46.740 --> 00:06:49.500]   Oh, you did a two-minute drive around the block.
[00:06:49.500 --> 00:06:50.500]   That's impressive."
[00:06:50.500 --> 00:06:53.780]   Because we know it doesn't generalize the same way.
[00:06:53.780 --> 00:06:58.540]   And this notion of generalization that's in our head often when we see a robot do something
[00:06:58.540 --> 00:07:01.660]   makes us think, "Oh, wow, we're so far along."
[00:07:01.660 --> 00:07:05.700]   But then when you start putting into practice, you want to commercialize it, as of course
[00:07:05.700 --> 00:07:10.660]   we're doing at Covariant, you realize it's all about the number of nines for liability
[00:07:10.660 --> 00:07:12.220]   you achieve.
[00:07:12.220 --> 00:07:16.940]   And I think that's where robotics is a bit more unique than most other AI applications
[00:07:16.940 --> 00:07:22.660]   because for most AI applications, you don't need that many nines to be helpful.
[00:07:22.660 --> 00:07:28.780]   If you have a spam filter and it removes even just 30% of your spam, that's already a help.
[00:07:28.780 --> 00:07:31.740]   Of course you want it to be better, but it's already helpful.
[00:07:31.740 --> 00:07:36.140]   But if a robot only succeeds 30% of the time, that means 70% of the time it's probably making
[00:07:36.140 --> 00:07:42.580]   a mess that takes even more effort to clean up than it was helping out.
[00:07:42.580 --> 00:07:47.180]   And so it's one of those things where the number of nines of reliability you need with
[00:07:47.180 --> 00:07:53.260]   a robot is just very high before they become actually valuable.
[00:07:53.260 --> 00:07:57.860]   And I think that's where it's so hard to gauge that because you see this cool 30-second clip
[00:07:57.860 --> 00:08:00.540]   and you're like, "Wow, it seems like things are good."
[00:08:00.540 --> 00:08:06.700]   And then you realize, "Well, if you watch it for an hour uninterrupted, it's not consistently
[00:08:06.700 --> 00:08:07.700]   that good."
[00:08:07.700 --> 00:08:10.740]   And now all of a sudden it's not viable.
[00:08:10.740 --> 00:08:16.460]   And so to get those higher levels of accuracy, maybe let's talk about Covariant if you're
[00:08:16.460 --> 00:08:17.460]   up for it.
[00:08:17.460 --> 00:08:23.460]   I mean, obviously a very outwardly successful startup.
[00:08:23.460 --> 00:08:27.460]   Are there breakthroughs that need to happen to get those?
[00:08:27.460 --> 00:08:35.500]   Is it science to get those extra levels of accuracy or is it fixing lots of little details?
[00:08:35.500 --> 00:08:39.220]   Does it feel more like engineering?
[00:08:39.220 --> 00:08:40.500]   I think it's actually both.
[00:08:40.500 --> 00:08:43.220]   I think you can knock it away with just doing one of them.
[00:08:43.220 --> 00:08:46.780]   So if we look at Covariant, what are we doing?
[00:08:46.780 --> 00:08:51.180]   Essentially we're trying to build a brain for robots to be able to see what's around
[00:08:51.180 --> 00:08:54.780]   them and react to it and do interesting things in the world.
[00:08:54.780 --> 00:09:02.100]   And we concluded that the first most relevant space to go into is warehousing and logistics.
[00:09:02.100 --> 00:09:04.820]   And so it means a robot doing pick and place.
[00:09:04.820 --> 00:09:08.860]   You go online, you order something, something has to be retrieved, put into a box.
[00:09:08.860 --> 00:09:14.660]   Well, there's a lot of essentially robots on wheels for many, many years doing the long
[00:09:14.660 --> 00:09:19.820]   distance travel of all the goods in these warehouses, but there isn't really manipulation
[00:09:19.820 --> 00:09:24.300]   robots helping out with the specific, like pick one object, place it there, pack it and
[00:09:24.300 --> 00:09:25.300]   so forth.
[00:09:25.300 --> 00:09:28.020]   And so that's the first thing we're focused on.
[00:09:28.020 --> 00:09:33.980]   And when you try to chase the level of reliability that you need, and of course also speed of
[00:09:33.980 --> 00:09:39.500]   operation, if you're reliable at one item per day, I mean, that's also not very useful,
[00:09:39.500 --> 00:09:42.180]   but you want speed and reliability.
[00:09:42.180 --> 00:09:47.940]   What you realize is that, well, first of all, even though we're an AI company, you can't
[00:09:47.940 --> 00:09:49.580]   ignore the hardware.
[00:09:49.580 --> 00:09:54.620]   If your hardware cannot pick something up, it's just not going to happen.
[00:09:54.620 --> 00:10:00.560]   So a lot of things can be picked with suction cups and that's a lot of what we do.
[00:10:00.560 --> 00:10:05.900]   But then if it doesn't match that paradigm, let's say you have something that's kind of
[00:10:05.900 --> 00:10:10.500]   netting of some kind, well, it has to be rerouted somewhere else.
[00:10:10.500 --> 00:10:15.700]   You also have to recognize that this is not for our robot reroute to a place where it
[00:10:15.700 --> 00:10:19.560]   can be picked with something else than a suction cup.
[00:10:19.560 --> 00:10:25.240]   But then when you get to the difficulties, what you realize is that you encounter a lot
[00:10:25.240 --> 00:10:28.740]   of things that you encounter in every machine learning application.
[00:10:28.740 --> 00:10:34.860]   That is, you typically need more data than you initially anticipated to train on.
[00:10:34.860 --> 00:10:38.080]   And so you need a way to collect that data.
[00:10:38.080 --> 00:10:42.940]   In robotics, collecting data means either going out in the physical world or having
[00:10:42.940 --> 00:10:44.780]   really good simulation.
[00:10:44.780 --> 00:10:47.540]   And simulation is getting better and better and definitely helps.
[00:10:47.540 --> 00:10:51.780]   But again, you actually need real world data.
[00:10:51.780 --> 00:10:56.240]   And now I get this chicken and egg problem because if you want actual real world data
[00:10:56.240 --> 00:11:00.720]   outside of your lab or office environment, you actually need to have something that kind
[00:11:00.720 --> 00:11:01.720]   of works.
[00:11:01.720 --> 00:11:03.960]   Otherwise, who's going to let you collect the data?
[00:11:03.960 --> 00:11:09.660]   And so it's a big challenge to bootstrap in, which of course we've been doing over the
[00:11:09.660 --> 00:11:12.020]   last couple of years.
[00:11:12.020 --> 00:11:16.880]   And then once you're getting the data to come in, and sometimes a whole other kind of fun
[00:11:16.880 --> 00:11:21.760]   part, because a lot of people will say, "Oh, just bigger data, it'll just work."
[00:11:21.760 --> 00:11:24.760]   That's actually not true.
[00:11:24.760 --> 00:11:29.560]   Once you start chasing many nines of reliability, you'll realize that you actually also need
[00:11:29.560 --> 00:11:35.720]   to re-architect the neural nets very often to be able to absorb that data.
[00:11:35.720 --> 00:11:40.720]   Choose a new loss function, find a different way to annotate to provide more signal to
[00:11:40.720 --> 00:11:42.920]   actually get to the levels that you want to get to.
[00:11:42.920 --> 00:11:47.000]   So it's really a mix of a lot of these things, how you collect your data, how you annotate
[00:11:47.000 --> 00:11:51.240]   it, what are the right losses to maximize signal you get, and then what are the neural
[00:11:51.240 --> 00:11:55.920]   net architectures that can actually absorb that signal?
[00:11:55.920 --> 00:12:03.980]   And so if you had started Covariant five years ago or a decade ago, would it have worked
[00:12:03.980 --> 00:12:05.160]   as well as it does now?
[00:12:05.160 --> 00:12:09.600]   Or I guess what's changed in the state of the art that kind of allows this application
[00:12:09.600 --> 00:12:11.740]   to work?
[00:12:11.740 --> 00:12:19.440]   I think the big reason I believed it would be possible to do this now with the right
[00:12:19.440 --> 00:12:26.960]   effort, and we're also proving it possible, is essentially the combination of progress
[00:12:26.960 --> 00:12:34.120]   in computer vision and in imitation/reinforcement learning.
[00:12:34.120 --> 00:12:37.080]   So computer vision, naturally the robot can't see.
[00:12:37.080 --> 00:12:40.320]   It's not going to pick and place reliably objects in a warehouse.
[00:12:40.320 --> 00:12:42.460]   I mean, it can't see them.
[00:12:42.460 --> 00:12:44.220]   How are you going to pick them?
[00:12:44.220 --> 00:12:49.280]   Because these are unstructured environments in many, many ways.
[00:12:49.280 --> 00:12:51.140]   But the vision of course is not enough.
[00:12:51.140 --> 00:12:54.020]   You need to understand how you're going to interact with these objects.
[00:12:54.020 --> 00:12:58.220]   Is this one that you can pick or will it actually fling others out of the bin?
[00:12:58.220 --> 00:13:03.040]   Is this a reliable way to pick it or will you rip the thing apart by picking it over
[00:13:03.040 --> 00:13:04.040]   there and so forth?
[00:13:04.040 --> 00:13:09.700]   So there's a kind of imitation learning, reinforcement learning aspect to that that is also really
[00:13:09.700 --> 00:13:10.700]   important.
[00:13:10.700 --> 00:13:20.220]   And what we saw in 2015, '16, '17 was just a ton of progress on all fronts of vision,
[00:13:20.220 --> 00:13:24.700]   imitation, reinforcement learning that made us believe that with the right additional
[00:13:24.700 --> 00:13:29.260]   push that's really oriented towards bringing it into the real world, that this should work.
[00:13:29.260 --> 00:13:34.780]   And luckily, yes, we were right and it is working.
[00:13:34.780 --> 00:13:36.140]   That's fantastic.
[00:13:36.140 --> 00:13:37.860]   Can you talk a little bit about imitation learning?
[00:13:37.860 --> 00:13:41.140]   I think a lot of people talk about it, but obviously you've done a lot of work in that
[00:13:41.140 --> 00:13:42.140]   field.
[00:13:42.140 --> 00:13:43.140]   Yeah.
[00:13:43.140 --> 00:13:50.860]   So maybe the place I would start is the thing often people are most excited about is reinforcement
[00:13:50.860 --> 00:13:53.420]   learning because it's learning from scratch.
[00:13:53.420 --> 00:13:59.500]   Your robot or your agent is just trial and error learning and gets scored periodically
[00:13:59.500 --> 00:14:01.980]   based on how well it's doing and then gets better over time.
[00:14:01.980 --> 00:14:06.880]   And of course, famous results, learn to play Atari games out at DeepMind and then out of
[00:14:06.880 --> 00:14:11.740]   my lab at Berkeley, robots learning to run, to get up and things like that.
[00:14:11.740 --> 00:14:19.100]   Now the tricky thing with reinforcement learning, despite all its beauty and I think being probably
[00:14:19.100 --> 00:14:25.940]   the most beautiful discipline to work in, is it can be very slow because learning from
[00:14:25.940 --> 00:14:32.260]   scratch is just, well, it takes forever for most problems.
[00:14:32.260 --> 00:14:38.080]   And so in practice, I often think of imitation learning as a way to bootstrap reinforcement
[00:14:38.080 --> 00:14:42.440]   learning agents who can keep learning on their own, but by first getting some examples from
[00:14:42.440 --> 00:14:50.000]   humans of how something is done and imitating that, which actually at its core in many situations
[00:14:50.000 --> 00:14:51.760]   comes down to supervised learning.
[00:14:51.760 --> 00:14:58.480]   So a person might maybe joystick a robot and in doing so they are telling the robot every
[00:14:58.480 --> 00:15:03.520]   moment what action they should apply to each joint or to each motor.
[00:15:03.520 --> 00:15:08.160]   And then the robot will actually do supervised learning, train a neural network from what
[00:15:08.160 --> 00:15:11.760]   it's seeing to the action the person commanded.
[00:15:11.760 --> 00:15:17.480]   And by doing so, that supervised learning model will actually be pretty good at generating
[00:15:17.480 --> 00:15:21.840]   similar behaviors to what the person was doing.
[00:15:21.840 --> 00:15:28.500]   And the beauty is once you have that, it turns out you have a really good starting point.
[00:15:28.500 --> 00:15:31.840]   And a lot of the challenges in reinforcement learning are about having a good starting
[00:15:31.840 --> 00:15:32.840]   point.
[00:15:32.840 --> 00:15:36.360]   Reinforcement learning will be very effective at fine tuning and zoning in on the kind of
[00:15:36.360 --> 00:15:41.560]   final details of your solution, but the exploration problem is really, really hard and you kind
[00:15:41.560 --> 00:15:45.920]   of sidestep it by saying, "Hey, you must know what the solution roughly looks like.
[00:15:45.920 --> 00:15:47.580]   No need to explore everything.
[00:15:47.580 --> 00:15:50.120]   Let's just imitate humans."
[00:15:50.120 --> 00:15:56.160]   And so yeah, I think imitation learning is for practical purposes, almost guaranteed
[00:15:56.160 --> 00:16:00.160]   the way to go in many applications, at least as a starting point.
[00:16:00.160 --> 00:16:05.680]   Even though as you know, in my kind of academic research, there's a lot more focus on reinforcement
[00:16:05.680 --> 00:16:10.000]   learning because it's sometimes the harder, more open-ended problem.
[00:16:10.000 --> 00:16:15.480]   So for example, in a factory, you'd have somebody control the robot, kind of aim a suction,
[00:16:15.480 --> 00:16:20.360]   pick up a thing, and then the robot kind of starts there by trying to imitate the actions
[00:16:20.360 --> 00:16:22.200]   that the human's taking.
[00:16:22.200 --> 00:16:25.520]   And then you switch to reinforcement learning strategy once it's kind of close to something
[00:16:25.520 --> 00:16:26.520]   good.
[00:16:26.520 --> 00:16:27.760]   Is that a good summary?
[00:16:27.760 --> 00:16:29.360]   That is one way to picture it.
[00:16:29.360 --> 00:16:35.800]   Then in practice, often you take further shortcuts because imagine you are joysticking a robot
[00:16:35.800 --> 00:16:37.840]   to pick something up.
[00:16:37.840 --> 00:16:45.240]   Well, could you be even faster by maybe not even using the robot, but just maybe in your
[00:16:45.240 --> 00:16:49.040]   hand holding the suction cup that the robot would be using and going there.
[00:16:49.040 --> 00:16:53.960]   And if you can track that motion as a human, you can demonstrate so much faster.
[00:16:53.960 --> 00:16:59.620]   Or if you are very confident about how things are done, you might say at times, "Hey, maybe
[00:16:59.620 --> 00:17:02.760]   for some situations, all I need is see the images.
[00:17:02.760 --> 00:17:07.160]   And in the images, I can already annotate directly where I would be doing what."
[00:17:07.160 --> 00:17:12.880]   And so there's kind of a trade-off between very precisely demonstrating what the robot
[00:17:12.880 --> 00:17:17.440]   should be doing, which is very informative, but might be time consuming to get that informative
[00:17:17.440 --> 00:17:18.440]   signal.
[00:17:18.440 --> 00:17:23.960]   And so the amount of information you get per time spent might still be low, even though
[00:17:23.960 --> 00:17:29.840]   each single demonstration has a lot of information versus high throughput data collection where
[00:17:29.840 --> 00:17:34.680]   you annotate fast or maybe a bit more noisy, but there's so much more data that it makes
[00:17:34.680 --> 00:17:35.680]   up for it.
[00:17:35.680 --> 00:17:39.200]   And so, I mean, there's always a lot of choices to be made there and it's not a clear cut,
[00:17:39.200 --> 00:17:41.520]   easy decision for any problem.
[00:17:41.520 --> 00:17:45.960]   But I think it is easy to know that you should definitely consider the spectrum of speeding
[00:17:45.960 --> 00:17:51.200]   up your data collection and maybe having a bit more noise on it because the neural nets
[00:17:51.200 --> 00:17:54.040]   will be robust to that.
[00:17:54.040 --> 00:17:55.040]   So then what happens?
[00:17:55.040 --> 00:18:01.520]   Because it's like, I picture reinforcement learning to beat every human at go.
[00:18:01.520 --> 00:18:04.520]   And it seems like such a powerful strategy, especially though you've kind of laid it out.
[00:18:04.520 --> 00:18:06.280]   It makes so much sense.
[00:18:06.280 --> 00:18:09.940]   The robot kind of tries the thing, it gets feedback and then improves.
[00:18:09.940 --> 00:18:15.000]   So what does it run into when you're actually in a factory and you're actually trying to
[00:18:15.000 --> 00:18:16.000]   pick something up?
[00:18:16.000 --> 00:18:21.020]   What kinds of errors creep in or why hasn't the problem just totally solved right now
[00:18:21.020 --> 00:18:24.160]   by that strategy?
[00:18:24.160 --> 00:18:31.120]   So when I look at the deployments we do at Covariant and what makes them successful and
[00:18:31.120 --> 00:18:44.160]   also hard at times, what I see is that even though some things might look very similar
[00:18:44.160 --> 00:18:53.320]   to humans, like you might think for a person, let's say doing maybe pick in place of pharmaceutical
[00:18:53.320 --> 00:19:04.480]   goods versus groceries versus apparel versus electrical supplies versus shoe boxes versus
[00:19:04.480 --> 00:19:05.480]   anything else.
[00:19:05.480 --> 00:19:10.000]   It's all kind of the same for humans.
[00:19:10.000 --> 00:19:13.240]   One person can do one thing, they can also do the other thing.
[00:19:13.240 --> 00:19:15.360]   Same day, no problem.
[00:19:15.360 --> 00:19:23.280]   But when I look at our today's learning systems, when you switch from one industry to another,
[00:19:23.280 --> 00:19:29.000]   actually, you need a good amount of new data for that.
[00:19:29.000 --> 00:19:34.620]   And so what I see is that within industries that we've already collected a lot of data,
[00:19:34.620 --> 00:19:42.120]   my sense is that I'd be surprised if it wouldn't work on the next one.
[00:19:42.120 --> 00:19:45.040]   But when you go to a new industry, there's a whole new data collection.
[00:19:45.040 --> 00:19:50.400]   And of course, at some point we'll have checked off all industries and it'll be all taken
[00:19:50.400 --> 00:19:57.320]   care of, but there's a way humans generalize that is still a bit different from the way
[00:19:57.320 --> 00:20:02.840]   I think our today's most viable AI robotic systems generalize.
[00:20:02.840 --> 00:20:07.720]   And I do think it actually ties back to where a lot of the academic research is these days,
[00:20:07.720 --> 00:20:15.360]   which is kind of massive pre-training and then fine tuning on new tasks.
[00:20:15.360 --> 00:20:20.320]   But if you want to get to many nines of reliability, just massive pre-training on arbitrary data
[00:20:20.320 --> 00:20:26.280]   and then just some quick, small data regime fine tuning, it's not clear that that's working
[00:20:26.280 --> 00:20:27.280]   today.
[00:20:27.280 --> 00:20:30.120]   It'll work for like 90%.
[00:20:30.120 --> 00:20:37.040]   Sure, you can get to 90% quickly, but 99, 99.9, it's harder to get to that.
[00:20:37.040 --> 00:20:44.720]   And I would say that's really, in robotic automation, that's kind of vision AI powered,
[00:20:44.720 --> 00:20:51.780]   I would say getting to somewhere 99.5 to 99.9 tends to often be the sweet spot where things
[00:20:51.780 --> 00:20:56.640]   become viable, commercially viable, where the amount of supervision required from a
[00:20:56.640 --> 00:20:59.200]   person becomes essentially negligible.
[00:20:59.200 --> 00:21:04.240]   And it's really a robot doing the work rather than the supervisor actually being just as
[00:21:04.240 --> 00:21:07.400]   busy as they were before when there was no robot.
[00:21:07.400 --> 00:21:12.200]   Obviously, self-driving cars, you need even more nines and that makes it, I think, even
[00:21:12.200 --> 00:21:18.800]   harder, but get to 99.5, 99.9, that's where things become viable.
[00:21:18.800 --> 00:21:22.600]   And I think that's just the challenge, ultimately.
[00:21:22.600 --> 00:21:25.300]   We call that a challenge of autonomy.
[00:21:25.300 --> 00:21:33.080]   It doesn't mean 100% always being exactly right, but getting that 99.5, 99.9 spot in
[00:21:33.080 --> 00:21:37.240]   the industry that the robot is operating in is really key.
[00:21:37.240 --> 00:21:42.360]   And how much does simulation or simulated data or physical simulation matter here?
[00:21:42.360 --> 00:21:45.680]   I know a lot of people have been talking about it, but it's a little unclear to me if this
[00:21:45.680 --> 00:21:52.040]   is kind of a theoretical thing or something that's really best practice in working in
[00:21:52.040 --> 00:21:53.040]   the real world right now.
[00:21:53.040 --> 00:21:55.000]   How do you think about that?
[00:21:55.000 --> 00:21:56.000]   Yeah.
[00:21:56.000 --> 00:21:59.960]   I mean, I think you probably remember from when you were spending some of your time at
[00:21:59.960 --> 00:22:07.160]   OpenAI and Josh Tobin and I and you and Wojcik, and we're working on this domain randomization
[00:22:07.160 --> 00:22:12.920]   approaches back in 2016, 17, right?
[00:22:12.920 --> 00:22:15.760]   And I would say a couple of lessons learned from that.
[00:22:15.760 --> 00:22:25.480]   One is that simulation can be surprisingly helpful even when it's not super realistic.
[00:22:25.480 --> 00:22:32.520]   Because that was kind of the domain randomization whole kind of spiel and result was, wow, even
[00:22:32.520 --> 00:22:37.440]   when none of the renderings look realistic, training in these simulations does help a
[00:22:37.440 --> 00:22:40.760]   lot in the real world.
[00:22:40.760 --> 00:22:44.560]   And I think that's still a very powerful thing to rely upon.
[00:22:44.560 --> 00:22:49.000]   And of course, some kind of counterpart of that is doing large data augmentation on real
[00:22:49.000 --> 00:22:51.280]   data.
[00:22:51.280 --> 00:22:54.300]   But I think ultimately you do need real data.
[00:22:54.300 --> 00:22:59.800]   You can't just get away with today's simulated data to get to high reliability for real world
[00:22:59.800 --> 00:23:00.800]   operation.
[00:23:00.800 --> 00:23:05.280]   But again, that's part of what's so interesting, exciting about robotics is that you need these
[00:23:05.280 --> 00:23:09.680]   high reliabilities to be valuable.
[00:23:09.680 --> 00:23:13.920]   And this is going to be very different than other application domains that are pure software
[00:23:13.920 --> 00:23:19.000]   where often there's a lot more room for error and already providing value.
[00:23:19.000 --> 00:23:23.600]   But what about something like laundry folding?
[00:23:23.600 --> 00:23:26.840]   I feel like a robot that could fold 90% of my clothes.
[00:23:26.840 --> 00:23:27.840]   I'd be pretty happy with that.
[00:23:27.840 --> 00:23:31.200]   I would buy that for sure.
[00:23:31.200 --> 00:23:35.480]   Yeah, I think that's a very good point.
[00:23:35.480 --> 00:23:40.640]   So maybe we should revisit laundry folding sometime and knock at your door and see if
[00:23:40.640 --> 00:23:43.960]   you'll take one from us.
[00:23:43.960 --> 00:23:47.280]   There are some subtleties though that maybe...
[00:23:47.280 --> 00:23:54.200]   So I think the big subtlety here is the following and why things like robotics and self-driving
[00:23:54.200 --> 00:23:59.320]   have this high reliability requirement is because it's not just that it's high reliability
[00:23:59.320 --> 00:24:04.800]   requirement is that when something goes wrong, often it causes a lot of work or a lot of
[00:24:04.800 --> 00:24:06.800]   damage.
[00:24:06.800 --> 00:24:14.640]   And it's not that when you have 99.9%, let's say, typically it's not the case that that
[00:24:14.640 --> 00:24:16.960]   means 99.9% are going great.
[00:24:16.960 --> 00:24:22.560]   And then one in a thousand is just like, no, the robot knows this one's too hard.
[00:24:22.560 --> 00:24:23.960]   Come and help me out.
[00:24:23.960 --> 00:24:27.320]   It's more likely that the one in a thousand that goes wrong is something that the robot
[00:24:27.320 --> 00:24:28.320]   actually makes a mistake.
[00:24:28.320 --> 00:24:30.680]   And maybe the wrong thing gets shipped off to somebody.
[00:24:30.680 --> 00:24:33.120]   They might say, okay, we can bound the cost on that.
[00:24:33.120 --> 00:24:37.040]   Somebody gets the wrong item or maybe somebody in a later station that's more typical would
[00:24:37.040 --> 00:24:38.960]   sanity check it and would fix it.
[00:24:38.960 --> 00:24:44.480]   And so it'd be some fixing work, but fixing work often is more work than doing the original
[00:24:44.480 --> 00:24:45.480]   work.
[00:24:45.480 --> 00:24:50.520]   And so you cause need for fixing work.
[00:24:50.520 --> 00:24:53.080]   And I think that's why it's so interesting.
[00:24:53.080 --> 00:25:00.080]   And when you talk about laundry, yeah, if the kind of threshold for you is if it falls
[00:25:00.080 --> 00:25:04.160]   in at least a few things and the rest can still be a mask the way it comes out of the
[00:25:04.160 --> 00:25:08.960]   dryer, then I think it's a great case.
[00:25:08.960 --> 00:25:14.840]   But if you wanted everything folded and just one item just sitting there on the side, not
[00:25:14.840 --> 00:25:16.880]   folded, I don't think that's how it's going to be.
[00:25:16.880 --> 00:25:20.080]   It's going to be like, it's going to maybe have folded almost everything.
[00:25:20.080 --> 00:25:24.440]   And at the end, it makes a mistake and everything's back on the floor and it can't reach the floor.
[00:25:24.440 --> 00:25:26.640]   And you have to come in and put it back on the table.
[00:25:26.640 --> 00:25:32.920]   I mean, I'm saying it a bit more cynical than it would actually be, but it's just that the
[00:25:32.920 --> 00:25:36.080]   reliability requirements are there for a reason typically.
[00:25:36.080 --> 00:25:40.680]   And that is because when it doesn't succeed, there's a lot of work related to fixing.
[00:25:40.680 --> 00:25:41.680]   Right.
[00:25:41.680 --> 00:25:42.680]   Right.
[00:25:42.680 --> 00:25:49.040]   And it does seem like collecting training data that really represents the real world
[00:25:49.040 --> 00:25:52.720]   might be harder than something like ImageNet.
[00:25:52.720 --> 00:25:59.160]   It does seem like there's not quite the same sets of useful data sets that everyone can
[00:25:59.160 --> 00:26:00.160]   can play with.
[00:26:00.160 --> 00:26:01.160]   Am I wrong?
[00:26:01.160 --> 00:26:05.800]   Am I not aware of the stuff going on in the space?
[00:26:05.800 --> 00:26:13.480]   I think it's not so clear how to collect data that's actually in distribution for these
[00:26:13.480 --> 00:26:19.200]   problems unless you are actively in the space helping solve these problems.
[00:26:19.200 --> 00:26:21.080]   I see.
[00:26:21.080 --> 00:26:26.360]   And so we all know that today's machine learning works way better in distribution than out
[00:26:26.360 --> 00:26:27.920]   of distribution.
[00:26:27.920 --> 00:26:36.080]   And so it is kind of, I mean, I'd say there is a reason that we're building this as a
[00:26:36.080 --> 00:26:38.180]   commercial solution.
[00:26:38.180 --> 00:26:42.760]   One way to think of it is, and that's part of it, is it's awesome to put something in
[00:26:42.760 --> 00:26:44.800]   the world that actually works and helps out.
[00:26:44.800 --> 00:26:49.160]   And so that's part of the goal and why we are excited.
[00:26:49.160 --> 00:26:56.280]   But part of it is exactly what you're getting to, which is my belief is that if we want
[00:26:56.280 --> 00:27:02.280]   to solve robotics in the foreseeable future, meaning not by first building an AGI and then
[00:27:02.280 --> 00:27:06.480]   let the AGI take care of it, but actually a more kind of direct approach to solving
[00:27:06.480 --> 00:27:13.520]   robotics problems, I think the only way to do it is to get the right kind of data by
[00:27:13.520 --> 00:27:21.360]   going into the real world problems with robots and collecting the exact data that's needed,
[00:27:21.360 --> 00:27:26.760]   and then training on that data, and then letting the robot run, improve based on what you see
[00:27:26.760 --> 00:27:29.000]   happen and iterate.
[00:27:29.000 --> 00:27:34.640]   And I think without that iteration process, I don't think you can do it.
[00:27:34.640 --> 00:27:35.880]   Maybe somebody can figure out how to do it.
[00:27:35.880 --> 00:27:36.880]   I don't know.
[00:27:36.880 --> 00:27:40.040]   I mean, I don't want to exclude somebody doing something really amazing, surprising somewhere,
[00:27:40.040 --> 00:27:42.040]   but my money is on that.
[00:27:42.040 --> 00:27:47.440]   The way these kind of near-term AI robotics problems will be solved is by being very focused
[00:27:47.440 --> 00:27:53.800]   on real world deployment, data collection, and on that loop.
[00:27:53.800 --> 00:27:59.240]   Do you imagine a world where this starts to work and we suddenly have robots doing lots
[00:27:59.240 --> 00:28:00.720]   and lots of tasks around us?
[00:28:00.720 --> 00:28:05.320]   I feel like voice recognition has kind of snuck up on me where, when I was a kid, it
[00:28:05.320 --> 00:28:06.880]   was incredibly annoying.
[00:28:06.880 --> 00:28:11.000]   And now it seems like most people use it for various tasks on their phone or Alexa.
[00:28:11.000 --> 00:28:17.080]   Do you sort of picture the same trajectory for robotics?
[00:28:17.080 --> 00:28:25.920]   What I see happen is kind of a, maybe sneak up is maybe one way to think of it, but what
[00:28:25.920 --> 00:28:32.200]   I see happen is a gradual increased capability in terms of where robots are viable.
[00:28:32.200 --> 00:28:42.400]   So until recently, it was only repeated motion type settings, carefully pre-programmed motions.
[00:28:42.400 --> 00:28:51.340]   Now what we're seeing, I would say roughly this year is a transition into feasibility
[00:28:51.340 --> 00:28:58.840]   for robots doing interesting things in warehouses, pick and place type tasks.
[00:28:58.840 --> 00:29:06.320]   And I'd say that's really the first place where robots are really looking at things,
[00:29:06.320 --> 00:29:13.120]   reacting to it, interacting with the objects, and then achieving something.
[00:29:13.120 --> 00:29:18.440]   And in that sense, it is interesting because it's a first.
[00:29:18.440 --> 00:29:25.560]   It's where it's happening first and there is no reason it couldn't expand from there.
[00:29:25.560 --> 00:29:30.560]   I mean, I can see all the work involved in terms of iterating over non-network architectures,
[00:29:30.560 --> 00:29:34.440]   data collection, loss functions, all the things we do to get to the reliability we need to
[00:29:34.440 --> 00:29:35.920]   get to.
[00:29:35.920 --> 00:29:45.320]   But I also see that that same process in principle should apply just as well to other domains.
[00:29:45.320 --> 00:29:51.760]   I think maybe agriculture, maybe some construction problems, maybe some more difficult manufacturing
[00:29:51.760 --> 00:29:55.200]   problems where it's not just repeated motion that can do it.
[00:29:55.200 --> 00:29:59.560]   And I think any of those kind of semi-structured environments, I would say where maybe it's
[00:29:59.560 --> 00:30:03.560]   not directly interacting with people, because I think that's always much, much harder because
[00:30:03.560 --> 00:30:07.280]   people are very unpredictable, but that kind of semi-structured environment, the robot
[00:30:07.280 --> 00:30:10.080]   can kind of do its thing.
[00:30:10.080 --> 00:30:15.520]   I think it could grow relatively quickly in the foreseeable future.
[00:30:15.520 --> 00:30:17.920]   And yeah, we'll see.
[00:30:17.920 --> 00:30:21.840]   I mean, it seems to me like the funny thing about, or the counterintuitive thing about
[00:30:21.840 --> 00:30:28.120]   software unlike hardware, right, is once it's working, copying it is free, right?
[00:30:28.120 --> 00:30:33.280]   So it makes sense that you would start in this really high value task in warehouses,
[00:30:33.280 --> 00:30:38.960]   but then if you could really pick things up, maybe I could let the robot loose in my house
[00:30:38.960 --> 00:30:42.120]   and clean some of this clutter out of here.
[00:30:42.120 --> 00:30:43.120]   It's very true.
[00:30:43.120 --> 00:30:49.000]   I mean, I could imagine essentially a Roomba with an arm on top of it.
[00:30:49.000 --> 00:30:50.000]   Seems useful, right?
[00:30:50.000 --> 00:30:51.000]   I don't know.
[00:30:51.000 --> 00:30:55.960]   But yeah, if anything's on the floor, it picks it up and puts it into a basket somewhere,
[00:30:55.960 --> 00:31:00.560]   or maybe even knows where to go deliver it in the house.
[00:31:00.560 --> 00:31:07.560]   That kind of pick and place should actually be easier than the warehouse situation, because
[00:31:07.560 --> 00:31:09.800]   you don't have a clutter of objects.
[00:31:09.800 --> 00:31:14.840]   Typically it's just maybe an isolated object on the floor here and the floor there.
[00:31:14.840 --> 00:31:19.420]   I think a big part of it is also when you think about the economics, it comes down to
[00:31:19.420 --> 00:31:21.960]   what you said, software is very cheap to copy.
[00:31:21.960 --> 00:31:28.180]   So the question is, when you go to hardware, if this robot picks things up from the floor
[00:31:28.180 --> 00:31:35.400]   and maybe it costs you, I don't know, a certain amount of money, well, unless it's doing that
[00:31:35.400 --> 00:31:39.940]   multiple times a day, you might not feel it's worth it, right?
[00:31:39.940 --> 00:31:46.360]   And that's also part of why I see robots arrive first in these semi-structured environments
[00:31:46.360 --> 00:31:52.200]   where warehouse, there's like all the things need to flow through and there's always a
[00:31:52.200 --> 00:31:53.640]   next thing to be picked.
[00:31:53.640 --> 00:31:56.240]   It's never quiet.
[00:31:56.240 --> 00:32:01.320]   And so once you have the physical robot, well, we know robots don't tire, they just keep
[00:32:01.320 --> 00:32:02.320]   running.
[00:32:02.320 --> 00:32:04.600]   And so you get to leverage that aspect.
[00:32:04.600 --> 00:32:10.520]   And to me, that's a part of where I always want to think about household robots is where
[00:32:10.520 --> 00:32:16.840]   the equation is harder than in, let's say a warehouse or maybe on a farm and so forth.
[00:32:16.840 --> 00:32:19.760]   But I mean, who knows?
[00:32:19.760 --> 00:32:22.840]   I mean, prices of robots will go down, right?
[00:32:22.840 --> 00:32:27.080]   So at some point that will be less of an issue.
[00:32:27.080 --> 00:32:31.820]   It also seems like a robot, I mean, how much does a robot cost with like a suction arm?
[00:32:31.820 --> 00:32:38.120]   It doesn't seem like it intrinsically needs to cost thousands of dollars, does it?
[00:32:38.120 --> 00:32:39.120]   That's a really good question.
[00:32:39.120 --> 00:32:46.160]   I mean, it might depend a lot on what your performance requirements are for these robots.
[00:32:46.160 --> 00:32:50.160]   So if you look at these car manufacturing robots, some of them are very expensive.
[00:32:50.160 --> 00:32:55.040]   They can easily cost a hundred thousand dollars, but also they can pick up an entire car.
[00:32:55.040 --> 00:32:59.800]   So that's a very strong robot and can do it for 10 years in a row, every minute pick up
[00:32:59.800 --> 00:33:01.060]   a new car.
[00:33:01.060 --> 00:33:04.340]   So you get that kind of reliability and strength.
[00:33:04.340 --> 00:33:08.720]   It's not going to be super cheap, but then yeah, if you need to pick up just a toy from
[00:33:08.720 --> 00:33:13.160]   the floor, that doesn't put a whole lot of strain on the joints or the motors of the
[00:33:13.160 --> 00:33:14.160]   robot.
[00:33:14.160 --> 00:33:18.640]   So it seems, and if it's cheaper, you might be happy to replace it every year.
[00:33:18.640 --> 00:33:19.640]   Why not?
[00:33:19.640 --> 00:33:24.960]   And so I think there's kind of a spectrum of robots to be built and today's robots that
[00:33:24.960 --> 00:33:29.640]   are mostly out there are on the end of the spectrum where it's like, it should work for
[00:33:29.640 --> 00:33:32.280]   10, 20 years and minimal maintenance.
[00:33:32.280 --> 00:33:36.760]   It should just work because it's part of an assembly line that should never come to a
[00:33:36.760 --> 00:33:40.120]   halt because that'll cost so much if it comes to a halt.
[00:33:40.120 --> 00:33:41.920]   But you're absolutely right.
[00:33:41.920 --> 00:33:46.120]   Once you go into homes, the design space that you're working in is very different.
[00:33:46.120 --> 00:33:49.240]   If it doesn't work for a day, it might not be a problem.
[00:33:49.240 --> 00:33:52.480]   Doesn't work in a car manufacturing line for a day, that might cost millions.
[00:33:52.480 --> 00:33:59.200]   Well, it seems also like, at least from my very limited work that I did with you at OpenAI,
[00:33:59.200 --> 00:34:04.640]   that there was this real sense that the robots that they were using had incredibly precise
[00:34:04.640 --> 00:34:05.640]   calibration.
[00:34:05.640 --> 00:34:09.600]   It could go to exactly an XYZ coordinate.
[00:34:09.600 --> 00:34:13.560]   And it sort of felt like with machine learning and actually if you can look at where you
[00:34:13.560 --> 00:34:19.440]   went, you could have a little less precision in the hardware and maybe actually even learn
[00:34:19.440 --> 00:34:23.360]   how to manipulate yourself and deal with maybe less perfect motors.
[00:34:23.360 --> 00:34:28.880]   I was kind of surprised that there wasn't more work in that area.
[00:34:28.880 --> 00:34:29.880]   You're absolutely right.
[00:34:29.880 --> 00:34:38.000]   That once you have a vision feedback loop, you can actually put a lot less strain on
[00:34:38.000 --> 00:34:40.360]   terms of repeatable motion.
[00:34:40.360 --> 00:34:48.280]   Some of these robots have sub-millimeter precision, some of them even micrometer precision, repeated
[00:34:48.280 --> 00:34:49.280]   motion.
[00:34:49.280 --> 00:34:51.240]   And when you can see, I mean, humans don't have that.
[00:34:51.240 --> 00:34:56.320]   You can't repeatedly reach the same point unless you have feedback.
[00:34:56.320 --> 00:34:59.120]   You feel you're making contact with something.
[00:34:59.120 --> 00:35:04.400]   And based on that, you adjust and you get the exact right spot that you want.
[00:35:04.400 --> 00:35:06.080]   And so, yeah, it's a very good point.
[00:35:06.080 --> 00:35:11.680]   And I think it's a kind of part of the design space that hasn't been explored that much.
[00:35:11.680 --> 00:35:15.160]   At Berkeley, actually for a little while, we were working on this project called the
[00:35:15.160 --> 00:35:16.160]   Blue Robot.
[00:35:16.160 --> 00:35:23.200]   What we were doing exactly that, the thinking was, if we have better intelligence of the
[00:35:23.200 --> 00:35:28.440]   robot, we don't need the same kind of blind precision.
[00:35:28.440 --> 00:35:32.240]   And we're actually able to bring price down for an arm quite a bit.
[00:35:32.240 --> 00:35:39.040]   I think it was maybe in parts even bought at small scale, maybe $2,000 or even a little
[00:35:39.040 --> 00:35:46.280]   less for a seven degree of freedom arm with a gripper, parallel jaw gripper.
[00:35:46.280 --> 00:35:51.060]   And could imagine if you buy these parts at scale, you can maybe cut the price in half
[00:35:51.060 --> 00:35:54.440]   and now you're down to $1,000 for an arm.
[00:35:54.440 --> 00:35:57.240]   Would it work for 10 years straight?
[00:35:57.240 --> 00:35:59.720]   Like some of the industrial robots?
[00:35:59.720 --> 00:36:01.720]   Probably not.
[00:36:01.720 --> 00:36:05.400]   Does it move as fast with blind precision?
[00:36:05.400 --> 00:36:06.400]   Definitely not.
[00:36:06.400 --> 00:36:12.000]   But do we need that for a practical home application?
[00:36:12.000 --> 00:36:13.000]   Probably not either.
[00:36:13.000 --> 00:36:16.160]   So yeah, I think it's a very interesting direction.
[00:36:16.160 --> 00:36:19.960]   We've kind of paused that project for a little bit, but I think there's a lot of opportunity
[00:36:19.960 --> 00:36:22.600]   to keep pushing that direction.
[00:36:22.600 --> 00:36:23.600]   Awesome.
[00:36:23.960 --> 00:36:28.520]   Well, I want to also sort of use some of this time for the questions that we got when we
[00:36:28.520 --> 00:36:31.040]   crowdsourced the question collection.
[00:36:31.040 --> 00:36:34.920]   And I would say, we were asking folks in our community, what should we ask?
[00:36:34.920 --> 00:36:39.400]   And I was kind of surprised because to me, you're a researcher, but I realized to a lot
[00:36:39.400 --> 00:36:42.000]   of people, you're more of an entrepreneur than a researcher.
[00:36:42.000 --> 00:36:46.360]   And a lot of the questions are around how you think about starting companies.
[00:36:46.360 --> 00:36:50.880]   So I wonder if you could just sort of say, you've started all these different companies.
[00:36:50.880 --> 00:36:54.160]   If you could sort of say what your process is of thinking of what to start and how you
[00:36:54.160 --> 00:36:55.560]   get something off the ground.
[00:36:55.560 --> 00:36:57.920]   I think a lot of people would love to hear that.
[00:36:57.920 --> 00:36:58.920]   Sure.
[00:36:58.920 --> 00:36:59.920]   Yeah.
[00:36:59.920 --> 00:37:06.920]   So maybe I'll start with some concrete examples because I think trying to be general is hard
[00:37:06.920 --> 00:37:08.000]   in this space.
[00:37:08.000 --> 00:37:11.280]   So how did we start Gradescope?
[00:37:11.280 --> 00:37:17.680]   So Gradescope is a company that right now provides AI to help any kind of instructors
[00:37:17.680 --> 00:37:21.800]   teaching assistants with grading of their student work, whether it's exams or homework
[00:37:21.800 --> 00:37:24.560]   or projects and so forth.
[00:37:24.560 --> 00:37:29.200]   And the way it started is essentially out of a need.
[00:37:29.200 --> 00:37:32.240]   And a lot of entrepreneurs will say that why they started their company, but it was our
[00:37:32.240 --> 00:37:37.040]   personal need as a professor at Berkeley and my teaching assistant at the time, Arjun Singh,
[00:37:37.040 --> 00:37:40.800]   other teaching assistants, Sergey Karayev.
[00:37:40.800 --> 00:37:45.800]   We're talking, we're looking at what we're doing with the grading work.
[00:37:45.800 --> 00:37:51.240]   And we feel like we have these stacks of paper and we're kind of passing these along and
[00:37:51.240 --> 00:37:56.480]   we all need to come together in the same room to be able to grade, or we need to pass this
[00:37:56.480 --> 00:37:59.840]   along and it'll be like a long delay because one person grades one day, the other person
[00:37:59.840 --> 00:38:00.840]   next day.
[00:38:00.840 --> 00:38:06.940]   And so we're like, well, if we just were to scan everything, we could just grade this
[00:38:06.940 --> 00:38:08.880]   on our laptops.
[00:38:08.880 --> 00:38:15.520]   And for me, one of the quirky things I was excited about, some TAs are kind of clever,
[00:38:15.520 --> 00:38:19.520]   not Arjun and Sergey, I mean, they're very smart, but I mean, are kind of clever in a
[00:38:19.520 --> 00:38:21.080]   selfish way, I would say.
[00:38:21.080 --> 00:38:26.280]   And they will book a flight that leaves the university right after their last final exam.
[00:38:26.280 --> 00:38:28.280]   And they're like, oh, sorry, I can't help with grading.
[00:38:28.280 --> 00:38:29.720]   I'll already be at home.
[00:38:29.720 --> 00:38:32.680]   I can't access the stack of exams.
[00:38:32.680 --> 00:38:35.880]   And so I'm like, well, this will be great because even the people who booked their flights
[00:38:35.880 --> 00:38:37.960]   early will be able to help with grading.
[00:38:37.960 --> 00:38:41.720]   And so the initial thinking was just, can we just make the, in some sense, the user
[00:38:41.720 --> 00:38:43.280]   interface of grading better?
[00:38:43.280 --> 00:38:46.360]   It wasn't actually starting from let's automate this.
[00:38:46.360 --> 00:38:50.340]   It was, if we scan, people can grade from wherever they are.
[00:38:50.340 --> 00:38:54.320]   And on the first exam thereafter, one of our TAs was grading in flight.
[00:38:54.320 --> 00:38:55.800]   Just like, and he was super excited.
[00:38:55.800 --> 00:38:57.100]   He's like, hey, I'm grading in flight.
[00:38:57.100 --> 00:38:59.760]   This is so cool.
[00:38:59.760 --> 00:39:04.480]   And so we found right away that it was really helpful to do everything digital rather than
[00:39:04.480 --> 00:39:06.680]   physical.
[00:39:06.680 --> 00:39:10.760]   And then at the time, it was just a project we used in our own class.
[00:39:10.760 --> 00:39:16.680]   And because it went so well, all the TAs were so happy, we passed it on to a bunch of my
[00:39:16.680 --> 00:39:20.120]   friends, professors at Berkeley and said, hey, do you want to try it for your class?
[00:39:20.120 --> 00:39:21.360]   Because the TAs really liked it.
[00:39:21.360 --> 00:39:27.320]   And so I think the next exam round, there was maybe like five to 10 professors using
[00:39:27.320 --> 00:39:30.720]   it in their class and they were really happy, but also they gave us a lot of feedback about
[00:39:30.720 --> 00:39:33.560]   things they were not perfectly happy with.
[00:39:33.560 --> 00:39:36.840]   And of course we go out and fix things.
[00:39:36.840 --> 00:39:40.500]   And that was kind of the early days where we're just kind of within Berkeley, just letting
[00:39:40.500 --> 00:39:42.960]   people use it and see what they think.
[00:39:42.960 --> 00:39:47.800]   And we hadn't planned to make a company per se, but it was definitely in our minds.
[00:39:47.800 --> 00:39:52.280]   Like if this goes well, if people really like it or we can see a path to make people like
[00:39:52.280 --> 00:39:54.680]   it, we should make it into a company probably.
[00:39:54.680 --> 00:39:59.320]   But for now, let's just see if we can even build a product people want to use to start
[00:39:59.320 --> 00:40:00.320]   off ourselves.
[00:40:00.320 --> 00:40:05.660]   But then we're probably representative of many others in the instructional space.
[00:40:05.660 --> 00:40:09.440]   And then some interesting things started to happen that got us even more motivated because
[00:40:09.440 --> 00:40:12.680]   you start, as I'm sure you've experienced yourself, you build something, you build it
[00:40:12.680 --> 00:40:13.680]   for yourself.
[00:40:13.680 --> 00:40:17.680]   Other people use it the same way, but then people start using it in different ways.
[00:40:17.680 --> 00:40:19.320]   And that's when things get really exciting.
[00:40:19.320 --> 00:40:26.840]   So the chemistry professors came back to us and they said, actually, they do quizzes pretty
[00:40:26.840 --> 00:40:31.520]   much every week for these pre-med students that need to be very calibrated by the end
[00:40:31.520 --> 00:40:32.960]   of their undergrad.
[00:40:32.960 --> 00:40:39.680]   And so they said, hey, thanks to your system, we can grade like this much faster, but actually
[00:40:39.680 --> 00:40:42.400]   we're not interested in grading faster.
[00:40:42.400 --> 00:40:47.800]   We want to spend the same amount of time on grading and now we can ask much deeper questions
[00:40:47.800 --> 00:40:50.840]   and still grade them at the same time that we could grade before.
[00:40:50.840 --> 00:40:54.000]   So we can move away from kind of canonical multiple choice, which is the only thing we
[00:40:54.000 --> 00:40:56.500]   could do before and do all these other things.
[00:40:56.500 --> 00:41:00.160]   And so that's where things get exciting, I think, because you see people pick up on what
[00:41:00.160 --> 00:41:02.520]   you're building and doing new things with it.
[00:41:02.520 --> 00:41:08.480]   And pretty soon thereafter, we actually started as a company as a very explicit decision.
[00:41:08.480 --> 00:41:09.560]   Let's make it a company.
[00:41:09.560 --> 00:41:13.840]   And we saw the path, of course, as we get more and more data to also build AI behind
[00:41:13.840 --> 00:41:14.840]   it.
[00:41:14.840 --> 00:41:17.800]   And so a very product driven start.
[00:41:17.800 --> 00:41:21.120]   Other things about generalize a little bit, one is product driven.
[00:41:21.120 --> 00:41:28.160]   The other thing I would say is it's for me, at least a lot of the fun in everything I
[00:41:28.160 --> 00:41:31.400]   do comes from working with great people.
[00:41:31.400 --> 00:41:39.320]   And so for every major project, especially company I've started, that's pretty much the
[00:41:39.320 --> 00:41:40.320]   starting point.
[00:41:40.320 --> 00:41:45.380]   Like, am I going to do it with people that I really admire that, you know, I know anything
[00:41:45.380 --> 00:41:47.880]   they do, I can trust is of the highest quality.
[00:41:47.880 --> 00:41:53.320]   I never have to, you know, think twice about anything anybody else is doing on the team.
[00:41:53.320 --> 00:41:56.800]   And we're just going to have, you know, we're going to move very fast because I think that's
[00:41:56.800 --> 00:41:57.800]   a big part of it.
[00:41:57.800 --> 00:42:02.080]   If you're a startup, I think the only way to succeed is to move very fast.
[00:42:02.080 --> 00:42:05.680]   If you move slow, it's likely not going to be a success.
[00:42:05.680 --> 00:42:11.920]   And so I think being a team that's really motivated and really qualified to move fast
[00:42:11.920 --> 00:42:14.360]   on what you're doing is really key.
[00:42:14.360 --> 00:42:18.840]   Otherwise, it's probably a losing proposition.
[00:42:18.840 --> 00:42:22.920]   And I would imagine you have a lot of practice with trying to find great people, like just
[00:42:22.920 --> 00:42:25.240]   looking for RAs for your lab.
[00:42:25.240 --> 00:42:29.520]   But is there anything different that you look for in terms of someone to start a company
[00:42:29.520 --> 00:42:34.960]   with versus someone that you want to do research with versus the way you grade students?
[00:42:34.960 --> 00:42:35.960]   Are those different qualities?
[00:42:35.960 --> 00:42:37.480]   Oh, they're very different.
[00:42:37.480 --> 00:42:43.280]   I mean, obviously there are some people who are great at everything, that's going to happen.
[00:42:43.280 --> 00:42:51.400]   But ultimately, I think what matters for a startup is, I think, I mean, you know this
[00:42:51.400 --> 00:42:56.160]   as good as anybody else might know it, but I mean, whoever has never started a company
[00:42:56.160 --> 00:42:58.960]   before, it always takes longer than you think.
[00:42:58.960 --> 00:43:03.040]   You think you're going to build this company and in a few months, it'll be this massive
[00:43:03.040 --> 00:43:06.780]   thing because clearly, it should only take a few months to build this thing.
[00:43:06.780 --> 00:43:11.920]   But it's really always in terms of few years or even like, if you look at any company that
[00:43:11.920 --> 00:43:20.000]   is actually big, big, it tends to take five to 10 years to get to really big size.
[00:43:20.000 --> 00:43:21.800]   And that's a long time.
[00:43:21.800 --> 00:43:29.640]   And so you need to look for people that actually are really excited about what you're going
[00:43:29.640 --> 00:43:30.640]   to go for.
[00:43:30.640 --> 00:43:35.680]   So for example, at Gradescope, everybody was really excited about helping instructors and
[00:43:35.680 --> 00:43:37.160]   always listening to the instructors.
[00:43:37.160 --> 00:43:41.280]   Of course, really good builders of what we're trying to build, but also really excited to
[00:43:41.280 --> 00:43:45.800]   understand what they want, what they need, how they can be better served.
[00:43:45.800 --> 00:43:51.040]   Obviously, that doesn't mean implementing every feature request that comes your way.
[00:43:51.040 --> 00:43:56.080]   Some requests are very noisy expressions of what they actually want and need.
[00:43:56.080 --> 00:43:59.240]   You got to interpret those requests.
[00:43:59.240 --> 00:44:07.400]   But being really passionate about the long-term is, I think, just critical because it takes
[00:44:07.400 --> 00:44:08.400]   a long time.
[00:44:08.400 --> 00:44:12.840]   Same at Covariant, we're all super excited about what robots can do in the world, how
[00:44:12.840 --> 00:44:16.160]   it can be so helpful in so many places.
[00:44:16.160 --> 00:44:23.760]   And I think if you're not, it's hard to stick with something because if you just do it because,
[00:44:23.760 --> 00:44:28.600]   "Oh, this is going to be a quick success," it's essentially never a quick success.
[00:44:28.600 --> 00:44:35.400]   It tends to be a long, long-fought success rather than a quick one, typically.
[00:44:35.400 --> 00:44:40.400]   And at Covariant, did you also start with a particular problem in mind or was that more
[00:44:40.400 --> 00:44:45.840]   like we want to do robotic stuff and then we were looking for a problem that suits it?
[00:44:45.840 --> 00:44:47.920]   It's actually very interesting what happened there.
[00:44:47.920 --> 00:44:52.440]   For the different founders, there's a different story of how they got to wanting to start
[00:44:52.440 --> 00:44:53.440]   Covariant.
[00:44:53.440 --> 00:45:01.040]   So four founders, so myself, then Rocky Duan, Peter Chen, Tianhao Zhang, all three of them
[00:45:01.040 --> 00:45:06.000]   were undergrads at Berkeley, then PhD students at Berkeley.
[00:45:06.000 --> 00:45:11.520]   And Rocky and Peter spent time at OpenAI before starting Covariant.
[00:45:11.520 --> 00:45:13.160]   And everybody had a kind of different view on things.
[00:45:13.160 --> 00:45:24.400]   But for me personally, the reason I got to it is essentially back in 2016, 2017, it felt
[00:45:24.400 --> 00:45:31.920]   like that transition point where I'd been working on robotics for so long and how AI
[00:45:31.920 --> 00:45:37.280]   can make robotics more capable, and it just seemed that all of a sudden things were becoming
[00:45:37.280 --> 00:45:39.960]   possible.
[00:45:39.960 --> 00:45:45.740]   And it was this kind of, in some sense, technology enabler that really got me excited.
[00:45:45.740 --> 00:45:48.400]   It's clear if you have capable robots that they can be very useful.
[00:45:48.400 --> 00:45:52.720]   But as long as they're not capable, well, they're not useful.
[00:45:52.720 --> 00:45:58.840]   And it seemed right at that time like, okay, it's not possible today in 2016, 2017, but
[00:45:58.840 --> 00:46:02.960]   with the right effort, I think we can do some really amazing things.
[00:46:02.960 --> 00:46:09.400]   And the path should be very feasible in the next several years to get to viable robots
[00:46:09.400 --> 00:46:13.020]   that are smart and do new things that weren't possible before.
[00:46:13.020 --> 00:46:18.520]   And so for me, in some sense, that was like a career long passion almost, working for
[00:46:18.520 --> 00:46:22.060]   PhD on AI and robotics, then as a professor for many years.
[00:46:22.060 --> 00:46:25.080]   And then like, "Hey, this might actually be practical now.
[00:46:25.080 --> 00:46:28.520]   I want to take that next step and build a company."
[00:46:28.520 --> 00:46:32.160]   But then also at the same time, I was like, "I cannot build a company on my own.
[00:46:32.160 --> 00:46:35.040]   It's not going to be successful.
[00:46:35.040 --> 00:46:37.360]   I want to build it with other people."
[00:46:37.360 --> 00:46:42.880]   So actually, I emailed around to my students.
[00:46:42.880 --> 00:46:46.680]   I don't know if it was just current students or current and former students, I forget.
[00:46:46.680 --> 00:46:53.560]   And I remember sending an email and saying, "Hey, I think AI is at an interesting point
[00:46:53.560 --> 00:46:56.920]   where a lot of applications are becoming possible.
[00:46:56.920 --> 00:47:03.880]   And I'm curious if anybody is thinking the same and maybe would be excited to take something
[00:47:03.880 --> 00:47:10.120]   into the real world rather than stay focused on writing the next papers."
[00:47:10.120 --> 00:47:16.880]   And well, Rocky and Peter replied saying they had been thinking the same thing, that the
[00:47:16.880 --> 00:47:21.320]   time is now to do something like that.
[00:47:21.320 --> 00:47:29.660]   And Tianhao at the time in my lab had the most impressive and relevant project breakthrough.
[00:47:29.660 --> 00:47:33.640]   And so we also went to talk to Tianhao, we're like, "Tianhao, what do you think?"
[00:47:33.640 --> 00:47:36.440]   He was a few months into his PhD.
[00:47:36.440 --> 00:47:44.440]   He was not planning, I think, to do that short a stint as a PhD student or to go do something
[00:47:44.440 --> 00:47:45.440]   else.
[00:47:45.440 --> 00:47:49.720]   But once we talked with him, he got really excited and yeah, the four of us took it from
[00:47:49.720 --> 00:47:50.720]   there.
[00:47:50.720 --> 00:47:51.720]   That's cool.
[00:47:51.720 --> 00:47:57.520]   One more question that I really wanted to make sure I got to is from an interview that
[00:47:57.520 --> 00:48:04.040]   I found earlier where you talked about how Andrew Ng, your advisor, told you to take
[00:48:04.040 --> 00:48:07.280]   a class on communication or improve your communication skills.
[00:48:07.280 --> 00:48:11.360]   And then you talked about how you think you're more of a communicator than anything else.
[00:48:11.360 --> 00:48:17.000]   And I guess I was curious, I have noticed that your communication skills are very good
[00:48:17.000 --> 00:48:20.800]   and do seem to be improved since I first knew you.
[00:48:20.800 --> 00:48:24.760]   Do you feel like there's been little tricks that you've learned that have made you a better
[00:48:24.760 --> 00:48:25.760]   communicator?
[00:48:25.760 --> 00:48:26.760]   Or is it really just practice?
[00:48:26.760 --> 00:48:31.520]   Or do you have any advice to people wanting to become better communicators?
[00:48:31.520 --> 00:48:32.520]   Yeah.
[00:48:32.520 --> 00:48:35.600]   So there's different kinds of communication, of course.
[00:48:35.600 --> 00:48:40.240]   And one of them is written and the other one is verbal, at least the two main ones for
[00:48:40.240 --> 00:48:41.240]   me.
[00:48:41.240 --> 00:48:48.520]   For writing, actually, here's how the story goes.
[00:48:48.520 --> 00:48:56.120]   So I'm a PhD student and I try to write my papers and I bring my copies of my drafts
[00:48:56.120 --> 00:49:00.840]   to Andrew Ng and he's my PhD advisor and he just looks at them and he said, "Oh, take
[00:49:00.840 --> 00:49:01.840]   a look."
[00:49:01.840 --> 00:49:08.080]   And then he gets back to me later and Andrew says, "Yeah, really good draft, great shape.
[00:49:08.080 --> 00:49:11.080]   I just left a few comments."
[00:49:11.080 --> 00:49:16.920]   And I go get my copy back and the copy is more red than black.
[00:49:16.920 --> 00:49:21.200]   And I'm like, "Okay, that's what Andrew calls just a few comments.
[00:49:21.200 --> 00:49:24.680]   I'm just already in great shape."
[00:49:24.680 --> 00:49:28.200]   And I look at all his comments and I'm just like, "Okay, these comments are great.
[00:49:28.200 --> 00:49:29.520]   I mean, these are no brainers.
[00:49:29.520 --> 00:49:31.660]   I should just incorporate all these comments.
[00:49:31.660 --> 00:49:33.160]   He just knows better.
[00:49:33.160 --> 00:49:36.240]   This is how I should be doing it."
[00:49:36.240 --> 00:49:41.080]   But I had a hard time seeing the pattern in terms of how to do it myself.
[00:49:41.080 --> 00:49:45.800]   And so I'm looking at him like, "Yes, Andrew's feedback is always making it better and I
[00:49:45.800 --> 00:49:50.720]   can easily tell it's making it better, but I don't know how to generate that."
[00:49:50.720 --> 00:49:55.320]   And then actually I was a student at Stanford at the time and I go to Stanford bookstore
[00:49:55.320 --> 00:50:01.800]   and I browse all the writing classes, books that the professors for writing classes were
[00:50:01.800 --> 00:50:03.740]   recommending students buy.
[00:50:03.740 --> 00:50:07.760]   And I browse essentially all of them in the bookstore, not reading, but kind of quick
[00:50:07.760 --> 00:50:08.760]   browsing.
[00:50:08.760 --> 00:50:13.760]   And then I took three of them home and I read them quite thoroughly and some of them had
[00:50:13.760 --> 00:50:16.280]   exercises and I worked on it quite thoroughly.
[00:50:16.280 --> 00:50:22.040]   And there's one by Williams called Writing Lessons in Clarity and Grace.
[00:50:22.040 --> 00:50:27.080]   And that one, when I started working through that one, it was just like everything made
[00:50:27.080 --> 00:50:28.080]   sense.
[00:50:28.080 --> 00:50:35.800]   Like every comment Andrew had left on my paper draft, it was just a thing they explained.
[00:50:35.800 --> 00:50:38.260]   Like this is something you want to pay attention to in writing.
[00:50:38.260 --> 00:50:42.340]   This is the way you want to structure your sentence or your paragraph or a sequence of
[00:50:42.340 --> 00:50:43.340]   sentences and all that stuff.
[00:50:43.340 --> 00:50:47.980]   And I was just like, all of a sudden, I think I can do this now.
[00:50:47.980 --> 00:50:52.260]   And so that book was really eyeopening.
[00:50:52.260 --> 00:50:57.320]   Verbal communication, let's see.
[00:50:57.320 --> 00:51:01.340]   One part of it is just, I mean, the nature of my job is a lot of practice.
[00:51:01.340 --> 00:51:02.340]   That's for sure.
[00:51:02.340 --> 00:51:06.860]   And same for writing, of course.
[00:51:06.860 --> 00:51:12.760]   Let's see.
[00:51:12.760 --> 00:51:16.700]   I think maybe kind of there is even in verbal communication, there's different things.
[00:51:16.700 --> 00:51:20.020]   There's one-on-one communication and there is kind of group communication.
[00:51:20.020 --> 00:51:28.120]   I think one-on-one is usually easier for most people just because, well, it's a conversation
[00:51:28.120 --> 00:51:30.660]   back and forth.
[00:51:30.660 --> 00:51:35.920]   In terms of group communication, I think that the main thing I've learned to pay attention
[00:51:35.920 --> 00:51:42.240]   to, and it's a very simple thing, but it helps a lot, is just if anybody hasn't spoken up
[00:51:42.240 --> 00:51:46.400]   in a meeting and just checking in with them.
[00:51:46.400 --> 00:51:50.600]   I mean, obviously not blatantly putting them on the spot and making them feel awkward if
[00:51:50.600 --> 00:51:56.720]   they have nothing they want to say, but finding ways to make sure people who are not speaking
[00:51:56.720 --> 00:52:02.160]   up maybe wanted to speak up, but just feel like they didn't get the opportunity.
[00:52:02.160 --> 00:52:08.160]   I think that's just a really helpful thing to get many more ideas to surface in any meeting.
[00:52:08.160 --> 00:52:09.160]   Cool.
[00:52:09.160 --> 00:52:10.160]   Thank you.
[00:52:10.160 --> 00:52:13.040]   That's super useful.
[00:52:13.040 --> 00:52:19.000]   We always end with two questions and the second to last one we always end with is, what's
[00:52:19.000 --> 00:52:23.640]   a topic in machine learning that you think doesn't get the attention that it deserves?
[00:52:23.640 --> 00:52:31.080]   Like a topic you would work on if you had a little bit of extra time to explore something.
[00:52:31.080 --> 00:52:40.560]   So I would argue as a professor with that hat on at Berkeley, there is always opportunity
[00:52:40.560 --> 00:52:44.480]   to explore new things because new students come in all the time asking for projects.
[00:52:44.480 --> 00:52:48.440]   So it's not like there's projects that are just kind of sitting there waiting because
[00:52:48.440 --> 00:52:50.520]   there's always new students who want to work on things.
[00:52:50.520 --> 00:52:54.200]   But so maybe I'll twist the question a little bit and I'll say some of the recent things
[00:52:54.200 --> 00:52:58.920]   I'm most excited about that we've started working on.
[00:52:58.920 --> 00:53:04.560]   One of them is play or kind of formalizing how kids play in reinforcement learning.
[00:53:04.560 --> 00:53:09.200]   This notion that kids don't get scored, there's no reward, no feedback most of the time.
[00:53:09.200 --> 00:53:11.280]   They just play.
[00:53:11.280 --> 00:53:14.400]   And can our agents or our robots do the same thing?
[00:53:14.400 --> 00:53:16.480]   Just play around in the environment?
[00:53:16.480 --> 00:53:20.680]   And essentially that is the reinforced learning equivalent of pre-training.
[00:53:20.680 --> 00:53:25.480]   And we know in computer vision, natural language processing, unsupervised pre-training is what
[00:53:25.480 --> 00:53:28.220]   powers all the latest and greatest models.
[00:53:28.220 --> 00:53:29.720]   So can we do the same thing in RL?
[00:53:29.720 --> 00:53:33.360]   Well, that means it has to be reward free, some kind of play.
[00:53:33.360 --> 00:53:37.280]   And so I think that's a really exciting area.
[00:53:37.280 --> 00:53:42.680]   The other area I'm really excited about, and for me was sometimes the most surprising result
[00:53:42.680 --> 00:53:49.580]   this past year in my own research was the kind of pre-trained transformers as universal
[00:53:49.580 --> 00:53:54.680]   competition engines paper that was led by Kevin Liu.
[00:53:54.680 --> 00:54:01.400]   And the idea there was that, hey, transformers are so good at being pre-trained language
[00:54:01.400 --> 00:54:03.040]   models.
[00:54:03.040 --> 00:54:04.840]   What if we just take a pre-trained language model?
[00:54:04.840 --> 00:54:09.240]   We put one linear layer in front, one linear layer in the back, but now the input is going
[00:54:09.240 --> 00:54:12.000]   to be an image and the output is going to be a classification of an image, or the input
[00:54:12.000 --> 00:54:15.480]   is going to be a protein sequence, the output is going to be some property of the protein
[00:54:15.480 --> 00:54:16.480]   sequence.
[00:54:16.480 --> 00:54:22.200]   And it actually kind of worked, which is really surprising to me because it means that somehow
[00:54:22.200 --> 00:54:29.400]   all these pre-trained layers, which were frozen for that new modality, somehow, well, we don't
[00:54:29.400 --> 00:54:33.200]   really understand it, but what in my mind is happening is something where it has a general
[00:54:33.200 --> 00:54:40.320]   compute pattern, a general pattern recognition in it that generalizes across different sensory
[00:54:40.320 --> 00:54:42.680]   modalities, which is really, really cool.
[00:54:42.680 --> 00:54:49.080]   I mean, of course it's better to train the whole network on the specific modality, but
[00:54:49.080 --> 00:54:54.120]   the fact that it already does quite well when it's a frozen pre-trained transformer and
[00:54:54.120 --> 00:55:00.880]   a different modality really surprised me and is something that I'm excited to keep digging
[00:55:00.880 --> 00:55:01.880]   into.
[00:55:01.880 --> 00:55:03.600]   That really is amazing and evocative.
[00:55:03.600 --> 00:55:08.240]   I can see why you're excited about that.
[00:55:08.240 --> 00:55:13.520]   And then finally, what in your experience has been the hardest parts of getting machine
[00:55:13.520 --> 00:55:15.840]   learning models to actually work in the real world?
[00:55:15.840 --> 00:55:19.160]   You've done it now at several different companies.
[00:55:19.160 --> 00:55:23.320]   What are the surprising pitfalls when you take a model that you've trained that seems
[00:55:23.320 --> 00:55:28.960]   to be working and then you try to build an actual useful thing around it?
[00:55:28.960 --> 00:55:30.160]   Yeah.
[00:55:30.160 --> 00:55:33.160]   So it varies a lot.
[00:55:33.160 --> 00:55:37.720]   So the cases I know best are of course, Gradescope and Covariant.
[00:55:37.720 --> 00:55:46.960]   So at Gradescope, essentially the way we did it is we build really good user interfaces
[00:55:46.960 --> 00:55:47.960]   around the model.
[00:55:47.960 --> 00:55:54.080]   So train models to effectively automate grading, but we knew in the beginning they're never
[00:55:54.080 --> 00:55:56.080]   going to work that well.
[00:55:56.080 --> 00:56:00.800]   Maybe not 99.99% performance, but much lower than that.
[00:56:00.800 --> 00:56:09.920]   And so we spend so much time on the UI of have it proposing things to the grader and
[00:56:09.920 --> 00:56:14.200]   then the grader can correct it and be really, really fast at getting through things.
[00:56:14.200 --> 00:56:20.000]   But this kind of human interface to supervise all the decisions was where I would say at
[00:56:20.000 --> 00:56:25.040]   least as much effort went into that and making that really good as went into the machine
[00:56:25.040 --> 00:56:27.200]   learning models behind it.
[00:56:27.200 --> 00:56:34.160]   I would say at Covariant it's in some sense very similar, but also a bit different.
[00:56:34.160 --> 00:56:37.480]   You can't just say, "Oh, we're going to put a great UI on it."
[00:56:37.480 --> 00:56:41.960]   Even when it's just in its beginning reliabilities, it already has to be very high reliability.
[00:56:41.960 --> 00:56:49.280]   So there, I think what's been interesting is for me that I've never in any other capacity
[00:56:49.280 --> 00:56:53.240]   chased multiple nines of reliability on any problem.
[00:56:53.240 --> 00:56:59.960]   And just that notion is just so different and it's been so interesting to go after that.
[00:56:59.960 --> 00:57:00.960]   Awesome.
[00:57:00.960 --> 00:57:02.460]   Well, thank you very much.
[00:57:02.460 --> 00:57:03.720]   This is super fun.
[00:57:03.720 --> 00:57:05.520]   Yeah, same here, Lucas.
[00:57:05.520 --> 00:57:07.480]   Thanks for having me.
[00:57:07.480 --> 00:57:11.880]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:57:11.880 --> 00:57:16.600]   to the show notes in the description where you can find links to all the papers that
[00:57:16.600 --> 00:57:20.760]   are mentioned, supplemental material, and a transcription that we worked really hard
[00:57:20.760 --> 00:57:21.760]   to produce.
[00:57:21.760 --> 00:57:22.760]   Check it out.
[00:57:23.160 --> 00:57:25.240]   you

