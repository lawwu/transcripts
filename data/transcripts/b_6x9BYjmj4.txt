
[00:00:00.000 --> 00:00:04.920]   Also, my head will live on YouTube and like always wait for an echo and
[00:00:04.920 --> 00:00:06.400]   then take my headphones off.
[00:00:06.400 --> 00:00:14.680]   I'm just waiting for YouTube to be live.
[00:00:14.680 --> 00:00:19.680]   I can hear myself, which is a good sign.
[00:00:19.680 --> 00:00:20.640]   Things look okay.
[00:00:20.640 --> 00:00:22.160]   Let me check my YouTube dashboard.
[00:00:22.160 --> 00:00:24.120]   She's excellent connection.
[00:00:24.120 --> 00:00:24.840]   Perfect.
[00:00:24.840 --> 00:00:28.560]   You should be able to see my face in HD.
[00:00:29.600 --> 00:00:30.880]   Let me take off my headphones.
[00:00:30.880 --> 00:00:31.400]   Um,
[00:00:31.400 --> 00:00:41.080]   awesome.
[00:00:41.080 --> 00:00:44.000]   Can someone from zoom please confirm if you can still hear me.
[00:00:44.000 --> 00:00:44.520]   Okay.
[00:00:44.520 --> 00:00:47.880]   I take my headphones off because I get an echo from them.
[00:00:47.880 --> 00:00:50.960]   Basically I can hear myself talk.
[00:00:50.960 --> 00:00:51.400]   Awesome.
[00:00:51.400 --> 00:00:52.000]   Thanks.
[00:00:52.000 --> 00:00:52.560]   Thanks guys.
[00:00:52.560 --> 00:00:53.400]   Thanks for confirming.
[00:00:53.400 --> 00:00:55.440]   I also see a lot of new people joining in.
[00:00:55.440 --> 00:00:56.200]   Welcome everyone.
[00:00:56.200 --> 00:00:56.640]   Thanks.
[00:00:56.640 --> 00:00:58.120]   Thanks for joining us.
[00:00:58.200 --> 00:01:02.560]   We have been into the session a few times, uh, which means we
[00:01:02.560 --> 00:01:04.280]   are at chapter 30 out of 15.
[00:01:04.280 --> 00:01:08.760]   Um, that is not to discourage you from asking questions that is
[00:01:08.760 --> 00:01:13.640]   either to encourage you, uh, for, uh, and encourage you to ask
[00:01:13.640 --> 00:01:18.200]   questions, uh, because we might be talking in a few advanced
[00:01:18.200 --> 00:01:21.960]   terminologies, although, uh, it's not really that advanced.
[00:01:21.960 --> 00:01:25.760]   If you feel you don't understand anything, please, please ask the questions.
[00:01:25.760 --> 00:01:27.040]   That's why we have the forums.
[00:01:27.040 --> 00:01:30.880]   That's why we are bits and biases who would these sessions.
[00:01:30.880 --> 00:01:34.920]   And that's why we love the fact that you take your time on a
[00:01:34.920 --> 00:01:37.080]   Sunday evening to join us or morning.
[00:01:37.080 --> 00:01:41.520]   Uh, so thanks again for joining and, uh, be sure to ask questions if you'd like.
[00:01:41.520 --> 00:01:45.000]   Let me share my screen.
[00:01:45.000 --> 00:01:46.880]   Uh, let's get to it.
[00:01:46.880 --> 00:01:51.800]   Awesome.
[00:01:51.800 --> 00:01:56.400]   Um, I don't need to move this zoom thing out of the way.
[00:01:56.400 --> 00:01:57.000]   Perfect.
[00:01:57.520 --> 00:02:00.520]   Pop the chart out and present my screen.
[00:02:00.520 --> 00:02:05.400]   So this is, I think, uh, the eighth session, which means we've been two
[00:02:05.400 --> 00:02:07.840]   months into this for two months.
[00:02:07.840 --> 00:02:12.960]   I, and reasons beyond me, people have been showing up to these recap, uh,
[00:02:12.960 --> 00:02:16.880]   reading groups, these lectures, although I wouldn't call them lectures that I've
[00:02:16.880 --> 00:02:20.520]   been hosting for reasons beyond my understanding, thanks for showing up
[00:02:20.520 --> 00:02:23.880]   every week, welcome if you knew, like I said, uh, any and all questions and
[00:02:23.880 --> 00:02:25.280]   anyone and everyone is welcome.
[00:02:25.280 --> 00:02:27.760]   So really appreciate you joining on a Sunday.
[00:02:27.760 --> 00:02:30.120]   Uh, and here's what we're going to learn today.
[00:02:30.120 --> 00:02:35.760]   So we've been reading this book by Manning publication, uh, and we're at
[00:02:35.760 --> 00:02:39.920]   chapter 12 and 13, although I don't strictly follow the flow of chapters
[00:02:39.920 --> 00:02:41.640]   for reason that reiterate real quick.
[00:02:41.640 --> 00:02:47.160]   Uh, but I do plan to cover the following things today, image segmentation, unit
[00:02:47.160 --> 00:02:49.360]   architecture, and image augmentation.
[00:02:49.360 --> 00:02:50.800]   There's a little bit more depth.
[00:02:52.840 --> 00:02:55.520]   Last week we had learned about training a tumor classifier.
[00:02:55.520 --> 00:03:02.680]   So just straightforward image classification and here's the agenda for today.
[00:03:02.680 --> 00:03:12.240]   Uh, people have asked earlier, what does Indian masala chai look like?
[00:03:12.240 --> 00:03:13.240]   This is what it looks like.
[00:03:13.240 --> 00:03:14.880]   So it's brownish in color.
[00:03:14.880 --> 00:03:19.800]   Um, when I refer to chai, that's what I refer to.
[00:03:20.040 --> 00:03:22.240]   Um, so awesome.
[00:03:22.240 --> 00:03:24.120]   Here was the homebook from last session.
[00:03:24.120 --> 00:03:26.440]   Uh, it was just suggested homework.
[00:03:26.440 --> 00:03:30.440]   Like always, there's no, uh, hard and fast requirement, but it does make me
[00:03:30.440 --> 00:03:32.760]   really sad when no one does any homework.
[00:03:32.760 --> 00:03:39.600]   And this week I'm not sad because, uh, I had two incredible, um, articles.
[00:03:39.600 --> 00:03:44.920]   Actually, this is, this is also an article, uh, also great walkthrough,
[00:03:44.920 --> 00:03:49.040]   also an incredible Kaggle kernel, uh, by Ravi.
[00:03:49.800 --> 00:03:55.400]   So what Ravi has done, he's created this notebook, uh, on the lunar 16 data set.
[00:03:55.400 --> 00:04:00.160]   And it's built on top of the original notebook by Sendex, who
[00:04:00.160 --> 00:04:02.120]   is also an incredible YouTuber.
[00:04:02.120 --> 00:04:07.240]   I said, also, I'm sorry, who is the incredible YouTuber of Python.
[00:04:07.240 --> 00:04:14.240]   Um, and, uh, you can tell how much effort goes into a notebook by just going
[00:04:14.240 --> 00:04:17.080]   through the code and the markdown cells.
[00:04:17.200 --> 00:04:21.880]   Uh, so this has been built on top of both the book and Sendex's kernel.
[00:04:21.880 --> 00:04:24.800]   And as you can see, it's very readable.
[00:04:24.800 --> 00:04:29.240]   Ravi has gone to the extent of commenting a lot of things,
[00:04:29.240 --> 00:04:30.840]   making the code very readable.
[00:04:30.840 --> 00:04:36.040]   Uh, I'm not sure if he hand drew this, but it looks like it, uh, correct me if
[00:04:36.040 --> 00:04:38.760]   I'm wrong, Ravi, but, uh, also massive effort here.
[00:04:38.760 --> 00:04:44.080]   And, uh, I know the code inside of the book was quite concise, at
[00:04:44.080 --> 00:04:45.440]   least on the GitHub repository.
[00:04:45.440 --> 00:04:49.840]   He's gone to the extent of commenting all of these lines,
[00:04:49.840 --> 00:04:51.640]   explaining every single thing.
[00:04:51.640 --> 00:04:54.600]   Uh, and I just spend time on Kaggle.
[00:04:54.600 --> 00:04:57.200]   I really want to encourage everyone to spend time on Kaggle.
[00:04:57.200 --> 00:05:01.800]   And as you get addicted to Kaggle, you'll see this differentiators between
[00:05:01.800 --> 00:05:05.360]   good kernels, great kernels, and just incredible kernels.
[00:05:05.360 --> 00:05:07.600]   So this is one of those things, right?
[00:05:07.600 --> 00:05:11.480]   On Kaggle, you don't care much about how readable your code is.
[00:05:12.440 --> 00:05:16.720]   You're aiming for different things, but of course, readability is one of them.
[00:05:16.720 --> 00:05:19.720]   So this is what also sets notebooks apart.
[00:05:19.720 --> 00:05:22.400]   I'm not going to go through the entirety of it.
[00:05:22.400 --> 00:05:23.720]   I really enjoy reading it.
[00:05:23.720 --> 00:05:29.080]   Uh, and as you can see, these, these are digitally hand drawn images.
[00:05:29.080 --> 00:05:30.840]   Uh, also quite neat.
[00:05:30.840 --> 00:05:35.760]   I have been apologizing to everyone for my handwriting, but Ravi has done this
[00:05:35.760 --> 00:05:39.280]   incredible job of even, you know, color matching different things.
[00:05:39.680 --> 00:05:43.560]   And I'm pointing these small details out because as you spend time on this, you
[00:05:43.560 --> 00:05:48.360]   start to have an eye for attention to detail or these subtle things are
[00:05:48.360 --> 00:05:50.040]   the things that jump out on you.
[00:05:50.040 --> 00:05:54.680]   So please take notes from these things and try to just
[00:05:54.680 --> 00:05:56.200]   emulate what Ravi is doing.
[00:05:56.200 --> 00:05:58.880]   And I'll post this link shortly in a minute.
[00:05:58.880 --> 00:06:03.600]   Also want to point out a great blog post by Amrik Ankh.
[00:06:03.600 --> 00:06:05.520]   I don't know where this came from.
[00:06:05.520 --> 00:06:12.320]   I definitely haven't covered ConfMixer, but Amrik Ankh has covered
[00:06:12.320 --> 00:06:13.960]   the PyTorch implementation of it.
[00:06:13.960 --> 00:06:16.440]   So I also want to briefly mention this.
[00:06:16.440 --> 00:06:21.120]   Um, as you can see, he talks about self-attention patches of
[00:06:21.120 --> 00:06:23.440]   images and, uh, VIT architectures.
[00:06:23.440 --> 00:06:27.520]   So clearly things that I haven't covered, uh, but this is also very
[00:06:27.520 --> 00:06:33.480]   readable blog posts and quite concise, neat, uh, summary of the paper.
[00:06:34.040 --> 00:06:40.600]   And this paper was quite very well discussed, um, on the Twitter community.
[00:06:40.600 --> 00:06:44.040]   At least I'm not sure of any other communities where things are, you
[00:06:44.040 --> 00:06:48.520]   know, so, so visible, but, uh, I know members of this study group
[00:06:48.520 --> 00:06:49.920]   have been pretty active on Twitter.
[00:06:49.920 --> 00:06:54.600]   If you're not, uh, please consider being active on there and, you know,
[00:06:54.600 --> 00:06:59.920]   following Ravi, Amrik Ankh, Matteo on, on Twitter, because if anything
[00:07:00.440 --> 00:07:04.280]   comes up in the community, the community is hyper active about it.
[00:07:04.280 --> 00:07:05.480]   And you can't miss it.
[00:07:05.480 --> 00:07:10.200]   Uh, and that is how you can write such incredible paper summaries.
[00:07:10.200 --> 00:07:17.680]   So coming back to today's discussion, I want to point everyone towards this link.
[00:07:17.680 --> 00:07:22.000]   Which someone has already posted in the chat.
[00:07:22.000 --> 00:07:22.500]   Thanks.
[00:07:22.500 --> 00:07:28.280]   Uh, for that, we'll be asking all questions in this thread and let me.
[00:07:28.280 --> 00:07:28.780]   Okay.
[00:07:28.780 --> 00:07:35.000]   Link Ravi's kernel and Amrik Ankh's blog here.
[00:07:35.000 --> 00:07:42.640]   This is Ravi's kernel.
[00:07:42.640 --> 00:07:48.720]   And this is Amrik Ankh's blog post here.
[00:07:48.720 --> 00:07:53.400]   Uh, you can simply reply with your questions into this thread.
[00:07:53.400 --> 00:07:57.360]   And, uh, I'm going to be monitoring this thread instead of the zoom
[00:07:57.360 --> 00:08:01.320]   chart and YouTube chat because we're live streaming to two different locations.
[00:08:01.320 --> 00:08:05.800]   I try to, but it's like quite hard for me to be on top of every
[00:08:05.800 --> 00:08:07.080]   single comment that comes in.
[00:08:07.080 --> 00:08:11.840]   So as my request, please consider asking your questions, uh, in this link.
[00:08:11.840 --> 00:08:16.360]   And I'll post that link into the zoom chat now and also pin it to YouTube.
[00:08:26.520 --> 00:08:28.760]   Uh, to the people who've been joining, I apologize.
[00:08:28.760 --> 00:08:32.400]   I cover this every single week and I always spend two, three minutes.
[00:08:32.400 --> 00:08:36.320]   Uh, but I also want to make this an amazing experience for people that
[00:08:36.320 --> 00:08:38.680]   watch later or watch this for the first time.
[00:08:38.680 --> 00:08:42.880]   So it's important for me to always reiterate on these things and, uh,
[00:08:42.880 --> 00:08:47.320]   always like my posts so that I can brag against my colleagues that I'm
[00:08:47.320 --> 00:08:49.040]   ahead of you in terms of likes.
[00:08:49.040 --> 00:08:55.520]   I'm just kidding, but, uh, this is a nice feature for basically bragging rights.
[00:08:56.160 --> 00:08:56.760]   Awesome guys.
[00:08:56.760 --> 00:08:59.720]   So we're all set to start a hasty agenda for today.
[00:08:59.720 --> 00:09:00.760]   Image segmentation.
[00:09:00.760 --> 00:09:02.840]   Uh, let's start with that.
[00:09:02.840 --> 00:09:06.360]   I'm going to recap.
[00:09:06.360 --> 00:09:09.560]   First of all, the things we started in the last group and then jump to it.
[00:09:09.560 --> 00:09:13.960]   Says as a reminder, please ask questions in this thread.
[00:09:13.960 --> 00:09:18.880]   So I'm following the fast day approach and I tried to copy some things from there,
[00:09:18.880 --> 00:09:21.400]   which means things aren't strictly in order.
[00:09:21.960 --> 00:09:26.680]   Um, but I'll try to recap the things from the previous group and
[00:09:26.680 --> 00:09:28.640]   then jump right into segmentation.
[00:09:28.640 --> 00:09:32.400]   This is not the order that the authors strictly follow.
[00:09:32.400 --> 00:09:37.120]   I know I jumped a few things, but we're at the point in this study group
[00:09:37.120 --> 00:09:41.920]   where, you know, I shouldn't be pointing every single detail out and everyone.
[00:09:41.920 --> 00:09:46.520]   Essentially, I can confirm that just by reading the stuff you all have
[00:09:46.520 --> 00:09:48.560]   been writing is quite advanced.
[00:09:48.560 --> 00:09:50.840]   So I tend to skip over a lot of details.
[00:09:51.200 --> 00:09:53.720]   Uh, I mentioned that again, because you're still very
[00:09:53.720 --> 00:09:55.160]   welcome to ask any questions.
[00:09:55.160 --> 00:09:57.000]   Don't let that fact discourage you.
[00:09:57.000 --> 00:09:59.160]   Uh, but I'll be glancing over a lot of things.
[00:09:59.160 --> 00:10:03.280]   So we learned about image augmentations last week.
[00:10:03.280 --> 00:10:11.000]   And one second.
[00:10:11.000 --> 00:10:17.360]   And I essentially explained by are these important?
[00:10:17.360 --> 00:10:18.520]   How are these helpful?
[00:10:18.600 --> 00:10:21.840]   Uh, what's the reason for augmenting images?
[00:10:21.840 --> 00:10:23.160]   Uh, we are the useful.
[00:10:23.160 --> 00:10:28.800]   We learned how to classify tumors, uh, with simple processes.
[00:10:28.800 --> 00:10:31.160]   I talked about what's the head of a network.
[00:10:31.160 --> 00:10:31.760]   What's the deal?
[00:10:31.760 --> 00:10:32.880]   What's the backbone.
[00:10:32.880 --> 00:10:36.520]   I suggested you all should switch out the backbone for different things.
[00:10:36.520 --> 00:10:38.480]   I didn't see that homework coming.
[00:10:38.480 --> 00:10:40.320]   So that's still pending for you all.
[00:10:40.320 --> 00:10:47.000]   Um, and then we looked at albumentations framework.
[00:10:47.280 --> 00:10:54.240]   So, uh, this is one of the incredible frameworks by Kagglers and also best
[00:10:54.240 --> 00:10:56.440]   practitioners in our community.
[00:10:56.440 --> 00:10:57.840]   Incredible framework.
[00:10:57.840 --> 00:11:01.800]   We'll be again, taking a deeper look at it and also trying to recap image
[00:11:01.800 --> 00:11:05.160]   segmentation or cover image segmentation as we do.
[00:11:05.160 --> 00:11:09.640]   So what exactly is image segmentation?
[00:11:09.640 --> 00:11:13.840]   Uh, to credit this image has been taken from taut sec.
[00:11:14.720 --> 00:11:22.000]   This is a GitHub repo, uh, that aims at providing a fast modular reference
[00:11:22.000 --> 00:11:24.320]   for implementing semantic segmentation.
[00:11:24.320 --> 00:11:27.880]   It's not been updated for a while, but I still think there are quite
[00:11:27.880 --> 00:11:29.160]   a few interesting things in there.
[00:11:29.160 --> 00:11:35.800]   I just copied the image from here to credit, but here are a few things
[00:11:35.800 --> 00:11:37.840]   that immediately should stand out to you.
[00:11:37.840 --> 00:11:42.680]   So I'm trying to contrast image segmentation with, uh, image classification.
[00:11:44.640 --> 00:11:48.960]   In image segmentation with color ring, every single damn
[00:11:48.960 --> 00:11:50.080]   thing inside of the image.
[00:11:50.080 --> 00:11:51.200]   Right.
[00:11:51.200 --> 00:11:55.520]   And we're not just randomly coloring things, right.
[00:11:55.520 --> 00:11:59.280]   Trees for good reason are green here.
[00:11:59.280 --> 00:12:05.800]   Uh, makes it easy to remember the sidewalk or footpath is pink.
[00:12:05.800 --> 00:12:12.840]   All pedestrians are blue, all vehicles or automobiles.
[00:12:13.640 --> 00:12:16.480]   So this looks like a truck or a pickup truck, depending
[00:12:16.480 --> 00:12:17.720]   on which country you're from.
[00:12:17.720 --> 00:12:24.000]   Um, this looks like a simple sedan, a hatchback, uh, all vehicles, all
[00:12:24.000 --> 00:12:25.760]   automobiles are red in this image.
[00:12:25.760 --> 00:12:27.680]   All stop signs are blue.
[00:12:27.680 --> 00:12:31.840]   So this is the importance of segmentation.
[00:12:31.840 --> 00:12:38.680]   We're literally trying to understand every single pixel inside of the image.
[00:12:38.680 --> 00:12:40.280]   So we're not just classifying things.
[00:12:40.280 --> 00:12:42.880]   We're understanding every single image.
[00:12:42.880 --> 00:12:45.000]   We're trying to understand every single pixel.
[00:12:45.000 --> 00:12:50.440]   So my first question, uh, and I expect answers in this thread is
[00:12:50.440 --> 00:12:54.520]   what, where do you think image segmentation really makes important
[00:12:54.520 --> 00:12:58.400]   or is, is a better choice over image classification?
[00:12:58.400 --> 00:13:01.160]   So I'll type it out.
[00:13:01.160 --> 00:13:08.120]   When would you rather use segmentation over classification?
[00:13:10.480 --> 00:13:14.680]   And I'll wait for a few answers to come in while I also glance over a few details.
[00:13:14.680 --> 00:13:21.400]   Um, so this guy also, if you observe is, uh, or just background
[00:13:21.400 --> 00:13:23.200]   has also been color coded.
[00:13:23.200 --> 00:13:31.160]   So essentially no single pixel goes without, uh, being, let's call it segmented here.
[00:13:31.160 --> 00:13:36.400]   And I assume as a human, I can consider these to be buildings.
[00:13:37.280 --> 00:13:41.520]   But another thing to point out here is things in the father background or
[00:13:41.520 --> 00:13:45.120]   things as they are away from you tend to have a lot of false positives.
[00:13:45.120 --> 00:13:47.360]   So what do I mean by false positives?
[00:13:47.360 --> 00:13:50.440]   I don't assume a tree will be growing on top of a building.
[00:13:50.440 --> 00:13:53.960]   It could be, but it's unlikely.
[00:13:53.960 --> 00:13:57.640]   So, uh, this feels like it's into the distance.
[00:13:57.640 --> 00:14:02.080]   And as you can also see, uh, here's another stop sign that I
[00:14:02.080 --> 00:14:04.200]   assume the image is guessing.
[00:14:04.840 --> 00:14:08.680]   And I don't expect a stop sign somewhere there on a building, right?
[00:14:08.680 --> 00:14:13.440]   So in the distance, there are a few false positives, but what matters to this
[00:14:13.440 --> 00:14:18.280]   particular image, uh, things closer to us, at least from a visual
[00:14:18.280 --> 00:14:19.960]   standpoint are well-marked.
[00:14:19.960 --> 00:14:22.240]   Um, Mr.
[00:14:22.240 --> 00:14:25.200]   Webov says maybe solid-line cars.
[00:14:25.200 --> 00:14:25.680]   Sure.
[00:14:25.680 --> 00:14:28.480]   Multi-class classification.
[00:14:28.480 --> 00:14:33.960]   Um, because don't you think we could still do multi-class classification.
[00:14:33.960 --> 00:14:37.080]   So when I said classification, it's not just binary classification.
[00:14:37.080 --> 00:14:40.800]   So we could totally do multi-class classification, right?
[00:14:40.800 --> 00:14:44.440]   Why would you want to use segmentation for that purpose?
[00:14:44.440 --> 00:15:01.360]   Um, Ravi's answers come in.
[00:15:01.400 --> 00:15:05.280]   I guess we use classification when we care about the presence or absence of an
[00:15:05.280 --> 00:15:05.720]   object.
[00:15:05.720 --> 00:15:06.520]   I'm sure.
[00:15:06.520 --> 00:15:12.040]   And we use segmentation when we care about the exact position of an object in image.
[00:15:12.040 --> 00:15:15.200]   Um, that's, that's a totally valid answer.
[00:15:15.200 --> 00:15:15.880]   Great answer.
[00:15:15.880 --> 00:15:16.600]   Thanks for that.
[00:15:16.600 --> 00:15:25.160]   When we also want to know the location of labels in the images.
[00:15:25.840 --> 00:15:31.640]   Um, so material we label objects inside of images.
[00:15:31.640 --> 00:15:34.320]   We aren't exactly looking for labels.
[00:15:34.320 --> 00:15:38.000]   I, I, I know your answer is right, but I'm just trying to correct your jargon a
[00:15:38.000 --> 00:15:38.600]   little bit.
[00:15:38.600 --> 00:15:47.160]   Um, we want to know the location of objects and then, um, we try to label them.
[00:15:47.160 --> 00:15:50.920]   So correct answer slightly off the jargon.
[00:15:52.160 --> 00:15:56.760]   Medical imaging, uh, classifying the pixel show totally valid answer.
[00:15:56.760 --> 00:15:58.840]   This is by Preet.
[00:15:58.840 --> 00:16:00.040]   Thanks for that answer.
[00:16:00.040 --> 00:16:01.960]   I'll take one last answer.
[00:16:01.960 --> 00:16:05.800]   And I think this is because trying to add more context.
[00:16:18.520 --> 00:16:24.360]   I was hoping to keep this cup full at least until the half session, but, uh,
[00:16:24.360 --> 00:16:26.400]   that's not how it goes with Jay and me.
[00:16:26.400 --> 00:16:29.480]   So it's already empty.
[00:16:29.480 --> 00:16:30.160]   Almost.
[00:16:30.160 --> 00:16:35.360]   I'll wait for because his answer in the meantime, I'm trying to run this exercise
[00:16:35.360 --> 00:16:39.080]   because I want to contrast two different techniques and, uh, there's another
[00:16:39.080 --> 00:16:42.440]   technique that I haven't talked about, but I'll mention it in a second.
[00:16:42.440 --> 00:16:44.120]   Okay.
[00:16:44.120 --> 00:16:45.480]   I think because might take a second.
[00:16:45.480 --> 00:16:46.960]   So I'll talk about that.
[00:16:47.000 --> 00:16:49.640]   There's also bounding box detection.
[00:16:49.640 --> 00:16:53.720]   So you're also, there are different techniques that come under image, uh,
[00:16:53.720 --> 00:16:59.440]   approaches this classification segmentation, localization, this overlap
[00:16:59.440 --> 00:17:06.080]   between these, uh, and of course these jargon can be mixed, but, um, when you're
[00:17:06.080 --> 00:17:13.040]   trying to localize one example inside of image or one class, uh, you usually draw
[00:17:13.040 --> 00:17:14.680]   bounding boxes around that thing.
[00:17:15.560 --> 00:17:18.480]   And in this image, you're trying to understand the scenario.
[00:17:18.480 --> 00:17:21.120]   So that's where you would want to use segmentation.
[00:17:21.120 --> 00:17:22.160]   If that makes sense.
[00:17:22.160 --> 00:17:27.800]   Basically classify different objects in one image.
[00:17:27.800 --> 00:17:31.760]   Um, because I think you could still still do that with
[00:17:31.760 --> 00:17:33.480]   multi-class classification, right?
[00:17:33.480 --> 00:17:38.840]   So what do you care about here is the location relative to other things.
[00:17:38.840 --> 00:17:41.200]   That's where segmentation really stands out, right?
[00:17:41.200 --> 00:17:45.360]   So you can also understand where is this person inside of the image.
[00:17:45.920 --> 00:17:51.640]   If I were to just classify, uh, or just count humans, I would rather draw a
[00:17:51.640 --> 00:17:54.520]   bounding box over them and just care about humans, right?
[00:17:54.520 --> 00:17:56.080]   Not care about everything else.
[00:17:56.080 --> 00:18:00.440]   I'm not sure if I'm confusing you more or answering your
[00:18:00.440 --> 00:18:02.560]   question, uh, feel free to follow up.
[00:18:02.560 --> 00:18:08.280]   So this was the reason, uh, for contrasting this.
[00:18:08.280 --> 00:18:10.800]   And I also want to point out the YOLO paper.
[00:18:11.240 --> 00:18:16.000]   It's it's one of the, uh, I can, so I never advise anyone to read any paper.
[00:18:16.000 --> 00:18:20.960]   This paper, I will, uh, strongly advise for you all to read because
[00:18:20.960 --> 00:18:34.280]   I think it was the YOLO V3 or YOLO paper that was honestly one of the funniest
[00:18:34.280 --> 00:18:36.920]   papers ever, uh, not the funniest.
[00:18:36.920 --> 00:18:39.200]   It's it's, it's an incredible technique for sure.
[00:18:40.080 --> 00:18:43.320]   Um, but, uh, fun read.
[00:18:43.320 --> 00:18:44.120]   That's what I mean.
[00:18:44.120 --> 00:18:46.040]   That's also the, uh, what the authors say.
[00:18:46.040 --> 00:18:51.200]   So please, please check out this paper.
[00:18:51.200 --> 00:18:55.000]   Uh, you'll, you'll be smiling at the paper instead of sweating.
[00:18:55.000 --> 00:18:57.600]   That's what most deep learning papers make me do.
[00:18:57.600 --> 00:19:03.480]   Um, but this technique, uh, essentially creates bounding boxes around images.
[00:19:03.480 --> 00:19:07.480]   Uh, that is how AI is advertised in a lot of advertisements.
[00:19:07.480 --> 00:19:10.000]   Um, that's not what we care about today.
[00:19:10.000 --> 00:19:14.680]   We care about segmentation and labeling every single pixel inside of an image.
[00:19:14.680 --> 00:19:22.880]   So going back, uh, to discussion, uh, someone's asking in the zoom
[00:19:22.880 --> 00:19:27.360]   chat, uh, to point out the links, please head over to this thread.
[00:19:27.360 --> 00:19:29.360]   I'll post the link again once for you.
[00:19:29.360 --> 00:19:35.480]   Um, so it's really tough for me to always monitor the zoom chat.
[00:19:35.480 --> 00:19:38.520]   Sometimes I catch an answer, but I don't want to upset everyone.
[00:19:38.520 --> 00:19:43.560]   So please, uh, head over to this link and that's where you'll find all of the answers.
[00:19:43.560 --> 00:19:45.800]   And I want to make sure no one's upset.
[00:19:45.800 --> 00:19:51.120]   So going back to the original topic and two days, two lectures behind in time, here
[00:19:51.120 --> 00:19:53.640]   was the steps for our tumor classifier.
[00:19:53.640 --> 00:19:55.240]   We would load the data.
[00:19:55.240 --> 00:19:58.000]   Uh, it's not as simple as creating a dataset class.
[00:19:58.000 --> 00:20:02.200]   It took us a decent amount, one and a half hour to cover that knowledge,
[00:20:02.200 --> 00:20:04.400]   probably more for me to implement.
[00:20:04.480 --> 00:20:09.320]   Uh, so we figured out how to combine different files and load in the data.
[00:20:09.320 --> 00:20:13.440]   We figured out how to mark the tumors and classify them.
[00:20:13.440 --> 00:20:16.560]   Furthermore, how to segment them.
[00:20:16.560 --> 00:20:22.920]   So now I want to jump to units and I'll start talking about those.
[00:20:22.920 --> 00:20:27.600]   Uh, but I have another question for you all, which I'll write here.
[00:20:29.280 --> 00:20:31.760]   When would you segment?
[00:20:31.760 --> 00:20:48.560]   So when do you think, uh, segmentation comes into this pipeline?
[00:20:48.560 --> 00:20:51.600]   Uh, is it after classification?
[00:20:51.600 --> 00:20:55.760]   Uh, we we're learning about segmentation definitely after classification, but do
[00:20:55.760 --> 00:21:00.800]   you think it should happen after classification and how do do these two
[00:21:00.800 --> 00:21:02.800]   relate to each other in the process?
[00:21:02.800 --> 00:21:03.560]   That's the question.
[00:21:03.560 --> 00:21:06.080]   And I'll let you all answer and go back to the units.
[00:21:06.080 --> 00:21:09.520]   Um, meanwhile, I'll read Adil's answer.
[00:21:09.520 --> 00:21:12.680]   Image segmentation is used when we want to understand the entire
[00:21:12.680 --> 00:21:16.080]   context and contents of an image.
[00:21:16.080 --> 00:21:17.360]   A perfect answer.
[00:21:17.360 --> 00:21:18.840]   I would say, Adil, thanks for this.
[00:21:18.840 --> 00:21:24.560]   So, uh, yeah, this is, I'll always use this answer from now.
[00:21:24.600 --> 00:21:26.120]   Thanks on, thanks so much Adil.
[00:21:26.120 --> 00:21:32.080]   So for zoom backgrounds, you don't image segmentation and for
[00:21:32.080 --> 00:21:34.240]   cell phone calls and medical images as well.
[00:21:34.240 --> 00:21:34.960]   Thanks.
[00:21:34.960 --> 00:21:35.480]   Thanks for that.
[00:21:35.480 --> 00:21:39.160]   So now I'll jump to explaining units.
[00:21:39.160 --> 00:21:48.840]   Um, and I'm already out of chair, but these are really one of the, uh,
[00:21:48.840 --> 00:21:52.600]   revolutionary ideas, so to speak.
[00:21:52.600 --> 00:21:56.000]   So here's the original paper that came out in 2015.
[00:21:56.000 --> 00:22:00.760]   It's one of the most highly cited papers in deep learning, I believe.
[00:22:00.760 --> 00:22:04.640]   And the title is unit convolutional networks for
[00:22:04.640 --> 00:22:06.640]   biomedical image segmentation.
[00:22:06.640 --> 00:22:12.120]   I won't go through this paper because, um, I plan on doing an entire
[00:22:12.120 --> 00:22:13.400]   paper reading group around this.
[00:22:13.400 --> 00:22:18.160]   That should happen next week, I believe, or the week after, but we'll
[00:22:18.160 --> 00:22:19.840]   cover every single line in that.
[00:22:19.840 --> 00:22:21.840]   For now, I'll just cover the highlights.
[00:22:22.840 --> 00:22:28.520]   So we've come up with this problem now or this approach rather of labeling
[00:22:28.520 --> 00:22:31.160]   every single pixel inside of an image, right?
[00:22:31.160 --> 00:22:35.400]   That's what we have decided to do with our nasty tumors that we're trying to
[00:22:35.400 --> 00:22:41.080]   detect in the images or in the 3d CT scans rather to be more accurate.
[00:22:41.080 --> 00:22:43.800]   Uh, from the lunar 16 dataset.
[00:22:43.800 --> 00:22:48.160]   So we want every single pixel to have some meaning and have some label like so.
[00:22:49.280 --> 00:22:52.280]   But so far we've only learned about resonates, right?
[00:22:52.280 --> 00:22:58.480]   We don't know how or what can we do to change this.
[00:22:58.480 --> 00:23:02.160]   Now, this was supposed to be a question to you all, but the title gives it away.
[00:23:02.160 --> 00:23:03.200]   So I'm not asking it.
[00:23:03.200 --> 00:23:07.440]   The answer to the original question would be, how would we change this?
[00:23:07.440 --> 00:23:12.440]   We would have to change the architecture and what architecture would be used.
[00:23:12.440 --> 00:23:15.880]   Uh, well, I'm not smart enough to do that, uh, or invent an architecture,
[00:23:15.880 --> 00:23:19.880]   but I know which one to use because that's what the authors suggest.
[00:23:19.880 --> 00:23:24.720]   We'll use the unit architecture from the paper that I just mentioned.
[00:23:24.720 --> 00:23:30.520]   It's called unit, uh, spoiler alert because of the U shape.
[00:23:30.520 --> 00:23:37.040]   So, uh, it's quite symmetrical, but not exactly.
[00:23:37.040 --> 00:23:43.360]   If you look closely, sorry, uh, I was trying to zoom in and it's scrolled.
[00:23:44.000 --> 00:23:48.440]   So if you look closely and this image is from the people, so these are the
[00:23:48.440 --> 00:23:53.760]   number of channels up above, and these are the number of, these
[00:23:53.760 --> 00:23:55.160]   are the X and Y dimensions.
[00:23:55.160 --> 00:24:00.160]   So the input dimensions are five 70 by five, five 72 by five 72.
[00:24:00.160 --> 00:24:03.320]   And the output dimensions are slightly smaller.
[00:24:03.320 --> 00:24:12.240]   So it's not exactly symmetrical, but almost quite the reason this really stood
[00:24:12.240 --> 00:24:17.160]   out, this paper was one of the most, uh, innovative ones was because it really
[00:24:17.160 --> 00:24:19.800]   taught us how to work on segmentation problems.
[00:24:19.800 --> 00:24:25.400]   Now, so far we've learned plenty about, uh, convolutional neural networks, right?
[00:24:25.400 --> 00:24:30.400]   And let me actually switch to one note so that I can write stuff.
[00:24:30.400 --> 00:24:33.520]   I'll also need to pull out my tablet.
[00:24:33.520 --> 00:24:34.560]   Sorry about that.
[00:24:34.560 --> 00:24:35.360]   I dropped something.
[00:24:35.360 --> 00:24:40.960]   One second, guys, I'm being quite clumsy and dropping a lot of things today.
[00:24:41.960 --> 00:24:42.760]   Sorry about that.
[00:24:42.760 --> 00:24:49.520]   As you can see, it's, it's live and it's not recorded.
[00:24:49.520 --> 00:24:53.360]   Uh, and everyone is pretty much used to me messing up life.
[00:24:53.360 --> 00:24:56.800]   Awesome.
[00:24:56.800 --> 00:24:58.720]   I have the keyboard tucked away now.
[00:24:58.720 --> 00:25:03.520]   So let me share one note and make this full screen.
[00:25:08.440 --> 00:25:14.800]   So let me move towards, um, the unit architecture inside of the book.
[00:25:14.800 --> 00:25:19.840]   Here we go.
[00:25:19.840 --> 00:25:26.920]   So one of the problems we have learned so far, uh, let me select the right color.
[00:25:26.920 --> 00:25:32.960]   So far we just learned about this side of the thing.
[00:25:32.960 --> 00:25:36.320]   So let's not look at the right side, but just looking on the left-hand side,
[00:25:36.320 --> 00:25:40.240]   this is a simple convolutional network.
[00:25:40.240 --> 00:25:42.200]   Well, almost, but not quite right.
[00:25:42.200 --> 00:25:49.120]   You have an input, uh, you apply some convolutional layers, some activation
[00:25:49.120 --> 00:25:51.040]   function, uh, most of the times it's ReLU.
[00:25:51.040 --> 00:25:55.040]   Then you apply max pooling, uh, maybe, maybe not.
[00:25:55.040 --> 00:25:56.520]   And you get an output, right?
[00:25:56.520 --> 00:26:00.320]   Now there's a fundamental problem here.
[00:26:00.320 --> 00:26:05.960]   See with image segmentation, if you remember the original image, we had
[00:26:05.960 --> 00:26:08.000]   looked at it, please excuse me for a second.
[00:26:08.000 --> 00:26:21.280]   Sorry.
[00:26:21.280 --> 00:26:27.360]   Um, so the fundamental problem here is we're trying to label every
[00:26:27.360 --> 00:26:30.560]   single pixel inside of an image.
[00:26:31.960 --> 00:26:37.400]   But if you look at just the left-hand side of things for now, so just to the
[00:26:37.400 --> 00:26:40.960]   left-hand side of this red line, that's a CNN, right?
[00:26:40.960 --> 00:26:43.840]   So far in history, we haven't invented unit.
[00:26:43.840 --> 00:26:46.640]   We're going back in time, going to 2014.
[00:26:46.640 --> 00:26:52.440]   So how do we perform in image segmentation?
[00:26:52.440 --> 00:26:53.120]   Well, you can't.
[00:26:53.120 --> 00:26:53.880]   Right.
[00:26:53.880 --> 00:26:56.080]   That's why it was quite a challenging problem.
[00:26:56.080 --> 00:26:59.320]   It still is one of the difficult problems in images.
[00:26:59.400 --> 00:27:04.120]   Uh, or in a computer vision, rather not, not images.
[00:27:04.120 --> 00:27:05.520]   That's, that's a more accurate term.
[00:27:05.520 --> 00:27:12.840]   So the reason for that is as you go into the CNN, which is just the left-hand
[00:27:12.840 --> 00:27:17.760]   side, right, the outputs become smaller and smaller and greater in depth.
[00:27:17.760 --> 00:27:24.360]   So the depth increases and the height cross width decreases, right?
[00:27:24.400 --> 00:27:29.400]   That's in line with a unit image as well.
[00:27:29.400 --> 00:27:34.400]   So just please, uh, play, pay close attention to the left-hand side.
[00:27:34.400 --> 00:27:36.920]   This is just a CNN model.
[00:27:36.920 --> 00:27:42.960]   And as we go deeper into the model, the X by Y dimensions
[00:27:42.960 --> 00:27:44.160]   become smaller and smaller.
[00:27:44.160 --> 00:27:44.880]   Right?
[00:27:44.880 --> 00:27:47.240]   Why is that a problem for segmentation?
[00:27:47.240 --> 00:27:51.240]   Because the outputs need to overlap with the input.
[00:27:52.800 --> 00:27:58.080]   So this 32 by 32 output doesn't do it for us, right?
[00:27:58.080 --> 00:28:01.440]   Because let's assume this is a five 72 by five 72 input.
[00:28:01.440 --> 00:28:05.000]   You cannot do anything with the output at all.
[00:28:05.000 --> 00:28:07.280]   So I stopped sharing my screen.
[00:28:07.280 --> 00:28:14.040]   As you can see, if you want to label every single pixel inside of what you're
[00:28:14.040 --> 00:28:18.080]   seeing right now, me, my monitor, this PC in the background, right?
[00:28:19.560 --> 00:28:26.520]   For you to be able to label it, you can't do it, but just a 32 by 32 segment of it.
[00:28:26.520 --> 00:28:33.160]   So you need at least an image that is a good fraction of the input.
[00:28:33.160 --> 00:28:38.080]   And that is what the unit architecture proposed.
[00:28:38.080 --> 00:28:43.600]   So going back to sharing my screen, the key change here
[00:28:43.600 --> 00:28:46.000]   was these up sampling layers.
[00:28:48.320 --> 00:28:52.160]   So whenever we perform convolutions, right, we're down sampling the image.
[00:28:52.160 --> 00:28:57.040]   How are we doing that?
[00:28:57.040 --> 00:29:07.240]   By increasing the receptive field of every single unit inside of
[00:29:07.240 --> 00:29:09.040]   every layer as we go into depth.
[00:29:09.040 --> 00:29:14.840]   So this one output has an effective receptive field of six by six.
[00:29:14.840 --> 00:29:17.040]   Through these functions.
[00:29:17.080 --> 00:29:19.040]   This is covered in depth inside of the book.
[00:29:19.040 --> 00:29:23.920]   But traditional convolutional neural networks had this problem.
[00:29:23.920 --> 00:29:28.200]   Where as you perform more operations, right?
[00:29:28.200 --> 00:29:34.640]   This corresponds to more, but what we want is every single output corresponding
[00:29:34.640 --> 00:29:37.880]   to almost every single input pixel.
[00:29:37.880 --> 00:29:42.760]   So to deal with that, I am not a researcher.
[00:29:42.760 --> 00:29:45.920]   This is just based on my reading from the book and the citation
[00:29:45.920 --> 00:29:49.120]   graph of this paper, we had already tried.
[00:29:49.120 --> 00:29:51.680]   Not we, I'm not included.
[00:29:51.680 --> 00:29:54.480]   The searchers had already tried up sampling.
[00:29:54.480 --> 00:29:57.960]   So what does up sampling do quite simply?
[00:29:57.960 --> 00:30:01.800]   It restores these dimensions, these original dimensions.
[00:30:01.800 --> 00:30:10.120]   So instead of the depth going up, now it goes down and the
[00:30:10.120 --> 00:30:12.200]   height and width increase.
[00:30:13.000 --> 00:30:18.880]   So, height and width for an image is this height by width.
[00:30:18.880 --> 00:30:20.280]   And then you have depth.
[00:30:20.280 --> 00:30:24.320]   So for, for your YouTube screen right now, it should be
[00:30:24.320 --> 00:30:27.840]   one nine two zero by one zero eight zero by three.
[00:30:27.840 --> 00:30:29.280]   So three is the depth.
[00:30:29.280 --> 00:30:33.760]   This is the height and one nine two zero is the width.
[00:30:33.760 --> 00:30:36.800]   So when I say the height and width increase that for the
[00:30:36.800 --> 00:30:38.880]   image and the depth decreases.
[00:30:39.840 --> 00:30:42.640]   Let's not focus on image segmentation.
[00:30:42.640 --> 00:30:43.040]   Sorry.
[00:30:43.040 --> 00:30:44.600]   Let's not focus on tumors.
[00:30:44.600 --> 00:30:48.000]   Let's not focus on CT scans because they don't have a three,
[00:30:48.000 --> 00:30:49.360]   a depth of three.
[00:30:49.360 --> 00:30:54.640]   Just broadly speaking, this is why unit architecture really
[00:30:54.640 --> 00:30:58.880]   succeeded because it figured out how to make this up sampling
[00:30:58.880 --> 00:30:59.720]   work really well.
[00:30:59.720 --> 00:31:07.360]   They introduced this trick before Resonance of skip connections.
[00:31:09.640 --> 00:31:15.600]   So that when we up sample and we can learn about what I'm
[00:31:15.600 --> 00:31:18.640]   sampling is I'm skipping those details for now, but believe
[00:31:18.640 --> 00:31:19.920]   me, take my word for it.
[00:31:19.920 --> 00:31:24.880]   There is a possible operation in Pyrator that you can call to
[00:31:24.880 --> 00:31:35.360]   change this let's say 32 by 32 by one zero two four block to let's
[00:31:35.360 --> 00:31:42.200]   say two 56 by two 56 by I don't know, one 28 let's, let's call
[00:31:42.200 --> 00:31:46.400]   it that there's an operator that can do that, but the problem
[00:31:46.400 --> 00:31:50.280]   here that still remains is how do we connect this with the
[00:31:50.280 --> 00:31:52.400]   input image or the original layers?
[00:31:52.400 --> 00:31:57.600]   So for that, we add these skip connections where we just
[00:31:57.600 --> 00:32:02.480]   concatenate these features simply.
[00:32:03.800 --> 00:32:05.240]   There are a few more details.
[00:32:05.240 --> 00:32:11.800]   So there's also dropout being applied and a few more key
[00:32:11.800 --> 00:32:13.320]   details in the unit paper.
[00:32:13.320 --> 00:32:16.640]   I'll save that for a later discussion, but I really wanted
[00:32:16.640 --> 00:32:18.720]   to explain this at a very high level.
[00:32:18.720 --> 00:32:25.120]   We care about image segmentation because we want to segment out
[00:32:25.120 --> 00:32:32.680]   a tumor from a 3d scan, a 3d CT scan, rather to be able to
[00:32:32.680 --> 00:32:34.760]   live label every single pixel.
[00:32:34.760 --> 00:32:38.400]   We need to find a way to connect the input to output pixels.
[00:32:38.400 --> 00:32:39.460]   Right.
[00:32:39.460 --> 00:32:50.720]   For that, first of all, we add a unit and then we add skip connections.
[00:32:50.720 --> 00:32:54.220]   Right.
[00:32:54.220 --> 00:32:58.000]   There are a few more details.
[00:32:58.000 --> 00:33:00.520]   Of course you need to use weighted loss.
[00:33:00.880 --> 00:33:05.680]   You need to use a dropouts, different forms of regularization for sure.
[00:33:05.680 --> 00:33:12.280]   But broadly speaking inside of a unit, this left side of the layer
[00:33:12.280 --> 00:33:20.040]   is known as an encoder or contraction.
[00:33:20.040 --> 00:33:27.880]   And this right side is known as I'm out of space guys.
[00:33:27.880 --> 00:33:31.320]   So I'll just mention it to be a decoder or expansion.
[00:33:31.320 --> 00:33:33.400]   So we take an input.
[00:33:33.400 --> 00:33:38.640]   Our input gets encoded.
[00:33:38.640 --> 00:33:43.600]   This gets trained somehow.
[00:33:43.600 --> 00:33:44.840]   We don't care about that.
[00:33:44.840 --> 00:33:49.640]   Then we decode this image to give us an output.
[00:33:49.640 --> 00:33:56.840]   Now, how do we create these labels that we saw over the image?
[00:33:56.840 --> 00:33:57.780]   Right.
[00:33:57.900 --> 00:34:03.220]   That's, that's the scientific, uh, tumor classifying questionnaire that still remains.
[00:34:03.220 --> 00:34:08.860]   So the output gives us a mask that we put over the image.
[00:34:08.860 --> 00:34:15.660]   So this is the mask being visualized and that is the unit architecture.
[00:34:15.660 --> 00:34:19.660]   So inside of the original people, I'm not going to cover the exact
[00:34:19.660 --> 00:34:21.540]   dimensions because we eventually will.
[00:34:21.540 --> 00:34:26.940]   But this was the original architecture and here were the key points from it.
[00:34:27.460 --> 00:34:32.180]   So it follows a U shaped network, hence the name unit, right?
[00:34:32.180 --> 00:34:33.700]   Looks like a U to me.
[00:34:33.700 --> 00:34:37.820]   It has this encoder decoder set up.
[00:34:37.820 --> 00:34:43.620]   And again, this is based on the book and based on my verifying the fact, but they
[00:34:43.620 --> 00:34:50.460]   introduced skip connection before resonance and they take a weighted loss, uh, for
[00:34:50.460 --> 00:34:52.220]   reasons that I won't explain today.
[00:34:54.460 --> 00:34:57.620]   Here are the key summaries, uh, key summary points of it.
[00:34:57.620 --> 00:35:00.980]   And let me look at any questions that might be coming in.
[00:35:00.980 --> 00:35:10.100]   Um, this is by Vasudev.
[00:35:10.100 --> 00:35:16.060]   So Vasudev says we'd want to segment, uh, after classification because we want to
[00:35:16.060 --> 00:35:18.340]   see final level details, but human is present.
[00:35:18.340 --> 00:35:19.340]   Um, okay.
[00:35:19.340 --> 00:35:21.500]   Let's see if that is the right answer or not.
[00:35:22.100 --> 00:35:26.900]   My guess is we should segment before classification so that we can only send
[00:35:26.900 --> 00:35:28.860]   the part of the image that may contain a tumor.
[00:35:28.860 --> 00:35:30.460]   So two conflicting answers.
[00:35:30.460 --> 00:35:31.620]   Let's see which one is right.
[00:35:31.620 --> 00:35:38.500]   Um, Sidheshwar says to train the model, we need to segment it before.
[00:35:38.500 --> 00:35:39.580]   Um, okay.
[00:35:39.580 --> 00:35:40.660]   Let's see if that is right.
[00:35:40.660 --> 00:35:48.700]   Before classification, maybe we can segment, not sure how, but then crop and classify.
[00:35:48.780 --> 00:35:53.500]   Um, we have more answers now in favor of segmenting earlier.
[00:35:53.500 --> 00:35:54.340]   Let's see.
[00:35:54.340 --> 00:35:56.220]   Uh, Mateo has cheated.
[00:35:56.220 --> 00:35:57.700]   He's looked at the answer from the book.
[00:35:57.700 --> 00:35:58.380]   It appears.
[00:35:58.380 --> 00:36:00.140]   I'm just looking, Mateo.
[00:36:00.140 --> 00:36:01.100]   It's still the right answer.
[00:36:01.100 --> 00:36:05.100]   Uh, segmentation is performed before classification to isolate modules.
[00:36:05.100 --> 00:36:07.900]   Um, let's see if he's right or not.
[00:36:07.900 --> 00:36:10.140]   UNIT has been out for a few years.
[00:36:10.140 --> 00:36:11.580]   Do you know of any segment?
[00:36:11.580 --> 00:36:16.940]   Uh, so to answer your question, I'll come back to this afterwards.
[00:36:17.060 --> 00:36:20.620]   Um, there have been insanely significant improvements for sure.
[00:36:20.620 --> 00:36:23.500]   Um, but I'll come back to that later.
[00:36:23.500 --> 00:36:26.580]   Up sampling is basically resizing.
[00:36:26.580 --> 00:36:27.460]   Not exactly.
[00:36:27.460 --> 00:36:29.780]   We will show I'll, I'll explain that further.
[00:36:29.780 --> 00:36:36.660]   So to break, uh, the final answer, we would want to segment before we
[00:36:36.660 --> 00:36:40.660]   classify, because again, uh, Mateo has looked at the book, so he's of course,
[00:36:40.660 --> 00:36:41.060]   right.
[00:36:41.060 --> 00:36:43.260]   And his logic is right as well.
[00:36:43.260 --> 00:36:46.540]   Of course, the logic for anyone who voted in favor.
[00:36:47.020 --> 00:36:51.780]   But the reason is, um, I'm trying to find some white space.
[00:36:51.780 --> 00:37:01.740]   So after you have your input image, you can segment a tumor, right?
[00:37:01.740 --> 00:37:09.060]   And then you can classify if it is, uh, malignant or been malignant or cancerous.
[00:37:09.060 --> 00:37:13.340]   So if it's cancerous or not, so that's why you would want to perform
[00:37:13.340 --> 00:37:15.700]   segmentation before classification.
[00:37:16.300 --> 00:37:21.540]   And after segmenting, you can then classify the tumor if it's
[00:37:21.540 --> 00:37:24.780]   cancerous or non-cancerous.
[00:37:24.780 --> 00:37:28.660]   And then you can, uh, perform other operations.
[00:37:28.660 --> 00:37:32.100]   I'm skipping the fact that you need to figure out a very tough way of
[00:37:32.100 --> 00:37:34.900]   inputting all of these details.
[00:37:34.900 --> 00:37:40.060]   Uh, you, we essentially took an entire lecture, a live stream to figure that
[00:37:40.060 --> 00:37:44.260]   out that happens before this, but after all of that is figured out, you can then
[00:37:44.260 --> 00:37:46.140]   segment the tumor and then classify it.
[00:37:47.140 --> 00:37:48.300]   So that's the short answer.
[00:37:48.300 --> 00:37:51.100]   Sorry.
[00:37:51.100 --> 00:37:52.300]   That's the complete answer.
[00:37:52.300 --> 00:37:55.300]   I'll take another sip of water.
[00:37:55.300 --> 00:38:10.180]   Someone had kindly pointed out that my throat really dries up by
[00:38:10.180 --> 00:38:12.620]   the end of the session sessions.
[00:38:12.620 --> 00:38:14.700]   Uh, so I'm trying to be proactive about that.
[00:38:15.700 --> 00:38:19.940]   Uh, so I apologize to anyone watching right now, later, uh, for the excessive
[00:38:19.940 --> 00:38:22.980]   breaks, but that's, that's the reason for those.
[00:38:22.980 --> 00:38:27.260]   So now that we know, how are we doing this and what are we doing this with?
[00:38:27.260 --> 00:38:28.740]   Let's see, how can we do it?
[00:38:28.740 --> 00:38:30.780]   It's simple 38 lines of codes.
[00:38:30.780 --> 00:38:32.940]   Uh, and how did I find this repository?
[00:38:32.940 --> 00:38:40.380]   I went to my favorite, uh, website, um, papers with code.
[00:38:40.380 --> 00:38:45.020]   And whenever you want to do this, uh, so it's for any code related to any
[00:38:45.020 --> 00:38:48.940]   paper, you can search for it on papers with good, and I just searched for unit.
[00:38:48.940 --> 00:38:50.140]   It took me to the paper.
[00:38:50.140 --> 00:38:50.940]   I believe.
[00:38:50.940 --> 00:38:59.220]   I think so.
[00:38:59.220 --> 00:39:01.860]   This is the paper and now you can see the code.
[00:39:01.860 --> 00:39:07.180]   Actually, um, I'll point this out further.
[00:39:07.180 --> 00:39:12.060]   So to answer your question, beaver Basu, uh, if you're trying to find,
[00:39:12.460 --> 00:39:16.660]   let me close these tabs, if you're trying to find techniques that build on top of
[00:39:16.660 --> 00:39:21.780]   these, right, or are an improvement for improvement, you'd want to look for image
[00:39:21.780 --> 00:39:24.380]   segment, semantic segmentation models.
[00:39:24.380 --> 00:39:33.780]   So if you go in this tab on papers with code.com, you can see, uh, the year and
[00:39:33.780 --> 00:39:38.620]   the number of papers that have come out over different techniques, and then
[00:39:38.620 --> 00:39:40.420]   you can find different techniques.
[00:39:40.420 --> 00:39:45.260]   So I would assume that the things that have come out later might be better, but
[00:39:45.260 --> 00:39:49.420]   this is a good area to explore different methods.
[00:39:49.420 --> 00:39:58.020]   And if I'm trying to find the code, I can simply click C code that takes me to this
[00:39:58.020 --> 00:40:05.180]   wonderful GitHub page, um, by miles, IAL.
[00:40:05.180 --> 00:40:08.300]   Um, and it's just 38 lines of code.
[00:40:08.420 --> 00:40:12.540]   This is why, when I mentioned in the beginning of this session, right, in the
[00:40:12.540 --> 00:40:16.460]   first lecture, uh, whatever framework you choose to cover, it doesn't really matter.
[00:40:16.460 --> 00:40:20.140]   So I'll cover the code inside of the book later.
[00:40:20.140 --> 00:40:24.180]   I'm doing something really silly right now, which is I'm showing you all of these
[00:40:24.180 --> 00:40:28.380]   layers that simply take care of pretty much everything inside of the book.
[00:40:28.380 --> 00:40:32.860]   We actually implement these things, but since it's been six years, which in deep
[00:40:32.860 --> 00:40:37.700]   learning is essentially like 600 years since the paper come out.
[00:40:38.700 --> 00:40:44.980]   Now this state of this state of the art technique that never really existed in
[00:40:44.980 --> 00:40:50.020]   2015 is just, is just a few lines of calls that we make.
[00:40:50.020 --> 00:40:55.660]   And it, I, it would be the same with, um, Keras as well.
[00:40:55.660 --> 00:41:00.420]   So no matter what framework you choose to learn, we all have chosen PyTorch or, uh,
[00:41:00.420 --> 00:41:03.340]   whatever hyper framework you choose to use.
[00:41:03.340 --> 00:41:05.540]   I invented that term right now.
[00:41:06.220 --> 00:41:11.300]   Sorry if it's misleading, but whatever meta framework you choose to use after
[00:41:11.300 --> 00:41:17.700]   this fast AI, uh, or any other framework that sits on top of PyTorch, most of them
[00:41:17.700 --> 00:41:21.540]   would have these functions implemented to make your life easier.
[00:41:21.540 --> 00:41:25.020]   So that's why it really doesn't matter what you do afterwards, but it's
[00:41:25.020 --> 00:41:26.780]   important to understand these details.
[00:41:26.780 --> 00:41:32.220]   So, uh, here's what we do to implement a unit.
[00:41:32.900 --> 00:41:40.780]   We call these down sampling layers and we go from smaller number of channels to
[00:41:40.780 --> 00:41:45.140]   greater number of channels at a very higher level, and then we up sample
[00:41:45.140 --> 00:41:46.980]   in an reverse direction.
[00:41:46.980 --> 00:41:53.500]   So we went from 64 down to one 24, 10, 24, and now we go from 10, 24 back to 64.
[00:41:53.500 --> 00:41:56.260]   And the forward pass.
[00:41:56.260 --> 00:42:02.260]   Uh, so this was for the model and for the forward pass, uh, we just call these
[00:42:02.260 --> 00:42:03.620]   and return the logits.
[00:42:03.620 --> 00:42:04.620]   That's it.
[00:42:04.620 --> 00:42:06.340]   That's, that's our unit.
[00:42:06.340 --> 00:42:08.140]   That's the entire architecture.
[00:42:08.140 --> 00:42:12.980]   That's the entire nine pages of paper and 38 lines of code.
[00:42:12.980 --> 00:42:19.060]   And let's see, let's look at another example of where this would be useful.
[00:42:19.060 --> 00:42:20.980]   Let me close this tab.
[00:42:20.980 --> 00:42:23.260]   Um, this one as well.
[00:42:23.260 --> 00:42:27.300]   So this we had, I believe sent as an email to everyone.
[00:42:27.300 --> 00:42:28.500]   So I won't cover this.
[00:42:28.500 --> 00:42:30.140]   Uh, let me close this tab.
[00:42:31.060 --> 00:42:33.420]   And we have, okay.
[00:42:33.420 --> 00:42:35.180]   Let's implementation as well.
[00:42:35.180 --> 00:42:38.140]   So I leave this as a homework.
[00:42:38.140 --> 00:42:54.220]   If anyone is interested, you would convert this into Pytorch.
[00:42:54.220 --> 00:42:56.420]   So that's, that's my suggested homework for this week.
[00:42:56.420 --> 00:42:58.700]   One of the suggested ones for sure.
[00:42:58.860 --> 00:43:05.420]   Um, and I want to actually cover this small report by my colleague, Stacey.
[00:43:05.420 --> 00:43:08.580]   So this is image mass for semantic segmentation.
[00:43:08.580 --> 00:43:12.060]   Let me also paste the link to this.
[00:43:12.060 --> 00:43:21.900]   Let me know in case that doesn't take you to the right one.
[00:43:21.900 --> 00:43:24.780]   Actually, let me just link off the collab repo.
[00:43:24.780 --> 00:43:26.100]   Oops.
[00:43:27.220 --> 00:43:30.340]   I press some button and now everything's missing.
[00:43:30.340 --> 00:43:34.580]   I think I've messed up.
[00:43:34.580 --> 00:43:38.940]   Um, Oh no.
[00:43:38.940 --> 00:43:39.580]   What did I do?
[00:43:39.580 --> 00:43:46.980]   Sorry guys.
[00:43:46.980 --> 00:43:50.180]   I, I can't see anything on my screen right now.
[00:43:50.180 --> 00:43:51.580]   I'm trying to debug that.
[00:43:51.580 --> 00:43:57.140]   Give me one second.
[00:43:57.140 --> 00:43:58.740]   I'll try to unplug my monitor.
[00:43:58.740 --> 00:44:14.820]   Sorry about that.
[00:44:14.820 --> 00:44:16.380]   I should be back now.
[00:44:16.380 --> 00:44:21.220]   And I still don't see anything on my screen, which is really strange.
[00:44:21.220 --> 00:44:27.020]   Oh, I see what I did.
[00:44:28.020 --> 00:44:29.340]   Sorry.
[00:44:29.340 --> 00:44:30.740]   Let me quickly fix this.
[00:44:30.740 --> 00:44:44.060]   Sorry about this, guys.
[00:44:44.060 --> 00:44:45.420]   This should just take a second.
[00:44:45.420 --> 00:44:48.460]   System preferences.
[00:44:48.460 --> 00:44:54.500]   I want to mirror my display.
[00:44:55.620 --> 00:44:57.180]   Should be fine now.
[00:44:57.180 --> 00:44:57.940]   Sorry about that.
[00:44:57.940 --> 00:45:01.980]   I'll turn back my camera.
[00:45:01.980 --> 00:45:03.660]   Okay.
[00:45:03.660 --> 00:45:07.180]   Could anyone please confirm if things are back?
[00:45:07.180 --> 00:45:10.220]   Let me share my screen again.
[00:45:10.220 --> 00:45:18.340]   Apparently I hit the shortcut on my MacBook to change the screen sharing to
[00:45:18.340 --> 00:45:19.100]   shared screen.
[00:45:19.100 --> 00:45:23.100]   Uh, thanks.
[00:45:23.100 --> 00:45:23.660]   Thanks for that.
[00:45:23.660 --> 00:45:24.860]   I'm really sorry about this.
[00:45:24.860 --> 00:45:30.940]   I, I would ideally like to learn what that shortcut is, but I started sweating
[00:45:30.940 --> 00:45:32.740]   because of it, not just because of it.
[00:45:32.740 --> 00:45:33.220]   I'm kidding.
[00:45:33.220 --> 00:45:34.700]   Sorry about that guys.
[00:45:34.700 --> 00:45:42.900]   Uh, so I was going to link off the collab notebook.
[00:45:42.900 --> 00:45:53.220]   By the way, does anyone know what the shortcut is to enable or
[00:45:53.220 --> 00:45:54.500]   disable screen mirroring?
[00:45:55.300 --> 00:45:56.020]   Just curious.
[00:45:56.020 --> 00:46:02.420]   So I'm, I'm going to walk through, uh, this collab and I've not authored it.
[00:46:02.420 --> 00:46:04.500]   This spins, this has been written by my colleagues.
[00:46:04.500 --> 00:46:09.020]   They see, uh, she's also an amazing machine learning engineer at Bateson
[00:46:09.020 --> 00:46:14.460]   biases, uh, and I'll shamelessly use her work to explain something in our session.
[00:46:14.460 --> 00:46:23.820]   This shows you how to, uh, segment every pixel inside of this particular data set.
[00:46:24.100 --> 00:46:25.220]   What is this data set?
[00:46:25.220 --> 00:46:31.340]   Uh, this is one of the cell driving car datasets called Berkeley
[00:46:31.340 --> 00:46:33.020]   deep drive, a hundred K data set.
[00:46:33.020 --> 00:46:36.540]   You can take a look at it, but essentially it's for cell drying cars.
[00:46:36.540 --> 00:46:42.460]   Uh, those were one of the suggested, uh, applications by all of you
[00:46:42.460 --> 00:46:43.740]   and you were quite right there.
[00:46:43.740 --> 00:46:47.100]   So we're going to be taking a look at that.
[00:46:47.100 --> 00:46:51.500]   This also shows you how to use weights and biases effectively for tracking such
[00:46:51.500 --> 00:46:53.860]   experiments and also visualizing your results.
[00:46:54.420 --> 00:46:57.580]   Uh, you can run this to understand that if you're interested, I'll
[00:46:57.580 --> 00:46:59.220]   just talk about the important bits.
[00:46:59.220 --> 00:47:03.820]   So Stacy here uses fast AI, uh, to do the classification.
[00:47:03.820 --> 00:47:08.740]   And I'm again, trying to amplify my point that it's really straightforward
[00:47:08.740 --> 00:47:09.940]   to do this in fast AI.
[00:47:09.940 --> 00:47:14.380]   So we import all of our required frameworks.
[00:47:14.380 --> 00:47:18.660]   We also install one DB since it doesn't come on Colab by default.
[00:47:19.460 --> 00:47:23.220]   Uh, you'll have to log in once so that you can log all of
[00:47:23.220 --> 00:47:24.660]   your metrics to your dashboard.
[00:47:24.660 --> 00:47:28.860]   And if you're curious about how that looks, uh, like
[00:47:28.860 --> 00:47:40.060]   this is the workspace.
[00:47:40.060 --> 00:47:42.380]   And essentially you can see all of these charts.
[00:47:42.380 --> 00:47:46.740]   Uh, so as a practitioner, you really care about different experiments.
[00:47:46.780 --> 00:47:53.140]   And as you can see, you can skim through all of the 168, uh, experiments
[00:47:53.140 --> 00:47:55.060]   or runs that have been tracked.
[00:47:55.060 --> 00:48:00.180]   And really after doing so many experiments, just focus on
[00:48:00.180 --> 00:48:01.500]   the ones that matter to you.
[00:48:01.500 --> 00:48:05.940]   So I can hide the ones that I don't want to look at and find those details.
[00:48:05.940 --> 00:48:11.060]   I can also take a look at the last table that has all of these.
[00:48:11.060 --> 00:48:16.580]   And then I can also perform filters on top of it, do different things.
[00:48:16.740 --> 00:48:19.860]   I won't go into depth of this, but if you're curious, you can check it out.
[00:48:19.860 --> 00:48:23.300]   Uh, you can check out the Colab notebook that should take you here.
[00:48:23.300 --> 00:48:26.980]   And now I'll try to find the Colab notebook.
[00:48:26.980 --> 00:48:27.500]   Here it is.
[00:48:27.500 --> 00:48:31.980]   So we've logged into weights and biases and you'll have to download
[00:48:31.980 --> 00:48:35.060]   this dataset also because it's not there on Colab or your
[00:48:35.060 --> 00:48:36.380]   Google drive for the first time.
[00:48:36.380 --> 00:48:46.660]   So for this particular example, here are the classes that we care about.
[00:48:47.020 --> 00:48:50.260]   So we're trying to segment between road, sidewalk, building, wall, fence,
[00:48:50.260 --> 00:48:51.660]   so on and so forth.
[00:48:51.660 --> 00:48:53.780]   These are the labels that we care about.
[00:48:53.780 --> 00:49:00.940]   We're going to read this in and we're going to read, uh, in an image mask.
[00:49:00.940 --> 00:49:06.620]   Of course, this would be provided along with the dataset and we would just operate.
[00:49:06.620 --> 00:49:10.620]   Ideally, it's always a good idea to just start with a fraction of the dataset.
[00:49:10.620 --> 00:49:11.700]   So we're going to use that.
[00:49:11.700 --> 00:49:15.420]   And I can glance over these details.
[00:49:15.420 --> 00:49:21.580]   So this is a fast detail that helps us create, I think a data class.
[00:49:21.580 --> 00:49:26.900]   So I can glance over this and we've written a fast callback.
[00:49:26.900 --> 00:49:30.980]   If you don't know what callbacks are, it's totally fine, but we're writing a way to.
[00:49:30.980 --> 00:49:34.260]   I'm trying to simplify it in my head.
[00:49:34.260 --> 00:49:40.900]   Um, we're trying to find a simple way of logging this onto our
[00:49:40.900 --> 00:49:42.340]   weights and biases dashboard.
[00:49:42.340 --> 00:49:43.620]   So that's what this is doing.
[00:49:44.060 --> 00:49:44.340]   Okay.
[00:49:44.340 --> 00:49:50.820]   Then we define accuracy and traffic accuracy on those things.
[00:49:50.820 --> 00:49:56.460]   And here are our hyper parameters that we can configure.
[00:49:56.460 --> 00:50:01.540]   So we define the image size, the batch size, epochs.
[00:50:01.540 --> 00:50:09.540]   Uh, now again, I've skipped over this detail for now, but here we use a resonate 18 backbone.
[00:50:10.220 --> 00:50:15.340]   So remember when we looked at the unit architecture, this could
[00:50:15.340 --> 00:50:17.500]   have different backbones also.
[00:50:17.500 --> 00:50:21.540]   So you would, you could change this network for now.
[00:50:21.540 --> 00:50:26.260]   We just use a resonate 18 encoder.
[00:50:26.260 --> 00:50:29.980]   So for the encoder backbone, this is what we're going to use.
[00:50:29.980 --> 00:50:34.140]   And we'll define other hyper parameters that I can skip over.
[00:50:34.140 --> 00:50:39.020]   Uh, remember we would want to perform some image augmentation as well.
[00:50:39.020 --> 00:50:40.220]   So that's what we define.
[00:50:40.220 --> 00:50:42.740]   And finally, here's our model.
[00:50:42.740 --> 00:50:46.340]   So we create a unit learner and that's about it.
[00:50:46.340 --> 00:50:50.180]   And then you pass all of the data set class to it.
[00:50:50.180 --> 00:50:55.860]   Uh, the architecture, the pre-trained configuration, matrix, weight, decay,
[00:50:55.860 --> 00:51:00.300]   batch normalization, and these callbacks that we have just defined.
[00:51:00.300 --> 00:51:02.100]   And now you can start training the model.
[00:51:02.100 --> 00:51:03.540]   That's it.
[00:51:05.100 --> 00:51:09.380]   That's how easy it is to get started with units also using fast
[00:51:09.380 --> 00:51:11.380]   AI or I assume other frameworks.
[00:51:11.380 --> 00:51:17.620]   So, um, I think I've covered everything I wanted to hear.
[00:51:17.620 --> 00:51:19.780]   Let me take a look if there are any questions.
[00:51:19.780 --> 00:51:21.740]   Okay.
[00:51:21.740 --> 00:51:25.700]   I don't see any questions, but I'll take another pause for any questions.
[00:51:25.700 --> 00:51:32.620]   So any, uh, before we move on further from here, any questions related to this
[00:51:32.620 --> 00:51:36.980]   particular collab notebook or unit architecture or image segmentation, please.
[00:51:36.980 --> 00:51:38.940]   Uh, this would be a good time to ask those.
[00:51:38.940 --> 00:51:52.580]   Let me also take a look at the YouTube chart.
[00:51:52.580 --> 00:51:54.660]   Um, Nope.
[00:51:54.660 --> 00:51:55.700]   I don't see any questions.
[00:51:55.700 --> 00:51:57.780]   Hopefully people are asking them in the right place.
[00:52:02.500 --> 00:52:02.820]   Awesome.
[00:52:02.820 --> 00:52:03.620]   This is incredible.
[00:52:03.620 --> 00:52:04.060]   See you guys.
[00:52:04.060 --> 00:52:06.460]   My scheme of gathering more likes is working.
[00:52:06.460 --> 00:52:08.460]   Now I can brag to my colleagues.
[00:52:08.460 --> 00:52:14.740]   So instead of replying, once I did three different replies, see, I'm just kidding.
[00:52:14.740 --> 00:52:15.260]   About that.
[00:52:15.260 --> 00:52:18.300]   Um, perfect.
[00:52:18.300 --> 00:52:20.260]   Let me close this tab as well.
[00:52:20.260 --> 00:52:28.220]   So I think next up in my agenda was, um, yes, I'm on the right path to cover
[00:52:28.220 --> 00:52:30.300]   image augmentations in a greater depth.
[00:52:30.300 --> 00:52:31.940]   Uh, Oh, that's wonderful.
[00:52:31.940 --> 00:52:33.780]   I'm getting more likes triple six.
[00:52:33.780 --> 00:52:34.460]   That's not good.
[00:52:34.460 --> 00:52:37.340]   Um, that's a scary number.
[00:52:37.340 --> 00:52:40.660]   So, uh, jokes aside enough jokes.
[00:52:40.660 --> 00:52:44.020]   Let's let's take a look at image augmentation in greater depth.
[00:52:44.020 --> 00:52:50.060]   This was a three year old blog post by me, which means it's absolutely, uh, outdated
[00:52:50.060 --> 00:52:54.860]   and a terrible idea to take a look at anything that's so old in machine
[00:52:54.860 --> 00:52:56.500]   learning, unless it's conceptual.
[00:52:56.500 --> 00:52:57.660]   And that's what this is.
[00:52:58.020 --> 00:53:02.940]   So instead of talking about the fast AI framework, I'll point you towards, uh,
[00:53:02.940 --> 00:53:12.460]   the details inside of, uh, this blog, essentially just the concepts.
[00:53:12.460 --> 00:53:19.340]   So here are a few use cases I had covered and, uh, I'm trying to revisit image
[00:53:19.340 --> 00:53:22.100]   augmentation and cover them in a greater depth this time.
[00:53:22.100 --> 00:53:24.700]   So let me post a link to this.
[00:53:25.700 --> 00:53:25.700]   Okay.
[00:53:25.700 --> 00:53:38.140]   There we go.
[00:53:38.140 --> 00:53:44.460]   Um, I'm trying to find a few use cases of, you know, building a soda
[00:53:44.460 --> 00:53:48.420]   neighborhood swimming pool detector, uh, quite an important task, building a
[00:53:48.420 --> 00:53:52.980]   medical image OCR, uh, let's see if that's important, a Google lens rip off.
[00:53:53.100 --> 00:53:58.180]   Um, a startup and a license plate detector.
[00:53:58.180 --> 00:54:00.500]   I've covered this article a few times.
[00:54:00.500 --> 00:54:02.340]   Uh, and it's of course outdated now.
[00:54:02.340 --> 00:54:05.340]   So this is another suggested homework for you all.
[00:54:05.340 --> 00:54:21.580]   So please rewrite this with the latest version of fast AI.
[00:54:22.020 --> 00:54:25.860]   Um, this is also one of my most well-recognized cognitive kernels, which
[00:54:25.860 --> 00:54:27.260]   means there's a demand for this.
[00:54:27.260 --> 00:54:32.180]   Uh, the recognition of course, doesn't have any correlation with the amount
[00:54:32.180 --> 00:54:37.180]   of effort that's gone into anything or the importance of it, uh, a few times.
[00:54:37.180 --> 00:54:40.700]   But, uh, that is just to suggest to you that, uh, this might be a
[00:54:40.700 --> 00:54:41.940]   good investment of your time.
[00:54:41.940 --> 00:54:44.220]   So I'd written this with fast AI v1.
[00:54:44.220 --> 00:54:46.420]   You could totally rewrite it with fast AI v2.
[00:54:46.420 --> 00:54:51.660]   Um, so the reason of using data augmentation anywhere is because it's
[00:54:51.660 --> 00:54:55.940]   one of the most common regularization techniques we use regularization
[00:54:55.940 --> 00:55:02.020]   to reduce overfitting and reduce image augmentation or data
[00:55:02.020 --> 00:55:03.260]   augmentation for the same.
[00:55:03.260 --> 00:55:07.060]   So whenever we're trying to augment our data, we're trying to change it a
[00:55:07.060 --> 00:55:12.460]   little bit so that the performance of our model can improve a little.
[00:55:12.460 --> 00:55:17.460]   And that is still cheaper than getting more data a lot of the times.
[00:55:17.900 --> 00:55:22.060]   So, uh, you know, for self-driving cars, rather than driving your Tesla
[00:55:22.060 --> 00:55:25.860]   around for another few hours, if you can image, if you can augment
[00:55:25.860 --> 00:55:29.780]   that image very easily, that is much, much, much easier.
[00:55:29.780 --> 00:55:35.340]   Uh, I mean, sure it might take your, uh, might take Tesla's coding
[00:55:35.340 --> 00:55:42.260]   department or Tesla's R&D department to come up with that for in a week,
[00:55:42.260 --> 00:55:47.100]   maybe longer than that in terms of software time, but that still
[00:55:47.100 --> 00:55:50.140]   saves the effort of collecting more data in the future.
[00:55:50.140 --> 00:55:53.820]   So that's why it's quite an important time, important task.
[00:55:53.820 --> 00:55:57.060]   Why do image augmentation work?
[00:55:57.060 --> 00:56:01.580]   I've already covered this, but even a small rotation of two degrees might
[00:56:01.580 --> 00:56:05.900]   make a huge difference to the model, even though it doesn't make
[00:56:05.900 --> 00:56:07.660]   a difference to the human eye.
[00:56:07.660 --> 00:56:12.780]   So what I'd done here, fast AI provides you with a lot of defaults and I just
[00:56:13.100 --> 00:56:18.020]   copied the default augmentation and you can do it in albumentations also.
[00:56:18.020 --> 00:56:21.220]   So just copy over the default stuff and see how does that work for you.
[00:56:21.220 --> 00:56:27.540]   And what I did was I trained one model with the augmentations off versus on,
[00:56:27.540 --> 00:56:30.700]   and I got much better results with the image augmentations on.
[00:56:30.700 --> 00:56:36.980]   So that was a just works example, but here are the list of transform support
[00:56:36.980 --> 00:56:39.500]   with by fast AI back at that time.
[00:56:40.060 --> 00:56:46.020]   And we've taken a look at albumentations also, but let's just run through these
[00:56:46.020 --> 00:56:50.020]   to try to understand the use cases of different image augmentation.
[00:56:50.020 --> 00:56:53.860]   Ideally we're learning PyTorch to apply to different areas, right?
[00:56:53.860 --> 00:56:55.220]   To apply to different things.
[00:56:55.220 --> 00:56:57.780]   So that's, I'd like to cover all of these.
[00:56:57.780 --> 00:57:02.740]   So instead of naming all of those, I'll give you a second to read through all of
[00:57:02.740 --> 00:57:10.940]   these and I let you ask any question related to any or whichever you think is
[00:57:10.940 --> 00:57:12.980]   new to you or fancy to you.
[00:57:12.980 --> 00:57:16.900]   I'm happy to go into depth, but I let you take a read of all of these.
[00:57:16.900 --> 00:57:30.460]   Do I see any questions?
[00:57:30.460 --> 00:57:31.140]   No, not yet.
[00:57:31.140 --> 00:57:36.140]   Okay.
[00:57:36.140 --> 00:57:38.660]   Let's start with different ones.
[00:57:38.660 --> 00:57:42.620]   So this code was stolen from the fast AI documentation.
[00:57:42.620 --> 00:57:47.860]   I do not claim ownership of it and I just used it to iterate over the sample image.
[00:57:47.860 --> 00:57:51.980]   So in this blog, I'll use the sample image of the skewed dog every single time to
[00:57:51.980 --> 00:57:56.300]   show you how different augmentations work and then also when use case where it's
[00:57:56.300 --> 00:57:57.260]   actually useful.
[00:57:57.260 --> 00:57:58.580]   So this is a base case.
[00:57:59.020 --> 00:58:03.660]   So you can always go back to this image, think of it in your head and come back to
[00:58:03.660 --> 00:58:04.160]   it.
[00:58:04.160 --> 00:58:06.940]   And then I'll show an actual example where it's useful.
[00:58:06.940 --> 00:58:08.700]   So we're rotating the image.
[00:58:08.700 --> 00:58:13.660]   No one takes a dog image like so or in inverted fashion.
[00:58:13.660 --> 00:58:18.380]   A real world use case where this would be useful is let's say satellite images.
[00:58:18.380 --> 00:58:24.060]   So it's summers maybe in your side of the world and you're trying to build a
[00:58:24.060 --> 00:58:26.140]   soda swimming pool detector.
[00:58:26.660 --> 00:58:30.540]   So you can rotate your images and these two may still look like satellite images
[00:58:30.540 --> 00:58:33.740]   and seem like new images.
[00:58:33.740 --> 00:58:36.700]   So that would work.
[00:58:36.700 --> 00:58:42.420]   Would anyone like to point out, uh, would this technique work for segmentation?
[00:58:42.420 --> 00:58:48.220]   So my question to you is if you're trying to rotate the images, is that a good idea
[00:58:48.220 --> 00:58:49.460]   for image segmentation?
[00:58:49.460 --> 00:58:55.780]   So if I just rotate my image, will that cause an increase in performance for image
[00:58:55.780 --> 00:58:56.780]   segmentation?
[00:58:56.780 --> 00:59:01.380]   Um, I'll wait for your answers to come in just to, just to ask it again.
[00:59:01.380 --> 00:59:09.100]   If I apply this particular augmentation of simply rotating the image by 90, 180 to
[00:59:09.100 --> 00:59:13.940]   70 degrees, will that help or hurt a segmentation model?
[00:59:13.940 --> 00:59:19.660]   I'll continue further or I'll, I'll just wait for a minute.
[00:59:19.660 --> 00:59:28.820]   Let me see if anyone's complaining in the zoom chat.
[00:59:28.820 --> 00:59:30.380]   No, things are good.
[00:59:30.380 --> 00:59:45.020]   I think rotation will give more or less the same result.
[00:59:45.020 --> 00:59:48.180]   Um, let's see if your answer is right, Babu.
[00:59:48.180 --> 00:59:52.060]   I'll wait for someone else to answer also, but I don't see anyone typing.
[00:59:52.060 --> 00:59:57.380]   So I let, I let you all think, but, um, it should help for this example.
[00:59:57.380 --> 01:00:03.820]   That would be a wrong answer because you can't simply rotate an image if you're
[01:00:03.820 --> 01:00:04.860]   trying to segment it.
[01:00:04.860 --> 01:00:06.620]   Why?
[01:00:06.620 --> 01:00:10.860]   You also need to augment the mask.
[01:00:10.860 --> 01:00:15.940]   You also need to rotate the labels.
[01:00:16.540 --> 01:00:18.780]   So we're not just detecting objects.
[01:00:18.780 --> 01:00:21.660]   We're also detecting their location in an image.
[01:00:21.660 --> 01:00:25.780]   And if you're training data has that location of different objects, you
[01:00:25.780 --> 01:00:27.380]   would also want to rotate that.
[01:00:27.380 --> 01:00:32.380]   That is one added challenge with image segmentation.
[01:00:32.380 --> 01:00:36.460]   I believe the latest fast way framework has a way of working with them.
[01:00:36.460 --> 01:00:40.540]   Uh, but instead I leave that as a homework to you for you to write.
[01:00:40.540 --> 01:00:45.140]   And instead I'll cover the mask augmentation from here and image
[01:00:45.140 --> 01:00:49.660]   augmentations from alimentation framework, which we had looked at, uh,
[01:00:49.660 --> 01:00:50.740]   in the last session.
[01:00:50.740 --> 01:00:56.060]   So I'll open that in a tab and come back to this and try to finish this first.
[01:00:56.060 --> 01:00:56.940]   Okay.
[01:00:56.940 --> 01:00:58.180]   No other comments coming in.
[01:00:58.180 --> 01:00:59.220]   So I'll continue further.
[01:00:59.220 --> 01:01:03.140]   Um, let's look at RGB and randomized.
[01:01:03.140 --> 01:01:08.100]   So this simply adds that green blue variance.
[01:01:08.100 --> 01:01:09.900]   Where would this be useful?
[01:01:09.900 --> 01:01:13.340]   I mean, uh, no one is taking trippy images of their cat.
[01:01:13.700 --> 01:01:14.300]   Hopefully.
[01:01:14.300 --> 01:01:20.860]   Um, but let's say you have a dataset that just has red cars, right?
[01:01:20.860 --> 01:01:26.540]   For that particular problem, uh, you want your day, uh, classified to learn
[01:01:26.540 --> 01:01:34.580]   about cars and not just something red as being a car there, you could change
[01:01:34.580 --> 01:01:38.820]   the color of the car and teach a model that he not every red looking object
[01:01:38.820 --> 01:01:43.740]   is a class, but also would want to change its color so that it's used to that.
[01:01:43.740 --> 01:01:46.380]   Um, you can vary the brightness.
[01:01:46.380 --> 01:01:49.180]   So brightness should be pretty straightforward.
[01:01:49.180 --> 01:01:56.740]   Um, thanks to Instagram and, uh, different places where we post our images, we have
[01:01:56.740 --> 01:02:00.060]   now a good eye for what's the right brightness.
[01:02:00.060 --> 01:02:03.460]   So you could also eyeball and say 0.5 is the best case.
[01:02:03.940 --> 01:02:09.420]   A real world use case that I've worked on, uh, is I was trying to detect this
[01:02:09.420 --> 01:02:13.140]   text on the back of this, uh, medicine.
[01:02:13.140 --> 01:02:16.940]   And if it's quite low, you would have to augment it.
[01:02:16.940 --> 01:02:20.780]   I did play around a lot with the brightness to make it work best.
[01:02:20.780 --> 01:02:23.580]   Same for contrast, same idea.
[01:02:23.580 --> 01:02:30.180]   Although on the internet, high contrast images for some reason work well for OCR
[01:02:30.180 --> 01:02:35.500]   problems also know his, his one thing I want to emphasize, I'll stop sharing my
[01:02:35.500 --> 01:02:40.660]   screen for OCR object, character recognition problems.
[01:02:40.660 --> 01:02:46.620]   You want your text to be maximum contrasted with the background, not just
[01:02:46.620 --> 01:02:51.940]   the image to have maximum contrast in a complete image.
[01:02:51.940 --> 01:02:54.260]   A maximum contrast is a filter.
[01:02:54.260 --> 01:02:57.100]   I wanted to turn on my zoom filter at this moment.
[01:02:57.100 --> 01:03:01.340]   I'd written in my notes, but in interest of messing up, which I've done enough
[01:03:01.340 --> 01:03:06.580]   today, I'll just let you all assume I've applied a cartoon filter that increases
[01:03:06.580 --> 01:03:10.820]   contrast that is different from having your text.
[01:03:10.820 --> 01:03:18.900]   So let's say this V being in a lot of contrast with the complete image or any
[01:03:18.900 --> 01:03:22.540]   other text that you might see on low in my, my video account.
[01:03:22.540 --> 01:03:32.980]   So, that's where you would want to play around with contrast for OCR.
[01:03:32.980 --> 01:03:37.460]   You can also crop into different parts of the image crop and pad it.
[01:03:37.460 --> 01:03:47.140]   A dihedron is this thing that I remember painfully from my chemistry lectures.
[01:03:47.140 --> 01:03:51.860]   It basically rotates a thing in an image in eight different angles.
[01:03:52.500 --> 01:03:58.740]   It works really well for again, satellite images, jitter adds random noise, and this
[01:03:58.740 --> 01:04:02.900]   can be quite useful to avoid overfitting.
[01:04:02.900 --> 01:04:12.380]   I can't think, I couldn't think at that time of a good example, but I still can't
[01:04:12.380 --> 01:04:15.460]   actually, maybe for medical images, adding jitter is helpful.
[01:04:15.460 --> 01:04:16.180]   I might be wrong.
[01:04:17.300 --> 01:04:23.580]   For perspective, I'd link this video by Apple, highly recommended, but
[01:04:23.580 --> 01:04:28.340]   perspective actually change changes the image in a way as if you're moving your
[01:04:28.340 --> 01:04:33.460]   camera about, so again, this is also helpful for OCR problems or anywhere
[01:04:33.460 --> 01:04:34.980]   where you're trying to focus on one thing.
[01:04:34.980 --> 01:04:40.660]   And I think that covers all of the things I wanted to mention from this block.
[01:04:41.020 --> 01:04:48.220]   So let's take a look at different mask augmentations inside of albumentation.
[01:04:48.220 --> 01:04:54.100]   So we import albumentation and we read the masks and images from the disk.
[01:04:54.100 --> 01:04:57.340]   Is this test text good to everyone?
[01:04:57.340 --> 01:04:59.780]   Could please someone confirm if I need to zoom in a bit.
[01:04:59.780 --> 01:05:05.540]   Is this readable to everyone?
[01:05:10.540 --> 01:05:11.460]   Um, preaches.
[01:05:11.460 --> 01:05:11.940]   Yes.
[01:05:11.940 --> 01:05:12.700]   So thanks.
[01:05:12.700 --> 01:05:13.860]   Mateo says yes as well.
[01:05:13.860 --> 01:05:14.180]   Okay.
[01:05:14.180 --> 01:05:14.580]   Thanks.
[01:05:14.580 --> 01:05:15.340]   Thanks guys.
[01:05:15.340 --> 01:05:24.260]   Um, so for instance segmentation, you also need to read multiple masks for images.
[01:05:24.260 --> 01:05:30.420]   Uh, I was considering explaining this, but I don't think I need to,
[01:05:30.420 --> 01:05:31.420]   it's already been covered.
[01:05:31.420 --> 01:05:36.340]   So we need to pass these images and masks to the augmentation pipeline
[01:05:36.340 --> 01:05:38.300]   and receive augmented images and masks.
[01:05:38.820 --> 01:05:42.300]   So we do that by simply passing it to the transform function.
[01:05:42.300 --> 01:05:46.300]   Again, to remind you, this is using a framework that is specifically
[01:05:46.300 --> 01:05:48.980]   for image augmentation, no question still.
[01:05:48.980 --> 01:05:52.620]   So that means it's obvious to everyone, but we had looked at albumentations
[01:05:52.620 --> 01:05:57.940]   last week and that's what we're continuing to do inside of this framework.
[01:05:57.940 --> 01:05:59.980]   That's how you work with mask augmentation.
[01:05:59.980 --> 01:06:04.060]   So you pass the images, you pass the masks, uh, you have the transform
[01:06:04.060 --> 01:06:08.020]   image and transform mask visualized like so.
[01:06:08.300 --> 01:06:12.020]   So as you can see, some, some rotation is going on, right?
[01:06:12.020 --> 01:06:16.620]   But pay close attention here, please.
[01:06:16.620 --> 01:06:24.300]   As the image has been rotated, the mask has also been rotated in conjunction.
[01:06:24.300 --> 01:06:31.780]   So if I just take this image and put it on top of, if I just take the mask and put
[01:06:31.780 --> 01:06:34.380]   it on top of the image, it overlaps perfectly.
[01:06:34.860 --> 01:06:40.220]   And that is one thing that is quite hard, quite easy to forget and quite hard to
[01:06:40.220 --> 01:06:41.580]   implement when you're in a hurry.
[01:06:41.580 --> 01:06:45.500]   Um, augmentation makes it really easy for you.
[01:06:45.500 --> 01:06:50.820]   So we're trying to detect houses or we're trying to mask houses in this image.
[01:06:50.820 --> 01:06:51.900]   Just segment those.
[01:06:51.900 --> 01:06:55.940]   Maybe, uh, you know, you're working with the government and you're trying to find
[01:06:55.940 --> 01:06:59.220]   where houses have been built legally or illegally, and you just take some
[01:06:59.220 --> 01:07:01.020]   satellite images and work with those.
[01:07:01.020 --> 01:07:03.380]   Uh, that's what this image does.
[01:07:03.420 --> 01:07:07.980]   And as you can see, we've also applied a few other augmentations, right?
[01:07:07.980 --> 01:07:12.460]   So we zoomed into the image a little bit, right?
[01:07:12.460 --> 01:07:17.820]   Uh, I did zoom in right now, but the high was a little longer here.
[01:07:17.820 --> 01:07:19.220]   It's not as long.
[01:07:19.220 --> 01:07:21.620]   We can see the van.
[01:07:21.620 --> 01:07:25.180]   Is it a little bit closer this time?
[01:07:25.180 --> 01:07:26.860]   The motorcycle appears to be bigger.
[01:07:26.860 --> 01:07:29.860]   So maybe we've cropped into the image and we've also changed
[01:07:29.860 --> 01:07:31.500]   the mask to go along with it.
[01:07:33.220 --> 01:07:35.580]   So you say, let's look at another example.
[01:07:35.580 --> 01:07:40.140]   Um, or actually let's, let's look at cool augmentation.
[01:07:40.140 --> 01:07:41.700]   Let's, let's take a look at both.
[01:07:41.700 --> 01:07:46.740]   So this is from the TJ salt identification, Kaggle competition,
[01:07:46.740 --> 01:07:48.820]   another suggested homework for you all.
[01:07:48.820 --> 01:07:51.700]   Uh, the more examples we look at, the more homework you'll get.
[01:07:51.700 --> 01:07:55.820]   So here's another suggestion from me that you can totally ignore, uh, but
[01:07:55.820 --> 01:07:58.580]   please feel free to check this competition out and you can work with it.
[01:07:59.860 --> 01:08:04.700]   So our, uh, task here, if I remember correctly was to identify salt
[01:08:04.700 --> 01:08:09.100]   lakes or salt, um, bodies.
[01:08:09.100 --> 01:08:12.180]   I might be wrong about this and you have the mass for it.
[01:08:12.180 --> 01:08:13.460]   So you could pad these.
[01:08:13.460 --> 01:08:17.660]   So what we've done here is we've applied a padding all around.
[01:08:17.660 --> 01:08:19.620]   You could send a crop and crop.
[01:08:19.620 --> 01:08:24.900]   So guys remember these are straightforward transforms, but
[01:08:24.900 --> 01:08:28.140]   applying these along with the mask is an added challenge.
[01:08:28.700 --> 01:08:33.420]   Um, fairly straightforward, but much harder with mass, right?
[01:08:33.420 --> 01:08:34.420]   And easy to mess up.
[01:08:34.420 --> 01:08:36.700]   So I'll be meditation is making our life easier.
[01:08:36.700 --> 01:08:42.180]   You could also apply the dihedral transform that I had mentioned.
[01:08:42.180 --> 01:08:44.860]   So as you can see, we've rotated the image here.
[01:08:44.860 --> 01:08:46.820]   Uh, you could also vertically flip it.
[01:08:46.820 --> 01:08:50.540]   You could randomly rotate it by 90 degrees.
[01:08:50.540 --> 01:08:52.820]   You could transpose it.
[01:08:53.060 --> 01:09:01.660]   So, uh, that would be rotating by 180 degrees or just taking a reflection
[01:09:01.660 --> 01:09:05.100]   along a diagonal, uh, basically transposing like you transpose a
[01:09:05.100 --> 01:09:07.380]   matrix since images are matrices.
[01:09:07.380 --> 01:09:16.820]   Uh, you could distort the grid, apply optical distortion, random size crops.
[01:09:16.820 --> 01:09:21.300]   So you crop in randomly and you could of course combine all of
[01:09:21.300 --> 01:09:23.140]   these for the best advantage.
[01:09:23.140 --> 01:09:30.220]   So these are the set of possible augmentations you could apply.
[01:09:30.220 --> 01:09:33.500]   Why did I mention these in the book?
[01:09:33.500 --> 01:09:39.020]   There's only one covered and I want you all to try different ones as another.
[01:09:39.020 --> 01:09:40.700]   You guys did homework.
[01:09:40.700 --> 01:09:44.740]   Uh, why do I suggest so many things for you to try out?
[01:09:44.740 --> 01:09:47.460]   First of all, we're here to learn how to apply these things.
[01:09:47.860 --> 01:09:53.020]   And also it makes for great blog posts or great experiments, which I really,
[01:09:53.020 --> 01:09:56.860]   really would encourage you all every single week, like I have
[01:09:56.860 --> 01:09:58.300]   been for the past nine weeks.
[01:09:58.300 --> 01:10:00.100]   Right.
[01:10:00.100 --> 01:10:03.700]   What you learn, right.
[01:10:03.700 --> 01:10:06.220]   What you learn about every single week, because it's a strong
[01:10:06.220 --> 01:10:11.100]   signal on your resume and also a great way to build a solid resume.
[01:10:11.100 --> 01:10:14.980]   No questions again.
[01:10:14.980 --> 01:10:16.380]   So I'll continue for that.
[01:10:16.380 --> 01:10:18.500]   These are the augmentations I wanted to cover.
[01:10:18.500 --> 01:10:24.660]   And I believe I'm at the point where I can start going through the books code.
[01:10:24.660 --> 01:10:30.380]   So I'll quickly take another sip of water and see if there are any, if there's
[01:10:30.380 --> 01:10:31.900]   anyone that wants to ask questions.
[01:10:31.900 --> 01:10:49.980]   Awesome.
[01:10:49.980 --> 01:10:53.900]   Uh, I think we have 20 minutes to go, so I should be able to cover
[01:10:53.900 --> 01:10:55.300]   most of the things I wanted to.
[01:10:55.300 --> 01:10:58.660]   Let me take a look at my notes here.
[01:10:58.900 --> 01:11:02.020]   Um, yep, I think we should be able to cover these things.
[01:11:02.020 --> 01:11:02.540]   Sure.
[01:11:02.540 --> 01:11:03.740]   Let's, let's jump into it.
[01:11:03.740 --> 01:11:11.460]   Um, so as I remember, as a reminder, this all is coming from the book, uh,
[01:11:11.460 --> 01:11:15.820]   GitHub repository, and I'll be shamelessly running through that, uh, without claiming
[01:11:15.820 --> 01:11:19.700]   ownership, of course I haven't modified any single function and we're
[01:11:19.700 --> 01:11:20.980]   just taking a look at this.
[01:11:20.980 --> 01:11:28.780]   Um, this particular notebook just showcases all of the segmentation
[01:11:28.780 --> 01:11:32.540]   and nodules, uh, I think I can glance over this without any problem.
[01:11:32.540 --> 01:11:36.820]   And same for this one.
[01:11:36.820 --> 01:11:49.140]   So inside of the util folder, uh, they have a fork of the original, uh, unit, I believe.
[01:11:57.100 --> 01:12:01.660]   And now you of course have a unit class, um, inside of PyTorch.
[01:12:01.660 --> 01:12:03.660]   So that's what they've used to implement it.
[01:12:03.660 --> 01:12:10.820]   I'm considering if I should explain all of these details or instead I can just,
[01:12:10.820 --> 01:12:14.740]   actually I should, I should just head over to the PyTorch docs
[01:12:14.740 --> 01:12:16.740]   and point these things out.
[01:12:25.580 --> 01:12:28.100]   So let's see, uh, what does this bring up?
[01:12:28.100 --> 01:12:33.540]   Hmm.
[01:12:33.540 --> 01:12:35.900]   You know, I did not bring up anything.
[01:12:35.900 --> 01:12:39.620]   Um, should I be searching for something else?
[01:12:39.620 --> 01:12:42.900]   Once I find the correct tab.
[01:12:42.900 --> 01:12:48.380]   Who was that one?
[01:12:48.380 --> 01:12:54.860]   Um, so it comes from an end or module, uh, which means I should be looking into that.
[01:12:54.860 --> 01:13:00.860]   I'm confusing myself here again.
[01:13:00.860 --> 01:13:03.020]   Okay.
[01:13:03.020 --> 01:13:07.180]   So this actually does not include the blocks that I had in mind.
[01:13:07.180 --> 01:13:13.060]   Um, and sorry, guys, I've managed to confuse myself.
[01:13:13.060 --> 01:13:22.780]   I wanted to not open papers with code.
[01:13:24.220 --> 01:13:28.180]   Um, not open paper space instead papers with code head over to unit.
[01:13:28.180 --> 01:13:36.220]   Um, let's see if that works.
[01:13:36.220 --> 01:13:42.500]   Third time is a charm.
[01:13:42.500 --> 01:13:43.500]   Let's see the code.
[01:13:43.500 --> 01:13:49.620]   What I was trying to do here is, uh, okay.
[01:13:49.620 --> 01:13:52.580]   This also imports these from this particular module.
[01:13:52.900 --> 01:14:01.540]   So I was just trying to point out, um, what these different layers to, uh, let's
[01:14:01.540 --> 01:14:04.700]   head over to an end or module and look inside of there.
[01:14:04.700 --> 01:14:07.220]   So unit,
[01:14:07.220 --> 01:14:18.060]   I may be confusing myself between fast and by touch now, because
[01:14:18.060 --> 01:14:19.580]   fast he has a unit learner.
[01:14:19.580 --> 01:14:21.900]   Uh, okay.
[01:14:21.900 --> 01:14:22.660]   Sorry about that.
[01:14:22.660 --> 01:14:26.060]   I assume that by touch now has this unit function.
[01:14:26.060 --> 01:14:28.980]   Uh, it's, it's in by touch.
[01:14:28.980 --> 01:14:30.340]   Sorry about the confusion.
[01:14:30.340 --> 01:14:35.180]   So I'll try to cover all of the details now.
[01:14:35.180 --> 01:14:43.540]   Um, in the model definition, you'd first define the constructor.
[01:14:43.540 --> 01:14:47.940]   Uh, and this time we define the in channels and one as one,
[01:14:47.940 --> 01:14:49.580]   a number of classes as two.
[01:14:50.620 --> 01:14:52.660]   We take a depth of five.
[01:14:52.660 --> 01:14:56.060]   Uh, so this has been copied from the original implementation, I believe.
[01:14:56.060 --> 01:14:56.820]   Um,
[01:14:56.820 --> 01:15:07.260]   and we assume up sampling mode in up conf and up sample, uh, layers.
[01:15:07.260 --> 01:15:09.140]   So that's what we've defined here.
[01:15:09.140 --> 01:15:12.980]   We also take care of the padding depth and channels.
[01:15:12.980 --> 01:15:13.820]   Um,
[01:15:17.300 --> 01:15:21.180]   and I'm trying to actually through it in the same time, while I'm trying to
[01:15:21.180 --> 01:15:22.580]   understand how much should I explain.
[01:15:22.580 --> 01:15:32.060]   So we append, uh, so essentially in any unit, there are two parts, right?
[01:15:32.060 --> 01:15:33.980]   The down sampling and the up sampling one.
[01:15:33.980 --> 01:15:40.860]   So we run through all of the layers and append the down part and up sampling
[01:15:40.860 --> 01:15:45.820]   part by going through all of the layers and essentially creating a list from those.
[01:15:46.820 --> 01:15:52.180]   And in our forward function, we iterate over this, these layers
[01:15:52.180 --> 01:15:53.860]   and apply average pooling.
[01:15:53.860 --> 01:15:58.900]   I want to point one detail out in the original paper, uh, implementation.
[01:15:58.900 --> 01:16:02.300]   And this also mentioned in the book, the first of all included average pooling,
[01:16:02.300 --> 01:16:04.500]   but later it's been updated to max pooling.
[01:16:04.500 --> 01:16:08.980]   So if you actually go through the details and get confused, max pooling
[01:16:08.980 --> 01:16:12.140]   is the one you should be applying here, not just average pooling.
[01:16:12.140 --> 01:16:13.380]   Please be careful with that.
[01:16:15.460 --> 01:16:18.860]   Um, I think other things are pretty straightforward here.
[01:16:18.860 --> 01:16:25.540]   So in unit con vlog, we now can import from an end module and, uh, call
[01:16:25.540 --> 01:16:27.660]   the constructor for unit conf block.
[01:16:27.660 --> 01:16:35.500]   This time we just applied con 2d and batch norm 2d.
[01:16:35.500 --> 01:16:37.540]   That's what we do in the book also.
[01:16:37.540 --> 01:16:39.900]   And that is a problem.
[01:16:39.900 --> 01:16:41.620]   That is a challenge we need to deal with.
[01:16:42.180 --> 01:16:47.460]   I'm going to stop sharing my screen because for a simple problem, right?
[01:16:47.460 --> 01:16:51.820]   If you're just trying to work with this particular image, things are good.
[01:16:51.820 --> 01:16:53.660]   You can work with a 2d image.
[01:16:53.660 --> 01:16:55.140]   This is a 2d image, right?
[01:16:55.140 --> 01:17:00.740]   I mean, it has three layers, red, green, blue, but it's still a 2d image, not a 3d
[01:17:00.740 --> 01:17:06.420]   image, unless you have a 3d monitor and for better or worse, you're using that
[01:17:06.420 --> 01:17:07.820]   to watch a PyTorch lecture.
[01:17:07.820 --> 01:17:09.620]   Uh, this is a 2d image.
[01:17:10.100 --> 01:17:11.980]   And we are working with CT scans.
[01:17:11.980 --> 01:17:20.380]   So the trick the authors apply here is to treat the layers of the 3d scan, which
[01:17:20.380 --> 01:17:22.700]   is black and white as the depth.
[01:17:22.700 --> 01:17:29.020]   So the CT scan is a 3d image that is black and white.
[01:17:29.020 --> 01:17:38.740]   And instead of passing on the RGB layers, we pass on the depth of the layers or
[01:17:38.740 --> 01:17:42.420]   every single slice to the unit.
[01:17:42.420 --> 01:17:46.700]   There are problems with this, of course, because you lose the
[01:17:46.700 --> 01:17:48.700]   complete spatial information, right?
[01:17:48.700 --> 01:17:53.980]   If my lungs have been scanned in a CT scan, and I just passed 333 layers or
[01:17:53.980 --> 01:17:58.380]   555 in this case, I lose the spatial information, right?
[01:17:58.380 --> 01:18:03.660]   Uh, of course you do, but we're working, uh, with limited computer
[01:18:03.660 --> 01:18:05.020]   memory, limited GPU memory.
[01:18:05.020 --> 01:18:06.180]   So that's what we're going to do.
[01:18:07.180 --> 01:18:11.460]   And, uh, that's something to be mindful of.
[01:18:11.460 --> 01:18:12.780]   That's why I pointed it out.
[01:18:12.780 --> 01:18:16.980]   Um, so that's what we work with here.
[01:18:16.980 --> 01:18:21.060]   And let's see if I need to cover anything else.
[01:18:21.060 --> 01:18:24.620]   I think most of the other things have been covered.
[01:18:34.980 --> 01:18:38.380]   I see a comment in zoom.
[01:18:38.380 --> 01:18:41.460]   Could you please ask that question on the discourse forums, please?
[01:18:41.460 --> 01:18:48.420]   Um, yeah, going through these layers again, uh, there's
[01:18:48.420 --> 01:18:50.020]   nothing interesting happening here.
[01:18:50.020 --> 01:18:52.180]   So I, I have two agendas in mind.
[01:18:52.180 --> 01:18:56.620]   I want to point out things like the 3d to 2d image conversion, uh,
[01:18:56.620 --> 01:18:58.060]   in my head every single time.
[01:18:58.060 --> 01:19:03.580]   And I also want to cover any new concepts that get introduced here.
[01:19:03.580 --> 01:19:06.500]   It's pretty much the same things that all of us have understood.
[01:19:06.500 --> 01:19:09.060]   So in the interest of your learning, I would skip over those.
[01:19:09.060 --> 01:19:14.980]   No new questions.
[01:19:14.980 --> 01:19:16.380]   I can continue further.
[01:19:16.380 --> 01:19:21.540]   Um, I think in interest of time, I'll come back to these next week.
[01:19:21.540 --> 01:19:25.740]   So for now, I'll just cover how do we load this data set.
[01:19:25.740 --> 01:19:32.660]   So remember now we're working with the same images, uh, that we have with so far,
[01:19:33.100 --> 01:19:39.140]   but we need to be able to create a mask for the tumors.
[01:19:39.140 --> 01:19:46.180]   Uh, and instead of making our lives easy and simply reading the annotations, uh,
[01:19:46.180 --> 01:19:51.820]   or simply providing a mask that we can just read in, we try something else here.
[01:19:51.820 --> 01:19:58.180]   Uh, we we're doing things the hard way to learn, which is we know
[01:19:58.180 --> 01:20:00.580]   the center of tumors from earlier.
[01:20:00.980 --> 01:20:04.620]   So if you're new to the group, we learned in the last few weeks of how to find the
[01:20:04.620 --> 01:20:10.460]   center of a tumor using simple, simple geometric tricks, which is take the radius
[01:20:10.460 --> 01:20:14.780]   divided by half in a small enough picture.
[01:20:14.780 --> 01:20:15.700]   That's the center.
[01:20:15.700 --> 01:20:22.860]   Now we do another simple trick, which I'll share my screen, uh, to one node the same.
[01:20:22.860 --> 01:20:30.180]   So now assuming you have the center, actually this it's there in the book.
[01:20:30.180 --> 01:20:31.660]   So let me find that particular.
[01:20:31.660 --> 01:20:38.980]   Yes, this is the one.
[01:20:38.980 --> 01:20:47.900]   So whenever people say, uh, computer science is useful sometimes for machine
[01:20:47.900 --> 01:20:50.820]   learning or deep learning, uh, not always.
[01:20:50.820 --> 01:20:58.780]   Um, but this is one of those rare examples where it is.
[01:20:58.900 --> 01:21:02.460]   So, uh, this is not to discourage you, but what we're trying to do here is we're
[01:21:02.460 --> 01:21:03.940]   applying a search algorithm.
[01:21:03.940 --> 01:21:07.100]   So if you're from computer science, you'd understand that if not, I'll explain it to
[01:21:07.100 --> 01:21:07.260]   you.
[01:21:07.260 --> 01:21:09.340]   So here's what we do.
[01:21:09.340 --> 01:21:12.780]   We start with this center of any tumor.
[01:21:12.780 --> 01:21:18.420]   So we take this layer and we start looking one cell outwards.
[01:21:18.420 --> 01:21:22.660]   So one pixel outwards.
[01:21:22.660 --> 01:21:28.620]   And we look inside of this particular box this time.
[01:21:29.620 --> 01:21:31.820]   What are we trying to look for?
[01:21:31.820 --> 01:21:45.620]   So we are trying to find the exact location of a tumor and we using the center and we
[01:21:45.620 --> 01:21:50.620]   looking outside in the neighborhood pixel by pixel.
[01:21:50.620 --> 01:21:52.580]   But how do we know where to stop?
[01:21:52.620 --> 01:22:01.540]   We had learned of, um, I'm trying to, I've actually forgotten the exact terminology.
[01:22:01.540 --> 01:22:07.860]   We had learned of a way to understand the density.
[01:22:07.860 --> 01:22:09.820]   There was one particular unit.
[01:22:09.820 --> 01:22:14.420]   It was called HU and I'm forgetting what does HU stand for?
[01:22:14.420 --> 01:22:21.180]   Uh, so I'll try to act smart and say, we learned about HU, which measures the
[01:22:21.180 --> 01:22:29.580]   density. So what we'll do is we'll keep looking in the neighborhood of the center
[01:22:29.580 --> 01:22:33.340]   till the density drops below a threshold.
[01:22:33.340 --> 01:22:37.340]   And for that, we'd say that's the end of the tumor.
[01:22:37.340 --> 01:22:44.540]   So we'll start with a radius of one, then increase the radius to two and then maybe
[01:22:44.540 --> 01:22:45.140]   to three.
[01:22:45.140 --> 01:22:49.980]   And then we'll slice out the outmost layer, which we look at.
[01:22:50.980 --> 01:22:58.020]   Because when we actually hit this, um, imagine implementing this, you would
[01:22:58.020 --> 01:23:00.180]   implement this inside of a while loop, right?
[01:23:00.180 --> 01:23:07.340]   So when this is true, your radius would be actually one more than the threshold.
[01:23:07.340 --> 01:23:10.660]   So we're trying to account for that and you try to subtract that.
[01:23:10.660 --> 01:23:14.260]   So that's what we are accounting for and taking care of here.
[01:23:14.260 --> 01:23:19.060]   This is how we create all of these masks.
[01:23:19.820 --> 01:23:27.220]   Or this is how we find all of the nodules, the tumors inside of every single CT
[01:23:27.220 --> 01:23:30.660]   scan. This is how we mark them using a search algorithm.
[01:23:30.660 --> 01:23:38.540]   Um, I wanted to cover all of this code also, but I think the books annotation are
[01:23:38.540 --> 01:23:39.660]   pretty spot on.
[01:23:39.660 --> 01:23:44.180]   So what I would suggest is in interest of time, I would request you all to simply
[01:23:44.180 --> 01:23:45.260]   read all of these.
[01:23:45.860 --> 01:23:51.740]   And I think we can start from here, but, uh, this is inside of our datasets,
[01:23:51.740 --> 01:23:53.380]   thought Python file.
[01:23:53.380 --> 01:23:56.900]   The one that I was just sharing, let me go back to that.
[01:23:56.900 --> 01:24:05.940]   Um, so you can run through all of this and try to understand the code.
[01:24:05.940 --> 01:24:08.620]   If there are things you don't understand, we can continue from here.
[01:24:08.620 --> 01:24:14.780]   Um, but I think it's, you all should be able to understand that to the people who
[01:24:14.780 --> 01:24:15.900]   have recently joined us.
[01:24:15.900 --> 01:24:18.460]   If things don't make sense, please feel free to ask.
[01:24:18.460 --> 01:24:26.940]   Um, I'll come back to my presentation now and remind you that we have a few minutes.
[01:24:26.940 --> 01:24:29.020]   So I'm happy to answer any questions.
[01:24:29.020 --> 01:24:31.900]   Uh, please, please ask them in this thread.
[01:24:31.900 --> 01:24:33.260]   I'll attend to those.
[01:24:33.260 --> 01:24:36.180]   And here's the suggested homework for today.
[01:24:36.180 --> 01:24:40.780]   Please check out the report, uh, that again, we will email you.
[01:24:41.740 --> 01:24:45.340]   Uh, if you signed up, if you've joined the zoom call, we will email it to you.
[01:24:45.340 --> 01:24:48.100]   If you haven't, it's there in the forum thread.
[01:24:48.100 --> 01:24:49.860]   So please feel free to check it out.
[01:24:49.860 --> 01:24:53.780]   Implement unit on a Kaggle competition.
[01:24:53.780 --> 01:24:58.700]   So, uh, any competition that has segmentation involved, please implement
[01:24:58.700 --> 01:25:00.780]   or consider implementing a unit there.
[01:25:00.780 --> 01:25:04.060]   Try a newer segmentation approach.
[01:25:04.060 --> 01:25:09.700]   I painfully, uh, after messing up my zoom screen share, uh, pointed you
[01:25:09.700 --> 01:25:11.220]   towards paperswithcode.com.
[01:25:11.580 --> 01:25:16.980]   Try looking for a newer image segmentation approach or a newer
[01:25:16.980 --> 01:25:23.340]   architecture than unit and blog about either or all of these.
[01:25:23.340 --> 01:25:27.580]   And I, I can promise that we'll come back to unit architecture.
[01:25:27.580 --> 01:25:29.940]   We'll I'll actually host a paper eating group on this.
[01:25:29.940 --> 01:25:32.260]   So, uh, please stay tuned for that.
[01:25:32.260 --> 01:25:36.500]   Um, and I have one more request for everyone.
[01:25:36.500 --> 01:25:42.980]   So I created this placeholder thread for us to thank the authors.
[01:25:42.980 --> 01:25:46.340]   I believe the next session will mostly be the last session.
[01:25:46.340 --> 01:25:54.740]   So if, um, you all have enjoyed this group, I really want to request you
[01:25:54.740 --> 01:25:57.060]   all to please see a few kind words.
[01:25:57.060 --> 01:26:00.500]   We'll be sending this to the authors pretty soon.
[01:26:00.500 --> 01:26:04.180]   Uh, and whenever we end or decide to wrap the session up.
[01:26:04.860 --> 01:26:09.620]   So, um, if you've enjoyed any bit of the book, or if you've learned
[01:26:09.620 --> 01:26:12.260]   anything from the book at all, this is not for us, this is not
[01:26:12.260 --> 01:26:13.700]   for Weights and Biases or for me.
[01:26:13.700 --> 01:26:19.140]   I sincerely just want to thank the authors who've created this incredible
[01:26:19.140 --> 01:26:21.340]   book that all of us have been learning from.
[01:26:21.340 --> 01:26:25.700]   Um, so consider taking some time out and writing a few things there.
[01:26:25.700 --> 01:26:28.540]   I'll of course edit this later and make it more accurate.
[01:26:28.540 --> 01:26:38.300]   Um, I see a question in the zoom chat.
[01:26:38.300 --> 01:26:43.980]   I don't have the recordings on a Google drive.
[01:26:43.980 --> 01:26:46.660]   That's because we have them on our YouTube page.
[01:26:46.660 --> 01:26:49.460]   I'll stop my screen sharing so that I can point that out.
[01:26:49.460 --> 01:26:53.580]   Uh, because you all don't want to see my YouTube recommendations right now.
[01:26:56.460 --> 01:26:57.100]   I'm just kidding.
[01:26:57.100 --> 01:27:02.340]   There are some problems with pointing out YouTube recommendations.
[01:27:02.340 --> 01:27:08.700]   So if you go to the Weights and Biases YouTube page, um, you should
[01:27:08.700 --> 01:27:10.220]   be able to see PyTorch study group.
[01:27:10.220 --> 01:27:14.140]   And inside of this playlist, you should be able to see all of the
[01:27:14.140 --> 01:27:15.940]   groups that we've hosted so far.
[01:27:15.940 --> 01:27:18.260]   Uh, the first one was with the author.
[01:27:18.260 --> 01:27:21.660]   And since then we've been trying to cover as many chapters as
[01:27:21.660 --> 01:27:23.500]   meaningfully possible every single week.
[01:27:23.500 --> 01:27:25.580]   So please check that out on our YouTube page.
[01:27:26.580 --> 01:27:30.140]   Um, I'm sorry, Vabav, you've been limited to three posts.
[01:27:30.140 --> 01:27:34.140]   So whenever you sign up on our forums, initially we try to limit any spam bots.
[01:27:34.140 --> 01:27:39.340]   Uh, because of that, I'll, I'll try to fix your limit after this.
[01:27:39.340 --> 01:27:44.820]   But his question is, um, he's working on a Kaggle competition.
[01:27:44.820 --> 01:27:51.300]   So should I approach, is this a live competition right now?
[01:27:51.300 --> 01:27:53.180]   See discount image classification.
[01:27:54.100 --> 01:28:00.100]   So their question is for,
[01:28:00.100 --> 01:28:07.780]   Oh, it's from, it's from four years ago.
[01:28:07.780 --> 01:28:10.940]   I assume you're looking at the same competition, but you're
[01:28:10.940 --> 01:28:12.740]   referring to this competition only.
[01:28:12.740 --> 01:28:17.620]   I assume the answer is yes.
[01:28:17.620 --> 01:28:19.620]   Uh, so in that case, it's totally fine.
[01:28:19.620 --> 01:28:23.340]   Otherwise I want you to point out the fact that it's against the rules of
[01:28:23.340 --> 01:28:28.420]   Kaggle to discuss any competition, uh, that is currently live outside of the
[01:28:28.420 --> 01:28:32.860]   forums, uh, please always remain mindful of that because we don't want to upset
[01:28:32.860 --> 01:28:33.900]   Kaggle in any way.
[01:28:33.900 --> 01:28:40.380]   Um, yes, please consider applying this approach and tell us how that goes.
[01:28:40.380 --> 01:28:43.180]   Uh, I had mentioned this in the last session.
[01:28:43.180 --> 01:28:49.100]   Uh, the most important thing on Kaggle is trying techniques instead of simply,
[01:28:49.140 --> 01:28:53.660]   uh, considering if you should apply them or not, you would actually learn much
[01:28:53.660 --> 01:28:56.540]   more, and this is not a personal remark towards you.
[01:28:56.540 --> 01:29:01.660]   I know it's, it's quite a strict remark, but please, please consider trying as
[01:29:01.660 --> 01:29:06.860]   many things as you can, uh, you know, going back in time and looking at unit,
[01:29:06.860 --> 01:29:10.300]   no one would have thought, uh, this would work or not.
[01:29:10.300 --> 01:29:16.180]   And someone did some surprisingly it worked, which was a new technique at that time.
[01:29:16.580 --> 01:29:20.260]   So even if you can take a technique that no one is applying to a particular
[01:29:20.260 --> 01:29:27.900]   problem and your gut intuition tells you that it should work, who knows, maybe,
[01:29:27.900 --> 01:29:33.540]   maybe you've considered an absolutely new path of, you know, working with different
[01:29:33.540 --> 01:29:34.180]   problems.
[01:29:34.180 --> 01:29:38.540]   Uh, so please, please try as many things as you can.
[01:29:38.540 --> 01:29:41.540]   I've already mentioned the homework.
[01:29:41.540 --> 01:29:44.140]   So I'll take one last look at any questions.
[01:29:45.100 --> 01:29:47.380]   Uh, no, I don't see any questions.
[01:29:47.380 --> 01:29:48.300]   So that's perfect.
[01:29:48.300 --> 01:29:54.460]   Um, I also want to point out one other study group.
[01:29:54.460 --> 01:29:59.460]   If anyone is interested, I'm actually working on creating the materials for my
[01:29:59.460 --> 01:30:01.540]   next course run.
[01:30:01.540 --> 01:30:05.540]   Uh, this will be the fast game machine learning course and a small group of us.
[01:30:05.540 --> 01:30:07.620]   Actually, I'm not doing the heavy lifting.
[01:30:07.620 --> 01:30:12.700]   All of the work has been done by these incredible people are working on
[01:30:12.700 --> 01:30:14.340]   remastering the course.
[01:30:15.340 --> 01:30:20.340]   So if you're interested in contributing to this, uh, please just reply to the
[01:30:20.340 --> 01:30:21.740]   thread or please just DM me.
[01:30:21.740 --> 01:30:22.940]   I'll add you to this group.
[01:30:22.940 --> 01:30:28.300]   Uh, we're working on this actively during the week and I'm happy to invite you to
[01:30:28.300 --> 01:30:28.500]   it.
[01:30:28.500 --> 01:30:35.900]   Um, I don't see any other notification for questions, which means I can stop my
[01:30:35.900 --> 01:30:39.700]   screen share, uh, thank the people on YouTube and zoom.
[01:30:39.700 --> 01:30:41.220]   So thanks everyone for joining.
[01:30:41.220 --> 01:30:42.940]   I'll end the live stream on YouTube.
[01:30:42.940 --> 01:30:45.980]   Now I'll see you next week, which will mostly be the last week.
[01:30:45.980 --> 01:30:51.260]   If you have been joining, please consider, uh, thanking us.
[01:30:51.260 --> 01:30:54.220]   Uh, sorry, not thanking us, thanking the authors.
[01:30:54.220 --> 01:30:55.700]   I was reading a comment on YouTube.
[01:30:55.700 --> 01:31:03.060]   Um, this one question that I can probably treat, this is by Brian.
[01:31:03.060 --> 01:31:04.180]   Hey Brian, good to see you.
[01:31:04.180 --> 01:31:07.780]   Brian is my colleague, uh, the head of data science at bits and biases.
[01:31:07.780 --> 01:31:11.700]   When the unit architecture was introduced, what was the intuition that suggested
[01:31:11.700 --> 01:31:13.020]   it might be useful?
[01:31:13.020 --> 01:31:19.740]   Um, from what I understand and for papers, uh, I try to follow the citation
[01:31:19.740 --> 01:31:22.940]   graph at that time, some unit architectures were already there.
[01:31:22.940 --> 01:31:29.300]   So the U shaped papers was trying, was starting to come up, but I think the
[01:31:29.300 --> 01:31:32.260]   reason why it worked was because of the skip connections.
[01:31:32.260 --> 01:31:35.620]   Uh, the authors just concatenated all of the layers.
[01:31:39.020 --> 01:31:41.780]   I'm not sure how did they come up with that intuition because
[01:31:41.780 --> 01:31:43.380]   this was before SNETs.
[01:31:43.380 --> 01:31:48.300]   I mean, in hindsight, it works really well, but maybe, uh, it was being
[01:31:48.300 --> 01:31:49.980]   discussed in academia somewhere.
[01:31:49.980 --> 01:31:51.340]   I might be wrong about this.
[01:31:51.340 --> 01:31:58.540]   Uh, so they were trying to find a way to make the U shaped networks work.
[01:31:58.540 --> 01:32:02.020]   Um, and somehow they arrived at this conclusion, I believe.
[01:32:02.020 --> 01:32:06.540]   I'm sorry.
[01:32:06.540 --> 01:32:08.300]   I don't have an exact answer for that.
[01:32:08.340 --> 01:32:11.940]   Uh, but if anyone else does, please, please help me out here.
[01:32:11.940 --> 01:32:19.820]   Uh, I have another comment on Zoom.
[01:32:19.820 --> 01:32:21.340]   I don't think it's related to that.
[01:32:21.340 --> 01:32:23.820]   Okay.
[01:32:23.820 --> 01:32:26.660]   I'll try to, maybe I'll try to find this over the week.
[01:32:26.660 --> 01:32:34.220]   Uh, the question is what was the intuition behind unit, uh, units author
[01:32:34.220 --> 01:32:36.060]   assuming it will work for segmentation.
[01:32:36.060 --> 01:32:36.860]   That's a great question.
[01:32:36.860 --> 01:32:38.180]   I'll, I'll come back to this next week.
[01:32:38.180 --> 01:32:39.060]   Thanks for asking that.
[01:32:39.060 --> 01:32:41.420]   Um, awesome.
[01:32:41.420 --> 01:32:42.860]   I think I can end the live stream.
[01:32:42.860 --> 01:32:44.140]   Thanks everyone for joining.
[01:32:44.140 --> 01:32:45.300]   I'll see you next week.
[01:32:45.300 --> 01:32:46.780]   Uh, consider doing the homework.
[01:32:46.780 --> 01:32:48.620]   If not, I won't be mad at you.
[01:32:48.620 --> 01:32:52.020]   Uh, but if you do, please share it with us.
[01:32:52.020 --> 01:32:55.660]   I'd be happy to share it with our community and I'll see you next week,
[01:32:55.660 --> 01:32:57.420]   which will mostly be the last session.
[01:32:57.420 --> 01:32:57.860]   Thanks.
[01:32:57.860 --> 01:32:58.540]   Thanks everyone.
[01:32:58.540 --> 01:32:59.380]   See you next week.
[01:32:59.380 --> 01:33:03.300]   week.
[01:33:03.300 --> 01:33:05.360]   you

