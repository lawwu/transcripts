
[00:00:00.000 --> 00:00:04.720]   We have 600 plus kind of roughly intact scrolls
[00:00:04.720 --> 00:00:06.040]   that we can't open.
[00:00:06.040 --> 00:00:07.160]   And I heard about this,
[00:00:07.160 --> 00:00:08.800]   and I thought that was incredibly exciting,
[00:00:08.800 --> 00:00:10.440]   like the idea that there was information
[00:00:10.440 --> 00:00:12.080]   from 2,000 years in the past.
[00:00:12.080 --> 00:00:14.200]   We don't know what's in these things.
[00:00:14.200 --> 00:00:15.720]   If we could read all of them,
[00:00:15.720 --> 00:00:19.300]   then that would give us approximately a doubling
[00:00:19.300 --> 00:00:21.560]   of the total text that we have from antiquity.
[00:00:21.560 --> 00:00:23.960]   If there are thousands more papyrus scrolls in there,
[00:00:23.960 --> 00:00:26.400]   and we now have the techniques to read them,
[00:00:26.400 --> 00:00:27.800]   then there's gold in that mud,
[00:00:27.800 --> 00:00:30.040]   and it's gotta be dug out.
[00:00:30.040 --> 00:00:32.720]   I just fundamentally don't believe the world is efficient.
[00:00:32.720 --> 00:00:35.840]   And so if I see an opportunity to do something,
[00:00:35.840 --> 00:00:37.840]   I used to, but I no longer have a reflexive reaction
[00:00:37.840 --> 00:00:39.560]   that says, oh, that must not be a good idea
[00:00:39.560 --> 00:00:42.240]   if it were a good idea, someone would already be doing it.
[00:00:42.240 --> 00:00:44.800]   Okay, today I have the pleasure of speaking
[00:00:44.800 --> 00:00:47.840]   with Nat Friedman, who was the CEO of GitHub
[00:00:47.840 --> 00:00:50.720]   from 2018 to 2021.
[00:00:50.720 --> 00:00:54.040]   Before that, he started and sold two companies,
[00:00:54.040 --> 00:00:55.760]   Zimian and Xamarin.
[00:00:55.760 --> 00:00:58.320]   And he is also the founder of AI Grant
[00:00:58.320 --> 00:01:00.000]   and California Yimby.
[00:01:00.000 --> 00:01:03.120]   And most recently, he is the organizer and funder
[00:01:03.120 --> 00:01:04.320]   of the Scroll Prize,
[00:01:04.320 --> 00:01:06.200]   which is where we'll start this conversation.
[00:01:06.200 --> 00:01:07.920]   So Nat, do you wanna tell the audience
[00:01:07.920 --> 00:01:09.600]   about what the Scroll Prize is?
[00:01:09.600 --> 00:01:11.760]   Well, we're calling it the Vesuvius Challenge.
[00:01:11.760 --> 00:01:12.600]   Okay, got it.
[00:01:12.600 --> 00:01:16.120]   And this is just this crazy and exciting thing
[00:01:16.120 --> 00:01:19.360]   I feel like incredibly honored to have gotten caught up in.
[00:01:19.360 --> 00:01:23.080]   But a couple of years ago, I was reading,
[00:01:23.080 --> 00:01:24.600]   it was the midst of COVID,
[00:01:24.600 --> 00:01:27.280]   and I think we were in lockdown.
[00:01:27.280 --> 00:01:28.640]   And like everybody, I was sort of falling
[00:01:28.640 --> 00:01:30.320]   into internet rabbit holes.
[00:01:30.320 --> 00:01:34.520]   And I just started reading about the eruption
[00:01:34.520 --> 00:01:38.240]   of Mount Vesuvius in Italy about 2000 years ago.
[00:01:38.240 --> 00:01:41.360]   And it turns out that when Vesuvius erupted,
[00:01:41.360 --> 00:01:44.000]   it was AD 79.
[00:01:44.000 --> 00:01:46.400]   It destroyed all the nearby towns.
[00:01:46.400 --> 00:01:48.240]   Everyone knows about Pompeii.
[00:01:48.240 --> 00:01:50.840]   But there was another nearby town called Herculaneum.
[00:01:50.840 --> 00:01:53.440]   And Herculaneum was sort of like the Beverly Hills
[00:01:53.440 --> 00:01:54.280]   to Pompeii.
[00:01:54.280 --> 00:01:57.960]   You know, big villas, big houses, fancy people.
[00:01:57.960 --> 00:02:01.600]   And in Herculaneum, there was one villa in particular
[00:02:01.600 --> 00:02:02.920]   that was enormous.
[00:02:02.920 --> 00:02:04.480]   And it had once been owned
[00:02:04.480 --> 00:02:07.280]   by the father-in-law of Julius Caesar,
[00:02:07.280 --> 00:02:09.760]   and so well-connected guy.
[00:02:09.760 --> 00:02:14.760]   And it was full of beautiful statues and marbles and art,
[00:02:14.760 --> 00:02:17.840]   but it was also the home to a huge library
[00:02:17.840 --> 00:02:19.720]   of papyrus scrolls.
[00:02:19.720 --> 00:02:21.240]   And so when the villa was buried,
[00:02:21.240 --> 00:02:25.760]   the volcano actually, it spit out enormous quantities
[00:02:25.760 --> 00:02:27.240]   of mud and ash.
[00:02:27.240 --> 00:02:30.320]   And it buried Herculaneum in particular
[00:02:30.320 --> 00:02:32.800]   in something like 20 meters of material.
[00:02:32.800 --> 00:02:34.120]   So it wasn't like a thin layer.
[00:02:34.120 --> 00:02:36.040]   It was a very thick layer.
[00:02:36.040 --> 00:02:38.680]   Those towns were buried and forgotten for hundreds of years.
[00:02:38.680 --> 00:02:43.240]   No one even knew exactly where they were until the 1700s.
[00:02:43.240 --> 00:02:47.240]   And so in 1750, a farm worker who was digging a well
[00:02:47.240 --> 00:02:50.120]   kind of in the outskirts of Herculaneum
[00:02:50.120 --> 00:02:55.120]   struck this marble paving stone of a path
[00:02:55.120 --> 00:02:57.320]   that had been at this huge villa.
[00:02:57.320 --> 00:02:59.640]   And of course, he was pretty far down.
[00:02:59.640 --> 00:03:01.840]   When he did that, he was 60 feet down.
[00:03:01.840 --> 00:03:04.680]   And then subsequently, this Swiss engineer came in
[00:03:04.680 --> 00:03:08.960]   and started digging tunnels from that well shaft,
[00:03:08.960 --> 00:03:11.040]   and they found all these treasures.
[00:03:11.040 --> 00:03:13.680]   And that was sort of the spirit at the time was like looting.
[00:03:13.680 --> 00:03:16.000]   They were taking out like incredible,
[00:03:16.000 --> 00:03:17.160]   they would, if they encountered a wall,
[00:03:17.160 --> 00:03:18.840]   they would just bust through it.
[00:03:18.840 --> 00:03:21.840]   And they were taking out these beautiful bronze statues
[00:03:21.840 --> 00:03:22.920]   that had survived.
[00:03:22.920 --> 00:03:25.200]   And along the way, they kept encountering these lumps
[00:03:25.200 --> 00:03:26.840]   of what looked like charcoal.
[00:03:26.840 --> 00:03:27.960]   They weren't sure what they were,
[00:03:27.960 --> 00:03:30.080]   and many were apparently thrown away
[00:03:30.080 --> 00:03:33.960]   until someone noticed a little bit of writing on one of them.
[00:03:33.960 --> 00:03:36.600]   And they realized they were papyrus scrolls.
[00:03:36.600 --> 00:03:38.640]   And there were hundreds, there may have been thousands
[00:03:38.640 --> 00:03:39.720]   of them.
[00:03:39.720 --> 00:03:43.760]   And so they had uncovered really this enormous library
[00:03:43.760 --> 00:03:45.840]   is the only library ever to have sort of survived
[00:03:45.840 --> 00:03:47.840]   in any form, even though it was badly damaged,
[00:03:47.840 --> 00:03:49.560]   and they were sort of carbonized,
[00:03:49.560 --> 00:03:51.560]   very fragile, deformed.
[00:03:51.560 --> 00:03:53.720]   The only one that survived since antiquity.
[00:03:53.720 --> 00:03:56.440]   In the open air, these papyrus scrolls
[00:03:56.440 --> 00:03:58.360]   in like a Mediterranean climate,
[00:03:58.360 --> 00:04:00.240]   they rot and they decay quickly.
[00:04:00.240 --> 00:04:02.240]   And so they'd have to be recopied by monks
[00:04:02.240 --> 00:04:05.520]   like every hundred years or so, maybe even less.
[00:04:05.520 --> 00:04:08.400]   And so we only have, it's estimated,
[00:04:08.400 --> 00:04:09.840]   something like less than 1%,
[00:04:09.840 --> 00:04:13.000]   less than 1% of all the writing from that period.
[00:04:13.000 --> 00:04:15.920]   And so to find underground hundreds
[00:04:15.920 --> 00:04:18.640]   of definitely not in good condition,
[00:04:18.640 --> 00:04:21.680]   but still present, you know, papyrus scrolls were,
[00:04:21.680 --> 00:04:23.200]   on a few of them you can make out the lettering
[00:04:23.200 --> 00:04:25.040]   was like this enormous discovery.
[00:04:25.040 --> 00:04:27.240]   People immediately in a well-meaning attempt
[00:04:27.240 --> 00:04:29.560]   to read them started trying to open them,
[00:04:29.560 --> 00:04:31.480]   but they're really fragile.
[00:04:31.480 --> 00:04:32.360]   Like, you know, they're like,
[00:04:32.360 --> 00:04:34.400]   they turn to ash in your hand.
[00:04:34.400 --> 00:04:36.200]   And so hundreds were destroyed.
[00:04:36.200 --> 00:04:40.080]   People did things like cut them with daggers down the middle
[00:04:40.080 --> 00:04:42.520]   and, you know, a bunch of little pieces would flake off
[00:04:42.520 --> 00:04:44.160]   and they'd try to like get a few letters
[00:04:44.160 --> 00:04:46.200]   off of a couple of pieces.
[00:04:46.200 --> 00:04:49.560]   And then eventually there was a monk named Piaggio,
[00:04:49.560 --> 00:04:52.920]   who's an Italian monk, and he devised this machine
[00:04:52.920 --> 00:04:55.440]   kind of under the care of the Vatican
[00:04:55.440 --> 00:04:58.360]   to unroll these things very, very slowly,
[00:04:58.360 --> 00:05:01.080]   like half a centimeter a day, something like that.
[00:05:01.080 --> 00:05:02.560]   And a typical scroll, I think,
[00:05:02.560 --> 00:05:05.600]   could be 15 or 20 or 30 feet long.
[00:05:05.600 --> 00:05:09.320]   And managed to successfully unroll a few of these.
[00:05:09.320 --> 00:05:13.560]   And they found on them Greek philosophical texts
[00:05:13.560 --> 00:05:15.520]   in the Epicurean tradition
[00:05:15.520 --> 00:05:19.280]   by this little-known philosopher named Philodemus.
[00:05:19.280 --> 00:05:21.880]   But we got kind of new text from antiquity,
[00:05:21.880 --> 00:05:25.080]   which is not a thing that happens all the time.
[00:05:25.080 --> 00:05:27.480]   Eventually people stopped trying
[00:05:27.480 --> 00:05:28.800]   to physically unroll these things
[00:05:28.800 --> 00:05:29.880]   because so many were destroyed.
[00:05:29.880 --> 00:05:33.680]   And in fact, some attempts to physically unroll the scrolls
[00:05:33.680 --> 00:05:38.400]   continued even into like the 2000s, like 1990s, 2000s,
[00:05:38.400 --> 00:05:40.480]   and they were destroyed.
[00:05:40.480 --> 00:05:44.920]   So the current situation is we have 600-plus
[00:05:44.920 --> 00:05:48.160]   kind of roughly intact scrolls that we can't open.
[00:05:48.160 --> 00:05:49.640]   And I heard about this
[00:05:49.640 --> 00:05:51.240]   and I thought that was incredibly exciting,
[00:05:51.240 --> 00:05:52.880]   like the idea that there was information
[00:05:52.880 --> 00:05:54.520]   from 2,000 years in the past,
[00:05:54.520 --> 00:05:56.920]   we don't know what's in these things.
[00:05:56.920 --> 00:05:59.080]   And obviously people are trying to develop new ways
[00:05:59.080 --> 00:06:01.200]   and new technologies to open them.
[00:06:01.200 --> 00:06:05.560]   And I read about a professor
[00:06:05.560 --> 00:06:09.040]   at the University of Kentucky, Brent Seals,
[00:06:09.040 --> 00:06:10.960]   who had been trying to scan these
[00:06:10.960 --> 00:06:14.480]   using increasingly advanced imaging techniques
[00:06:14.480 --> 00:06:17.560]   and then use computer vision techniques and machine learning
[00:06:17.560 --> 00:06:20.520]   to kind of virtually unroll them without ever opening them.
[00:06:20.520 --> 00:06:22.480]   And they tried a lot of different things,
[00:06:22.480 --> 00:06:25.440]   but their most recent attempt in 2019
[00:06:25.440 --> 00:06:29.840]   was to take the scrolls to a particle accelerator
[00:06:29.840 --> 00:06:33.600]   in Oxford, England, called the Diamond Flight Source,
[00:06:33.600 --> 00:06:34.800]   and to make essentially
[00:06:34.800 --> 00:06:37.760]   an incredibly high-resolution CT scan
[00:06:37.760 --> 00:06:39.880]   of a sort of 3D X-ray scan.
[00:06:39.880 --> 00:06:42.320]   And they needed really high-energy photons
[00:06:42.320 --> 00:06:43.520]   in order to do this,
[00:06:43.520 --> 00:06:48.040]   and they were able to take scans at eight microns,
[00:06:48.040 --> 00:06:50.720]   so these really quite tiny voxels,
[00:06:50.720 --> 00:06:52.480]   which they thought would be sufficient.
[00:06:52.480 --> 00:06:54.040]   And I thought this was like the coolest thing ever.
[00:06:54.040 --> 00:06:55.040]   You know, we're using technology
[00:06:55.040 --> 00:06:57.600]   to sort of read this lost information from the past,
[00:06:57.600 --> 00:06:59.520]   and I sort of waited for the news
[00:06:59.520 --> 00:07:02.080]   that they had been decoded successfully.
[00:07:02.080 --> 00:07:04.320]   So that was, you know, 2020.
[00:07:04.320 --> 00:07:05.680]   And then I think COVID hit,
[00:07:05.680 --> 00:07:08.280]   everybody got a little bit slowed down by that.
[00:07:08.280 --> 00:07:10.600]   And last year, I found myself wondering,
[00:07:10.600 --> 00:07:11.920]   I wonder what happened to, you know,
[00:07:11.920 --> 00:07:15.240]   Dr. Seals and his scroll project.
[00:07:15.240 --> 00:07:16.520]   And I reached out,
[00:07:16.520 --> 00:07:19.680]   and it turned out they'd been making really good progress.
[00:07:19.680 --> 00:07:21.640]   You know, they'd gotten some machine learning models
[00:07:21.640 --> 00:07:25.960]   to start to identify ink inside of the scrolls,
[00:07:25.960 --> 00:07:29.360]   but they hadn't yet extracted words or passages.
[00:07:29.360 --> 00:07:31.320]   It's very challenging.
[00:07:31.320 --> 00:07:34.160]   And I invited him to come out to California and hang out,
[00:07:34.160 --> 00:07:37.040]   and to my shock, he did.
[00:07:37.040 --> 00:07:42.040]   And we got to talking and decided to team up
[00:07:42.040 --> 00:07:44.440]   and try to crack this thing.
[00:07:44.440 --> 00:07:47.480]   And the approach that we've settled on to do that
[00:07:47.480 --> 00:07:50.320]   is to actually launch an open competition.
[00:07:50.320 --> 00:07:53.480]   We're gonna, we've done a ton of work with his team
[00:07:53.480 --> 00:07:56.000]   to get the data into a shape where,
[00:07:56.000 --> 00:07:57.800]   and the tools and techniques
[00:07:57.800 --> 00:08:00.280]   and just the broad understanding of the materials
[00:08:00.280 --> 00:08:03.880]   into a shape where smart people can kind of approach it
[00:08:03.880 --> 00:08:05.440]   and get productive easily.
[00:08:05.440 --> 00:08:10.800]   And then I'm putting up together with Daniel Gross a prize,
[00:08:10.800 --> 00:08:13.600]   you know, sort of like an X prize or something like that
[00:08:13.600 --> 00:08:16.680]   for the first person or team who can actually read
[00:08:16.680 --> 00:08:18.200]   like substantial amounts of real text
[00:08:18.200 --> 00:08:20.840]   from one of these scrolls without opening them.
[00:08:20.840 --> 00:08:23.640]   And so we're launching that this week.
[00:08:23.640 --> 00:08:27.400]   You know, I guess maybe it's when this airs, I don't know.
[00:08:27.400 --> 00:08:28.760]   The stakes are kind of big.
[00:08:28.760 --> 00:08:30.840]   Like what gets me excited are the stakes.
[00:08:30.840 --> 00:08:34.600]   So the six or 800 scrolls that are there,
[00:08:34.600 --> 00:08:37.680]   it's estimated that if we could read all of them
[00:08:37.680 --> 00:08:40.760]   and you know, somehow the technique works
[00:08:40.760 --> 00:08:43.400]   and it generalizes to all the scrolls,
[00:08:43.400 --> 00:08:47.040]   then that would give us approximately a doubling
[00:08:47.040 --> 00:08:49.200]   of the total texts that we have from antiquity.
[00:08:49.200 --> 00:08:51.040]   This is what historians and classicists tell me.
[00:08:51.040 --> 00:08:53.560]   So it's not like, oh, we would get like a 5% bump
[00:08:53.560 --> 00:08:56.920]   or a 10% bump in the total ancient Roman or Greek text.
[00:08:56.920 --> 00:08:58.480]   It would be like, no, we get all of the texts
[00:08:58.480 --> 00:09:01.880]   that we have, again, multiple Shakespeare's
[00:09:01.880 --> 00:09:03.640]   is sort of one of the units that I've heard.
[00:09:03.640 --> 00:09:05.560]   So that would be significant.
[00:09:05.560 --> 00:09:06.680]   I mean, we don't know what's in there.
[00:09:06.680 --> 00:09:08.760]   You know, we've got a few Philodemus texts.
[00:09:08.760 --> 00:09:11.040]   Those are of some interest,
[00:09:11.040 --> 00:09:15.960]   but there could be lost epic poems or God knows what.
[00:09:15.960 --> 00:09:17.320]   So I'm really excited.
[00:09:17.320 --> 00:09:20.480]   And I think, you know, my bet is there's like a 50% chance
[00:09:20.480 --> 00:09:25.480]   that someone will encounter this opportunity
[00:09:25.480 --> 00:09:27.760]   and get the data and get nerd sniped by it.
[00:09:27.760 --> 00:09:29.960]   And we'll solve it this year.
[00:09:29.960 --> 00:09:31.000]   - I mean, really, it is something
[00:09:31.000 --> 00:09:32.080]   out of a science fiction novel.
[00:09:32.080 --> 00:09:33.000]   You know, it's like something you'd read
[00:09:33.000 --> 00:09:34.880]   in "Neil Stephens" or something.
[00:09:34.880 --> 00:09:37.880]   I was talking to Professor Seals before,
[00:09:37.880 --> 00:09:39.880]   and apparently the shock went both ways
[00:09:39.880 --> 00:09:41.400]   because the first few emails, he was like,
[00:09:41.400 --> 00:09:42.520]   this has got to be spam.
[00:09:42.520 --> 00:09:45.080]   Like, no way, Nat Friedman is reaching out
[00:09:45.080 --> 00:09:46.360]   and has found out about this prize.
[00:09:46.360 --> 00:09:49.440]   - That's really funny because he was really pretty hard
[00:09:49.440 --> 00:09:51.200]   to get in touch with.
[00:09:51.200 --> 00:09:54.160]   So like, I emailed him a couple of times,
[00:09:54.160 --> 00:09:56.040]   just like didn't respond.
[00:09:56.040 --> 00:10:01.040]   And so I was like, so I asked my admin, Emily,
[00:10:01.040 --> 00:10:04.040]   to call the secretary of his department
[00:10:04.040 --> 00:10:05.960]   and say like, Mr. Friedman requested me.
[00:10:05.960 --> 00:10:07.600]   And then like, he knew there was something
[00:10:07.600 --> 00:10:08.440]   like actually going on there.
[00:10:08.440 --> 00:10:09.840]   And so he finally got on the phone with me
[00:10:09.840 --> 00:10:12.560]   and we got on Zoom and he's like,
[00:10:12.560 --> 00:10:13.840]   why are you interested in this?
[00:10:13.840 --> 00:10:15.800]   (all laughing)
[00:10:15.800 --> 00:10:16.760]   I mean, I love Brent.
[00:10:16.760 --> 00:10:17.760]   He's fantastic.
[00:10:17.760 --> 00:10:21.280]   And I think, you know, we're like friends now.
[00:10:21.280 --> 00:10:25.400]   And I think we found that we think alike about this.
[00:10:25.400 --> 00:10:26.520]   And I think he's reached the point
[00:10:26.520 --> 00:10:29.160]   where he just really wants to crack.
[00:10:29.160 --> 00:10:30.800]   You know, they've taken this right up
[00:10:30.800 --> 00:10:32.040]   to the one yard line.
[00:10:32.040 --> 00:10:33.800]   Like, this is doable at this point.
[00:10:33.800 --> 00:10:37.360]   They've demonstrated, I think, every key component,
[00:10:37.360 --> 00:10:39.800]   but putting it all together, improving the quality,
[00:10:39.800 --> 00:10:41.240]   doing it at the scale of a whole scroll,
[00:10:41.240 --> 00:10:43.040]   this is still very hard work.
[00:10:43.040 --> 00:10:45.880]   And an open competition seems like the most efficient way
[00:10:45.880 --> 00:10:46.760]   to get it done.
[00:10:46.760 --> 00:10:48.320]   - Before we get into the state of the data
[00:10:48.320 --> 00:10:50.480]   and the different possible solutions,
[00:10:50.480 --> 00:10:53.200]   I want to make tangible like what could be gained
[00:10:53.200 --> 00:10:54.440]   if we can unwrap these.
[00:10:54.440 --> 00:10:57.160]   So you said there's a few more thousand scrolls.
[00:10:57.160 --> 00:11:00.120]   Are we talking about the ones in Philodemus' lair?
[00:11:00.120 --> 00:11:02.400]   Are we talking about the ones in other lairs?
[00:11:02.400 --> 00:11:04.120]   - You know, you'd think if you find this crazy villa
[00:11:04.120 --> 00:11:07.240]   that was owned by Julius Caesar's father-in-law,
[00:11:07.240 --> 00:11:09.280]   that we just like dig the whole thing out.
[00:11:09.280 --> 00:11:12.680]   But in fact, most of the exploration occurred
[00:11:12.680 --> 00:11:16.400]   in the 1700s through the Swiss engineers' tunnels
[00:11:16.400 --> 00:11:17.240]   underground.
[00:11:17.240 --> 00:11:18.680]   So it was never, the villa was never dug out
[00:11:18.680 --> 00:11:20.120]   and exposed to the air.
[00:11:20.120 --> 00:11:23.440]   You went down 50, 60 feet and then you dig tunnels.
[00:11:23.440 --> 00:11:25.880]   And, you know, again, they were looking for treasure,
[00:11:25.880 --> 00:11:28.600]   not like a full archeological exploration.
[00:11:28.600 --> 00:11:31.320]   So they mostly got treasure.
[00:11:31.320 --> 00:11:34.240]   In the '90s, some additional excavations were done
[00:11:34.240 --> 00:11:35.920]   kind of at the edge of the villa
[00:11:35.920 --> 00:11:37.480]   and they discovered a couple of things.
[00:11:37.480 --> 00:11:38.520]   First, they discovered this like,
[00:11:38.520 --> 00:11:40.440]   it was a seaside villa that faced the ocean.
[00:11:40.440 --> 00:11:43.800]   It was right on the water before the volcano erupted.
[00:11:43.800 --> 00:11:46.040]   The eruption actually pushed the shoreline out
[00:11:46.040 --> 00:11:49.320]   by depositing so much additional mud there.
[00:11:49.320 --> 00:11:51.640]   So it's no longer right by the ocean apparently.
[00:11:51.640 --> 00:11:53.280]   I've actually never been.
[00:11:53.280 --> 00:11:57.880]   And they also found that there were two additional floors
[00:11:57.880 --> 00:11:59.640]   in the villa that the tunnels
[00:11:59.640 --> 00:12:01.640]   apparently had never excavated.
[00:12:01.640 --> 00:12:05.000]   And so at most, a third of the villa has been excavated.
[00:12:05.000 --> 00:12:07.600]   Now they also know when they were discovering
[00:12:07.600 --> 00:12:10.600]   these papyrus scrolls that they found basically
[00:12:10.600 --> 00:12:13.800]   one little room where most of the scrolls were.
[00:12:13.800 --> 00:12:17.080]   And these were mostly these Philodemus texts,
[00:12:17.080 --> 00:12:19.080]   at least that's what we know.
[00:12:19.080 --> 00:12:21.880]   And they found apparently several revisions
[00:12:21.880 --> 00:12:23.800]   sometimes of the same text.
[00:12:23.800 --> 00:12:25.800]   And so they think, the hypothesis is,
[00:12:25.800 --> 00:12:28.120]   this was actually Philodemus' working library.
[00:12:28.120 --> 00:12:31.320]   He worked here, this sort of Epicurean philosopher.
[00:12:31.320 --> 00:12:34.320]   And in the hallways though,
[00:12:34.320 --> 00:12:36.800]   they occasionally found other scrolls,
[00:12:36.800 --> 00:12:38.800]   including crates of them.
[00:12:38.800 --> 00:12:41.080]   And the belief is, at least this is what historians
[00:12:41.080 --> 00:12:43.280]   have told me, and I'm no expert,
[00:12:43.280 --> 00:12:45.320]   but what they have told me is they think
[00:12:45.320 --> 00:12:47.040]   that the main library in this villa
[00:12:47.040 --> 00:12:49.000]   has probably not been excavated.
[00:12:49.000 --> 00:12:52.200]   And that the main library may be a Latin library
[00:12:52.200 --> 00:12:56.960]   and may contain literary texts, historical texts,
[00:12:56.960 --> 00:12:59.200]   other things, and that it could be much larger.
[00:12:59.200 --> 00:13:01.520]   Now I don't know how prone these classicists
[00:13:01.520 --> 00:13:03.240]   are to wishful thinking.
[00:13:03.240 --> 00:13:06.040]   It is a romantic idea, but they have some evidence
[00:13:06.040 --> 00:13:09.520]   in the presence of these partly evacuated sort of scrolls
[00:13:09.520 --> 00:13:12.800]   that were found in hallways and that sort of thing.
[00:13:12.800 --> 00:13:15.360]   So there are descriptions, I've since gone and read
[00:13:15.360 --> 00:13:20.360]   a bunch of the first-hand accounts of the excavations,
[00:13:20.360 --> 00:13:22.320]   and there are these heartbreaking descriptions
[00:13:22.320 --> 00:13:26.360]   of them finding an entire case of scrolls in Latin
[00:13:26.360 --> 00:13:28.080]   and accidentally destroying it
[00:13:28.080 --> 00:13:30.480]   as they tried to get it out of the mud.
[00:13:30.480 --> 00:13:34.480]   And there were maybe 30 scrolls or something in there.
[00:13:34.480 --> 00:13:37.040]   So there clearly was some other stuff
[00:13:37.040 --> 00:13:38.240]   that we just haven't got to.
[00:13:38.240 --> 00:13:43.240]   - I mean, you made some scrolls in the furnace, right?
[00:13:43.760 --> 00:13:47.040]   - Yeah, so papyrus, this is papyrus,
[00:13:47.040 --> 00:13:49.960]   and it's a reed, it's a grassy reed
[00:13:49.960 --> 00:13:51.920]   that grows on the Nile in Egypt.
[00:13:51.920 --> 00:13:54.080]   And for thousands of years, many thousands of years,
[00:13:54.080 --> 00:13:55.600]   they've been making paper out of it.
[00:13:55.600 --> 00:13:59.040]   And the way they do it is they take the kind of outer rind
[00:13:59.040 --> 00:14:03.240]   off of the papyrus, and then they cut the inner core
[00:14:03.240 --> 00:14:06.440]   into these strips, and they lay the strips out
[00:14:06.440 --> 00:14:08.160]   kind of parallel to one another,
[00:14:08.160 --> 00:14:11.040]   and then they put another layer at 90 degrees
[00:14:11.040 --> 00:14:13.760]   to that bottom layer, and they press it together
[00:14:13.760 --> 00:14:17.200]   in a press or under stones and let it dry out.
[00:14:17.200 --> 00:14:19.840]   And that's papyrus, essentially.
[00:14:19.840 --> 00:14:21.400]   And then they'll take some of those sheets
[00:14:21.400 --> 00:14:23.000]   and kind of glue them together with paste,
[00:14:23.000 --> 00:14:26.280]   made out of flour usually, and get a long scroll.
[00:14:26.280 --> 00:14:27.760]   And you can still buy it.
[00:14:27.760 --> 00:14:28.920]   I bought this on Amazon.
[00:14:28.920 --> 00:14:33.320]   And it's interesting because it's got a lot of texture,
[00:14:33.320 --> 00:14:36.120]   you know, those fibrous ridges of the papyrus plant.
[00:14:36.120 --> 00:14:39.720]   And so when you write on it, you really feel the texture.
[00:14:39.720 --> 00:14:42.200]   And I got it because I wanted to understand
[00:14:42.200 --> 00:14:44.040]   sort of what are these artifacts that we're working with.
[00:14:44.040 --> 00:14:47.760]   And so we made an attempt to simulate carbonizing
[00:14:47.760 --> 00:14:50.640]   a few of these, so we basically took a Dutch oven.
[00:14:50.640 --> 00:14:52.760]   Because when you carbonize something and you make charcoal,
[00:14:52.760 --> 00:14:54.360]   it's not like burning it with oxygen.
[00:14:54.360 --> 00:14:55.840]   You sort of remove the oxygen,
[00:14:55.840 --> 00:14:58.160]   heat it up, and let it carbonize.
[00:14:58.160 --> 00:14:59.920]   So we tried to simulate that with a Dutch oven,
[00:14:59.920 --> 00:15:01.720]   which is probably imperfect,
[00:15:01.720 --> 00:15:04.800]   and left it in the oven at 500 degrees Fahrenheit
[00:15:04.800 --> 00:15:07.160]   for maybe, I don't know how long these were,
[00:15:07.160 --> 00:15:09.560]   but our biggest attempt was like five or six hours.
[00:15:09.560 --> 00:15:12.480]   And they really, and so these things are incredibly light.
[00:15:12.480 --> 00:15:13.920]   And if you try to unfold them,
[00:15:13.920 --> 00:15:17.160]   they just fall apart in your hand very readily.
[00:15:17.160 --> 00:15:18.840]   I assume these are in somewhat better shape
[00:15:18.840 --> 00:15:19.880]   than the ones that were found
[00:15:19.880 --> 00:15:22.640]   because these were not in a volcanic eruption,
[00:15:22.640 --> 00:15:24.960]   and like covered in mud.
[00:15:24.960 --> 00:15:27.480]   I think the volcano was probably,
[00:15:27.480 --> 00:15:29.760]   maybe that mud was hotter than my oven can go.
[00:15:29.760 --> 00:15:31.200]   So, and it just flakes, you know,
[00:15:31.200 --> 00:15:32.520]   just sort of just squeeze it.
[00:15:32.520 --> 00:15:34.400]   It's just dust in your hand.
[00:15:34.400 --> 00:15:37.200]   And so we actually tried to replicate
[00:15:37.200 --> 00:15:39.600]   many of the heartbreaking 1700s,
[00:15:39.600 --> 00:15:42.000]   you know, 18th century unrolling techniques.
[00:15:42.000 --> 00:15:45.040]   Like they used rosewater, for example,
[00:15:45.040 --> 00:15:46.600]   or they tried to use different oils
[00:15:46.600 --> 00:15:48.160]   to soften it and unroll it.
[00:15:48.160 --> 00:15:50.400]   And most of them are just very destructive.
[00:15:50.400 --> 00:15:51.600]   They poured mercury into it
[00:15:51.600 --> 00:15:52.680]   'cause they thought mercury would slip
[00:15:52.680 --> 00:15:54.040]   between the layers potentially.
[00:15:54.040 --> 00:15:56.760]   So, so yeah, this is sort of what they look like.
[00:15:56.760 --> 00:16:00.200]   They, you know, they shrink and they turn to ash.
[00:16:00.200 --> 00:16:01.840]   - Yeah, for those listening, by the way,
[00:16:01.840 --> 00:16:03.800]   it kind of looks like a black,
[00:16:03.800 --> 00:16:05.920]   I mean, just imagine sort of the ash of a cigar,
[00:16:05.920 --> 00:16:09.160]   but a blacker, and it crumbles the same way.
[00:16:09.160 --> 00:16:12.400]   It's just a blistered black piece of rolled up papyrus.
[00:16:12.400 --> 00:16:14.760]   - Yeah, and they blister, the layers can separate,
[00:16:14.760 --> 00:16:16.280]   they can fuse.
[00:16:16.280 --> 00:16:18.360]   - And so this happened in 79 AD, right?
[00:16:18.360 --> 00:16:21.280]   So we know that anything before that could be in here,
[00:16:21.280 --> 00:16:22.960]   which I guess could include-
[00:16:22.960 --> 00:16:23.800]   - Yeah, so what could be in there?
[00:16:23.800 --> 00:16:25.720]   I don't know, you know, we,
[00:16:25.720 --> 00:16:29.600]   you and I have speculated about this.
[00:16:29.600 --> 00:16:31.720]   Well, I think, you know, it would be extremely exciting
[00:16:31.720 --> 00:16:33.800]   not to just get more Epicurean philosophy,
[00:16:33.800 --> 00:16:35.200]   although that's fine too.
[00:16:36.120 --> 00:16:39.040]   But almost anything would be interesting and additive.
[00:16:39.040 --> 00:16:42.560]   Like dreams are, I think it would maybe have a big impact
[00:16:42.560 --> 00:16:44.320]   to find something about early Christianity,
[00:16:44.320 --> 00:16:47.240]   like a contemporaneous mention of early Christianity.
[00:16:47.240 --> 00:16:49.520]   Maybe there'd be something that, you know,
[00:16:49.520 --> 00:16:52.760]   the church wouldn't want, that would be exciting to me.
[00:16:52.760 --> 00:16:55.840]   Maybe there'd be something, you know, some color or detail
[00:16:55.840 --> 00:16:58.880]   from someone commenting on Christianity or Jesus.
[00:16:58.880 --> 00:17:01.520]   That like, I think that would be a very big deal.
[00:17:01.520 --> 00:17:04.000]   We have no such things as far as I know.
[00:17:04.000 --> 00:17:06.720]   Other things that would be cool would be old stuff,
[00:17:06.720 --> 00:17:07.720]   like even older stuff.
[00:17:07.720 --> 00:17:10.960]   So there were several scrolls already found in there
[00:17:10.960 --> 00:17:13.200]   that they know were hundreds of years old
[00:17:13.200 --> 00:17:14.280]   when the villa was buried.
[00:17:14.280 --> 00:17:15.880]   So the villa was probably constructed
[00:17:15.880 --> 00:17:18.800]   about a hundred years prior, is my understanding.
[00:17:18.800 --> 00:17:22.160]   And they can tell from the style of writing.
[00:17:22.160 --> 00:17:23.720]   They can date, you know,
[00:17:23.720 --> 00:17:25.480]   they can date some of these scrolls.
[00:17:25.480 --> 00:17:28.280]   And so there was some old stuff in there
[00:17:28.280 --> 00:17:30.760]   and the Library of Alexandria was burned
[00:17:30.760 --> 00:17:33.200]   80 or 90 years prior.
[00:17:33.200 --> 00:17:36.640]   And so, again, maybe wishful thinking,
[00:17:36.640 --> 00:17:38.480]   but there's some rumors
[00:17:38.480 --> 00:17:41.440]   that some of those scrolls were evacuated
[00:17:41.440 --> 00:17:43.120]   and maybe some of them would have ended up
[00:17:43.120 --> 00:17:45.840]   at this substantial, prominent, you know,
[00:17:45.840 --> 00:17:47.520]   Mediterranean villa.
[00:17:47.520 --> 00:17:48.480]   God knows what would be in there.
[00:17:48.480 --> 00:17:49.800]   That would be really cool.
[00:17:49.800 --> 00:17:51.200]   I think it'd be great to find literature.
[00:17:51.200 --> 00:17:52.840]   Like personally, I think that would be exciting,
[00:17:52.840 --> 00:17:55.400]   like beautiful new poems or stories.
[00:17:55.400 --> 00:17:59.440]   We just don't have a ton because so little survived.
[00:17:59.440 --> 00:18:02.880]   And so I think that would be fun.
[00:18:02.880 --> 00:18:05.960]   - I think you had the best crazy idea
[00:18:05.960 --> 00:18:07.480]   for what could be in there,
[00:18:07.480 --> 00:18:10.240]   which was text, which was GPT watermarked.
[00:18:10.240 --> 00:18:13.200]   That would be a creepy feeling.
[00:18:13.200 --> 00:18:17.080]   - I still can't get over just how much of a plot
[00:18:17.080 --> 00:18:18.840]   of a sci-fi novel this is like, right?
[00:18:18.840 --> 00:18:23.000]   Like potentially the biggest intact library
[00:18:23.000 --> 00:18:26.040]   from the ancient world that has been sort of stopped
[00:18:26.040 --> 00:18:28.680]   like a debugger because of this volcano.
[00:18:28.680 --> 00:18:31.200]   And I mean, just the philosophers
[00:18:31.200 --> 00:18:34.800]   of antiquity forgotten, like the earliest gospels.
[00:18:34.800 --> 00:18:35.960]   There's so much interesting stuff there,
[00:18:35.960 --> 00:18:39.560]   but let's talk about what the data looks like.
[00:18:39.560 --> 00:18:42.840]   So you mentioned that they've been CT scanned
[00:18:42.840 --> 00:18:45.120]   and that they built these machine learning techniques
[00:18:45.120 --> 00:18:48.800]   to do segmentation and the unrolling.
[00:18:48.800 --> 00:18:50.920]   What would it take to get from there
[00:18:50.920 --> 00:18:54.200]   to an actual, to understand the actual content
[00:18:54.200 --> 00:18:55.560]   of what is within?
[00:18:55.560 --> 00:18:58.880]   - Dr. Seals actually pioneered this field
[00:18:58.880 --> 00:19:02.720]   of what he calls and is now widely called virtual unwrapping.
[00:19:02.720 --> 00:19:05.000]   And he did it actually not with these Herculaneum scrolls.
[00:19:05.000 --> 00:19:06.400]   These things are like expert mode.
[00:19:06.400 --> 00:19:07.280]   They're so difficult.
[00:19:07.280 --> 00:19:08.520]   I'll tell you why soon.
[00:19:08.520 --> 00:19:11.560]   But he did it initially with a scroll
[00:19:11.560 --> 00:19:13.680]   that was found in the Dead Sea in Israel.
[00:19:13.680 --> 00:19:15.360]   It's called the En-Gedi scroll.
[00:19:15.360 --> 00:19:16.560]   And it was carbonized actually
[00:19:16.560 --> 00:19:18.880]   under like slightly similar circumstances.
[00:19:18.880 --> 00:19:21.040]   I think there was a temple that was burned.
[00:19:21.040 --> 00:19:23.080]   The papyrus scroll was in a box.
[00:19:23.080 --> 00:19:25.080]   So it kind of, it's like a Dutch oven.
[00:19:25.080 --> 00:19:27.520]   It kind of carbonized in the same way.
[00:19:27.520 --> 00:19:29.720]   And so it was not openable.
[00:19:29.720 --> 00:19:31.280]   It just fall apart.
[00:19:31.280 --> 00:19:32.120]   And so the question was,
[00:19:32.120 --> 00:19:35.480]   could you non-destructively read the contents of it?
[00:19:35.480 --> 00:19:40.480]   And so he did this 3D x-ray, the CT scan of the scroll
[00:19:40.480 --> 00:19:44.480]   and then was able to do two things.
[00:19:44.480 --> 00:19:48.440]   First, the ink gave a great x-ray signature.
[00:19:48.440 --> 00:19:51.720]   And so it looked very different from the papyrus.
[00:19:51.720 --> 00:19:53.680]   It was a high contrast.
[00:19:53.680 --> 00:19:55.440]   And then second, he was able to segment
[00:19:55.440 --> 00:19:59.240]   the wines of the scroll throughout the entire body
[00:19:59.240 --> 00:20:03.120]   of the scroll and identify each layer.
[00:20:03.120 --> 00:20:05.480]   And then just geometrically unroll it
[00:20:05.480 --> 00:20:08.480]   using fairly normal flattening computer vision
[00:20:08.480 --> 00:20:10.240]   sort of techniques.
[00:20:10.240 --> 00:20:12.720]   And then read the contents of it.
[00:20:12.720 --> 00:20:14.320]   And it turned out to be, I think,
[00:20:14.320 --> 00:20:16.960]   an early part of the book of Leviticus,
[00:20:16.960 --> 00:20:18.880]   something of the Old Testament or the Torah.
[00:20:18.880 --> 00:20:23.480]   And that was like a landmark achievement.
[00:20:23.480 --> 00:20:25.080]   And so then the next idea was to apply
[00:20:25.080 --> 00:20:26.920]   those same techniques to this case.
[00:20:26.920 --> 00:20:29.200]   And so, okay, this is proven hard.
[00:20:29.200 --> 00:20:32.400]   I think there's a couple things that make it difficult.
[00:20:32.400 --> 00:20:34.760]   One is that, the primary one is that
[00:20:34.760 --> 00:20:38.480]   the ink used on the Herculaneum papyri,
[00:20:38.480 --> 00:20:41.360]   it is not very absorbent of x-ray.
[00:20:41.360 --> 00:20:44.680]   Like it basically seems to be equally absorbent of x-ray
[00:20:44.680 --> 00:20:48.560]   as the papyrus or very close, certainly not perfectly.
[00:20:48.560 --> 00:20:50.920]   And so you don't have this nice bright lettering
[00:20:50.920 --> 00:20:54.840]   that shows up kind of on your tomographic 3D x-ray.
[00:20:54.840 --> 00:20:57.760]   So you have to somehow develop new techniques
[00:20:57.760 --> 00:20:59.440]   for finding the ink in there.
[00:20:59.440 --> 00:21:00.680]   So that's sort of problem one,
[00:21:00.680 --> 00:21:02.600]   and it's been a major challenge.
[00:21:02.600 --> 00:21:04.000]   And then the second problem is
[00:21:04.000 --> 00:21:06.080]   the scrolls are just real messed up.
[00:21:06.080 --> 00:21:09.640]   Like they were long and tightly wound,
[00:21:09.640 --> 00:21:13.540]   highly distorted by the volcanic mud,
[00:21:13.540 --> 00:21:18.000]   which not only heated them, but partly deformed them.
[00:21:18.000 --> 00:21:21.880]   And so just the segmentation problem
[00:21:21.880 --> 00:21:25.360]   of identifying each of these layers throughout the scrolls,
[00:21:25.360 --> 00:21:27.220]   it's doable, but it's hard.
[00:21:27.220 --> 00:21:29.440]   Those are a couple of challenges.
[00:21:29.440 --> 00:21:30.560]   And then the other challenge, of course,
[00:21:30.560 --> 00:21:32.360]   is just getting access to scrolls
[00:21:32.360 --> 00:21:34.320]   and taking them to a particle accelerator.
[00:21:34.320 --> 00:21:35.880]   So you have to like have scroll access
[00:21:35.880 --> 00:21:39.840]   and particle accelerator access and time on those.
[00:21:39.840 --> 00:21:41.760]   It's expensive and difficult.
[00:21:41.760 --> 00:21:46.120]   And Dr. Seals did the hard work of making all that happen.
[00:21:46.120 --> 00:21:49.800]   And so the good news is very recently,
[00:21:49.800 --> 00:21:51.480]   just in the last couple of months,
[00:21:51.480 --> 00:21:53.300]   his lab has demonstrated
[00:21:53.300 --> 00:21:55.680]   with a convolutional neural network,
[00:21:55.680 --> 00:21:59.500]   the ability to actually recognize ink inside these x-rays.
[00:21:59.500 --> 00:22:04.500]   And I look at the x-ray scans and I cannot,
[00:22:04.500 --> 00:22:06.360]   at least in any of the renderings that we've seen,
[00:22:06.360 --> 00:22:08.160]   I can't see the ink, but the machine learning model
[00:22:08.160 --> 00:22:10.560]   can pick up on sort of very subtle patterns
[00:22:10.560 --> 00:22:12.600]   in the x-ray absorption at high resolution
[00:22:12.600 --> 00:22:16.520]   inside these volumes in order to identify ink.
[00:22:16.520 --> 00:22:17.480]   And we've seen that.
[00:22:17.480 --> 00:22:18.300]   And so you might ask, okay,
[00:22:18.300 --> 00:22:20.160]   like how do you train a model to do that?
[00:22:20.160 --> 00:22:22.520]   Because you need some kind of ground truth data
[00:22:22.520 --> 00:22:24.040]   to train the model.
[00:22:24.040 --> 00:22:27.040]   So the big insight that they had
[00:22:27.040 --> 00:22:30.680]   was to train on broken off fragments of the papyrus.
[00:22:30.680 --> 00:22:32.600]   So as people tried to open these over the years,
[00:22:32.600 --> 00:22:35.760]   you know, in Italy, they destroyed many of them,
[00:22:35.760 --> 00:22:38.280]   but they saved some of the pieces that broke off.
[00:22:38.280 --> 00:22:41.960]   And on some of those pieces, you can kind of see lettering.
[00:22:41.960 --> 00:22:46.520]   And if you take an infrared image of the fragment,
[00:22:46.520 --> 00:22:48.360]   then you can really see the lettering pretty well
[00:22:48.360 --> 00:22:49.200]   in some cases.
[00:22:49.200 --> 00:22:51.980]   And so they think it's 930 nanometers.
[00:22:51.980 --> 00:22:53.940]   They take this little infrared image.
[00:22:53.940 --> 00:22:55.440]   Now you've got some ground truth.
[00:22:55.440 --> 00:22:59.980]   Then you do a CT scan of that broken off fragment
[00:22:59.980 --> 00:23:03.440]   and you try to align it, register it with the image.
[00:23:03.440 --> 00:23:07.240]   And then you have data that you can use potentially
[00:23:07.240 --> 00:23:08.720]   to train a model.
[00:23:08.720 --> 00:23:12.600]   And that turned out to work in the case of the fragments.
[00:23:12.600 --> 00:23:15.240]   Okay, so now I think this is sort of the why now.
[00:23:15.240 --> 00:23:19.000]   This is why I think launching this challenge now
[00:23:19.000 --> 00:23:23.560]   is the right time because we have a lot of reasons
[00:23:23.560 --> 00:23:24.760]   to believe it can work.
[00:23:24.760 --> 00:23:26.080]   Like in the core techniques,
[00:23:26.080 --> 00:23:27.680]   the core pieces have been demonstrated.
[00:23:27.680 --> 00:23:29.760]   It just all has to be put together
[00:23:29.760 --> 00:23:32.960]   at the scale of these really complicated scrolls.
[00:23:32.960 --> 00:23:36.680]   And so, so yeah, I think if you can do the segmentation,
[00:23:36.680 --> 00:23:39.160]   which is probably a lot of work,
[00:23:39.160 --> 00:23:40.900]   maybe there's some way to automate it.
[00:23:40.900 --> 00:23:43.560]   And then you can figure out how to apply these models
[00:23:43.560 --> 00:23:46.760]   inside the body of a scroll and not just to these fragments,
[00:23:46.760 --> 00:23:50.600]   then it seems like you could probably read lots of text.
[00:23:50.600 --> 00:23:54.000]   - Why did you decide to do it in the form of a prize
[00:23:54.000 --> 00:23:56.520]   rather than just like giving a grant to the team
[00:23:56.520 --> 00:23:57.440]   that was already pursuing it
[00:23:57.440 --> 00:23:58.680]   or maybe some other team that wants to take it up?
[00:23:58.680 --> 00:23:59.880]   - We talked about that.
[00:23:59.880 --> 00:24:05.080]   But I think what we basically concluded
[00:24:05.080 --> 00:24:06.760]   was the search space of different ways
[00:24:06.760 --> 00:24:08.340]   you could solve this is pretty big.
[00:24:08.340 --> 00:24:11.900]   And we just wanted to get it done as quickly as possible.
[00:24:11.900 --> 00:24:15.360]   So having a contest means lots of people
[00:24:15.360 --> 00:24:16.500]   are gonna try lots of things
[00:24:16.500 --> 00:24:19.320]   and someone's gonna figure it out quickly.
[00:24:19.320 --> 00:24:22.240]   Many eyes may make it shallow as a task.
[00:24:22.240 --> 00:24:24.280]   And so I think that's the main thing.
[00:24:24.280 --> 00:24:25.880]   Like probably someone could do it,
[00:24:25.880 --> 00:24:28.240]   but I think this will just be a lot more efficient.
[00:24:28.240 --> 00:24:29.080]   And it's fun too.
[00:24:29.080 --> 00:24:29.920]   I think this is fun.
[00:24:29.920 --> 00:24:32.280]   Like I think it's interesting to do a contest
[00:24:32.280 --> 00:24:34.920]   and who knows who will solve it or how.
[00:24:34.920 --> 00:24:37.560]   People may, they may not even use machine learning.
[00:24:37.560 --> 00:24:40.120]   You know, we think that's the most likely approach
[00:24:40.120 --> 00:24:41.120]   for recognizing the ink,
[00:24:41.120 --> 00:24:43.000]   but they may find some other approach
[00:24:43.000 --> 00:24:43.840]   that we haven't thought of.
[00:24:43.840 --> 00:24:45.000]   - One question people might have
[00:24:45.000 --> 00:24:48.700]   is that you have these visible fragments mapped out.
[00:24:48.700 --> 00:24:53.080]   Do we expect them to correspond to the burned off
[00:24:53.080 --> 00:24:56.120]   or the ashen carbonized squirrels
[00:24:56.120 --> 00:24:57.600]   that you can do machine learning on?
[00:24:57.600 --> 00:24:59.240]   The ground truth of one can correspond to the other.
[00:24:59.240 --> 00:25:00.620]   - I think this is a very legitimate concern.
[00:25:00.620 --> 00:25:01.460]   They're different.
[00:25:01.460 --> 00:25:03.360]   Like when you have a broken off fragment,
[00:25:03.360 --> 00:25:05.440]   there's air above the ink.
[00:25:05.440 --> 00:25:09.100]   So when you CT scan it, you have kind of ink next to air.
[00:25:09.100 --> 00:25:10.720]   Inside of a wrapped scroll,
[00:25:10.720 --> 00:25:12.760]   the ink might be next to papyrus, right?
[00:25:12.760 --> 00:25:15.000]   'Cause it's pushing up against the next layer.
[00:25:15.000 --> 00:25:17.660]   And your model has to,
[00:25:17.660 --> 00:25:19.600]   your model may not know what to do with that.
[00:25:19.600 --> 00:25:22.960]   And so yeah, I think this is one of the challenges
[00:25:22.960 --> 00:25:25.480]   and sort of how you take these models
[00:25:25.480 --> 00:25:26.440]   that are trained on fragments
[00:25:26.440 --> 00:25:29.360]   and translate them to the slightly different environment.
[00:25:29.360 --> 00:25:31.640]   But maybe there's parts of the scroll
[00:25:31.640 --> 00:25:33.080]   where there is air on the inside.
[00:25:33.080 --> 00:25:33.920]   I mean, we know that to be true.
[00:25:33.920 --> 00:25:35.400]   You can sort of see that here.
[00:25:35.400 --> 00:25:38.200]   And so I think it should at least partly work
[00:25:38.200 --> 00:25:40.040]   and clever people can probably figure out
[00:25:40.040 --> 00:25:41.520]   how to make it completely work.
[00:25:41.520 --> 00:25:43.920]   - Yeah, so you said the odds are about 50/50.
[00:25:43.920 --> 00:25:45.200]   What makes you think that?
[00:25:45.200 --> 00:25:46.520]   - It can be done? - Yeah.
[00:25:46.520 --> 00:25:49.120]   - I think it can be done because we've recognized ink
[00:25:49.120 --> 00:25:52.440]   from a CT scan on the fragments.
[00:25:52.440 --> 00:25:55.500]   And I think everything else is probably geometry
[00:25:55.500 --> 00:25:56.620]   and computer vision.
[00:25:56.620 --> 00:25:58.880]   The scans are very high resolution.
[00:25:58.880 --> 00:26:01.880]   So they're eight microns, eight micrometers.
[00:26:01.880 --> 00:26:04.640]   And they're taken, if you kind of stood a scroll on end
[00:26:04.640 --> 00:26:07.280]   like this, they're taken in these slices through it.
[00:26:07.280 --> 00:26:08.440]   Right, like this.
[00:26:08.440 --> 00:26:10.920]   So it's like this in the Z-axis from bottom to top,
[00:26:10.920 --> 00:26:12.640]   there are these slices.
[00:26:12.640 --> 00:26:14.360]   And the way they're represented on disk
[00:26:14.360 --> 00:26:16.640]   is each slice is a TIFF file.
[00:26:16.640 --> 00:26:18.760]   And for the full scrolls,
[00:26:18.760 --> 00:26:21.280]   each slice is like 100 something megabytes.
[00:26:21.280 --> 00:26:23.440]   So they're quite high resolution.
[00:26:23.440 --> 00:26:25.520]   And then if you stack, for example, 100 of these,
[00:26:25.520 --> 00:26:26.580]   they're eight microns, right?
[00:26:26.580 --> 00:26:28.800]   So 100 of these is 0.8 millimeters.
[00:26:28.800 --> 00:26:31.040]   So millimeter is pretty small.
[00:26:31.040 --> 00:26:35.360]   So they're fairly, we think the resolution is good enough,
[00:26:35.360 --> 00:26:37.760]   or at least right on the edge of good enough
[00:26:37.760 --> 00:26:38.880]   that it should be possible.
[00:26:38.880 --> 00:26:42.100]   There's sort of like, seem to be six or eight pixels,
[00:26:42.100 --> 00:26:48.100]   for voxels, I guess, across an entire layer of papyrus.
[00:26:48.100 --> 00:26:49.420]   That's probably enough.
[00:26:49.420 --> 00:26:51.800]   And we've also seen with the machine learning models,
[00:26:51.800 --> 00:26:55.000]   Dr. Seals has got some PhD students
[00:26:55.000 --> 00:26:58.400]   who have actually demonstrated this at eight microns.
[00:26:58.400 --> 00:27:02.760]   So I think that the ink recognition will work.
[00:27:02.760 --> 00:27:04.080]   I think the data is in it.
[00:27:04.080 --> 00:27:06.480]   The data is clearly physically in the scrolls, right?
[00:27:06.480 --> 00:27:09.600]   The ink was carbonized, the papyrus was carbonized,
[00:27:09.600 --> 00:27:13.780]   but not as, like a lot of data actually physically survived.
[00:27:13.780 --> 00:27:16.200]   And then the question is,
[00:27:16.200 --> 00:27:18.480]   did the data make it into the scans?
[00:27:18.480 --> 00:27:20.540]   And I think that's very likely,
[00:27:20.540 --> 00:27:23.800]   based on the results that we've seen so far.
[00:27:23.800 --> 00:27:26.680]   And so I think it's just about a smart person solving this,
[00:27:26.680 --> 00:27:28.560]   and, or a smart group of people,
[00:27:28.560 --> 00:27:30.040]   or just a dogged group of people
[00:27:30.040 --> 00:27:33.040]   who do a lot of manual work that could also be true.
[00:27:33.040 --> 00:27:35.180]   You may have to be smart and dogged.
[00:27:35.180 --> 00:27:38.640]   But, and I think that's where most of my uncertainty is.
[00:27:38.640 --> 00:27:41.040]   This, just like whether somebody does it.
[00:27:41.040 --> 00:27:43.080]   - Yeah, I mean, if a quarter of a million dollars
[00:27:43.080 --> 00:27:44.640]   doesn't motivate you.
[00:27:44.640 --> 00:27:46.160]   - Yeah, I think money's good, yep.
[00:27:46.160 --> 00:27:47.000]   I mean, there's a lot of money
[00:27:47.000 --> 00:27:47.960]   in machine learning these days.
[00:27:47.960 --> 00:27:49.680]   - That's true as well.
[00:27:49.680 --> 00:27:52.080]   Do we have enough data in the form of scrolls
[00:27:52.080 --> 00:27:55.880]   that have been mapped out to be able to train a model,
[00:27:55.880 --> 00:27:57.120]   if that's the best way to go?
[00:27:57.120 --> 00:27:58.560]   I guess, 'cause one question somebody might have is,
[00:27:58.560 --> 00:28:00.600]   listen, if you already have this ground truth,
[00:28:00.600 --> 00:28:03.320]   why hasn't Dr. Seale's team already been able
[00:28:03.320 --> 00:28:04.280]   to just train a model? - I think they will.
[00:28:04.280 --> 00:28:07.240]   I think if we just let them do it, they'll get it solved.
[00:28:07.240 --> 00:28:08.320]   It might take a little bit longer,
[00:28:08.320 --> 00:28:11.400]   'cause it's not a huge number of people,
[00:28:11.400 --> 00:28:13.880]   and there is a big search space here.
[00:28:13.880 --> 00:28:15.960]   But I mean, yeah, if we didn't launch this contest,
[00:28:15.960 --> 00:28:17.720]   I'd still think this would get solved.
[00:28:17.720 --> 00:28:19.160]   But it might take several years.
[00:28:19.160 --> 00:28:22.640]   And I think this way, it's likely to happen this year.
[00:28:22.640 --> 00:28:24.200]   - Okay, and what happens,
[00:28:24.200 --> 00:28:27.040]   let's say the prize is solved,
[00:28:27.040 --> 00:28:28.280]   somebody figures out how to do this,
[00:28:28.280 --> 00:28:30.920]   and we can read the first scroll.
[00:28:30.920 --> 00:28:32.360]   You mentioned that these other layers
[00:28:32.360 --> 00:28:33.200]   haven't been excavated.
[00:28:33.200 --> 00:28:35.920]   How is the world gonna react?
[00:28:35.920 --> 00:28:37.040]   Let's say we get one of these mapped out.
[00:28:37.040 --> 00:28:38.200]   How's it work out? - That's my personal hope
[00:28:38.200 --> 00:28:39.040]   for this.
[00:28:39.040 --> 00:28:41.520]   I always like to look for these cheap leverage hacks,
[00:28:41.520 --> 00:28:44.200]   these moments where you can do a relatively small thing,
[00:28:44.200 --> 00:28:46.440]   and you kick a pebble, and you get an avalanche.
[00:28:46.440 --> 00:28:50.600]   And the theory is, and Brent shares this theory,
[00:28:50.600 --> 00:28:53.440]   the theory is that if you can read one scroll,
[00:28:53.440 --> 00:28:56.240]   like just one scroll, and we only have two scanned scrolls.
[00:28:56.240 --> 00:28:58.520]   There's hundreds of surviving scrolls.
[00:28:58.520 --> 00:29:01.480]   It's relatively expensive to book a particle accelerator.
[00:29:01.480 --> 00:29:04.480]   So if you can scan one scroll, and you know it works,
[00:29:04.480 --> 00:29:05.720]   and you can generalize the technique out,
[00:29:05.720 --> 00:29:07.240]   and it's gonna work on these other scrolls,
[00:29:07.240 --> 00:29:09.800]   then the money, which is probably low millions,
[00:29:09.800 --> 00:29:12.840]   maybe only $1 million to scan the remaining scrolls
[00:29:12.840 --> 00:29:15.040]   will just arrive.
[00:29:15.040 --> 00:29:19.120]   It's too sweet of a prize for that not to happen.
[00:29:19.120 --> 00:29:24.120]   And the urgency and kind of return
[00:29:24.120 --> 00:29:26.840]   on excavating the rest of the villa
[00:29:26.840 --> 00:29:28.520]   will be incredibly obvious too.
[00:29:28.520 --> 00:29:31.480]   Because if there are thousands more papyrus scrolls in there,
[00:29:31.480 --> 00:29:33.640]   and we now have the techniques to read them,
[00:29:33.640 --> 00:29:35.040]   then there's gold in that mud.
[00:29:35.040 --> 00:29:37.400]   And it's gotta be dug out.
[00:29:37.400 --> 00:29:42.400]   And it's amazing how little money there is for archeology.
[00:29:42.400 --> 00:29:46.120]   It's literally for decades, no one's been digging there.
[00:29:46.120 --> 00:29:50.120]   So that's my hope, is that this is the catalyst
[00:29:50.120 --> 00:29:52.200]   that it works, somebody reads it,
[00:29:52.200 --> 00:29:55.480]   they get a lot of glory, we all get to feel great,
[00:29:55.480 --> 00:29:59.400]   and then the diggers arrive in Herculaneum,
[00:29:59.400 --> 00:30:00.320]   and they dig out the rest.
[00:30:00.320 --> 00:30:03.080]   - I wonder if the budget for archeological movies
[00:30:03.080 --> 00:30:05.720]   and games like Uncharted or Indiana Jones
[00:30:05.720 --> 00:30:06.840]   is bigger than the actual budget
[00:30:06.840 --> 00:30:09.360]   to do real world archeology.
[00:30:09.360 --> 00:30:11.080]   But I was talking to some of the people
[00:30:11.080 --> 00:30:13.280]   before this interview, and that's one thing they emphasized
[00:30:13.280 --> 00:30:16.280]   is your ability to find these leverage points.
[00:30:16.280 --> 00:30:17.560]   For example, with California EMB,
[00:30:17.560 --> 00:30:20.000]   I don't know the exact amount you seeded it with,
[00:30:20.000 --> 00:30:22.800]   but for that amount of money,
[00:30:22.800 --> 00:30:25.880]   and for an institution that is that new,
[00:30:25.880 --> 00:30:27.280]   it is one of the very few institutions
[00:30:27.280 --> 00:30:30.000]   that has had a significant amount of political influence.
[00:30:30.000 --> 00:30:31.760]   If you look at the state of EMB in California
[00:30:31.760 --> 00:30:33.440]   and nationally today, I guess,
[00:30:33.440 --> 00:30:34.600]   how do you identify these things?
[00:30:34.600 --> 00:30:37.080]   How do you see, I mean, there's plenty of people
[00:30:37.080 --> 00:30:38.720]   who have money who get into history
[00:30:38.720 --> 00:30:40.160]   or get into whatever subject,
[00:30:40.160 --> 00:30:41.520]   very few do something about it.
[00:30:41.520 --> 00:30:42.360]   How do you figure out where to--
[00:30:42.360 --> 00:30:44.720]   - Yeah, you know, I'm a little bit mystified
[00:30:44.720 --> 00:30:47.600]   by why people don't do more things too.
[00:30:47.600 --> 00:30:51.800]   Like, I think, first of all, I don't know.
[00:30:51.800 --> 00:30:52.640]   Maybe you can tell me.
[00:30:52.640 --> 00:30:53.760]   Why are more people doing things?
[00:30:53.760 --> 00:30:56.040]   Like, I think most rich people are boring
[00:30:56.040 --> 00:30:58.480]   and they should do more cool things.
[00:30:58.480 --> 00:31:01.860]   So I'm hoping that they do that now.
[00:31:01.860 --> 00:31:04.720]   But yeah, I mean, I don't know.
[00:31:04.720 --> 00:31:07.240]   I think part of it is I just fundamentally
[00:31:07.240 --> 00:31:08.920]   don't believe the world is efficient.
[00:31:08.920 --> 00:31:12.060]   And so if I see an opportunity to do something,
[00:31:12.060 --> 00:31:13.860]   I don't have a, I used to,
[00:31:13.860 --> 00:31:15.800]   but I no longer have a reflexive reaction that says,
[00:31:15.800 --> 00:31:17.160]   oh, that must not be a good idea.
[00:31:17.160 --> 00:31:19.680]   If it were a good idea, someone would already be doing it.
[00:31:19.680 --> 00:31:21.840]   Like, someone must be taking care of housing policy
[00:31:21.840 --> 00:31:22.880]   in California, right?
[00:31:22.880 --> 00:31:25.800]   Or somebody must be taking care of this or that.
[00:31:25.800 --> 00:31:28.720]   And so I think, first, I don't have that filter
[00:31:28.720 --> 00:31:31.560]   that says the world's efficient, don't bother.
[00:31:31.560 --> 00:31:33.060]   Someone's probably got it covered.
[00:31:33.060 --> 00:31:35.800]   And then the second thing is I kind of have learned
[00:31:35.800 --> 00:31:38.680]   to trust my enthusiasm.
[00:31:38.680 --> 00:31:40.620]   You know, this gets me in trouble too.
[00:31:40.620 --> 00:31:44.080]   But if I get like really enthusiastic about something
[00:31:44.080 --> 00:31:46.800]   and that enthusiasm kind of persists,
[00:31:46.800 --> 00:31:49.600]   I just indulge it and just think,
[00:31:49.600 --> 00:31:51.960]   oh yeah, I'm gonna go to like, you know,
[00:31:51.960 --> 00:31:54.080]   like I like doing the things I'm enthusiastic about.
[00:31:54.080 --> 00:31:57.220]   And so I just kind of let myself be impulsive.
[00:31:57.220 --> 00:31:59.520]   And so frequently what you do,
[00:31:59.520 --> 00:32:02.040]   there's this great image that I found and tweeted,
[00:32:02.040 --> 00:32:04.480]   which said, we do these things not because they are easy,
[00:32:04.480 --> 00:32:06.720]   but because we thought they would be easy.
[00:32:06.720 --> 00:32:08.400]   And so, yeah, like that's frequently what happens
[00:32:08.400 --> 00:32:11.560]   is like the commitment to do it is impulsive
[00:32:11.560 --> 00:32:13.160]   'cause, and it's done out of enthusiasm.
[00:32:13.160 --> 00:32:14.800]   And then you get into it and you're like, oh my God,
[00:32:14.800 --> 00:32:17.840]   this is like really much harder than we expected.
[00:32:17.840 --> 00:32:20.600]   But then you're sort of committed and you're stuck
[00:32:20.600 --> 00:32:22.200]   and you're gonna have to get it done.
[00:32:22.200 --> 00:32:23.760]   Like, I thought this project
[00:32:23.760 --> 00:32:25.160]   would be relatively straightforward.
[00:32:25.160 --> 00:32:27.040]   I'm just gonna take the data and put it up.
[00:32:27.040 --> 00:32:28.560]   But of course everything is,
[00:32:28.560 --> 00:32:31.640]   and truly 99% of the work has already been done
[00:32:31.640 --> 00:32:36.280]   by Dr. Seals and his team at the University of Kentucky.
[00:32:36.280 --> 00:32:37.720]   I am a kind of carpetbagger.
[00:32:37.720 --> 00:32:40.000]   I've shown up at the end here to like,
[00:32:40.000 --> 00:32:41.840]   try to do a new piece of it.
[00:32:41.840 --> 00:32:42.680]   But-
[00:32:42.680 --> 00:32:43.560]   - The last mile is often the hardest.
[00:32:43.560 --> 00:32:45.960]   - Well, I mean, it's turned out to be fractal anyway.
[00:32:45.960 --> 00:32:47.720]   Like, just like all the little bits
[00:32:47.720 --> 00:32:50.480]   that you have to get right to do a thing and have it work.
[00:32:50.480 --> 00:32:52.880]   And I hope we got all of them.
[00:32:52.880 --> 00:32:55.400]   But I think that's part of it is just like, yeah,
[00:32:55.400 --> 00:32:56.760]   not believing the world's efficient,
[00:32:56.760 --> 00:32:59.040]   then just like allowing your enthusiasm
[00:32:59.040 --> 00:33:00.200]   to cause you to commit to something
[00:33:00.200 --> 00:33:02.600]   that turns out to be a lot of work and really hard.
[00:33:02.600 --> 00:33:04.920]   And then you just are like stubborn and don't wanna fail.
[00:33:04.920 --> 00:33:06.240]   And so you keep at it.
[00:33:06.240 --> 00:33:07.920]   I don't know, I think that's it.
[00:33:07.920 --> 00:33:09.480]   - Yeah, I don't know.
[00:33:09.480 --> 00:33:12.000]   I feel like the efficiency point,
[00:33:12.000 --> 00:33:13.880]   do you think that's particularly true just of things
[00:33:13.880 --> 00:33:15.360]   like California or this,
[00:33:15.360 --> 00:33:18.160]   where there isn't a direct monetary incentive or?
[00:33:18.160 --> 00:33:19.000]   - No, I mean, look,
[00:33:19.000 --> 00:33:20.920]   certainly parts of the world are more efficient than others.
[00:33:20.920 --> 00:33:21.760]   - Right.
[00:33:21.760 --> 00:33:24.120]   - And you can't assume equal levels
[00:33:24.120 --> 00:33:25.560]   of inefficiency everywhere.
[00:33:25.560 --> 00:33:29.000]   But I'm like constantly surprised
[00:33:29.000 --> 00:33:32.800]   by how even in areas you expect to be very efficient,
[00:33:32.800 --> 00:33:35.200]   there are things that are sort of in plain sight that no,
[00:33:35.200 --> 00:33:37.640]   and it's not that I see them and others don't.
[00:33:37.640 --> 00:33:39.720]   There's lots of stuff I don't see too.
[00:33:39.720 --> 00:33:42.920]   I was talking to some traders at a hedge fund recently,
[00:33:42.920 --> 00:33:44.400]   and I asked them,
[00:33:44.400 --> 00:33:46.520]   I was trying to understand the role secrets play
[00:33:46.520 --> 00:33:48.080]   in the success of a hedge fund.
[00:33:48.080 --> 00:33:50.040]   And the reason I was interested in that
[00:33:50.040 --> 00:33:52.200]   is because I think the AI labs
[00:33:52.200 --> 00:33:55.200]   are going to enter a new similar dynamic
[00:33:55.200 --> 00:33:56.720]   where their secrets are very valuable.
[00:33:56.720 --> 00:34:00.760]   Like if you have a 50% training efficiency improvement
[00:34:00.760 --> 00:34:03.200]   and your training runs cost $100 million,
[00:34:03.200 --> 00:34:05.480]   that is a $50 million secret that you have
[00:34:05.480 --> 00:34:06.920]   that you want to keep.
[00:34:06.920 --> 00:34:09.240]   And hedge funds do that kind of thing routinely.
[00:34:09.240 --> 00:34:11.120]   And so I asked some traders
[00:34:11.120 --> 00:34:13.240]   at a very successful hedge fund,
[00:34:13.240 --> 00:34:19.120]   if you had maybe your smartest trader get on Twitch
[00:34:19.120 --> 00:34:22.120]   for 10 minutes once a month,
[00:34:22.120 --> 00:34:23.800]   and on that Twitch stream,
[00:34:23.800 --> 00:34:28.560]   describe their 30 day old trading strategies, right?
[00:34:28.560 --> 00:34:29.400]   So not your current ones,
[00:34:29.400 --> 00:34:31.240]   but the ones that are a month old,
[00:34:31.240 --> 00:34:34.400]   how would that affect your business
[00:34:34.400 --> 00:34:35.640]   after 12 months of doing that?
[00:34:35.640 --> 00:34:37.080]   So 12 months, 10 minutes a month,
[00:34:37.080 --> 00:34:38.600]   30 day look back,
[00:34:38.600 --> 00:34:40.480]   so it's two hours in a year.
[00:34:40.480 --> 00:34:42.520]   And to my shock,
[00:34:42.520 --> 00:34:46.400]   they told me 80% reduction in their profits.
[00:34:46.400 --> 00:34:48.080]   Like it would have a huge impact.
[00:34:48.080 --> 00:34:48.920]   And then I asked, okay,
[00:34:48.920 --> 00:34:52.920]   so how long would the look back window have to be
[00:34:52.920 --> 00:34:55.960]   before it would have like a relatively small effect
[00:34:55.960 --> 00:34:56.800]   in your business?
[00:34:56.800 --> 00:34:58.360]   And they said 10 years.
[00:34:58.360 --> 00:35:01.280]   So like that I think is just quite strong evidence
[00:35:01.280 --> 00:35:02.760]   that the world's not perfectly efficient
[00:35:02.760 --> 00:35:05.800]   'cause these folks make billions of dollars
[00:35:05.800 --> 00:35:09.120]   using secrets that could be related in like an hour
[00:35:09.120 --> 00:35:11.120]   or something like that.
[00:35:11.120 --> 00:35:13.120]   And yet others don't have them
[00:35:13.120 --> 00:35:14.600]   or their secrets wouldn't work.
[00:35:14.600 --> 00:35:19.320]   And so I think there are different levels of efficiency
[00:35:19.320 --> 00:35:20.160]   in the world,
[00:35:20.160 --> 00:35:21.320]   but on the whole,
[00:35:21.320 --> 00:35:24.720]   our like default estimate of how efficient the world is,
[00:35:24.720 --> 00:35:27.400]   is far too charitable.
[00:35:27.400 --> 00:35:28.800]   On the particular point, by the way,
[00:35:28.800 --> 00:35:31.960]   of AI labs potentially starting secrets,
[00:35:31.960 --> 00:35:34.080]   I mean, you have this sort of strange norm
[00:35:34.080 --> 00:35:36.400]   of different people from different AI labs,
[00:35:36.400 --> 00:35:38.600]   not only being friends,
[00:35:38.600 --> 00:35:40.120]   but like often living together, right?
[00:35:40.120 --> 00:35:41.800]   So it would be like Oppenheimer living
[00:35:41.800 --> 00:35:44.040]   with somebody working on the Russian atomic bomb
[00:35:44.040 --> 00:35:44.880]   or something like that.
[00:35:44.880 --> 00:35:46.400]   Do you think those norms will persist
[00:35:46.400 --> 00:35:48.560]   once the value of the secrets is realized?
[00:35:48.560 --> 00:35:51.040]   Yeah, I was just wondering about that some more today.
[00:35:51.040 --> 00:35:54.040]   I mean, it seems to be sort of slowing,
[00:35:54.040 --> 00:35:57.160]   you know, they seem to be trying to close the valves,
[00:35:57.160 --> 00:36:00.400]   but I think there's a lot of things working against them
[00:36:00.400 --> 00:36:01.240]   in this regard.
[00:36:01.240 --> 00:36:03.720]   So one is, again, that the secrets are relatively simple.
[00:36:03.720 --> 00:36:07.640]   Two is that you coming off this academic norm of publishing
[00:36:07.640 --> 00:36:10.320]   and really like the entire culture
[00:36:10.320 --> 00:36:12.600]   is based on sort of sharing and publishing.
[00:36:12.600 --> 00:36:13.560]   You know, three is, as you said,
[00:36:13.560 --> 00:36:14.920]   they all live in group houses,
[00:36:14.920 --> 00:36:16.480]   some are in polycules, you know,
[00:36:16.480 --> 00:36:19.720]   there's just a lot of intermixing.
[00:36:19.720 --> 00:36:21.600]   And then it's all in California
[00:36:21.600 --> 00:36:23.840]   and California is non-compete state.
[00:36:23.840 --> 00:36:24.920]   We don't have non-competes.
[00:36:24.920 --> 00:36:28.000]   And so you'd have to change the culture,
[00:36:28.000 --> 00:36:30.240]   get everybody their own house and move to Connecticut.
[00:36:30.240 --> 00:36:32.520]   And then, you know, maybe it would work.
[00:36:32.520 --> 00:36:36.080]   You know, I think ML engineer salaries
[00:36:36.080 --> 00:36:38.720]   and compensation packages will probably be adjusted
[00:36:38.720 --> 00:36:41.400]   to try to, you know, address this
[00:36:41.400 --> 00:36:44.160]   'cause you don't want your secrets walking out the door.
[00:36:44.160 --> 00:36:47.400]   There are engineers, you know, Igor Babushkin, for example,
[00:36:47.400 --> 00:36:50.800]   who has just, I believe, joined Twitter.
[00:36:50.800 --> 00:36:51.640]   I think, is that right?
[00:36:51.640 --> 00:36:53.520]   Elon hired him to train.
[00:36:53.520 --> 00:36:55.200]   I think that's public, is that right?
[00:36:55.200 --> 00:36:56.120]   I think it is.
[00:36:56.120 --> 00:36:56.960]   - It will be now.
[00:36:56.960 --> 00:37:03.240]   - I mean, Igor's really, really great guy and brilliant,
[00:37:03.240 --> 00:37:05.600]   but he also happens to have trained state-of-the-art models
[00:37:05.600 --> 00:37:07.240]   at DeepMind and OpenAI.
[00:37:07.240 --> 00:37:09.200]   And so, you know, like that's the set of people
[00:37:09.200 --> 00:37:11.320]   who have that set of, you know,
[00:37:11.320 --> 00:37:13.040]   I don't know whether that's a consideration
[00:37:13.040 --> 00:37:14.480]   or how big of an effect that is,
[00:37:14.480 --> 00:37:18.440]   but it's the kind of thing that it would make sense to value
[00:37:18.440 --> 00:37:20.880]   if you think there are sort of valuable secrets
[00:37:20.880 --> 00:37:22.840]   that have not yet proliferated.
[00:37:22.840 --> 00:37:25.640]   So I think they're gonna try to slow it down.
[00:37:25.640 --> 00:37:28.240]   Publishing has certainly slowed down dramatically already,
[00:37:28.240 --> 00:37:30.480]   but I think there's just a long way to go
[00:37:30.480 --> 00:37:32.360]   before you're anywhere in like hedge fund
[00:37:32.360 --> 00:37:34.360]   or Manhattan Project territory,
[00:37:34.360 --> 00:37:37.320]   and probably secrets will still have
[00:37:37.320 --> 00:37:38.960]   a relatively short half-life.
[00:37:38.960 --> 00:37:40.680]   - As somebody who has been involved in open source
[00:37:40.680 --> 00:37:43.000]   your entire life, are you happy that this is the way
[00:37:43.000 --> 00:37:43.920]   that AI has turned out,
[00:37:43.920 --> 00:37:46.520]   or do you think that this is less than optimal?
[00:37:46.520 --> 00:37:47.440]   - Well, I don't know.
[00:37:47.440 --> 00:37:48.840]   My opinion's been changing.
[00:37:50.120 --> 00:37:54.160]   I have increasing worries about kind of safety issues,
[00:37:54.160 --> 00:37:58.680]   like not the hijacked version of safety,
[00:37:58.680 --> 00:38:03.680]   but some industrial accident type situations or misuse.
[00:38:03.680 --> 00:38:06.200]   And so I do think there's some, we're not in that world,
[00:38:06.200 --> 00:38:08.200]   and I'm not particularly concerned about it
[00:38:08.200 --> 00:38:09.720]   in the short term.
[00:38:09.720 --> 00:38:11.200]   But in the long term, I do think there are worlds
[00:38:11.200 --> 00:38:14.920]   that we should be a little bit concerned about,
[00:38:14.920 --> 00:38:17.160]   although I don't know what to do about,
[00:38:17.280 --> 00:38:21.800]   where, yeah, like bad things happen.
[00:38:21.800 --> 00:38:23.440]   The probability mass, my belief though,
[00:38:23.440 --> 00:38:25.680]   is that it's probably better in the whole
[00:38:25.680 --> 00:38:28.200]   for more people to get to tinker with
[00:38:28.200 --> 00:38:30.840]   and use these models, at least in their current state.
[00:38:30.840 --> 00:38:33.840]   And so, for example, when Georgi Gergenoff this weekend
[00:38:33.840 --> 00:38:37.160]   did a four-bit quantization of the Lama model
[00:38:37.160 --> 00:38:41.000]   and got it inferencing on a M1 or M2,
[00:38:41.000 --> 00:38:42.760]   I was very excited, and I got that running,
[00:38:42.760 --> 00:38:43.880]   and it's like fun to play with.
[00:38:43.880 --> 00:38:47.120]   Now I've got a model, it's very good,
[00:38:47.120 --> 00:38:49.800]   it's almost GPT-3 quality, runs on my laptop,
[00:38:49.800 --> 00:38:53.000]   and I've sort of grown up in this world of the tinkerers
[00:38:53.000 --> 00:38:56.280]   and open-source folks, and the more access you have,
[00:38:56.280 --> 00:38:57.480]   the more things you can try.
[00:38:57.480 --> 00:39:01.800]   And so I think I do find myself very attracted to that.
[00:39:01.800 --> 00:39:06.800]   - I guess that is the scientist and the ideas
[00:39:06.800 --> 00:39:08.400]   part of what is being shared,
[00:39:08.400 --> 00:39:11.400]   but there's also another part about the actual substance,
[00:39:11.400 --> 00:39:14.400]   right, so like the uranium and the sort of atom bomb analogy.
[00:39:14.400 --> 00:39:17.800]   As, I guess, different sources of data realize
[00:39:17.800 --> 00:39:21.240]   how valuable their data is for trading newer models,
[00:39:21.240 --> 00:39:23.160]   do you think that these things will become harder to scrape,
[00:39:23.160 --> 00:39:25.600]   LibGen, Archive, are these going to become
[00:39:25.600 --> 00:39:26.440]   rate-limited in some way,
[00:39:26.440 --> 00:39:27.960]   or what are you expecting there?
[00:39:27.960 --> 00:39:29.720]   - Well, first, there's so much data on the internet.
[00:39:29.720 --> 00:39:31.680]   I mean, the two kind of primitives
[00:39:31.680 --> 00:39:33.520]   that you need to build models are you need lots of data,
[00:39:33.520 --> 00:39:35.240]   we have that in the form of the internet,
[00:39:35.240 --> 00:39:36.920]   we digitize the whole world into the internet,
[00:39:36.920 --> 00:39:39.760]   and then you have, you need these GPUs,
[00:39:39.760 --> 00:39:40.880]   which we have because of video games.
[00:39:40.880 --> 00:39:42.560]   You take like the internet and video game hardware
[00:39:42.560 --> 00:39:43.400]   and you smash them together
[00:39:43.400 --> 00:39:45.080]   and you get machine learning models,
[00:39:45.080 --> 00:39:46.480]   and they're both commodities.
[00:39:46.480 --> 00:39:48.600]   And so I think the data,
[00:39:48.600 --> 00:39:50.880]   I don't think anyone in the open source world
[00:39:50.880 --> 00:39:52.880]   is really going to be data limited for a long time.
[00:39:52.880 --> 00:39:54.400]   There's so much that's out there.
[00:39:54.400 --> 00:39:57.600]   Probably people who have like proprietary data sets
[00:39:57.600 --> 00:40:01.160]   that are readily scrapable have been shutting those down,
[00:40:01.160 --> 00:40:05.520]   you know, so get your scraping in now if you need to do it,
[00:40:05.520 --> 00:40:07.240]   but that's just on the margin.
[00:40:07.240 --> 00:40:09.960]   I still think there's quite a lot
[00:40:09.960 --> 00:40:11.440]   that's out there to work with.
[00:40:11.440 --> 00:40:12.760]   So I think, look, there's going to be a time,
[00:40:12.760 --> 00:40:13.680]   this is the year of proliferation.
[00:40:13.680 --> 00:40:14.920]   This is a week of proliferation.
[00:40:14.920 --> 00:40:16.320]   Like we're going to see four or five
[00:40:16.320 --> 00:40:18.240]   major AI announcements this week.
[00:40:18.240 --> 00:40:20.440]   You know, new models, new APIs, new platforms,
[00:40:20.440 --> 00:40:23.080]   new tools from all the different vendors.
[00:40:23.080 --> 00:40:25.040]   In a way, this, you know, they're all looking forward.
[00:40:25.040 --> 00:40:27.160]   My Herculaneum project is looking backwards.
[00:40:27.160 --> 00:40:28.320]   (laughing)
[00:40:28.320 --> 00:40:30.560]   I think it's extremely exciting and cool,
[00:40:30.560 --> 00:40:32.160]   but it is sort of a funny contrast.
[00:40:32.160 --> 00:40:35.040]   - Okay, so that, I guess before we delve deeper into AI,
[00:40:35.040 --> 00:40:36.680]   I do want to talk about GitHub.
[00:40:36.680 --> 00:40:40.320]   So I think we should start with, you were at Microsoft,
[00:40:40.320 --> 00:40:41.720]   and at some point you realized
[00:40:41.720 --> 00:40:44.800]   that GitHub is very valuable and worth acquiring.
[00:40:44.800 --> 00:40:45.800]   How did you realize that,
[00:40:45.800 --> 00:40:48.160]   and how did you convince Microsoft to purchase GitHub?
[00:40:48.160 --> 00:40:50.200]   - Well, so I had started a company called Xamarin
[00:40:50.200 --> 00:40:53.520]   together with Miguel de Acaza and Joseph Hill,
[00:40:53.520 --> 00:40:56.360]   and we had built kind of mobile tools and platforms,
[00:40:56.360 --> 00:40:59.960]   and Microsoft acquired the company in 2016.
[00:40:59.960 --> 00:41:02.860]   And I was excited about that, I thought it was great,
[00:41:02.860 --> 00:41:06.280]   but to be honest, I didn't actually expect
[00:41:06.280 --> 00:41:10.640]   or plan to spend more than kind of a year or so there.
[00:41:10.640 --> 00:41:11.800]   But when I got in there,
[00:41:11.800 --> 00:41:14.440]   I got exposed to what Satya was doing
[00:41:14.440 --> 00:41:16.320]   and just the quality of his leadership team.
[00:41:16.320 --> 00:41:17.560]   I was really impressed.
[00:41:17.560 --> 00:41:21.200]   And actually, I think I saw him
[00:41:21.200 --> 00:41:23.160]   in the first week or so I was there and he asked me,
[00:41:23.160 --> 00:41:25.000]   "What do you think we should do at Microsoft?"
[00:41:25.000 --> 00:41:27.280]   And I said, "Well, I think we should buy GitHub."
[00:41:27.280 --> 00:41:28.320]   - And when would this have been?
[00:41:28.320 --> 00:41:29.160]   - This was like my first week,
[00:41:29.160 --> 00:41:32.640]   it was like March or April of 2016.
[00:41:32.640 --> 00:41:33.480]   - Okay.
[00:41:33.480 --> 00:41:37.840]   - And then he said, "Yeah, it's a good idea,
[00:41:37.840 --> 00:41:40.120]   "we thought about it, I'm not sure we can get away with it,"
[00:41:40.120 --> 00:41:41.280]   or something like that.
[00:41:41.280 --> 00:41:43.320]   And then it was about a year later,
[00:41:43.320 --> 00:41:44.800]   a little more than a year later,
[00:41:44.800 --> 00:41:48.120]   yeah, I wrote him an email, just a memo.
[00:41:48.120 --> 00:41:50.200]   I sort of said, "I think it's time to do this."
[00:41:50.200 --> 00:41:52.840]   There was some noise that Google was sniffing around,
[00:41:52.840 --> 00:41:56.160]   I think that may have been manufactured by the GitHub team,
[00:41:56.160 --> 00:41:57.760]   but it was a good catalyst 'cause it was something
[00:41:57.760 --> 00:42:00.240]   I thought made a lot of sense for Microsoft to do anyway.
[00:42:00.240 --> 00:42:02.280]   And so I wrote an email to Satya,
[00:42:02.280 --> 00:42:04.280]   sort of a little memo saying,
[00:42:04.280 --> 00:42:05.280]   "Hey, I think we should buy GitHub,
[00:42:05.280 --> 00:42:07.680]   "here's why, here's what we should do with it."
[00:42:07.680 --> 00:42:10.400]   And the basic argument was,
[00:42:10.400 --> 00:42:13.880]   developers are making IT purchasing decisions now.
[00:42:13.880 --> 00:42:16.520]   It used to be this sort of IT thing,
[00:42:16.520 --> 00:42:18.800]   and now developers are leading that purchase
[00:42:18.800 --> 00:42:21.720]   and it's this sort of major shift
[00:42:21.720 --> 00:42:25.120]   in how software products are acquired.
[00:42:25.120 --> 00:42:27.320]   And Microsoft really was an IT company,
[00:42:27.400 --> 00:42:29.520]   it was not a developer company
[00:42:29.520 --> 00:42:32.920]   in the way most of its purchases were made,
[00:42:32.920 --> 00:42:35.720]   but it was founded as a developer company, right?
[00:42:35.720 --> 00:42:40.080]   And so Microsoft's first product was a programming language.
[00:42:40.080 --> 00:42:42.200]   Yeah, I said, "Look, the challenge that we have is,
[00:42:42.200 --> 00:42:44.160]   "there's an entire new generation of developers
[00:42:44.160 --> 00:42:45.880]   "who have no affinity with Microsoft,
[00:42:45.880 --> 00:42:49.720]   "and the largest collection of them is at GitHub.
[00:42:49.720 --> 00:42:51.480]   "And if we acquire this,
[00:42:51.480 --> 00:42:54.680]   "and we do a merely competent job of running it,
[00:42:55.520 --> 00:42:57.880]   "we can earn the right to be considered
[00:42:57.880 --> 00:43:01.000]   "by these developers for all the other products that we do."
[00:43:01.000 --> 00:43:02.680]   And to my surprise, Satya replied
[00:43:02.680 --> 00:43:04.640]   in like six or seven minutes and said,
[00:43:04.640 --> 00:43:06.120]   "I think this is very good thinking,
[00:43:06.120 --> 00:43:08.000]   "let's meet next week or so and talk about it."
[00:43:08.000 --> 00:43:10.960]   And I ended up at this conference room with him
[00:43:10.960 --> 00:43:13.520]   and Amy Hood and Scott Guthrie and Kevin Scott
[00:43:13.520 --> 00:43:15.360]   and several other people.
[00:43:15.360 --> 00:43:18.660]   And they said, "Okay, tell us what you're thinking."
[00:43:18.660 --> 00:43:21.480]   And I kind of did a little 20 minute ramble on it.
[00:43:21.480 --> 00:43:24.140]   And Satya said, "Yeah, I think we should do it.
[00:43:24.140 --> 00:43:26.640]   "And why don't we run it independently like LinkedIn,
[00:43:26.640 --> 00:43:28.520]   "Nat, you'll be the CEO."
[00:43:28.520 --> 00:43:30.400]   And he said, "Do you think we can get it for two billion?"
[00:43:30.400 --> 00:43:33.080]   And I said, "We can try."
[00:43:33.080 --> 00:43:36.520]   And three weeks later, he said,
[00:43:36.520 --> 00:43:40.160]   "Okay, go do this, Scott will support you on this."
[00:43:40.160 --> 00:43:41.720]   Three weeks later, we had a signed term sheet
[00:43:41.720 --> 00:43:43.060]   and an announced deal.
[00:43:43.060 --> 00:43:45.720]   And then it was an amazing experience for me.
[00:43:45.720 --> 00:43:47.220]   I'd been there less than two years,
[00:43:47.220 --> 00:43:49.800]   and Microsoft was made up of and run by a lot of people
[00:43:49.800 --> 00:43:51.240]   who'd been there for many years.
[00:43:51.240 --> 00:43:54.040]   And they trusted me with this really big project.
[00:43:54.040 --> 00:43:58.000]   And it made me feel really good to be trusted and empowered.
[00:43:58.000 --> 00:44:01.180]   And I had grown up in the open source world.
[00:44:01.180 --> 00:44:03.280]   And so for me to get an opportunity to run GitHub,
[00:44:03.280 --> 00:44:04.720]   it's like, I don't know, getting appointed mayor
[00:44:04.720 --> 00:44:08.080]   of your hometown or something like that, it felt cool.
[00:44:08.080 --> 00:44:10.440]   And I really wanted to do a good job for developers.
[00:44:10.440 --> 00:44:13.300]   And so that's how it happened.
[00:44:13.300 --> 00:44:15.520]   - That's actually one of the things I want to ask you about,
[00:44:15.520 --> 00:44:18.060]   because often when something succeeds,
[00:44:18.060 --> 00:44:20.160]   we kind of think it was inevitable that it would succeed.
[00:44:20.160 --> 00:44:23.260]   But at the time, I remember, I mean, it was a while back,
[00:44:23.260 --> 00:44:25.220]   but I remember that there was a huge amount of skepticism.
[00:44:25.220 --> 00:44:26.380]   I would go on like Hacker News,
[00:44:26.380 --> 00:44:27.900]   and the top thing would be the blog post
[00:44:27.900 --> 00:44:30.000]   about how Microsoft's going to mess up GitHub.
[00:44:30.000 --> 00:44:33.360]   And I guess people have, those concerns
[00:44:33.360 --> 00:44:35.500]   have been alleviated throughout the years.
[00:44:35.500 --> 00:44:38.520]   But how did you deal with that skepticism
[00:44:38.520 --> 00:44:39.360]   and deal with that distrust?
[00:44:39.360 --> 00:44:40.840]   - Well, I was really paranoid about it.
[00:44:40.840 --> 00:44:42.320]   And I really cared about what developers thought.
[00:44:42.320 --> 00:44:43.440]   I think there's always this question
[00:44:43.440 --> 00:44:44.680]   of who are you performing for?
[00:44:44.680 --> 00:44:46.040]   Like, who do you actually really care about?
[00:44:46.040 --> 00:44:48.320]   Sort of who's the audience that's in your head
[00:44:48.320 --> 00:44:51.400]   that you're trying to do a good job for,
[00:44:51.400 --> 00:44:53.980]   in press, or in the respect of, whatever it is?
[00:44:53.980 --> 00:44:58.180]   And though I love Microsoft and care a lot about Satya
[00:44:58.180 --> 00:45:01.980]   and everyone there, I really cared about the developers.
[00:45:01.980 --> 00:45:03.900]   You know, I'd grown up in this open-source world.
[00:45:03.900 --> 00:45:07.180]   And so for me to do a bad job with this central institution
[00:45:07.180 --> 00:45:09.780]   and open-source would have been a devastating feeling for me.
[00:45:09.780 --> 00:45:11.020]   It was very important to me not to.
[00:45:11.020 --> 00:45:13.460]   So that was sort of the first thing is just that I cared.
[00:45:13.460 --> 00:45:15.460]   And then the second thing is that the deal leaked.
[00:45:15.460 --> 00:45:16.900]   It was going to be announced, I think, on a Monday.
[00:45:16.900 --> 00:45:18.060]   It leaked on a Friday.
[00:45:18.060 --> 00:45:21.060]   And Microsoft's buying GitHub.
[00:45:21.060 --> 00:45:24.820]   And the whole weekend, there were terrible posts online.
[00:45:24.820 --> 00:45:27.220]   You know, people saying, we've got to evacuate GitHub
[00:45:27.220 --> 00:45:28.540]   as quickly as possible.
[00:45:28.540 --> 00:45:31.460]   And we're like, oh my god, that's terrible.
[00:45:31.460 --> 00:45:33.420]   And then Monday, we put the announcement out.
[00:45:33.420 --> 00:45:35.660]   And we said, we're acquiring GitHub.
[00:45:35.660 --> 00:45:38.020]   It's going to run as an independent company.
[00:45:38.020 --> 00:45:40.380]   And then it said, Nat Friedman's going to be CEO.
[00:45:40.380 --> 00:45:42.900]   And I had--
[00:45:42.900 --> 00:45:44.580]   I don't want to overstate or whatever.
[00:45:44.580 --> 00:45:45.900]   But I think a couple people were like, oh,
[00:45:45.900 --> 00:45:47.140]   Nat comes from open-source.
[00:45:47.140 --> 00:45:48.540]   You know, he spent some time in open-source.
[00:45:48.540 --> 00:45:50.460]   So Nat's going to be running independently.
[00:45:50.460 --> 00:45:53.380]   So I don't think they were really that calmed down.
[00:45:53.380 --> 00:45:55.220]   But at least a few people thought,
[00:45:55.220 --> 00:45:57.020]   like, oh, maybe I'll give this a few months
[00:45:57.020 --> 00:45:59.740]   and just see what happens before I migrate off.
[00:45:59.740 --> 00:46:04.500]   And then my first day as CEO, after we got the deal closed,
[00:46:04.500 --> 00:46:09.020]   like 9 AM the first day, I was in this room.
[00:46:09.020 --> 00:46:11.980]   And we got on Zoom and all the heads of engineering
[00:46:11.980 --> 00:46:13.340]   and product.
[00:46:13.340 --> 00:46:14.620]   And I think maybe--
[00:46:14.620 --> 00:46:16.300]   I don't know what people were expecting.
[00:46:16.300 --> 00:46:18.900]   But I think maybe they were expecting some kind of longer
[00:46:18.900 --> 00:46:20.900]   term strategy or something.
[00:46:20.900 --> 00:46:22.020]   But I came in.
[00:46:22.020 --> 00:46:24.460]   I said, there was this GitHub had no official feedback
[00:46:24.460 --> 00:46:26.420]   mechanism that was publicly available.
[00:46:26.420 --> 00:46:29.660]   But there were several GitHub repos, the community members
[00:46:29.660 --> 00:46:30.220]   that started.
[00:46:30.220 --> 00:46:34.020]   Isaac from NPM had started one where he'd just
[00:46:34.020 --> 00:46:36.100]   been allowing people to give GitHub feedback.
[00:46:36.100 --> 00:46:38.500]   And people had been voting on this stuff for years.
[00:46:38.500 --> 00:46:41.180]   And I kind of shared my screen and put that up,
[00:46:41.180 --> 00:46:43.940]   sorted by votes, and said, like, we're
[00:46:43.940 --> 00:46:47.300]   going to pick one thing from this list
[00:46:47.300 --> 00:46:49.380]   and fix it by the end of the day and ship that,
[00:46:49.380 --> 00:46:50.980]   just one thing.
[00:46:50.980 --> 00:46:55.940]   And I think people were like, this is the new CEO strategy.
[00:46:55.940 --> 00:46:57.340]   And they were like, I don't know.
[00:46:57.340 --> 00:46:59.220]   We can't-- you have to do database migrations.
[00:46:59.220 --> 00:47:00.180]   Can't do that in a day.
[00:47:00.180 --> 00:47:03.740]   And then someone's like, well, maybe we can do this.
[00:47:03.740 --> 00:47:06.900]   We actually have a half implementation of this.
[00:47:06.900 --> 00:47:08.780]   And we eventually found something
[00:47:08.780 --> 00:47:10.460]   that we could fix by the end of the day.
[00:47:10.460 --> 00:47:12.820]   And what I'm thinking is, what I'm thinking,
[00:47:12.820 --> 00:47:14.780]   what I hope I said was, well, we need
[00:47:14.780 --> 00:47:19.340]   to show the world is that GitHub cares about developers,
[00:47:19.340 --> 00:47:20.860]   not that it cares about Microsoft.
[00:47:20.860 --> 00:47:23.540]   Like, if the first thing we did after the acquisition
[00:47:23.540 --> 00:47:26.620]   was to add Skype integration, developers
[00:47:26.620 --> 00:47:28.940]   would have said, oh, we're not your priority.
[00:47:28.940 --> 00:47:31.140]   Like, you have new priorities now.
[00:47:31.140 --> 00:47:33.340]   And so the idea was just to find ways
[00:47:33.340 --> 00:47:36.020]   to make it better for the people who use it
[00:47:36.020 --> 00:47:39.540]   and have them see that we cared about that immediately.
[00:47:39.540 --> 00:47:41.420]   And so I said, we're going to do this today.
[00:47:41.420 --> 00:47:42.260]   And then we're going to do it every day
[00:47:42.260 --> 00:47:43.660]   for the next 100 days.
[00:47:43.660 --> 00:47:46.860]   And it was cool, because I think it created some really good
[00:47:46.860 --> 00:47:48.620]   feedback loops, at least for me.
[00:47:48.620 --> 00:47:51.820]   One was, you ship things, and then people are like, oh, hey,
[00:47:51.820 --> 00:47:53.780]   I've been wanting to see this fixed for years.
[00:47:53.780 --> 00:47:54.900]   And now it's fixed.
[00:47:54.900 --> 00:47:56.780]   It's a relatively simple thing.
[00:47:56.780 --> 00:48:00.700]   So you get this sort of nice, dopaminergic feedback loop
[00:48:00.700 --> 00:48:01.780]   going there.
[00:48:01.780 --> 00:48:05.260]   And then people in the team feel the excitement
[00:48:05.260 --> 00:48:06.860]   of shipping stuff.
[00:48:06.860 --> 00:48:09.300]   I think GitHub was a company that had a little bit of stage
[00:48:09.300 --> 00:48:11.660]   fright about shipping previously and sort of break
[00:48:11.660 --> 00:48:14.740]   that static friction and ship a little bit more.
[00:48:14.740 --> 00:48:15.580]   I think felt good.
[00:48:15.580 --> 00:48:17.620]   And then the other one is just the learning loop.
[00:48:17.620 --> 00:48:19.460]   By trying to do lots of small things,
[00:48:19.460 --> 00:48:23.140]   I got exposed to, like, OK, this team is really good.
[00:48:23.140 --> 00:48:26.260]   Or this part of the code has a lot of tech debt.
[00:48:26.260 --> 00:48:29.300]   Or, hey, we shipped that, and it was actually kind of bad.
[00:48:29.300 --> 00:48:32.700]   How come that design got out?
[00:48:32.700 --> 00:48:35.500]   Whereas if the project had been some six-month thing,
[00:48:35.500 --> 00:48:37.540]   I'm not sure my learning would have been quite as
[00:48:37.540 --> 00:48:38.820]   quick about the company.
[00:48:39.180 --> 00:48:41.980]   Still things I missed and mistakes I made, for sure.
[00:48:41.980 --> 00:48:45.700]   But that was part of how I think--
[00:48:45.700 --> 00:48:47.620]   no one knows counterfactually whether that
[00:48:47.620 --> 00:48:48.820]   made a big difference or not.
[00:48:48.820 --> 00:48:50.700]   But I do think that earned some trust.
[00:48:50.700 --> 00:48:52.660]   I mean, most acquisitions don't go well.
[00:48:52.660 --> 00:48:54.340]   Not only do they not go as well, but they
[00:48:54.340 --> 00:48:55.540]   don't go well at all, right?
[00:48:55.540 --> 00:48:58.700]   As we're seeing in the last few months with the CERN one.
[00:48:58.700 --> 00:49:02.900]   Well, why do most acquisitions fail or fail to go well?
[00:49:02.900 --> 00:49:03.620]   Yeah, it is true.
[00:49:03.620 --> 00:49:06.380]   Most acquisitions are destructive of value.
[00:49:06.380 --> 00:49:08.700]   What is the value of a company?
[00:49:08.700 --> 00:49:12.020]   In an innovative industry, the value of the company, a lot
[00:49:12.020 --> 00:49:14.180]   of it boils down to its ability culturally
[00:49:14.180 --> 00:49:17.860]   to produce new innovations and is
[00:49:17.860 --> 00:49:21.540]   some sensitive harmonic of cultural elements
[00:49:21.540 --> 00:49:23.580]   that sets that up, that makes that possible.
[00:49:23.580 --> 00:49:25.900]   And it's quite fragile, I think.
[00:49:25.900 --> 00:49:27.700]   And so if you take a culture that
[00:49:27.700 --> 00:49:30.660]   has achieved some productive harmonic
[00:49:30.660 --> 00:49:32.780]   and you put it inside of another culture that's really
[00:49:32.780 --> 00:49:36.220]   different, the kind of mismatch of that
[00:49:36.220 --> 00:49:39.380]   can destroy the productivity of the company.
[00:49:39.380 --> 00:49:41.340]   So I think that maybe one way to think about it
[00:49:41.340 --> 00:49:43.100]   is companies are a little bit fragile.
[00:49:43.100 --> 00:49:49.380]   And so when you acquire them, it's relatively easy
[00:49:49.380 --> 00:49:49.980]   to break them.
[00:49:49.980 --> 00:49:52.660]   I mean, they're also more durable than people
[00:49:52.660 --> 00:49:54.020]   think in many cases, too.
[00:49:54.020 --> 00:49:55.380]   I would say another version of it
[00:49:55.380 --> 00:49:59.460]   is the people who really care leave.
[00:49:59.460 --> 00:50:01.500]   And so the people who really care about building
[00:50:01.500 --> 00:50:03.820]   great products and serving the customers,
[00:50:03.820 --> 00:50:05.980]   maybe they don't want to work for the acquirer.
[00:50:05.980 --> 00:50:08.460]   And the set of people that are really
[00:50:08.460 --> 00:50:14.460]   load-bearing around kind of the long-term success is small.
[00:50:14.460 --> 00:50:16.620]   And when they leave or get disempowered,
[00:50:16.620 --> 00:50:18.420]   you get very different behaviors.
[00:50:18.420 --> 00:50:21.260]   And then, so I want to go into the story of CoPilot
[00:50:21.260 --> 00:50:22.420]   because--
[00:50:22.420 --> 00:50:25.500]   until [INAUDIBLE] I guess it was like the most widely used
[00:50:25.500 --> 00:50:27.900]   application of the modern AI models.
[00:50:27.900 --> 00:50:29.340]   Whatever part of the story you're
[00:50:29.340 --> 00:50:30.460]   willing to share in public.
[00:50:30.460 --> 00:50:32.660]   Yeah, I mean, I've talked about this a little bit.
[00:50:32.660 --> 00:50:38.460]   I mean, so look, GPT-3 came out in May, I think, of 2020.
[00:50:38.460 --> 00:50:41.740]   And I saw it and it really blew my mind.
[00:50:41.740 --> 00:50:42.860]   I thought it was amazing.
[00:50:42.860 --> 00:50:45.700]   And I was CEO of GitHub at that time.
[00:50:45.700 --> 00:50:48.460]   And I thought, like, I don't know what,
[00:50:48.460 --> 00:50:50.420]   but we've got to build some product with this.
[00:50:50.420 --> 00:50:52.460]   This is-- you know, we've got to build something.
[00:50:52.460 --> 00:50:57.660]   And so Satya had, at I think Kevin Scott's urging,
[00:50:57.660 --> 00:51:02.220]   already invested in open AI like a year before GPT-3 came out.
[00:51:02.220 --> 00:51:03.420]   Like, this is quite amazing.
[00:51:03.420 --> 00:51:05.020]   He invested like a billion dollars.
[00:51:05.020 --> 00:51:06.820]   By the way, do you know why he knew that open AI would be
[00:51:06.820 --> 00:51:08.060]   worth investing in at that point?
[00:51:08.060 --> 00:51:09.180]   I don't know, actually.
[00:51:09.180 --> 00:51:10.460]   I've never asked him.
[00:51:10.460 --> 00:51:11.660]   But yeah, I'm not sure.
[00:51:11.660 --> 00:51:12.580]   That's a good question.
[00:51:12.580 --> 00:51:16.340]   I mean, I think open AI had already had some successes that
[00:51:16.340 --> 00:51:18.300]   were noticeable.
[00:51:18.300 --> 00:51:21.580]   And I think if you're Satya and you're
[00:51:21.580 --> 00:51:25.460]   running this multi-trillion dollar company,
[00:51:25.460 --> 00:51:27.780]   you're trying to execute well and serve your customers.
[00:51:27.780 --> 00:51:31.300]   But you're always looking for the next gigantic wave
[00:51:31.300 --> 00:51:34.260]   that is going to upend the technology industry.
[00:51:34.260 --> 00:51:36.300]   It's not just about trying to win cloud.
[00:51:36.300 --> 00:51:38.420]   It's like, OK, what comes after cloud?
[00:51:38.420 --> 00:51:42.420]   And so you want-- you have to make some big bets.
[00:51:42.420 --> 00:51:45.740]   And I think he thought AI could be one.
[00:51:45.740 --> 00:51:50.140]   And I think Kevin Scott deserves a lot of credit for really
[00:51:50.140 --> 00:51:52.940]   advocating for that aggressively.
[00:51:52.940 --> 00:51:56.180]   And I think Sam Altman did a good job
[00:51:56.180 --> 00:51:59.020]   of building that partnership because he
[00:51:59.020 --> 00:52:02.660]   knew that he needed access to the resources of a company
[00:52:02.660 --> 00:52:06.700]   like Microsoft to build large-scale AI and eventually
[00:52:06.700 --> 00:52:08.020]   AGI.
[00:52:08.020 --> 00:52:10.900]   And so I think it was some combination of those three
[00:52:10.900 --> 00:52:13.300]   people kind of coming together to make it happen.
[00:52:13.300 --> 00:52:15.220]   But I still think it was a very prescient bet.
[00:52:15.220 --> 00:52:16.300]   I've said that to people.
[00:52:16.300 --> 00:52:17.460]   And they've said, well, a billion dollars
[00:52:17.460 --> 00:52:18.500]   is not a lot for Microsoft.
[00:52:18.500 --> 00:52:20.100]   But there were a lot of other companies
[00:52:20.100 --> 00:52:22.680]   that could have spent a billion dollars to do that and did not.
[00:52:22.680 --> 00:52:25.580]   So I still think that deserves a lot of credit.
[00:52:25.580 --> 00:52:28.020]   OK, so GPT-3 comes out.
[00:52:28.020 --> 00:52:32.700]   I pinged Sam and Greg, I think, Brockman, at OpenAI.
[00:52:32.700 --> 00:52:35.140]   And they were like, yeah, we've already
[00:52:35.140 --> 00:52:38.260]   been experimenting with GPT-3 and derivative models
[00:52:38.260 --> 00:52:39.580]   in coding contexts.
[00:52:39.580 --> 00:52:41.580]   Let's definitely work on something.
[00:52:41.580 --> 00:52:43.820]   And to me, at least, and a few other people,
[00:52:43.820 --> 00:52:47.220]   it was not incredibly obvious what the product would be.
[00:52:47.220 --> 00:52:50.560]   Now, I think it's trivially obvious, autocomplete.
[00:52:50.560 --> 00:52:52.260]   My gosh, isn't that what the models do?
[00:52:52.260 --> 00:52:55.220]   But at the time, actually, my first thought
[00:52:55.220 --> 00:52:58.980]   was that it was probably going to be a Q&A chatbot stack
[00:52:58.980 --> 00:53:00.660]   overflow type of thing.
[00:53:00.660 --> 00:53:03.900]   And so that was actually the first thing we prototyped.
[00:53:03.900 --> 00:53:07.700]   So we grabbed a couple of engineers, this guy Uga,
[00:53:07.700 --> 00:53:10.740]   who had come in from an acquisition that we'd done,
[00:53:10.740 --> 00:53:14.540]   and Alex Gravely, and started prototyping.
[00:53:14.540 --> 00:53:16.540]   And the first prototype was a chatbot.
[00:53:16.540 --> 00:53:20.620]   And what we discovered first was that the demos were fabulous.
[00:53:20.620 --> 00:53:22.660]   Every AI product has a fantastic demo.
[00:53:22.660 --> 00:53:25.100]   You get this wow moment.
[00:53:25.100 --> 00:53:28.300]   So that turns out to be maybe not a sufficient condition
[00:53:28.300 --> 00:53:29.980]   for a product to be good.
[00:53:29.980 --> 00:53:31.900]   Because it was just, at the time,
[00:53:31.900 --> 00:53:33.700]   the models were just not reliable enough.
[00:53:33.700 --> 00:53:34.900]   They were not good enough.
[00:53:34.900 --> 00:53:35.900]   I ask you a question.
[00:53:35.900 --> 00:53:38.580]   25% of the time, you give me an incredible answer that I love.
[00:53:38.580 --> 00:53:41.540]   75% of the time, your answer is useless or wrong.
[00:53:41.540 --> 00:53:43.420]   It's not a great product experience.
[00:53:43.420 --> 00:53:45.620]   And so then we started thinking about code synthesis.
[00:53:45.620 --> 00:53:47.660]   And our first attempts at this were actually
[00:53:47.660 --> 00:53:50.220]   large chunks of code synthesis, like synthesizing
[00:53:50.220 --> 00:53:51.820]   whole function bodies.
[00:53:51.860 --> 00:53:54.700]   And we built some tools to do that
[00:53:54.700 --> 00:53:56.140]   and put them in the editor.
[00:53:56.140 --> 00:53:59.980]   And that also was not really that satisfying.
[00:53:59.980 --> 00:54:02.420]   And so the next thing that we tried
[00:54:02.420 --> 00:54:05.660]   was to just do simple, small-scale autocomplete
[00:54:05.660 --> 00:54:07.340]   with the large models.
[00:54:07.340 --> 00:54:10.620]   And we used the kind of IntelliSense drop-down UI
[00:54:10.620 --> 00:54:11.620]   to do that.
[00:54:11.620 --> 00:54:14.820]   And that was better, definitely pretty good.
[00:54:14.820 --> 00:54:16.900]   But the UI was not quite right.
[00:54:16.900 --> 00:54:20.540]   And we lost the ability to do this large-scale synthesis.
[00:54:20.540 --> 00:54:23.700]   We still had that, but the UI for that wasn't good.
[00:54:23.700 --> 00:54:26.580]   And we had it, I think, so that to get a function body
[00:54:26.580 --> 00:54:28.380]   synthesized, you would hit a key.
[00:54:28.380 --> 00:54:30.720]   And then-- I don't know why this was the idea everyone
[00:54:30.720 --> 00:54:32.180]   had at the time, but several people
[00:54:32.180 --> 00:54:35.700]   had this idea that it should display multiple options
[00:54:35.700 --> 00:54:36.660]   for the function body.
[00:54:36.660 --> 00:54:39.220]   And then the user would read them and pick the right one.
[00:54:39.220 --> 00:54:42.020]   And I think the idea was that we would use that human feedback
[00:54:42.020 --> 00:54:43.020]   to improve the model.
[00:54:43.020 --> 00:54:44.860]   But that turned out to be a bad experience,
[00:54:44.860 --> 00:54:47.780]   because first you had to hit a key and explicitly request it.
[00:54:47.780 --> 00:54:49.540]   Then you had to wait for it.
[00:54:49.540 --> 00:54:53.100]   And then you had to read three different versions
[00:54:53.100 --> 00:54:53.980]   of a block of code.
[00:54:53.980 --> 00:54:55.580]   Reading one version of a block of code
[00:54:55.580 --> 00:54:56.820]   takes some cognitive effort.
[00:54:56.820 --> 00:54:59.420]   Doing it three times takes more cognitive effort.
[00:54:59.420 --> 00:55:02.580]   And then most often the result of that
[00:55:02.580 --> 00:55:04.700]   was none of them were good or you
[00:55:04.700 --> 00:55:08.060]   didn't know which one to pick.
[00:55:08.060 --> 00:55:10.340]   So that was also like you're putting a lot of energy
[00:55:10.340 --> 00:55:12.820]   and you're not getting a lot out, sort of frustrating.
[00:55:12.820 --> 00:55:16.020]   So once we had that single-line completion working,
[00:55:16.020 --> 00:55:18.340]   I think Alex had the idea of saying
[00:55:18.340 --> 00:55:22.900]   we can use the cursor position in the AST
[00:55:22.900 --> 00:55:25.540]   to figure out heuristically whether you're
[00:55:25.540 --> 00:55:27.500]   at the beginning of a block in the code or not.
[00:55:27.500 --> 00:55:29.700]   And if it's not the beginning of a block, just complete a line.
[00:55:29.700 --> 00:55:31.120]   If it's the beginning of a block,
[00:55:31.120 --> 00:55:35.140]   show in line a full block completion.
[00:55:35.140 --> 00:55:38.740]   So the number of tokens you request and when you stop
[00:55:38.740 --> 00:55:41.380]   gets altered automatically with no user interaction.
[00:55:41.380 --> 00:55:44.140]   And then the idea of using this gray text like Gmail
[00:55:44.140 --> 00:55:46.540]   had done in the editor.
[00:55:46.540 --> 00:55:48.780]   And so we got that implemented and it was really
[00:55:48.780 --> 00:55:51.420]   only kind of once all those pieces came together
[00:55:51.420 --> 00:55:53.660]   and we started using a model that was small enough
[00:55:53.660 --> 00:55:56.460]   to be low latency but big enough to be accurate
[00:55:56.460 --> 00:56:00.060]   that we reached the point where the median new user loved
[00:56:00.060 --> 00:56:02.660]   Copilot and wouldn't stop using it.
[00:56:02.660 --> 00:56:06.160]   And that took four months, five months of just tinkering
[00:56:06.160 --> 00:56:07.380]   and sort of exploring.
[00:56:07.380 --> 00:56:10.260]   There were other dead ends that we had along the way.
[00:56:10.260 --> 00:56:12.900]   And then, yeah, I think that then it
[00:56:12.900 --> 00:56:15.420]   became quite obvious that it was good because we
[00:56:15.420 --> 00:56:18.380]   had hundreds of internal users who were GitHub engineers.
[00:56:18.380 --> 00:56:20.740]   And I remember the first time I looked at the retention
[00:56:20.740 --> 00:56:23.020]   numbers, they were extremely high.
[00:56:23.020 --> 00:56:26.180]   It was like, I remember, 60-plus percent after 30 days
[00:56:26.180 --> 00:56:27.020]   from first install.
[00:56:27.020 --> 00:56:28.520]   Like, if you installed it, the chance
[00:56:28.520 --> 00:56:31.500]   that you were still using it after 30 days was over 60%.
[00:56:31.500 --> 00:56:33.100]   And it's a very intrusive product.
[00:56:33.100 --> 00:56:35.660]   I mean, it's sort of always popping UI up.
[00:56:35.660 --> 00:56:39.300]   And so if you don't like it, you will disable it.
[00:56:39.300 --> 00:56:41.960]   Indeed, 40-something percent of people did disable it.
[00:56:41.960 --> 00:56:43.700]   But those are very high retention numbers
[00:56:43.700 --> 00:56:47.060]   for like an alpha first version of a product
[00:56:47.060 --> 00:56:48.900]   that you're using all day.
[00:56:48.900 --> 00:56:52.780]   And so then I was just incredibly excited to launch it.
[00:56:52.780 --> 00:56:55.300]   And now it's improved dramatically since then.
[00:56:55.300 --> 00:56:57.380]   Yeah, sounds very similar to the Gmail story, right?
[00:56:57.380 --> 00:56:59.140]   It's an incredibly valuable insight.
[00:56:59.140 --> 00:57:01.460]   And then it becomes obvious that it needs to go outside.
[00:57:01.460 --> 00:57:03.900]   OK, we'll go back to the AI stuff in a second.
[00:57:03.900 --> 00:57:06.940]   But some more GitHub questions.
[00:57:06.940 --> 00:57:10.340]   By what point will, if ever, will GitHub profiles replace
[00:57:10.340 --> 00:57:11.980]   resumes for programmers?
[00:57:11.980 --> 00:57:13.020]   That's a good question.
[00:57:13.020 --> 00:57:14.860]   I mean, I think they're a contributing element
[00:57:14.860 --> 00:57:17.220]   to how people try to understand a person now.
[00:57:17.220 --> 00:57:19.780]   But I don't think they're like a definitive resume.
[00:57:19.780 --> 00:57:22.420]   We introduced readmes on profiles when I was there.
[00:57:22.420 --> 00:57:24.180]   And I was excited about that, because I
[00:57:24.180 --> 00:57:26.460]   thought it gave people some degree of personalization.
[00:57:26.460 --> 00:57:31.420]   I think many thousands of people have done that.
[00:57:31.420 --> 00:57:32.300]   Yeah, I don't know.
[00:57:32.300 --> 00:57:34.420]   There's forces to push in the other direction, too,
[00:57:34.420 --> 00:57:37.540]   on that one, where people don't want their activity and skills
[00:57:37.540 --> 00:57:39.020]   to be as legible.
[00:57:39.020 --> 00:57:41.060]   And there may be some adverse selection
[00:57:41.060 --> 00:57:45.700]   as well, where the people with the most elite skills,
[00:57:45.700 --> 00:57:49.460]   it's rather gauche for them to signal their competence
[00:57:49.460 --> 00:57:50.940]   on their profile.
[00:57:50.940 --> 00:57:53.360]   So there's some weird social dynamics that feed into it,
[00:57:53.360 --> 00:57:53.860]   too.
[00:57:53.860 --> 00:57:55.660]   But I will say, I think it effectively
[00:57:55.660 --> 00:57:59.300]   has this role for people who are breaking through today.
[00:57:59.300 --> 00:58:01.140]   One of the best ways to break through--
[00:58:01.140 --> 00:58:03.060]   I know many people who are in this situation.
[00:58:03.060 --> 00:58:05.980]   You were born in Argentina.
[00:58:05.980 --> 00:58:08.260]   You're a very sharp person, but you
[00:58:08.260 --> 00:58:13.540]   didn't grow up in a highly connected or prosperous network
[00:58:13.540 --> 00:58:15.460]   family, et cetera.
[00:58:15.460 --> 00:58:17.500]   And yet, you know you're really capable.
[00:58:17.500 --> 00:58:19.660]   And you just want to get connected
[00:58:19.660 --> 00:58:23.140]   to the most elite communities in the world.
[00:58:23.140 --> 00:58:24.980]   And so if you're good at programming,
[00:58:24.980 --> 00:58:26.540]   you can join open source communities
[00:58:26.540 --> 00:58:28.100]   and contribute to them.
[00:58:28.100 --> 00:58:31.500]   And you can very quickly accrete a global reputation
[00:58:31.500 --> 00:58:35.300]   for your talent, which is legible to many companies
[00:58:35.300 --> 00:58:37.180]   and individuals around the world.
[00:58:37.180 --> 00:58:39.940]   And suddenly, you find yourself getting a job
[00:58:39.940 --> 00:58:42.700]   and moving maybe to the US, or maybe not moving,
[00:58:42.700 --> 00:58:44.540]   or you end up at a great startup.
[00:58:44.540 --> 00:58:47.220]   I mean, I know a lot of people who've deliberately
[00:58:47.220 --> 00:58:52.060]   pursued the strategy of building reputation in open source,
[00:58:52.060 --> 00:58:55.340]   and then you've got the sail up, and the wind catches you,
[00:58:55.340 --> 00:58:59.260]   and you've got a career.
[00:58:59.260 --> 00:59:02.220]   And so I think it plays that role in that sense.
[00:59:02.220 --> 00:59:05.180]   But in other communities, like in machine learning research,
[00:59:05.180 --> 00:59:06.700]   this is not how you--
[00:59:06.700 --> 00:59:08.340]   there's 1,000 people.
[00:59:08.340 --> 00:59:12.500]   Their reputation is more on archive than it is on GitHub.
[00:59:12.500 --> 00:59:14.580]   So I don't know that it will ever be comprehensive.
[00:59:14.580 --> 00:59:16.740]   Are there any other industries for which
[00:59:16.740 --> 00:59:19.620]   proof of work of this kind will eat more
[00:59:19.620 --> 00:59:22.620]   into the way in which people are hired?
[00:59:22.620 --> 00:59:27.740]   Well, I think there's a labor market dynamic in software
[00:59:27.740 --> 00:59:32.220]   where the really high quality talent is so in demand,
[00:59:32.220 --> 00:59:35.100]   and the supply is so much less than the demand.
[00:59:35.100 --> 00:59:39.340]   That it shifts power onto the developers
[00:59:39.340 --> 00:59:42.100]   such that they can require of their employers
[00:59:42.100 --> 00:59:44.580]   that they be allowed to work in public.
[00:59:44.580 --> 00:59:47.620]   And because-- and then when they do that,
[00:59:47.620 --> 00:59:49.700]   they develop an external reputation,
[00:59:49.700 --> 00:59:53.100]   which is this asset they can port between companies.
[00:59:53.100 --> 00:59:55.260]   And if the labor market dynamics weren't like that,
[00:59:55.260 --> 00:59:58.780]   if programming well were less economically valuable,
[00:59:58.780 --> 01:00:02.300]   then they would not--
[01:00:02.300 --> 01:00:04.720]   they would not have-- companies wouldn't let them do that.
[01:00:04.720 --> 01:00:06.660]   They wouldn't let them publish a bunch of stuff publicly.
[01:00:06.660 --> 01:00:07.900]   They'd say, that's a rule.
[01:00:07.900 --> 01:00:09.580]   And that used to be the case, in fact.
[01:00:09.580 --> 01:00:12.260]   And so as softwares become more valuable,
[01:00:12.260 --> 01:00:17.500]   developers-- the leverage of a single super talented developer
[01:00:17.500 --> 01:00:18.420]   has gone up.
[01:00:18.420 --> 01:00:21.700]   And they've been able to demand, over the last several decades,
[01:00:21.700 --> 01:00:23.700]   the ability to work in public.
[01:00:23.700 --> 01:00:26.300]   And I think that's not going away.
[01:00:26.300 --> 01:00:27.700]   Other than that, what has been--
[01:00:27.700 --> 01:00:29.060]   I mean, we talked about this a little bit.
[01:00:29.060 --> 01:00:30.640]   But what has been the impact of developers
[01:00:30.640 --> 01:00:32.180]   being more empowered in organizations,
[01:00:32.180 --> 01:00:35.320]   even ones that are not traditionally IT organizations?
[01:00:35.320 --> 01:00:38.920]   Yeah, I mean, software is kind of magic, right?
[01:00:38.920 --> 01:00:42.840]   I mean, you can write a for loop and do something a lot of times.
[01:00:42.840 --> 01:00:46.360]   And when you build large organizations at scale,
[01:00:46.360 --> 01:00:48.020]   one of the things that does surprise you
[01:00:48.020 --> 01:00:50.520]   is the degree to which you need to systematize
[01:00:50.520 --> 01:00:52.920]   the behavior of the people who are working.
[01:00:52.920 --> 01:00:56.600]   Like, when I first was starting companies and building sales
[01:00:56.600 --> 01:01:00.280]   teams, I had this wrong idea coming from the world
[01:01:00.280 --> 01:01:03.360]   as a programmer that salespeople were
[01:01:03.360 --> 01:01:06.960]   like hyper-aggressive, hyper-entrepreneurial,
[01:01:06.960 --> 01:01:10.080]   making promises to the customer that the product wouldn't do.
[01:01:10.080 --> 01:01:12.200]   And that the main challenge you had with salespeople
[01:01:12.200 --> 01:01:16.280]   was restraining them from going out and aggressively cutting
[01:01:16.280 --> 01:01:18.220]   deals that shouldn't be cut.
[01:01:18.220 --> 01:01:22.040]   And what I discovered is that that does exist sometimes.
[01:01:22.040 --> 01:01:23.960]   But the much more common case is that you
[01:01:23.960 --> 01:01:27.960]   need to build a systematic sales playbook, which
[01:01:27.960 --> 01:01:31.640]   is almost a script that you run on your sales team,
[01:01:31.640 --> 01:01:33.640]   where your sales reps know the process they
[01:01:33.640 --> 01:01:37.200]   need to follow to exercise this repeatable sales motion
[01:01:37.200 --> 01:01:39.160]   and get a deal closed.
[01:01:39.160 --> 01:01:41.760]   And so I just had bad ideas there.
[01:01:41.760 --> 01:01:43.920]   I didn't know that that was how the world worked.
[01:01:43.920 --> 01:01:49.760]   But software is a way to systematize and scale out
[01:01:49.760 --> 01:01:54.400]   a valuable process extremely efficiently.
[01:01:54.400 --> 01:01:59.000]   And I think the more digitized the world has become,
[01:01:59.000 --> 01:02:01.920]   the more valuable software becomes,
[01:02:01.920 --> 01:02:04.160]   and the more valuable become the developers who
[01:02:04.160 --> 01:02:05.880]   can create it, essentially.
[01:02:05.880 --> 01:02:07.600]   Would 25-year-old Nat be surprised
[01:02:07.600 --> 01:02:11.000]   with how well open source worked and how pervasive it is?
[01:02:11.000 --> 01:02:12.400]   Yeah, I think that's true.
[01:02:12.400 --> 01:02:16.760]   Yeah, I think we all have this image when we're young
[01:02:16.760 --> 01:02:21.040]   that these institutions are these implacable edifices that
[01:02:21.040 --> 01:02:24.040]   are evil and all-powerful and are
[01:02:24.040 --> 01:02:27.800]   able to, with master plans, substantially orchestrate
[01:02:27.800 --> 01:02:29.040]   the world.
[01:02:29.040 --> 01:02:30.720]   And that is sometimes a little bit true.
[01:02:30.720 --> 01:02:36.640]   But they're very vulnerable to these new ideas and new forces
[01:02:36.640 --> 01:02:39.520]   and new communications media and stuff like that.
[01:02:39.520 --> 01:02:45.360]   So right now, I think our institutions overall
[01:02:45.360 --> 01:02:46.780]   look relatively weak.
[01:02:46.780 --> 01:02:49.120]   And certainly, they're weaker than I thought they were back
[01:02:49.120 --> 01:02:49.400]   then.
[01:02:49.400 --> 01:02:50.880]   So I thought Microsoft-- honestly,
[01:02:50.880 --> 01:02:52.680]   I thought Microsoft could stop open source.
[01:02:52.680 --> 01:02:54.200]   I thought that was a possibility.
[01:02:54.200 --> 01:02:56.200]   They can do some patent move.
[01:02:56.200 --> 01:03:00.720]   And there's a master plan to ring fence open source in.
[01:03:00.720 --> 01:03:04.560]   And that didn't end up being the case.
[01:03:04.560 --> 01:03:06.480]   In fact, Microsoft, when we bought GitHub,
[01:03:06.480 --> 01:03:10.320]   we pledged all of our patent portfolio to open source.
[01:03:10.320 --> 01:03:12.520]   That was one of the things that we did as part of it.
[01:03:12.520 --> 01:03:15.580]   And so that was a kind of poetic moment for me,
[01:03:15.580 --> 01:03:19.200]   having been on the other side of patent discussions in the past
[01:03:19.200 --> 01:03:22.560]   to be a part of an instrumental in Microsoft
[01:03:22.560 --> 01:03:23.640]   making that pledge.
[01:03:23.640 --> 01:03:24.880]   That was quite crazy.
[01:03:24.880 --> 01:03:25.600]   Oh, that's really interesting.
[01:03:25.600 --> 01:03:28.200]   It wasn't that there was some business strategic reason.
[01:03:28.200 --> 01:03:30.440]   More so, it was just an idea whose time had come.
[01:03:30.440 --> 01:03:33.280]   Well, GitHub had made such a pledge.
[01:03:33.280 --> 01:03:36.240]   And so I think, in part, in acquiring GitHub,
[01:03:36.240 --> 01:03:39.600]   we had to either try to annul that pledge
[01:03:39.600 --> 01:03:40.840]   or sign up to it ourselves.
[01:03:40.840 --> 01:03:43.720]   And so there was sort of a moment of a forced choice.
[01:03:43.720 --> 01:03:47.920]   But everyone at Microsoft thought it was a good idea,
[01:03:47.920 --> 01:03:48.480]   too.
[01:03:48.480 --> 01:03:49.840]   So I think, in many senses, it was
[01:03:49.840 --> 01:03:50.760]   a moment whose time had come.
[01:03:50.760 --> 01:03:53.200]   And the GitHub acquisition was a forcing function.
[01:03:53.200 --> 01:03:56.120]   What do you make of critics of modern open source,
[01:03:56.120 --> 01:03:57.620]   like Richard Stallman, or people who
[01:03:57.620 --> 01:03:59.240]   look at it for free software, saying
[01:03:59.240 --> 01:04:03.480]   that, well, corporations might advocate for open source
[01:04:03.480 --> 01:04:07.040]   because of practical reasons for getting good code.
[01:04:07.040 --> 01:04:10.160]   The real value of software and the real way
[01:04:10.160 --> 01:04:12.160]   the software should be made, it should be free,
[01:04:12.160 --> 01:04:15.040]   and that you can replicate it, you can change it,
[01:04:15.040 --> 01:04:17.120]   you can modify it, and you can completely view it.
[01:04:17.120 --> 01:04:19.640]   And that the ethical values about that
[01:04:19.640 --> 01:04:21.840]   should be more important than the practical values.
[01:04:21.840 --> 01:04:23.280]   What do you make of that critique of open source?
[01:04:23.280 --> 01:04:25.480]   I think those are the things that he wants.
[01:04:25.480 --> 01:04:28.400]   And I think the thing that maybe he hasn't updated
[01:04:28.400 --> 01:04:31.800]   is that maybe not everyone else wants that.
[01:04:31.800 --> 01:04:33.720]   He has this idea that people want freedom
[01:04:33.720 --> 01:04:36.440]   from the tyranny of a proprietary intellectual
[01:04:36.440 --> 01:04:37.880]   property license.
[01:04:37.880 --> 01:04:41.000]   But what people really want is freedom
[01:04:41.000 --> 01:04:44.160]   from having to configure their graphics card or sound driver
[01:04:44.160 --> 01:04:45.240]   or something like that.
[01:04:45.240 --> 01:04:47.360]   They want their computer to kind of work.
[01:04:47.360 --> 01:04:49.760]   There are places where freedom is really valuable,
[01:04:49.760 --> 01:04:52.080]   but there's always this thing of like,
[01:04:52.080 --> 01:04:53.680]   I have a prescriptive ideology that I'd
[01:04:53.680 --> 01:04:56.480]   like to impose on the world versus this thing of like,
[01:04:56.480 --> 01:05:00.000]   I will try to develop the best observational model for what
[01:05:00.000 --> 01:05:04.040]   people actually want, whether I want them to want it or not.
[01:05:04.040 --> 01:05:08.440]   And I think Richard is strongly in the former camp.
[01:05:08.440 --> 01:05:10.720]   Well, what is the most underrated license, by the way?
[01:05:10.720 --> 01:05:11.800]   I mean, I don't know.
[01:05:11.800 --> 01:05:13.720]   Maybe the MIT license is still underrated
[01:05:13.720 --> 01:05:17.360]   because it's just so simple and bare.
[01:05:17.360 --> 01:05:19.000]   Nadia Ekbal had a book recently where
[01:05:19.000 --> 01:05:21.760]   she argued that the key constraint on open source
[01:05:21.760 --> 01:05:25.800]   software and on the time of the people who maintain it
[01:05:25.800 --> 01:05:27.680]   is the community aspect of software.
[01:05:27.680 --> 01:05:31.120]   They had to deal with feature requests and discussions
[01:05:31.120 --> 01:05:33.840]   and maintaining for different platforms and things like that.
[01:05:33.840 --> 01:05:35.380]   And it wasn't the actual code itself,
[01:05:35.380 --> 01:05:37.880]   but rather this sort of extracurricular aspect
[01:05:37.880 --> 01:05:39.280]   that was the main constraint.
[01:05:39.280 --> 01:05:40.440]   Do you think that is the constraint
[01:05:40.440 --> 01:05:41.200]   for open source software?
[01:05:41.200 --> 01:05:42.920]   Like, how do you see what is holding back
[01:05:42.920 --> 01:05:43.920]   more open source software?
[01:05:43.920 --> 01:05:45.640]   Yeah, I mean, I think by and large,
[01:05:45.640 --> 01:05:47.680]   I would say that there is not a problem.
[01:05:47.680 --> 01:05:50.400]   Meaning open source software continues to be developed,
[01:05:50.400 --> 01:05:52.000]   continues to be broadly used.
[01:05:52.000 --> 01:05:54.200]   And there's areas where it works better
[01:05:54.200 --> 01:05:55.720]   and areas where it works less well.
[01:05:55.720 --> 01:05:59.120]   But it's sort of winning in all the areas
[01:05:59.120 --> 01:06:03.520]   where large scale coordination and editorial control
[01:06:03.520 --> 01:06:04.600]   are not necessary.
[01:06:04.600 --> 01:06:07.800]   And so it tends to be great at infrastructure,
[01:06:07.800 --> 01:06:10.760]   standalone components, and very, very horizontal things
[01:06:10.760 --> 01:06:12.440]   like operating systems.
[01:06:12.440 --> 01:06:15.960]   And it tends to be worse at user experiences and things
[01:06:15.960 --> 01:06:18.640]   that where you need a sort of dictatorial aesthetic
[01:06:18.640 --> 01:06:22.000]   or an editorial control.
[01:06:22.000 --> 01:06:24.040]   I've had debates with Dylan Field at Figma
[01:06:24.040 --> 01:06:26.720]   as to why it is that we don't have lots of good open source
[01:06:26.720 --> 01:06:27.380]   applications.
[01:06:27.380 --> 01:06:29.640]   And I've always thought it had something
[01:06:29.640 --> 01:06:32.160]   to do with this governance dynamic of, gosh,
[01:06:32.160 --> 01:06:34.400]   it's such a pain to coordinate with tons of people
[01:06:34.400 --> 01:06:35.360]   who all sort of feel like they have
[01:06:35.360 --> 01:06:38.280]   a right to try to push the project in one way or another.
[01:06:38.280 --> 01:06:40.420]   Whereas in a hierarchical corporation,
[01:06:40.420 --> 01:06:42.960]   there can be a head of this product
[01:06:42.960 --> 01:06:45.120]   or a CEO or founder or designer who just says,
[01:06:45.120 --> 01:06:46.160]   we're doing it this way.
[01:06:46.160 --> 01:06:49.080]   And you can really align things in one direction
[01:06:49.080 --> 01:06:51.040]   very, very easily.
[01:06:51.040 --> 01:06:53.500]   Dylan has argued to me that it might be because there's just
[01:06:53.500 --> 01:06:55.760]   fewer designers, people with good design
[01:06:55.760 --> 01:06:56.720]   sense than open source.
[01:06:56.720 --> 01:06:58.680]   I think that might be a contributing factor too.
[01:06:58.680 --> 01:07:00.800]   But I think it's still mostly the governance thing.
[01:07:00.800 --> 01:07:03.400]   And I think that's what Nadi is pointing at also.
[01:07:03.400 --> 01:07:05.200]   You're running a project.
[01:07:05.200 --> 01:07:06.480]   You gave it to people for free.
[01:07:06.480 --> 01:07:09.320]   For some reason, giving people something for free
[01:07:09.320 --> 01:07:11.060]   creates the sense of entitlement.
[01:07:11.060 --> 01:07:13.560]   And then they feel like they have the right to demand your time
[01:07:13.560 --> 01:07:15.620]   and push things around and give you input
[01:07:15.620 --> 01:07:18.660]   and you want to be polite, and it's very draining.
[01:07:18.660 --> 01:07:22.140]   So I think that where that coordination burden is lower
[01:07:22.140 --> 01:07:25.340]   is where open source tends to succeed more.
[01:07:25.340 --> 01:07:28.060]   And probably software and other new forms of governance
[01:07:28.060 --> 01:07:30.620]   can improve that and expand the territory
[01:07:30.620 --> 01:07:32.340]   that open source can succeed in.
[01:07:32.340 --> 01:07:36.300]   Yeah, I mean, theoretically, those two things are consistent.
[01:07:36.300 --> 01:07:38.460]   You could have very tight control over governance
[01:07:38.460 --> 01:07:40.560]   while the code itself is open source.
[01:07:40.560 --> 01:07:42.240]   This happens in programming languages.
[01:07:42.240 --> 01:07:43.480]   Languages can't be designed--
[01:07:43.480 --> 01:07:45.600]   I mean, they often are eventually--
[01:07:45.600 --> 01:07:47.900]   set in stone and then advanced by committee.
[01:07:47.900 --> 01:07:50.280]   But yeah, I mean, certainly you have these benign dictators
[01:07:50.280 --> 01:07:53.440]   of languages who enforce a strong set of ideas.
[01:07:53.440 --> 01:07:56.400]   They have a vision, master plan.
[01:07:56.400 --> 01:07:59.100]   So that would be the argument that's most on Dylan's side.
[01:07:59.100 --> 01:08:00.680]   It's like, hey, it works for languages.
[01:08:00.680 --> 01:08:03.400]   Why can't it work for end user applications?
[01:08:03.400 --> 01:08:05.080]   I think the thing you need to do, though,
[01:08:05.080 --> 01:08:06.280]   to build a good end user application
[01:08:06.280 --> 01:08:08.080]   is not only have a good aesthetic and idea,
[01:08:08.080 --> 01:08:09.920]   but somehow establish a tight feedback
[01:08:09.920 --> 01:08:12.840]   loop with a set of users where you can give it--
[01:08:12.840 --> 01:08:13.680]   Torcash, try this.
[01:08:13.680 --> 01:08:14.120]   Oh, my gosh.
[01:08:14.120 --> 01:08:14.960]   OK, that's not what you need.
[01:08:14.960 --> 01:08:15.460]   OK.
[01:08:15.460 --> 01:08:18.360]   And so doing that is so hard, even in a company
[01:08:18.360 --> 01:08:20.800]   where you have total hierarchical control
[01:08:20.800 --> 01:08:23.160]   of the team in theory, and everyone really
[01:08:23.160 --> 01:08:25.400]   wants the same thing, and everyone's salary and stock
[01:08:25.400 --> 01:08:28.200]   options depend on the product being accepted by these users.
[01:08:28.200 --> 01:08:32.000]   It still fails many times in that scenario.
[01:08:32.000 --> 01:08:34.460]   Then additionally doing that in the context of open source,
[01:08:34.460 --> 01:08:36.480]   I think it's just slightly too hard.
[01:08:36.480 --> 01:08:38.680]   The reason you acquired GitHub, as you said,
[01:08:38.680 --> 01:08:40.960]   is that there seemed to be this sort of complementarity
[01:08:40.960 --> 01:08:43.120]   between Microsoft and GitHub's missions.
[01:08:43.120 --> 01:08:45.840]   And I guess that's been proven out over the last few years.
[01:08:45.840 --> 01:08:49.840]   Should there be more of these collaborations
[01:08:49.840 --> 01:08:51.040]   and acquisitions?
[01:08:51.040 --> 01:08:52.920]   Should there be more tech conglomerates?
[01:08:52.920 --> 01:08:54.720]   Like, would that be good for the system?
[01:08:54.720 --> 01:08:58.720]   I don't know if it's good, but I think there are--
[01:08:58.720 --> 01:09:01.400]   yes, it is certainly efficient in many ways.
[01:09:01.400 --> 01:09:03.360]   And I think we are seeing a conglomeration occur
[01:09:03.360 --> 01:09:06.080]   because the math is sort of pretty simple.
[01:09:06.080 --> 01:09:09.720]   If you are a large company and you have a lot of customers,
[01:09:09.720 --> 01:09:12.080]   then the thing that you've achieved
[01:09:12.080 --> 01:09:13.840]   is this very expensive and difficult thing
[01:09:13.840 --> 01:09:15.680]   of building distribution and relationships
[01:09:15.680 --> 01:09:17.480]   with lots of customers.
[01:09:17.480 --> 01:09:23.120]   And that is as hard or harder and takes longer and more money
[01:09:23.120 --> 01:09:25.360]   than just inventing the product in the first place.
[01:09:25.360 --> 01:09:26.840]   And so if you can then go and just
[01:09:26.840 --> 01:09:28.640]   buy the product for a small amount of money
[01:09:28.640 --> 01:09:31.000]   and make it available to all of your customers,
[01:09:31.000 --> 01:09:35.400]   then there's often an immediate really obvious gain
[01:09:35.400 --> 01:09:36.960]   from doing that.
[01:09:36.960 --> 01:09:38.640]   And so in that sense, acquisitions
[01:09:38.640 --> 01:09:39.480]   make a ton of sense.
[01:09:39.480 --> 01:09:41.480]   And I've been surprised that the large companies
[01:09:41.480 --> 01:09:44.040]   haven't done many more acquisitions in the past
[01:09:44.040 --> 01:09:46.240]   until I got into a big company and started
[01:09:46.240 --> 01:09:47.400]   trying to do acquisitions.
[01:09:47.400 --> 01:09:49.200]   And I saw that there are strong elements
[01:09:49.200 --> 01:09:52.280]   of the internal dynamics that make it hard.
[01:09:52.280 --> 01:09:54.880]   There's not-- it's easier to spend $100 million
[01:09:54.880 --> 01:09:58.000]   on employees internally to do a project
[01:09:58.000 --> 01:10:01.200]   than to spend $100 million to buy a company.
[01:10:01.200 --> 01:10:03.120]   The dollars are treated differently.
[01:10:03.120 --> 01:10:05.200]   The approval processes are different.
[01:10:05.200 --> 01:10:08.120]   The kind of cultural buy-in processes are different.
[01:10:08.120 --> 01:10:10.360]   And then to the point of the discussion we had earlier,
[01:10:10.360 --> 01:10:12.720]   many acquisitions do fail.
[01:10:12.720 --> 01:10:14.920]   And when an acquisition fails, it's
[01:10:14.920 --> 01:10:17.040]   somehow louder and more embarrassing
[01:10:17.040 --> 01:10:20.040]   than when some new product effort you've spun up
[01:10:20.040 --> 01:10:22.440]   doesn't quite work out as well.
[01:10:22.440 --> 01:10:25.200]   And so I think there's lots of internal reasons,
[01:10:25.200 --> 01:10:28.000]   some somewhat justified and some less so,
[01:10:28.000 --> 01:10:29.360]   that they haven't been doing it.
[01:10:29.360 --> 01:10:31.040]   But just from an economic point of view,
[01:10:31.040 --> 01:10:33.280]   it seemed like it makes sense to see more acquisitions
[01:10:33.280 --> 01:10:34.160]   than we've seen.
[01:10:34.160 --> 01:10:35.640]   Well, why did you leave?
[01:10:35.640 --> 01:10:38.240]   I think as much as I loved Microsoft,
[01:10:38.240 --> 01:10:41.480]   and certainly as much as I love GitHub, I really truly--
[01:10:41.480 --> 01:10:44.880]   I still feel tremendous love for GitHub and everything
[01:10:44.880 --> 01:10:47.320]   that it means to the people who use it.
[01:10:47.320 --> 01:10:50.080]   I didn't really want to be a part of a giant company
[01:10:50.080 --> 01:10:50.960]   anymore.
[01:10:50.960 --> 01:10:54.400]   And I think building Copilot was an example of this.
[01:10:54.400 --> 01:10:57.600]   It wouldn't have been possible without OpenAI and Microsoft
[01:10:57.600 --> 01:10:58.760]   and GitHub.
[01:10:58.760 --> 01:11:00.880]   But building it also required navigating
[01:11:00.880 --> 01:11:03.560]   this really large group of people
[01:11:03.560 --> 01:11:06.160]   between Microsoft and OpenAI and GitHub.
[01:11:06.160 --> 01:11:08.200]   And you reach a point where you're
[01:11:08.200 --> 01:11:12.680]   spending a ton of time on just navigating and coordinating
[01:11:12.680 --> 01:11:13.880]   lots of people.
[01:11:13.880 --> 01:11:15.640]   And I just find that less energizing.
[01:11:15.640 --> 01:11:18.120]   Back to enthusiasm, just my enthusiasm for that
[01:11:18.120 --> 01:11:19.600]   was not as high.
[01:11:19.600 --> 01:11:24.920]   And I was torn about it because I truly love GitHub,
[01:11:24.920 --> 01:11:25.600]   the product.
[01:11:25.600 --> 01:11:29.520]   And there was so much more I still knew we could do.
[01:11:29.520 --> 01:11:31.440]   But I was proud of what we'd done.
[01:11:31.440 --> 01:11:36.000]   And I miss the team, and I miss working on GitHub.
[01:11:36.000 --> 01:11:38.200]   It was really an honor for me.
[01:11:38.200 --> 01:11:40.080]   But yeah, it was time for me to go do something.
[01:11:40.080 --> 01:11:41.160]   I was always a startup guy.
[01:11:41.160 --> 01:11:42.440]   I always liked small teams.
[01:11:42.440 --> 01:11:45.560]   And I wanted to go back to the smaller, more nimble
[01:11:45.560 --> 01:11:46.240]   environment.
[01:11:46.240 --> 01:11:48.440]   OK, so we'll get to it in a second.
[01:11:48.440 --> 01:11:53.520]   But first, I want to ask about nat.org and the 300 words
[01:11:53.520 --> 01:11:54.920]   to list there, which is, I think,
[01:11:54.920 --> 01:11:58.400]   one of the most interesting and, I guess,
[01:11:58.400 --> 01:12:03.080]   very Straussian list of 300 words I've seen anywhere.
[01:12:03.080 --> 01:12:06.000]   But I'm just going to mention some of these
[01:12:06.000 --> 01:12:08.120]   and get some of your commentary.
[01:12:08.120 --> 01:12:09.760]   You should probably work on raising
[01:12:09.760 --> 01:12:11.200]   the ceiling, not the floor.
[01:12:11.200 --> 01:12:11.680]   Yeah.
[01:12:11.680 --> 01:12:13.360]   Why?
[01:12:13.360 --> 01:12:17.000]   Yeah, I mean, well, first, I say probably.
[01:12:17.000 --> 01:12:19.360]   But what does it mean to raise the ceiling or the floor?
[01:12:19.360 --> 01:12:21.280]   I just observed a lot of projects
[01:12:21.280 --> 01:12:26.440]   that set out to raise the floor, meaning, gosh, we are fine.
[01:12:26.440 --> 01:12:27.960]   But they are not.
[01:12:27.960 --> 01:12:32.120]   And we need to go help them with our superior prosperity
[01:12:32.120 --> 01:12:35.760]   and understanding of their situation.
[01:12:35.760 --> 01:12:39.440]   And many of those projects fail.
[01:12:39.440 --> 01:12:42.360]   So for example, there were a lot of attempts
[01:12:42.360 --> 01:12:47.160]   to bring internet to Africa by large and wealthy tech
[01:12:47.160 --> 01:12:49.680]   companies in American universities.
[01:12:49.680 --> 01:12:52.120]   And I won't say they all had no effect.
[01:12:52.120 --> 01:12:52.800]   That's not true.
[01:12:52.800 --> 01:12:57.360]   But many of them were far short of successful.
[01:12:57.360 --> 01:12:58.360]   There were satellites.
[01:12:58.360 --> 01:12:59.360]   There were balloons.
[01:12:59.360 --> 01:13:02.880]   There were high-altitude drones.
[01:13:02.880 --> 01:13:06.320]   There were mesh network laptops that were pursued
[01:13:06.320 --> 01:13:07.400]   by all these companies.
[01:13:07.400 --> 01:13:09.640]   And by the way, by perfectly well-meaning, incredibly
[01:13:09.640 --> 01:13:13.160]   talented people who I think did, in some cases,
[01:13:13.160 --> 01:13:15.280]   see some success, but overall, probably much less
[01:13:15.280 --> 01:13:16.840]   than they ever hoped.
[01:13:16.840 --> 01:13:20.000]   But if you go to Africa, there is internet now.
[01:13:20.000 --> 01:13:23.080]   And the way internet got there is the technologies
[01:13:23.080 --> 01:13:25.440]   that we developed to raise the ceiling in the richest
[01:13:25.440 --> 01:13:29.060]   part of the world, which were cell phones and cell towers.
[01:13:29.060 --> 01:13:31.160]   I mean, in the movie "Wall Street" from the '80s,
[01:13:31.160 --> 01:13:33.000]   he's got that gigantic brick cell phone.
[01:13:33.000 --> 01:13:35.700]   That thing cost like $10,000 at the time.
[01:13:35.700 --> 01:13:38.720]   That was a ceiling-raising technology.
[01:13:38.720 --> 01:13:43.200]   It eventually went down the learning curve and became cheap.
[01:13:43.200 --> 01:13:44.920]   And the cell towers and cell phones,
[01:13:44.920 --> 01:13:48.120]   eventually, we've got now hundreds of millions or billions
[01:13:48.120 --> 01:13:49.360]   of them in Africa.
[01:13:49.360 --> 01:13:53.240]   And it was that initially ceiling-raising technology
[01:13:53.240 --> 01:13:58.280]   and then the force of capitalism that made it work.
[01:13:58.280 --> 01:14:03.560]   In the end, it was not any deus ex machina technology solution
[01:14:03.560 --> 01:14:06.000]   that it was intended to raise the floor.
[01:14:06.000 --> 01:14:08.520]   And so I think there's something about that that's
[01:14:08.520 --> 01:14:12.220]   not just an incidental example.
[01:14:12.220 --> 01:14:14.080]   But on my website, I say probably,
[01:14:14.080 --> 01:14:18.720]   because there are some examples where I think people set out
[01:14:18.720 --> 01:14:20.520]   to kind of raise the floor and say,
[01:14:20.520 --> 01:14:24.160]   no one should ever die of smallpox again.
[01:14:24.160 --> 01:14:27.600]   No one should ever die of guinea worm again.
[01:14:27.600 --> 01:14:28.660]   And they succeed.
[01:14:28.660 --> 01:14:30.400]   And I wouldn't want to discourage that from happening.
[01:14:30.400 --> 01:14:32.520]   But I think on balance, we have too many attempts
[01:14:32.520 --> 01:14:35.120]   to do that that look good, feel good, sound good,
[01:14:35.120 --> 01:14:37.920]   and don't matter, and in some cases,
[01:14:37.920 --> 01:14:39.880]   have the opposite of the effect they intend to.
[01:14:39.880 --> 01:14:41.480]   Here's another one.
[01:14:41.480 --> 01:14:44.480]   And this is under the EMH section.
[01:14:44.480 --> 01:14:47.040]   In many cases, it's more accurate to model the world
[01:14:47.040 --> 01:14:49.120]   as 500 people than 8 billion.
[01:14:49.120 --> 01:14:50.120]   Now, here's my question.
[01:14:50.120 --> 01:14:53.000]   What are the 8 billion minus 500 people doing?
[01:14:53.000 --> 01:14:54.840]   Like, why are there only 500 people?
[01:14:54.840 --> 01:14:56.800]   Yeah, I mean, I don't know exactly.
[01:14:56.800 --> 01:14:58.200]   It's a good question.
[01:14:58.200 --> 01:14:59.360]   I ask people that a lot.
[01:14:59.360 --> 01:15:02.000]   I mean, the more I've sort of done in life,
[01:15:02.000 --> 01:15:03.960]   the more I've been mystified by this.
[01:15:03.960 --> 01:15:05.680]   Like, oh, somebody must be doing x.
[01:15:05.680 --> 01:15:08.360]   And then you hear there's a few people doing x.
[01:15:08.360 --> 01:15:10.760]   And then you look into it, they're not actually doing x.
[01:15:10.760 --> 01:15:14.240]   They're doing kind of some version of it that's not that.
[01:15:14.240 --> 01:15:17.760]   And so all the kind of best moments in life
[01:15:17.760 --> 01:15:20.880]   occur when you find something that to you is totally obvious
[01:15:20.880 --> 01:15:23.280]   that clearly somebody must be doing, but no one is doing.
[01:15:23.280 --> 01:15:25.320]   I mean, Mark Zuckerberg says this about founding Facebook.
[01:15:25.320 --> 01:15:26.740]   Like, surely the big companies will eventually
[01:15:26.740 --> 01:15:28.600]   do this and create this social and identity
[01:15:28.600 --> 01:15:30.080]   layer on the internet.
[01:15:30.080 --> 01:15:31.080]   Microsoft will do this.
[01:15:31.080 --> 01:15:32.560]   But no, none of them were.
[01:15:32.560 --> 01:15:33.520]   And he did it.
[01:15:33.520 --> 01:15:34.480]   So what are they doing?
[01:15:34.480 --> 01:15:38.440]   OK, so I think the first thing is many people
[01:15:38.440 --> 01:15:40.880]   throughout the world are optimizing local conditions.
[01:15:40.880 --> 01:15:43.520]   So they're working in their town, their community.
[01:15:43.520 --> 01:15:44.960]   They're doing something there.
[01:15:44.960 --> 01:15:47.040]   And so the set of people that are kind of thinking
[01:15:47.040 --> 01:15:49.960]   about kind of global conditions is just naturally
[01:15:49.960 --> 01:15:53.000]   narrowed by the structure of the economy.
[01:15:53.000 --> 01:15:53.720]   That's number one.
[01:15:53.720 --> 01:15:55.560]   I think number two is most people really
[01:15:55.560 --> 01:15:57.440]   are quite memetic.
[01:15:57.440 --> 01:16:00.480]   And I think we all are, including me.
[01:16:00.480 --> 01:16:03.120]   We get a lot of ideas from other people.
[01:16:03.120 --> 01:16:06.160]   And so our ideas are not our own.
[01:16:06.160 --> 01:16:07.760]   We kind of got them from somebody else.
[01:16:07.760 --> 01:16:09.640]   It's kind of copy-paste.
[01:16:09.640 --> 01:16:12.520]   And so you have to work really hard not to do that
[01:16:12.520 --> 01:16:13.720]   and to be de-correlated.
[01:16:13.720 --> 01:16:16.440]   And I think this is even more true today
[01:16:16.440 --> 01:16:17.800]   because of the internet.
[01:16:17.800 --> 01:16:23.960]   I don't know if Albert Einstein, as a patent clerk,
[01:16:23.960 --> 01:16:26.500]   wouldn't he have just been on Twitter just getting
[01:16:26.500 --> 01:16:27.960]   the same ideas as everybody else?
[01:16:27.960 --> 01:16:29.840]   Like, what do you have as de-correlated ideas?
[01:16:29.840 --> 01:16:32.600]   So I think the internet's correlated us more.
[01:16:32.600 --> 01:16:36.400]   The exception would be really disagreeable people who
[01:16:36.400 --> 01:16:37.920]   are just naturally disagreeable.
[01:16:37.920 --> 01:16:39.960]   And so I think the future belongs
[01:16:39.960 --> 01:16:42.800]   to the autists in some sense because they don't care
[01:16:42.800 --> 01:16:46.160]   what other people think as much.
[01:16:46.160 --> 01:16:49.160]   Those of us on the spectrum, in any sense, I think,
[01:16:49.160 --> 01:16:50.680]   are in that category.
[01:16:50.680 --> 01:16:52.160]   Then, yeah, I think--
[01:16:52.160 --> 01:16:54.360]   and then we have this belief that the world's efficient
[01:16:54.360 --> 01:16:55.200]   and it isn't.
[01:16:55.200 --> 01:16:56.320]   I think that's part of it.
[01:16:56.320 --> 01:16:59.760]   So the other thing is that the world is
[01:16:59.760 --> 01:17:01.160]   so fractal and so interesting.
[01:17:01.160 --> 01:17:06.640]   I mean, Herculaneum papyri is this corner of the world
[01:17:06.640 --> 01:17:09.720]   that I find totally fascinating, but I
[01:17:09.720 --> 01:17:12.600]   don't have any anticipation that 8 billion people should
[01:17:12.600 --> 01:17:14.640]   be thinking about that or that that should
[01:17:14.640 --> 01:17:15.960]   be a priority for everyone.
[01:17:15.960 --> 01:17:17.120]   OK, here's another one.
[01:17:17.120 --> 01:17:20.400]   Large-scale engineering projects are more soluble in IQ
[01:17:20.400 --> 01:17:21.120]   than they appear.
[01:17:21.120 --> 01:17:22.280]   And here's my question.
[01:17:22.280 --> 01:17:24.400]   Does that make you think that the impact of AI tools
[01:17:24.400 --> 01:17:27.560]   like Copilot will be bigger or smaller because of engineering?
[01:17:27.560 --> 01:17:29.840]   Because one way to look at Copilot is, like, actually,
[01:17:29.840 --> 01:17:32.400]   it seems like he was probably less than the average engineer,
[01:17:32.400 --> 01:17:34.040]   so maybe it'll have less impact, right?
[01:17:34.040 --> 01:17:36.320]   Yeah, but I think it increases the productivity of--
[01:17:36.320 --> 01:17:38.280]   like, it definitely increases the productivity
[01:17:38.280 --> 01:17:41.000]   of the average engineer to bring them higher up.
[01:17:41.000 --> 01:17:43.920]   And I think it increases the productivity of the best
[01:17:43.920 --> 01:17:45.680]   engineers as well.
[01:17:45.680 --> 01:17:47.400]   Certainly, a lot of the people I consider
[01:17:47.400 --> 01:17:50.520]   to be the best engineers tell me that they find it increases
[01:17:50.520 --> 01:17:52.440]   their productivity a lot.
[01:17:52.440 --> 01:17:56.360]   So yeah, I think AI is going to completely change--
[01:17:56.360 --> 01:18:00.080]   it's really interesting how so much of what's
[01:18:00.080 --> 01:18:04.280]   happened in AI has been sort of soft, fictional work.
[01:18:04.280 --> 01:18:05.160]   You have mid-journey.
[01:18:05.160 --> 01:18:06.400]   You have copywriting.
[01:18:06.400 --> 01:18:09.200]   You have, gosh, Claude from Anthropica is so literary.
[01:18:09.200 --> 01:18:11.120]   It writes poetry so well.
[01:18:11.120 --> 01:18:14.040]   Except for Copilot, which is this real hard area where
[01:18:14.040 --> 01:18:17.240]   the code has to compile, has to be syntactically correct.
[01:18:17.240 --> 01:18:19.600]   It has to work and pass the tests.
[01:18:19.600 --> 01:18:22.200]   And we see this steady improvement curve
[01:18:22.200 --> 01:18:24.640]   where now already, on average, more than half of the code
[01:18:24.640 --> 01:18:25.640]   is written by Copilot.
[01:18:25.640 --> 01:18:29.280]   I think when it shipped, it was like low 20s.
[01:18:29.280 --> 01:18:30.720]   And so it's really improved a lot
[01:18:30.720 --> 01:18:32.760]   as the models have gotten better and the prompting
[01:18:32.760 --> 01:18:33.720]   has gotten better.
[01:18:33.720 --> 01:18:39.120]   But I don't see any reason why that won't be like 95%.
[01:18:39.120 --> 01:18:40.640]   It seems very likely to me.
[01:18:40.640 --> 01:18:43.000]   And so I think-- I don't know what that world looks like.
[01:18:43.000 --> 01:18:45.280]   It seems like we might have more special purpose and less
[01:18:45.280 --> 01:18:46.320]   general purpose software.
[01:18:46.320 --> 01:18:48.240]   Like right now, we use general purpose tools
[01:18:48.240 --> 01:18:50.800]   like spreadsheets and things like this a lot.
[01:18:50.800 --> 01:18:53.400]   But part of that has to do with the cost of creating software.
[01:18:53.400 --> 01:18:55.760]   And so once you have much cheaper software,
[01:18:55.760 --> 01:18:57.600]   do you create more special purpose software?
[01:18:57.600 --> 01:19:00.280]   That's a possibility.
[01:19:00.280 --> 01:19:04.680]   Every company, just a custom piece of code, in a sense.
[01:19:04.680 --> 01:19:07.060]   Maybe that's the kind of future we're headed towards.
[01:19:07.060 --> 01:19:08.520]   So yeah, I think we're going to see
[01:19:08.520 --> 01:19:12.020]   enormous amounts of change in software development.
[01:19:12.020 --> 01:19:12.840]   Another one.
[01:19:12.840 --> 01:19:16.040]   The cultural prohibition on micromanagement is harmful.
[01:19:16.040 --> 01:19:18.520]   Great individuals should be fully empowered
[01:19:18.520 --> 01:19:20.000]   to exercise their judgment.
[01:19:20.000 --> 01:19:22.880]   And the rebuttal to this is, if you micromanage
[01:19:22.880 --> 01:19:25.400]   or prevent people from learning and to develop
[01:19:25.400 --> 01:19:26.600]   their own judgment--
[01:19:26.600 --> 01:19:29.160]   Yeah, so imagine you go into some company.
[01:19:29.160 --> 01:19:30.200]   They hire Dorkesh.
[01:19:30.200 --> 01:19:33.200]   And you do a great job with the first project they give you.
[01:19:33.200 --> 01:19:35.760]   And so you're-- everyone's really impressed.
[01:19:35.760 --> 01:19:37.600]   Man, Dorkesh, he made the right decisions.
[01:19:37.600 --> 01:19:38.720]   He worked really hard.
[01:19:38.720 --> 01:19:40.920]   He figured out exactly what needed to be done.
[01:19:40.920 --> 01:19:42.800]   And he did it extremely well.
[01:19:42.800 --> 01:19:44.360]   And so over time, you get promoted
[01:19:44.360 --> 01:19:45.860]   into positions of greater authority.
[01:19:45.860 --> 01:19:47.360]   And the reason the company is doing this
[01:19:47.360 --> 01:19:50.160]   is they want you to do that again, but at a bigger scale.
[01:19:50.160 --> 01:19:52.040]   Do it again, but 10 times bigger.
[01:19:52.040 --> 01:19:54.040]   The whole product instead of part of the product,
[01:19:54.040 --> 01:19:55.840]   or 10 products instead of one.
[01:19:55.840 --> 01:20:01.360]   And so the company is telling you, you have great judgment.
[01:20:01.360 --> 01:20:05.160]   And we want you to exercise that at a greater scale.
[01:20:05.160 --> 01:20:08.720]   Meanwhile, the culture is telling you,
[01:20:08.720 --> 01:20:12.000]   as you get promoted, you should suspend your judgment
[01:20:12.000 --> 01:20:15.200]   more and more and defer your judgment to your team.
[01:20:15.200 --> 01:20:18.240]   And so there's some equilibrium there.
[01:20:18.240 --> 01:20:20.120]   And I think we're just out of equilibrium
[01:20:20.120 --> 01:20:23.440]   right now, where the cultural prohibition is too strong.
[01:20:23.440 --> 01:20:25.340]   And I think maybe in the--
[01:20:25.340 --> 01:20:27.840]   I don't know if this is true or not, but maybe in the '80s,
[01:20:27.840 --> 01:20:30.200]   I would have felt the other side of this,
[01:20:30.200 --> 01:20:32.600]   that we have too much micromanagement.
[01:20:32.600 --> 01:20:34.280]   I think the other problem that people have
[01:20:34.280 --> 01:20:37.280]   is that they don't like micromanagement
[01:20:37.280 --> 01:20:40.720]   because they don't want bad managers to micromanage.
[01:20:40.720 --> 01:20:42.000]   So you have some bad managers.
[01:20:42.000 --> 01:20:43.840]   They have no expertise in the area.
[01:20:43.840 --> 01:20:45.640]   They're just kind of people managers.
[01:20:45.640 --> 01:20:47.520]   And they're starting to micromanage something
[01:20:47.520 --> 01:20:49.760]   that they don't understand where their judgment is bad.
[01:20:49.760 --> 01:20:53.160]   And my answer to that is stop empowering bad managers.
[01:20:53.160 --> 01:20:54.040]   Don't have them.
[01:20:54.040 --> 01:20:55.280]   Just don't have bad managers.
[01:20:55.280 --> 01:20:57.480]   Promote and empower people who have great judgment
[01:20:57.480 --> 01:21:00.360]   and do understand the subject matter that they're working on.
[01:21:00.360 --> 01:21:03.440]   If I work for you, and I just know you have better judgment,
[01:21:03.440 --> 01:21:04.880]   and you come in, and you say, now,
[01:21:04.880 --> 01:21:05.960]   you're launching the scroll thing,
[01:21:05.960 --> 01:21:08.160]   and I think you've got the file format wrong,
[01:21:08.160 --> 01:21:10.200]   here's how you should do it, I would welcome that,
[01:21:10.200 --> 01:21:11.640]   even though it's micromanagement.
[01:21:11.640 --> 01:21:13.880]   Because it's going to make us more successful,
[01:21:13.880 --> 01:21:14.800]   and I'm going to learn something from that.
[01:21:14.800 --> 01:21:16.960]   And I know your judgment is better than mine in this case.
[01:21:16.960 --> 01:21:18.760]   Or at least we're going to have a conversation about it.
[01:21:18.760 --> 01:21:20.200]   And we're both going to get smarter.
[01:21:20.200 --> 01:21:21.920]   So I think on balance, yeah, there
[01:21:21.920 --> 01:21:24.200]   are cases where people have excellent judgment.
[01:21:24.200 --> 01:21:28.040]   And we should encourage them to exercise it.
[01:21:28.040 --> 01:21:33.160]   And sometimes things will go wrong when you do that.
[01:21:33.160 --> 01:21:36.100]   But on balance, you will get far more excellence out of it.
[01:21:36.100 --> 01:21:38.680]   And yeah, we should empower individuals
[01:21:38.680 --> 01:21:39.680]   who have great judgment.
[01:21:39.680 --> 01:21:40.280]   Yeah, yeah.
[01:21:40.280 --> 01:21:41.980]   There's a quote about Napoleon that if he
[01:21:41.980 --> 01:21:45.320]   could have been in every single theater of every single battle
[01:21:45.320 --> 01:21:48.080]   he was part of, that he would have never lost a battle.
[01:21:48.080 --> 01:21:50.520]   I was talking to somebody who worked with you at GitHub.
[01:21:50.520 --> 01:21:53.560]   And she emphasized to me, and it's really remarkable to me,
[01:21:53.560 --> 01:21:56.120]   that even the applications that were being shipped out
[01:21:56.120 --> 01:21:59.480]   to engineers, how much of the actual suggestions
[01:21:59.480 --> 01:22:01.800]   and the actual design came from you directly,
[01:22:01.800 --> 01:22:04.760]   which is kind of remarkable to me that as CEO you would--
[01:22:04.760 --> 01:22:06.380]   Yeah, and you can probably find people
[01:22:06.380 --> 01:22:08.240]   you can talk to who think that was terrible.
[01:22:08.240 --> 01:22:11.600]   But the question is always, does that scale?
[01:22:11.600 --> 01:22:14.160]   And the answer is, it does not scale.
[01:22:14.160 --> 01:22:15.960]   It doesn't.
[01:22:15.960 --> 01:22:19.600]   But the set of people who really do have great judgment--
[01:22:19.600 --> 01:22:21.440]   like, the experience that I had as CEOs,
[01:22:21.440 --> 01:22:23.720]   I was terrified all the time that there
[01:22:23.720 --> 01:22:26.520]   was someone in the company who really knew exactly what to do
[01:22:26.520 --> 01:22:27.800]   and had excellent judgment.
[01:22:27.800 --> 01:22:31.360]   But because of cultural forces, that person wasn't empowered.
[01:22:31.360 --> 01:22:34.280]   That person was not allowed to exercise their judgment
[01:22:34.280 --> 01:22:35.600]   and make decisions.
[01:22:35.600 --> 01:22:37.880]   And so when I would think and talk about this,
[01:22:37.880 --> 01:22:41.520]   that was the fear that it was coming from.
[01:22:41.520 --> 01:22:43.280]   They were in some consensus environment
[01:22:43.280 --> 01:22:45.360]   where their good ideas were getting whittled down
[01:22:45.360 --> 01:22:47.960]   by lots of conversations with other people,
[01:22:47.960 --> 01:22:50.840]   and a politeness, and a desire not to micromanage.
[01:22:50.840 --> 01:22:54.200]   And so we were ending up with some kind of average thing.
[01:22:54.200 --> 01:22:57.400]   And I would rather kind of have more high-variance outcomes,
[01:22:57.400 --> 01:22:59.480]   where you either get something that's excellent,
[01:22:59.480 --> 01:23:05.160]   because it is the expressed vision of a really good auteur,
[01:23:05.160 --> 01:23:07.460]   or you get just a disaster, and it didn't work.
[01:23:07.460 --> 01:23:09.960]   And so now you know it didn't work, and you can start over.
[01:23:09.960 --> 01:23:12.160]   I would rather have those more high-variance outcomes.
[01:23:12.160 --> 01:23:13.160]   And I think it's worth--
[01:23:13.160 --> 01:23:14.120]   it's a worthy trade.
[01:23:14.120 --> 01:23:15.240]   OK, let's talk about AI.
[01:23:15.240 --> 01:23:16.000]   Yeah.
[01:23:16.000 --> 01:23:19.280]   What percentage of the economy is basically text-to-text?
[01:23:19.280 --> 01:23:20.760]   Yeah, I mean, it's a good question.
[01:23:20.760 --> 01:23:25.480]   We've done the Bureau of Labor Statistics analysis of this.
[01:23:25.480 --> 01:23:29.880]   And yeah, it's not the majority of the economy
[01:23:29.880 --> 01:23:30.800]   or anything like that.
[01:23:30.800 --> 01:23:33.300]   We're in the low double-digit percentages.
[01:23:33.300 --> 01:23:35.000]   The thing that I think is hard to predict
[01:23:35.000 --> 01:23:39.640]   is what happens over time as the cost of text-to-text goes down.
[01:23:39.640 --> 01:23:41.720]   And yeah, I don't know.
[01:23:41.720 --> 01:23:43.260]   I don't know what that's going to do.
[01:23:43.260 --> 01:23:45.520]   But yeah, there's plenty of revenue to be got now.
[01:23:45.520 --> 01:23:47.840]   I mean, one way you can think about it is, OK,
[01:23:47.840 --> 01:23:52.280]   we have all these benchmarks for machine learning models.
[01:23:52.280 --> 01:23:56.480]   And there's Lombarda, and there's this, and there's that.
[01:23:56.480 --> 01:24:00.200]   And those are really only useful and only
[01:24:00.200 --> 01:24:03.040]   exist because we haven't deployed the models, really,
[01:24:03.040 --> 01:24:04.200]   at scale.
[01:24:04.200 --> 01:24:07.360]   And so we don't have a sense of what they're actually good at.
[01:24:07.360 --> 01:24:10.680]   The best metric would probably be something
[01:24:10.680 --> 01:24:13.960]   like what percentage of economic tasks can they do?
[01:24:13.960 --> 01:24:18.160]   Or on a gig marketplace like Upwork, for example,
[01:24:18.160 --> 01:24:22.220]   what fraction of Upwork jobs can GPT-4 do?
[01:24:22.220 --> 01:24:23.800]   I think it's an interesting question.
[01:24:23.800 --> 01:24:27.520]   My guess is extremely low right now, autonomously.
[01:24:27.520 --> 01:24:29.880]   But over time, it will grow.
[01:24:29.880 --> 01:24:33.280]   And then the question is, what does that do for Upwork?
[01:24:33.280 --> 01:24:34.160]   I mean, I don't know.
[01:24:34.160 --> 01:24:37.920]   It's probably a $5 billion GMV marketplace,
[01:24:37.920 --> 01:24:39.160]   something like that.
[01:24:39.160 --> 01:24:40.080]   Does it grow?
[01:24:40.080 --> 01:24:43.440]   Does it become $15 billion or $50 billion?
[01:24:43.440 --> 01:24:46.520]   Does it shrink because the cost of text-to-text tasks
[01:24:46.520 --> 01:24:48.480]   goes down?
[01:24:48.480 --> 01:24:49.020]   I don't know.
[01:24:49.020 --> 01:24:52.120]   My bet would be that we find more and more ways
[01:24:52.120 --> 01:24:57.320]   to use text-to-text to advance progress.
[01:24:57.320 --> 01:25:00.400]   And so overall, there's a lot more demand for it.
[01:25:00.400 --> 01:25:01.560]   But yeah, I guess we'll see.
[01:25:01.560 --> 01:25:03.000]   And at what point does it happen?
[01:25:03.000 --> 01:25:05.520]   So I mean, GPT-3 has been a rounding error in terms
[01:25:05.520 --> 01:25:07.160]   of overall economic impact.
[01:25:07.160 --> 01:25:09.360]   Does it happen with GPT-4 or GPT-5,
[01:25:09.360 --> 01:25:11.560]   where we see billions of dollars of usage?
[01:25:11.560 --> 01:25:14.880]   I've got early access to GPT-4, and I've gotten to use it a lot.
[01:25:14.880 --> 01:25:19.320]   And I honestly can't tell you the answer to that,
[01:25:19.320 --> 01:25:21.920]   because it's so hard to discover what these things can do
[01:25:21.920 --> 01:25:23.240]   that the prior ones couldn't do.
[01:25:23.240 --> 01:25:25.760]   I just was talking to someone last night who told me,
[01:25:25.760 --> 01:25:28.840]   oh, GPT-4 is actually really good at Korean and Japanese,
[01:25:28.840 --> 01:25:30.680]   and GPT-3 is much worse at those.
[01:25:30.680 --> 01:25:35.300]   And so it's actually a real step change for those languages.
[01:25:35.300 --> 01:25:39.380]   And yeah, I think people didn't know how good GPT-3 was
[01:25:39.380 --> 01:25:42.380]   until it got instruction tuned for chat GPT
[01:25:42.380 --> 01:25:43.860]   and was put out in that format.
[01:25:43.860 --> 01:25:45.900]   And so I think there's kind of--
[01:25:45.900 --> 01:25:47.580]   you can imagine the pre-trained models
[01:25:47.580 --> 01:25:49.800]   as kind of unrefined crude oil.
[01:25:49.800 --> 01:25:52.300]   And then once they've been kind of RLHF'd and trained
[01:25:52.300 --> 01:25:54.340]   and then put out into the world, people can--
[01:25:54.340 --> 01:25:55.620]   they can find the value.
[01:25:55.620 --> 01:25:57.480]   What part of the AI narrative is wrong
[01:25:57.480 --> 01:25:59.220]   in the over-optimistic direction?
[01:25:59.220 --> 01:26:04.220]   The probably over-optimistic case
[01:26:04.220 --> 01:26:07.140]   from both the people who are fearful of what will happen
[01:26:07.140 --> 01:26:09.660]   and from people who are expecting
[01:26:09.660 --> 01:26:15.020]   great economic benefits is that we're definitely
[01:26:15.020 --> 01:26:18.980]   in this realm of diminishing returns from scale.
[01:26:18.980 --> 01:26:20.980]   So for example, I think GPT-4, my guess
[01:26:20.980 --> 01:26:24.460]   is two orders of magnitude more expensive to train than GPT-3,
[01:26:24.460 --> 01:26:28.560]   but clearly not two orders of magnitude more capable.
[01:26:28.560 --> 01:26:29.900]   Now, is it two orders of magnitude
[01:26:29.900 --> 01:26:31.020]   more economically valuable?
[01:26:31.020 --> 01:26:32.340]   That would also surprise me.
[01:26:32.340 --> 01:26:34.880]   And so I think it's possible, when you're in these sigmoids
[01:26:34.880 --> 01:26:37.180]   where you kind of are going up this exponential
[01:26:37.180 --> 01:26:38.940]   and then you start to asymptote, it
[01:26:38.940 --> 01:26:42.580]   can be difficult to tell if that's going to happen.
[01:26:42.580 --> 01:26:45.420]   So I think, yeah, the idea that we might not
[01:26:45.420 --> 01:26:48.700]   run into hard problems or that scaling will continue
[01:26:48.700 --> 01:26:51.540]   to be worth it on a dollar's basis, I think,
[01:26:51.540 --> 01:26:55.580]   are reasons to be a little bit more
[01:26:55.580 --> 01:26:57.620]   pessimistic than the people who have
[01:26:57.620 --> 01:27:01.100]   high certainty of, I don't know, GDP increasing by 50% per month
[01:27:01.100 --> 01:27:03.060]   or something like that, which I think
[01:27:03.060 --> 01:27:04.180]   some people are predicting.
[01:27:04.180 --> 01:27:05.480]   But on the whole, I'm very optimistic.
[01:27:05.480 --> 01:27:07.980]   So you're asking me to make the bear case for something
[01:27:07.980 --> 01:27:08.940]   I'm very bullish about.
[01:27:08.940 --> 01:27:09.260]   All right.
[01:27:09.260 --> 01:27:10.740]   No, that's why I asked you to make the bear case,
[01:27:10.740 --> 01:27:12.700]   because I know a lot in the whole year.
[01:27:12.700 --> 01:27:14.740]   I want to ask you about these foundation models.
[01:27:14.740 --> 01:27:17.460]   What is the stable equilibrium, you think, of how many of them
[01:27:17.460 --> 01:27:18.220]   there will be?
[01:27:18.220 --> 01:27:21.340]   Will it be a sort of oligopoly, like Uber and Lyft, where--
[01:27:21.340 --> 01:27:25.560]   I think there will probably be wide-scale proliferation.
[01:27:25.560 --> 01:27:28.180]   And you sort of asked me, what are the structural forces that
[01:27:28.180 --> 01:27:29.900]   are pro-proliferation and the structural forces that
[01:27:29.900 --> 01:27:30.860]   are pro-concentration?
[01:27:30.860 --> 01:27:34.140]   So I think the pro-proliferation case is a bit stronger.
[01:27:34.140 --> 01:27:37.020]   So the pro-proliferation case is they're
[01:27:37.020 --> 01:27:39.580]   actually not that hard to train.
[01:27:39.580 --> 01:27:41.880]   You can kind of-- the best practices will promulgate.
[01:27:41.880 --> 01:27:44.460]   You can kind of write them down on a couple of sheets of paper.
[01:27:44.460 --> 01:27:47.060]   And to the extent that secrets are developed that
[01:27:47.060 --> 01:27:49.860]   improve training, those are relatively simple,
[01:27:49.860 --> 01:27:53.220]   and they get copied around easily, number one.
[01:27:53.220 --> 01:27:54.900]   Number two, the data is mostly public.
[01:27:54.900 --> 01:27:56.700]   It's mostly kind of data from the internet.
[01:27:56.700 --> 01:27:59.140]   Number three, the hardware is mostly commodity,
[01:27:59.140 --> 01:28:03.140]   and the hardware is improving quickly and getting
[01:28:03.140 --> 01:28:05.260]   much more efficient.
[01:28:05.260 --> 01:28:08.060]   And then I think there's a lot of techniques
[01:28:08.060 --> 01:28:10.180]   that kind of overcome--
[01:28:10.180 --> 01:28:11.940]   I think some of these labs potentially
[01:28:11.940 --> 01:28:16.060]   have 50%, 100%, 200% training efficiency improvement
[01:28:16.060 --> 01:28:16.620]   techniques.
[01:28:16.620 --> 01:28:18.520]   And so there's just a lot of low-hanging fruit
[01:28:18.520 --> 01:28:21.080]   on the technique side of things.
[01:28:21.080 --> 01:28:22.540]   And so we're seeing it happen.
[01:28:22.540 --> 01:28:23.460]   I mean, it's happening this week,
[01:28:23.460 --> 01:28:25.880]   and it's happening this year, is that we're getting, like,
[01:28:25.880 --> 01:28:27.340]   a lot of proliferation.
[01:28:27.380 --> 01:28:29.460]   The only case against proliferation
[01:28:29.460 --> 01:28:32.340]   is that you'll get concentration because of training costs.
[01:28:32.340 --> 01:28:36.420]   And I don't know that that's true.
[01:28:36.420 --> 01:28:41.740]   I just don't have confidence that the trillion-dollar model
[01:28:41.740 --> 01:28:45.340]   will be much more valuable than the $100 billion model,
[01:28:45.340 --> 01:28:49.260]   and that even it will be necessary to spend
[01:28:49.260 --> 01:28:50.740]   a trillion dollars training it.
[01:28:50.740 --> 01:28:53.500]   Like, maybe there will be so many techniques available
[01:28:53.500 --> 01:28:55.740]   for improving efficiency that--
[01:28:55.740 --> 01:28:58.660]   how much are you willing to spend on researchers-defined
[01:28:58.660 --> 01:29:02.420]   techniques if you're willing to spend a trillion on training?
[01:29:02.420 --> 01:29:04.420]   That's a lot of bounties for new techniques,
[01:29:04.420 --> 01:29:06.740]   and some smart people are going to take those bounties.
[01:29:06.740 --> 01:29:08.340]   How different will these models be?
[01:29:08.340 --> 01:29:09.860]   Will it just be sort of everybody
[01:29:09.860 --> 01:29:11.540]   chasing the same exact marginal improvement,
[01:29:11.540 --> 01:29:13.280]   leading to the same marginal capabilities?
[01:29:13.280 --> 01:29:15.500]   Or will they have entirely different repertoires
[01:29:15.500 --> 01:29:17.260]   of skills and abilities?
[01:29:17.260 --> 01:29:19.360]   Right now, back to the memetic point,
[01:29:19.360 --> 01:29:21.700]   they're all pretty similar, I would say.
[01:29:21.700 --> 01:29:24.940]   I mean, basically the same rough techniques.
[01:29:24.940 --> 01:29:26.980]   What's happened is an alien substance
[01:29:26.980 --> 01:29:29.900]   has sort of landed on Earth, and we
[01:29:29.900 --> 01:29:32.180]   are trying to figure out what we can build with it.
[01:29:32.180 --> 01:29:35.940]   And I think we're in this multiple overhangs here.
[01:29:35.940 --> 01:29:37.900]   We have sort of a compute overhang, where there's much
[01:29:37.900 --> 01:29:39.700]   more compute in the world than is currently
[01:29:39.700 --> 01:29:42.660]   being used to train models, like much, much more.
[01:29:42.660 --> 01:29:46.020]   I think the biggest models are trained on maybe 10-ish thousand
[01:29:46.020 --> 01:29:48.740]   GPUs, but there's millions of GPUs.
[01:29:48.740 --> 01:29:51.180]   And so, OK, there's the compute overhang.
[01:29:51.180 --> 01:29:53.340]   And then we have, I think, a capability and technique
[01:29:53.340 --> 01:29:56.940]   overhang, where there's lots of good ideas that are coming out,
[01:29:56.940 --> 01:29:59.100]   and we haven't figured out how best to assemble them
[01:29:59.100 --> 01:30:01.020]   all together, but that's just a matter of time,
[01:30:01.020 --> 01:30:03.060]   kind of until people do that.
[01:30:03.060 --> 01:30:06.740]   And then those capabilities haven't reached--
[01:30:06.740 --> 01:30:09.140]   because many of them are in the hands of the labs,
[01:30:09.140 --> 01:30:11.900]   they haven't reached the tinkerers of the world.
[01:30:11.900 --> 01:30:14.740]   And I think that is where the new--
[01:30:14.740 --> 01:30:17.340]   like, what can this thing actually do?
[01:30:17.340 --> 01:30:19.580]   Until you get your hands on it, you don't really know.
[01:30:19.580 --> 01:30:21.940]   I think OpenAI were themselves surprised
[01:30:21.940 --> 01:30:25.380]   by how explosively Chat GPT has grown.
[01:30:25.380 --> 01:30:27.420]   I don't think they put Chat GPT out expecting
[01:30:27.420 --> 01:30:28.740]   that to be the big announcement.
[01:30:28.740 --> 01:30:30.740]   I think they thought GPT-4 was going
[01:30:30.740 --> 01:30:32.420]   to be their big announcement.
[01:30:32.420 --> 01:30:34.780]   And I think it still probably is and will be big,
[01:30:34.780 --> 01:30:37.940]   but Chat GPT really surprised them.
[01:30:37.940 --> 01:30:42.540]   And I think it's hard to predict what people will do with it
[01:30:42.540 --> 01:30:44.580]   and what they'll find valuable and what works.
[01:30:44.580 --> 01:30:45.660]   And so you need tinkerers.
[01:30:45.660 --> 01:30:49.140]   So it basically goes from hardware to researchers
[01:30:49.140 --> 01:30:50.940]   to tinkerers to products.
[01:30:50.940 --> 01:30:52.060]   That's the pipe.
[01:30:52.060 --> 01:30:52.860]   That's the cascade.
[01:30:52.860 --> 01:30:53.380]   Yeah, yeah.
[01:30:53.380 --> 01:30:55.300]   When I was scheduling my interview with Ilya,
[01:30:55.300 --> 01:30:57.340]   it was originally supposed to be around the time
[01:30:57.340 --> 01:30:59.020]   that Chat GPT came out.
[01:30:59.020 --> 01:31:02.140]   And so their cons person tells me, listen,
[01:31:02.140 --> 01:31:04.020]   just so you know, this interview will
[01:31:04.020 --> 01:31:05.220]   be scheduled around the time.
[01:31:05.220 --> 01:31:06.980]   We're going to make a minor announcement.
[01:31:06.980 --> 01:31:08.900]   It isn't-- it's not the thing you're thinking.
[01:31:08.900 --> 01:31:09.660]   It's not GPT-4.
[01:31:09.660 --> 01:31:11.140]   But it's just a minor thing.
[01:31:11.140 --> 01:31:15.900]   So they didn't expect what it ended up being.
[01:31:15.900 --> 01:31:18.180]   Have incumbents gotten smarter than before?
[01:31:18.180 --> 01:31:20.500]   So it seems like Microsoft was able to integrate
[01:31:20.500 --> 01:31:22.060]   this new technology real well.
[01:31:22.060 --> 01:31:23.940]   There's been two really big shifts
[01:31:23.940 --> 01:31:26.380]   in the way incumbents behave in the last 20 years
[01:31:26.380 --> 01:31:27.340]   that I've seen.
[01:31:27.340 --> 01:31:29.300]   The first is that it used to be incumbents got
[01:31:29.300 --> 01:31:31.460]   disrupted by startups all the time.
[01:31:31.460 --> 01:31:33.260]   You had example after example of this
[01:31:33.260 --> 01:31:35.940]   in the mini-computer, microcomputer era, et cetera.
[01:31:35.940 --> 01:31:40.820]   And then Clay Christensen wrote "The Innovator's Dilemma."
[01:31:40.820 --> 01:31:44.740]   And I think what happened was that everyone read it.
[01:31:44.740 --> 01:31:47.140]   And they said, oh, disruption is this thing that occurs.
[01:31:47.140 --> 01:31:49.560]   And we have this innovator's dilemma where we get disrupted
[01:31:49.560 --> 01:31:50.900]   because the new thing is cheaper.
[01:31:50.900 --> 01:31:52.620]   And we can't let that happen.
[01:31:52.620 --> 01:31:55.260]   And they became determined not to let that happen.
[01:31:55.260 --> 01:31:57.500]   And they mostly learned how to avoid it.
[01:31:57.500 --> 01:31:59.420]   They learned that you have to be willing to do
[01:31:59.420 --> 01:32:00.340]   some cannibalization.
[01:32:00.340 --> 01:32:02.760]   And you have to be willing to set up separate sales channels
[01:32:02.760 --> 01:32:05.020]   for the new thing and so forth.
[01:32:05.020 --> 01:32:08.100]   And so we've had a lot of stability in incumbents
[01:32:08.100 --> 01:32:10.020]   for the last 15 years or so.
[01:32:10.020 --> 01:32:12.780]   And I think that's maybe why.
[01:32:12.780 --> 01:32:14.460]   That's my theory.
[01:32:14.460 --> 01:32:16.220]   So that's the first major step change.
[01:32:16.220 --> 01:32:17.980]   And then the second one is, man, they
[01:32:17.980 --> 01:32:19.540]   are paying a ton of attention to AI.
[01:32:19.540 --> 01:32:22.220]   If you look at the prior platform revolutions like cloud,
[01:32:22.220 --> 01:32:26.020]   mobile, internet, web, PC, all the incumbents
[01:32:26.020 --> 01:32:28.940]   derided the new platform and said, gosh,
[01:32:28.940 --> 01:32:30.260]   no one's going to use web apps.
[01:32:30.260 --> 01:32:33.500]   Everyone will use full desktop apps, rich applications.
[01:32:33.500 --> 01:32:35.660]   And so there was always this sort
[01:32:35.660 --> 01:32:37.060]   of laughing at the new thing.
[01:32:37.060 --> 01:32:40.100]   The iPhone was laughed at by incumbents.
[01:32:40.100 --> 01:32:42.420]   And that is not happening at all with AI.
[01:32:42.420 --> 01:32:44.180]   Now, we may be at peak hype cycle
[01:32:44.180 --> 01:32:46.260]   and we're going to enter the trough of despair.
[01:32:46.260 --> 01:32:47.940]   I kind of don't think so, though.
[01:32:47.940 --> 01:32:49.640]   I think people are taking it seriously.
[01:32:49.640 --> 01:32:52.500]   And every live player CEO is adopting it aggressively
[01:32:52.500 --> 01:32:53.860]   in their company.
[01:32:53.860 --> 01:32:56.140]   So yeah, I think incumbents have gotten smarter.
[01:32:56.140 --> 01:32:56.860]   All right.
[01:32:56.860 --> 01:32:59.700]   So let me ask you some questions that we got from Twitter.
[01:32:59.700 --> 01:33:02.740]   This is a former guest and, I guess,
[01:33:02.740 --> 01:33:03.980]   mutual friend Austin Vernon.
[01:33:03.980 --> 01:33:05.100]   Oh, yeah.
[01:33:05.100 --> 01:33:08.660]   Nat is one of those people that seems unreasonably effective.
[01:33:08.660 --> 01:33:12.620]   What parts of that are innate and what did he have to learn?
[01:33:12.620 --> 01:33:15.180]   Well, that's very nice of Austin to say.
[01:33:15.180 --> 01:33:15.740]   I don't know.
[01:33:15.740 --> 01:33:18.580]   I mean, I think-- we talked a little bit about this before,
[01:33:18.580 --> 01:33:21.940]   but I think I just have a high willingness to try things
[01:33:21.940 --> 01:33:23.800]   and get caught up in new projects
[01:33:23.800 --> 01:33:25.420]   and then I don't want to stop doing it.
[01:33:25.420 --> 01:33:27.780]   And so I think I just have a relatively low activation
[01:33:27.780 --> 01:33:29.540]   energy to try something.
[01:33:29.540 --> 01:33:32.140]   And I'm willing to sort of impulsively jump into stuff.
[01:33:32.140 --> 01:33:35.500]   And many of those things don't work, but enough of them
[01:33:35.500 --> 01:33:40.140]   do that I guess I've been able to accomplish a few things.
[01:33:40.140 --> 01:33:42.260]   The other thing I would say, to be honest with you,
[01:33:42.260 --> 01:33:45.460]   is that I do not consider myself accomplished or successful.
[01:33:45.460 --> 01:33:48.140]   Like, my self-image is that I haven't really
[01:33:48.140 --> 01:33:50.660]   done anything of tremendous consequence.
[01:33:50.660 --> 01:33:56.540]   And I don't feel like I have this giant sort of bed
[01:33:56.540 --> 01:34:00.100]   of achievements that I can go to sleep on every night.
[01:34:00.100 --> 01:34:01.620]   I think-- and I try--
[01:34:01.620 --> 01:34:04.580]   and I think that's truly how I feel.
[01:34:04.580 --> 01:34:06.740]   I'm kind of an insecure overachiever.
[01:34:06.740 --> 01:34:08.360]   I don't really feel good about myself
[01:34:08.360 --> 01:34:10.340]   unless I'm doing good work.
[01:34:10.340 --> 01:34:13.460]   But I also have kind of cultivated--
[01:34:13.460 --> 01:34:16.460]   tried to cultivate a forward-looking view where
[01:34:16.460 --> 01:34:19.020]   I try not to be incredibly nostalgic about the past.
[01:34:19.020 --> 01:34:21.880]   I don't keep lots of trophies or anything like that.
[01:34:21.880 --> 01:34:23.660]   I go into some people's offices, and there's
[01:34:23.660 --> 01:34:25.940]   like things on the wall and trophies of all the things
[01:34:25.940 --> 01:34:27.140]   they've accomplished.
[01:34:27.140 --> 01:34:29.980]   And that always seemed really icky to me.
[01:34:29.980 --> 01:34:30.820]   So I don't know.
[01:34:30.820 --> 01:34:33.500]   I just had a sort of revulsion to that.
[01:34:33.500 --> 01:34:35.220]   Is that why you took down your blog?
[01:34:35.220 --> 01:34:36.220]   Yeah.
[01:34:36.220 --> 01:34:38.460]   Yeah, I just wanted to move forward.
[01:34:38.460 --> 01:34:41.780]   Simeon asks for your takes on alignment.
[01:34:41.780 --> 01:34:45.540]   Quote, "He seems to invest both in capabilities and alignment,
[01:34:45.540 --> 01:34:48.860]   which is the best move under a very small set of beliefs."
[01:34:48.860 --> 01:34:50.660]   So he's curious to hear a reasoning there.
[01:34:50.660 --> 01:34:51.660]   Yeah.
[01:34:51.660 --> 01:34:52.860]   Well, I'm not--
[01:34:52.860 --> 01:34:53.860]   well, I guess we'll see.
[01:34:53.860 --> 01:34:55.780]   I'm not sure capabilities and alignment end up
[01:34:55.780 --> 01:34:59.180]   being these opposing forces.
[01:34:59.180 --> 01:35:02.340]   It may be that capabilities are very important for alignment.
[01:35:02.340 --> 01:35:06.320]   It may be that alignment is very important for capabilities.
[01:35:06.320 --> 01:35:07.700]   I mean, I think I believe--
[01:35:07.700 --> 01:35:09.320]   I think a lot of people believe, and I
[01:35:09.320 --> 01:35:12.980]   included in this, that AI can have tremendous benefits,
[01:35:12.980 --> 01:35:16.940]   but that there's a small chance of really bad outcomes.
[01:35:16.940 --> 01:35:19.340]   Maybe some people think it's a large chance.
[01:35:19.340 --> 01:35:23.300]   And so I think the solutions, if they exist,
[01:35:23.300 --> 01:35:25.100]   are likely to be technical.
[01:35:25.100 --> 01:35:27.060]   They're probably some combination of technical
[01:35:27.060 --> 01:35:28.740]   and prescriptive.
[01:35:28.740 --> 01:35:30.820]   So it's probably a piece of code in a README file.
[01:35:30.820 --> 01:35:33.860]   It says, if you want to build aligned AIs, use this code
[01:35:33.860 --> 01:35:36.980]   and don't do this, or something like that.
[01:35:36.980 --> 01:35:41.220]   And so, yeah, I think that's really important,
[01:35:41.220 --> 01:35:43.220]   and more people should try to actually build
[01:35:43.220 --> 01:35:44.100]   technical solutions.
[01:35:44.100 --> 01:35:45.300]   I think one of the big things that's missing
[01:35:45.300 --> 01:35:46.900]   that sort of perplexes me is there's
[01:35:46.900 --> 01:35:50.420]   no open source technical alignment community.
[01:35:50.420 --> 01:35:53.700]   There's no one actually just implementing
[01:35:53.700 --> 01:35:56.700]   in open source the best alignment tools.
[01:35:56.700 --> 01:35:59.140]   There's a lot of philosophizing and talking,
[01:35:59.140 --> 01:36:02.420]   and then there's a lot of behind closed doors interpretability
[01:36:02.420 --> 01:36:03.980]   and alignment work.
[01:36:03.980 --> 01:36:05.900]   And I think we're going to end up,
[01:36:05.900 --> 01:36:08.540]   because the alignment people have this belief that they
[01:36:08.540 --> 01:36:11.100]   shouldn't release their work, in a world where there's
[01:36:11.100 --> 01:36:14.020]   a lot of open source, pure capabilities work,
[01:36:14.020 --> 01:36:17.660]   and no open source alignment work for a little while.
[01:36:17.660 --> 01:36:19.180]   And then hopefully that'll change.
[01:36:19.180 --> 01:36:21.220]   So yeah, I wanted to, on the margin,
[01:36:21.220 --> 01:36:22.820]   invest in people doing alignment.
[01:36:22.820 --> 01:36:23.980]   Seems like that's important.
[01:36:23.980 --> 01:36:25.980]   I thought Sydney was kind of an example of this.
[01:36:25.980 --> 01:36:28.740]   You had Microsoft essentially release an unaligned AI,
[01:36:28.740 --> 01:36:31.100]   and I think the world sort of said, hmm,
[01:36:31.100 --> 01:36:32.740]   sort of threatening its users.
[01:36:32.740 --> 01:36:35.340]   That seems a little bit strange.
[01:36:35.340 --> 01:36:38.740]   If Microsoft can't put a leash on this thing, who can?
[01:36:38.740 --> 01:36:40.900]   And so I think there will be more interest in it,
[01:36:40.900 --> 01:36:43.020]   and I hope there's open communities.
[01:36:43.020 --> 01:36:44.620]   - That was so enduring for some reason.
[01:36:44.620 --> 01:36:47.780]   It threatening you just made it so much more lovable.
[01:36:47.780 --> 01:36:50.100]   - Yeah, I think it's like the only reason it wasn't scary
[01:36:50.100 --> 01:36:52.580]   is because it wasn't hooked up to anything.
[01:36:52.580 --> 01:36:55.100]   If it was hooked up to HR systems,
[01:36:55.100 --> 01:36:57.860]   or if it could post jobs or something like that,
[01:36:57.860 --> 01:37:01.060]   then I don't know, on a gig worker site or something,
[01:37:01.060 --> 01:37:02.580]   I think it could have been scary.
[01:37:02.580 --> 01:37:04.740]   - All right, before we go, where can people learn more
[01:37:04.740 --> 01:37:06.540]   about the Vesuvius Challenge?
[01:37:06.540 --> 01:37:09.700]   - Yeah, so VesuviusChallenge is at scrollprize.org,
[01:37:09.700 --> 01:37:13.300]   S-C-R-O-L-L-P-R-I-Z-E.org.
[01:37:13.300 --> 01:37:16.780]   Yeah, check it out, and I think it's very likely
[01:37:16.780 --> 01:37:18.380]   that somebody listening to this could be the one
[01:37:18.380 --> 01:37:21.740]   who wins the grand prize and decodes the scrolls.
[01:37:21.740 --> 01:37:22.940]   - Okay, excellent, awesome.
[01:37:22.940 --> 01:37:24.820]   Okay, well, Nad, this was a true pleasure.
[01:37:24.820 --> 01:37:25.660]   - Thanks for catching up. - Thanks so much
[01:37:25.660 --> 01:37:26.500]   for coming on the podcast.
[01:37:26.500 --> 01:37:28.420]   - Thanks for having me.
[01:37:28.420 --> 01:37:29.460]   - Hey, everybody.
[01:37:29.460 --> 01:37:31.540]   I hope you enjoyed that episode.
[01:37:31.540 --> 01:37:34.860]   Just wanted to let you know that in order to help pay
[01:37:34.860 --> 01:37:38.220]   for the bills associated with this podcast,
[01:37:38.220 --> 01:37:40.340]   I'm turning on paid subscriptions
[01:37:40.340 --> 01:37:44.640]   on my SEP stack at varkashpatel.com.
[01:37:44.640 --> 01:37:48.780]   No important content on this podcast will ever be paywalled,
[01:37:48.780 --> 01:37:51.500]   so please don't donate if you have to think twice
[01:37:51.500 --> 01:37:53.240]   before buying a cup of coffee.
[01:37:53.240 --> 01:37:56.420]   But if you have the means and you've enjoyed this podcast
[01:37:56.420 --> 01:37:58.140]   or gotten some kind of value out of it,
[01:37:58.140 --> 01:38:00.000]   I would really appreciate your support.
[01:38:00.000 --> 01:38:02.200]   As always, the most helpful thing you can do
[01:38:02.200 --> 01:38:03.980]   is to share the podcast.
[01:38:03.980 --> 01:38:05.700]   Send it to people you think might enjoy it,
[01:38:05.700 --> 01:38:07.820]   put it in Twitter, your group chats, et cetera.
[01:38:07.820 --> 01:38:09.700]   It just splits the world.
[01:38:09.700 --> 01:38:10.940]   Appreciate you listening.
[01:38:10.940 --> 01:38:12.100]   I'll see you next time.
[01:38:12.100 --> 01:38:13.320]   Cheers.
[01:38:13.320 --> 01:38:15.900]   (gentle music)
[01:38:15.900 --> 01:38:18.480]   (gentle music)
[01:38:18.480 --> 01:38:22.480]   [music]

