
[00:00:00.000 --> 00:00:09.520]   How do you connect the offline metrics that you have in anything you're doing in any model
[00:00:09.520 --> 00:00:15.760]   in the lab to what's going to be the real impact that that model has on your product?
[00:00:15.760 --> 00:00:20.120]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:20.120 --> 00:00:22.280]   and I'm your host, Lukas Biewald.
[00:00:22.280 --> 00:00:30.480]   Xavier Amatrian is co-founder and CTO of CureEye, a ML-based primary care chat system that we're
[00:00:30.480 --> 00:00:32.440]   going to talk about today.
[00:00:32.440 --> 00:00:37.280]   Before that, he was VP of engineering at a website called Quora, which I absolutely love.
[00:00:37.280 --> 00:00:43.360]   And before that, he ran the recommendation system at Netflix, which was especially famous
[00:00:43.360 --> 00:00:46.640]   for the Netflix recommendation prize.
[00:00:46.640 --> 00:00:49.000]   I could not be more excited to talk to him today.
[00:00:49.000 --> 00:00:51.480]   I want to start with talking about what you're working on now.
[00:00:51.480 --> 00:00:56.440]   You've had a really long and interesting career in ML, but it probably makes sense to talk
[00:00:56.440 --> 00:00:57.440]   about CureEye, right?
[00:00:57.440 --> 00:00:58.440]   Is that- Yeah, CureEye.
[00:00:58.440 --> 00:00:59.440]   CureEye.
[00:00:59.440 --> 00:01:05.360]   First, can you tell me what CureEye does before we get into how machine learning fits in?
[00:01:05.360 --> 00:01:06.360]   Yeah.
[00:01:06.360 --> 00:01:14.080]   I mean, the basic level is an end-to-end virtual primary care service.
[00:01:14.080 --> 00:01:20.080]   It provides everything that you could need from your primary care doctor, but it provides
[00:01:20.080 --> 00:01:24.880]   it through an application, through chat.
[00:01:24.880 --> 00:01:31.240]   And our goal is to provide the best possible healthcare at the lowest possible price and
[00:01:31.240 --> 00:01:35.600]   make it very accessible and very affordable for everyone in the world, while at the same
[00:01:35.600 --> 00:01:37.280]   time increasing quality.
[00:01:37.280 --> 00:01:43.200]   And the way to enable that is using technology and more concretely, AI and machine learning,
[00:01:43.200 --> 00:01:44.200]   right?
[00:01:44.320 --> 00:01:51.320]   We feel like one of the things you can do through machine learning and AI is to automate
[00:01:51.320 --> 00:01:53.600]   and therefore make things more efficient.
[00:01:53.600 --> 00:01:54.900]   That's pretty obvious.
[00:01:54.900 --> 00:01:59.200]   But the other thing that might not be so obvious is that you can also make things higher quality,
[00:01:59.200 --> 00:02:00.200]   right?
[00:02:00.200 --> 00:02:06.760]   And that's very much related to the notion of data-driven decision-making algorithms
[00:02:06.760 --> 00:02:11.880]   and science in general, which should be behind all the medical decisions.
[00:02:11.880 --> 00:02:17.200]   So the combination of quality accessibility is what drives our product.
[00:02:17.200 --> 00:02:24.520]   But again, our product is basically a virtual primary care service that is provided through
[00:02:24.520 --> 00:02:27.600]   an application and through a chat-based interaction.
[00:02:27.600 --> 00:02:29.120]   And so could I use it today?
[00:02:29.120 --> 00:02:30.120]   Is it...
[00:02:30.120 --> 00:02:35.480]   If I had a health issue, I could talk to a virtual...
[00:02:35.480 --> 00:02:36.480]   Yeah.
[00:02:36.480 --> 00:02:39.320]   We're now available in seven states in the US.
[00:02:39.320 --> 00:02:42.840]   So that's... let me make sure I don't miss any.
[00:02:42.840 --> 00:02:50.840]   It's California, Florida, it's Illinois, Ohio, South Carolina, and North Carolina.
[00:02:50.840 --> 00:02:52.640]   So those are the seven states.
[00:02:52.640 --> 00:02:56.580]   We plan on being available in the 50 states by the end of the summer.
[00:02:56.580 --> 00:02:59.480]   So we're expanding rapidly.
[00:02:59.480 --> 00:03:04.000]   And the only reason we're not in the other 50 states is because there's legal implications
[00:03:04.000 --> 00:03:07.840]   of expanding and you need a different license for all the different states.
[00:03:07.840 --> 00:03:13.000]   But yes, if you're in one of those seven states, you can download it and start using it for
[00:03:13.000 --> 00:03:14.400]   free.
[00:03:14.400 --> 00:03:18.160]   And after the free trial, the price is very affordable too.
[00:03:18.160 --> 00:03:23.960]   So it's $7.99 a month and you can use it as many times, no copays, you don't pay per use
[00:03:23.960 --> 00:03:28.680]   it, it's just like a flat fee and you get everything, including prescriptions.
[00:03:28.680 --> 00:03:32.420]   You can go and pick up your prescription in the pharmacy, go to your lab test if you need
[00:03:32.420 --> 00:03:38.760]   any blood test or anything, and we do all of that through a network of partnerships.
[00:03:38.760 --> 00:03:46.360]   And the healthcare team, which I'm sure we'll get into, is a combination of humans and AI.
[00:03:46.360 --> 00:03:51.420]   So it's like maybe triage is the questions and the ones that are easier, the AI tries
[00:03:51.420 --> 00:03:55.960]   to answer and then the harder ones go to a human or how do you think about that?
[00:03:55.960 --> 00:03:58.480]   Yeah, it's a great question.
[00:03:58.480 --> 00:04:01.840]   And that's typically the traditional approach.
[00:04:01.840 --> 00:04:07.360]   You put the AI up front and then whatever the AI decides it can do, it does, and then
[00:04:07.360 --> 00:04:09.160]   you pass the rest to humans.
[00:04:09.160 --> 00:04:11.380]   We go well beyond that.
[00:04:11.380 --> 00:04:19.160]   We consider the AI to be just another member of the team and the AI never leaves the room.
[00:04:19.160 --> 00:04:22.600]   So what it will do is it will call other people.
[00:04:22.600 --> 00:04:29.120]   We have a care team that is composed of clinical associates, medical assistants, and then licensed
[00:04:29.120 --> 00:04:33.720]   physicians in all the states that we operate, and then the AI.
[00:04:33.720 --> 00:04:40.880]   Now the AI will sometimes, as you said, will take over the interaction and just drive it.
[00:04:40.880 --> 00:04:47.040]   And whenever it's either finished with whatever task it was doing or not sure, it will call
[00:04:47.040 --> 00:04:53.260]   in the physician, but it then stays in the room and it provides assessment and augmentation
[00:04:53.260 --> 00:04:54.720]   to the physician.
[00:04:54.720 --> 00:04:58.980]   So it's both user facing and doctor facing.
[00:04:58.980 --> 00:05:04.400]   So the AI is kind of like the connection between the two ends.
[00:05:04.400 --> 00:05:09.400]   And very importantly, in order to understand this, I think it was kind of implicit on what
[00:05:09.400 --> 00:05:13.700]   I was describing, the doctors are part of our QRI care team.
[00:05:13.700 --> 00:05:18.440]   So it's like part of the team that is not only providing the care, but also helping
[00:05:18.440 --> 00:05:24.040]   us develop the product and helping the system and the algorithms learn from the data that
[00:05:24.040 --> 00:05:25.040]   we're generating.
[00:05:25.040 --> 00:05:29.100]   So we call this, this is the so-called learning healthcare system, right?
[00:05:29.100 --> 00:05:36.420]   Because we are, at the same time, the AI is helping and augmenting the doctors and the
[00:05:36.420 --> 00:05:41.740]   doctors are learning from the AI, but very importantly, the AI is learning from being
[00:05:41.740 --> 00:05:46.340]   part of this team and from the data that is being gathered as part of this end to end
[00:05:46.340 --> 00:05:47.340]   process.
[00:05:47.340 --> 00:05:51.060]   So how is the AI augmenting the doctors?
[00:05:51.060 --> 00:05:59.140]   Is it like suggesting links to kind of go to for research or like auto completing possible
[00:05:59.140 --> 00:06:00.140]   responses?
[00:06:00.140 --> 00:06:04.820]   How does it actually work from a doctor's perspective?
[00:06:04.820 --> 00:06:07.240]   So the AI is doing all the above.
[00:06:07.240 --> 00:06:11.340]   So yes, it is doing all of that.
[00:06:11.340 --> 00:06:16.340]   I mean, as you know, people think of the AI as sort of like a magical entity that exists
[00:06:16.340 --> 00:06:17.340]   somewhere.
[00:06:17.340 --> 00:06:20.940]   But in fact, the AI, what it is, is a combination of different algorithms that are controlled
[00:06:20.940 --> 00:06:22.940]   by some protocol, right?
[00:06:22.940 --> 00:06:27.220]   So there's different machine learning algorithms doing different things and all of them are
[00:06:27.220 --> 00:06:29.700]   augmenting the doctors in different ways.
[00:06:29.700 --> 00:06:37.760]   But in a typical or in a simple scenario, what will happen is the AI will be part of
[00:06:37.760 --> 00:06:42.700]   the so-called history taking, and it will start by asking questions to the patients,
[00:06:42.700 --> 00:06:48.540]   presenting that as entities in electronic health record, it will call in the doctor
[00:06:48.540 --> 00:06:54.140]   and then it will say, "Hey, I have a differential diagnosis, which is a set of possible diagnosis
[00:06:54.140 --> 00:06:56.780]   that I think this could be happening.
[00:06:56.780 --> 00:06:57.940]   Now you take it from here.
[00:06:57.940 --> 00:07:01.920]   But by the way, I can also suggest questions that you could ask the patient if you want
[00:07:01.920 --> 00:07:04.560]   to dig into any of these things."
[00:07:04.560 --> 00:07:09.540]   So the doctor at that point can say, "Oh, wait, this could be COVID.
[00:07:09.540 --> 00:07:10.540]   Hold on.
[00:07:10.540 --> 00:07:15.580]   I can suggest a few questions that I could ask the patient to either confirm or invalidate
[00:07:15.580 --> 00:07:17.380]   the hypothesis that it's COVID."
[00:07:17.380 --> 00:07:23.420]   And then the algorithm will suggest questions that either confirm or not that particular
[00:07:23.420 --> 00:07:24.420]   hypothesis.
[00:07:24.420 --> 00:07:29.740]   As it's going along, it's extracting things from the text because these are all chat-based,
[00:07:29.740 --> 00:07:31.980]   so it's extracting things from the text.
[00:07:31.980 --> 00:07:33.900]   It's highlighting important things.
[00:07:33.900 --> 00:07:38.300]   It's also summarizing the conversation for the next doctor that comes in to sort of like
[00:07:38.300 --> 00:07:40.420]   get a summary.
[00:07:40.420 --> 00:07:45.980]   And even going all the way to suggesting treatment if the doctor needs suggestion for a treatment
[00:07:45.980 --> 00:07:48.340]   once the diagnosis has been confirmed.
[00:07:48.340 --> 00:07:55.620]   Very importantly, the AI or the algorithms never make the final decision to either diagnose
[00:07:55.620 --> 00:07:56.820]   or to treat.
[00:07:56.820 --> 00:07:58.980]   That's always on a physician.
[00:07:58.980 --> 00:08:03.540]   And we always say it's very important in this kind of environment to have the physician
[00:08:03.540 --> 00:08:07.900]   in the loop and to have sort of like the physician make the final decision.
[00:08:07.900 --> 00:08:15.140]   That we can augment them and make them much more efficient, but also better quality, right?
[00:08:15.140 --> 00:08:22.140]   Because in our offline analysis in the labs, our diagnosis algorithms, for example, are
[00:08:22.140 --> 00:08:24.740]   higher accuracy than the average physician.
[00:08:24.740 --> 00:08:27.500]   So we're pretty confident, and they keep getting better.
[00:08:27.500 --> 00:08:32.260]   We're pretty confident that those diagnosis algorithms are going to be better than most
[00:08:32.260 --> 00:08:33.260]   physicians.
[00:08:33.260 --> 00:08:35.900]   And even with that, we're not saying we're just going to make the diagnosis.
[00:08:35.900 --> 00:08:39.660]   We're just presenting it to the physician and saying, "Hey, this could be one of these
[00:08:39.660 --> 00:08:42.340]   three things or these 10 things.
[00:08:42.340 --> 00:08:45.380]   How do you want to go from here?"
[00:08:45.380 --> 00:08:51.700]   So I mean, I could totally see from kind of communicating with the patient standpoint,
[00:08:51.700 --> 00:08:55.380]   including me, that it would be kind of comforting to say, "Hey, the doctor always makes the
[00:08:55.380 --> 00:08:57.060]   final decision."
[00:08:57.060 --> 00:09:00.020]   But this is more of an interview about real world AI.
[00:09:00.020 --> 00:09:06.100]   And it does seem like with, like example, chess used to be before, I think, alpha chess
[00:09:06.100 --> 00:09:10.180]   or maybe the latest version of Stockfish, the best chess programs were like these hybrid
[00:09:10.180 --> 00:09:11.620]   systems with the human in the loop.
[00:09:11.620 --> 00:09:17.860]   But then at some point, the AI got good enough that the human in the loop only messes things
[00:09:17.860 --> 00:09:18.860]   up, right?
[00:09:18.860 --> 00:09:24.180]   I mean, do you ever have cases where you think that the ML system works better than a human
[00:09:24.180 --> 00:09:31.180]   operator and maybe it shouldn't actually give the final decision to a doctor?
[00:09:31.180 --> 00:09:32.940]   It's a great question.
[00:09:32.940 --> 00:09:38.300]   I think, as I said before, generally speaking, it's not that hard.
[00:09:38.300 --> 00:09:44.420]   Well, I mean, it's taking us a few years, but it's not "that hard" to get an algorithm
[00:09:44.420 --> 00:09:47.060]   that is better than the average physician.
[00:09:47.060 --> 00:09:52.820]   Now that being said, it's much harder to get an algorithm that is better than the combination
[00:09:52.820 --> 00:09:55.400]   of the human plus the AI.
[00:09:55.400 --> 00:10:03.860]   So even in the examples that you mentioned, the combination of humans plus AI in chess,
[00:10:03.860 --> 00:10:09.660]   if the human is relatively good, meaning a professional player, it's hard to beat, right?
[00:10:09.660 --> 00:10:15.920]   So an AI alone versus a combination of AI plus human is hard to beat.
[00:10:15.920 --> 00:10:20.580]   But in the case of healthcare, one of the important things to understand is that it
[00:10:20.580 --> 00:10:22.660]   is an imperfect information game, right?
[00:10:22.660 --> 00:10:29.860]   So it's not about if you had the perfect information, the algorithm would probably always beat the
[00:10:29.860 --> 00:10:30.860]   human, right?
[00:10:30.860 --> 00:10:36.380]   And you would be very easy to just beat the human with sort of like all the perfect information
[00:10:36.380 --> 00:10:37.380]   in the world.
[00:10:37.380 --> 00:10:44.060]   However, in the case of medicine, healthcare, there's a lot that goes on with empathizing
[00:10:44.060 --> 00:10:50.460]   with the patient, understanding even things that are called like social determinants,
[00:10:50.460 --> 00:10:51.460]   where do they come from?
[00:10:51.460 --> 00:10:53.140]   How are they going to understand?
[00:10:53.140 --> 00:10:59.700]   How can you communicate the possibility of something being likely or not?
[00:10:59.700 --> 00:11:07.100]   And that is very hard to do if you're not a human that is trained to have this level
[00:11:07.100 --> 00:11:09.220]   of empathy, so to speak, right?
[00:11:09.220 --> 00:11:17.000]   So there's the interesting question, and I keep talking to people that have very different
[00:11:17.000 --> 00:11:19.020]   opinions with that, right?
[00:11:19.020 --> 00:11:26.220]   There is the purely extreme rational opinion that all you want to have from the outcome
[00:11:26.220 --> 00:11:31.420]   as a patient is have a list of possible diagnosis with a probability, and you'll be able to
[00:11:31.420 --> 00:11:32.940]   interpret them.
[00:11:32.940 --> 00:11:36.740]   And if you're a hyper rational person, that is true.
[00:11:36.740 --> 00:11:42.100]   You want to know if you have a 0.2% probability of having cancer, you want to know that there
[00:11:42.100 --> 00:11:45.900]   is 0.2 probability, and you think you can deal with it.
[00:11:45.900 --> 00:11:52.180]   The reality is that most people don't know how to interpret that, right?
[00:11:52.180 --> 00:11:55.460]   What does that even mean, a 0.2 probability of having cancer?
[00:11:55.460 --> 00:12:00.260]   And do you want to communicate that, or do you want to interpret that and then follow
[00:12:00.260 --> 00:12:04.740]   the patient along and make sure that that probability doesn't get to a point that is
[00:12:04.740 --> 00:12:06.460]   more likely than not?
[00:12:06.460 --> 00:12:11.460]   And I think that's where the human judgment is really key, and that's very different from
[00:12:11.460 --> 00:12:18.420]   a pure probability that is output from any kind of machine learning algorithm.
[00:12:18.420 --> 00:12:20.620]   Interesting.
[00:12:20.620 --> 00:12:25.420]   I guess I would think that I would actually want to have the clear probabilities, but
[00:12:25.420 --> 00:12:28.460]   maybe everyone thinks that and they don't really want that.
[00:12:28.460 --> 00:12:31.180]   No, I think you're probably right.
[00:12:31.180 --> 00:12:36.460]   If you are in the tech bubble, so to speak, and you're rational and you play music and
[00:12:36.460 --> 00:12:43.700]   you're a mathematician or you like math, you think you can very rationally deal with those
[00:12:43.700 --> 00:12:49.140]   kinds of probabilities and work with them, but there's a lot of people that are not like
[00:12:49.140 --> 00:12:50.140]   that.
[00:12:50.140 --> 00:12:57.100]   And that's where the empathizing and understanding who you're talking to, it's really key.
[00:12:57.100 --> 00:13:03.420]   One of the important aspects, which is somewhat connected to what we're talking is, in particular,
[00:13:03.420 --> 00:13:12.220]   our service, we are not designing it for the tech savvy people of Silicon Valley or anywhere.
[00:13:12.220 --> 00:13:17.780]   We're really using technology to provide a very accessible and high quality service for
[00:13:17.780 --> 00:13:23.500]   people that usually don't even have access to high quality healthcare and they're underinsured,
[00:13:23.500 --> 00:13:25.700]   uninsured and so on.
[00:13:25.700 --> 00:13:36.060]   So we need to understand the social background of how are these people going to be interacting
[00:13:36.060 --> 00:13:43.660]   with the technology and how they are going to need the human aspect of the technology
[00:13:43.660 --> 00:13:49.340]   to help them even understand what's happening and how to react to it.
[00:13:49.340 --> 00:13:53.700]   So I think that's also very important because, I mean, we could get it, this is more of a
[00:13:53.700 --> 00:14:00.620]   philosophical, but we get usually blamed in tech companies, that we're design things only
[00:14:00.620 --> 00:14:03.180]   thinking of people like us.
[00:14:03.180 --> 00:14:05.980]   And then you realize, and particularly in healthcare, it's very interesting because
[00:14:05.980 --> 00:14:11.540]   as soon as you start talking to doctors and to anyone from medical profession, you understand
[00:14:11.540 --> 00:14:14.460]   it's like, gosh, yeah, the way of thinking is different.
[00:14:14.460 --> 00:14:21.660]   It's like even how they think, it's not purely mathematical and you need to have a level
[00:14:21.660 --> 00:14:26.980]   of sort of understanding of the different ways that people interpret and process information.
[00:14:26.980 --> 00:14:33.020]   Now that being said, I'm not saying that the traditional paternalistic view of medicine
[00:14:33.020 --> 00:14:34.020]   is good.
[00:14:34.020 --> 00:14:38.580]   The one which the doctor knew everything and wouldn't say anything to the patient, say,
[00:14:38.580 --> 00:14:39.860]   trust me, I know the truth.
[00:14:39.860 --> 00:14:43.940]   You have to do what I'm telling you, but I'm not even going to say what your diagnosis
[00:14:43.940 --> 00:14:44.940]   is.
[00:14:44.940 --> 00:14:45.940]   No, I am totally against that.
[00:14:45.940 --> 00:14:50.080]   And I think there needs to be a middle ground and the patients need to have access to their
[00:14:50.080 --> 00:14:57.060]   data and we need to be transparent with what's going on and give information as much as possible.
[00:14:57.060 --> 00:15:00.060]   And that's part of our model too, for sure.
[00:15:00.060 --> 00:15:08.620]   So going back to a comment you made earlier that your diagnosis is better than the average
[00:15:08.620 --> 00:15:13.980]   doctor, or I guess that your system is better than the average doctor.
[00:15:13.980 --> 00:15:16.180]   My first question on that is how would you even know that?
[00:15:16.180 --> 00:15:20.680]   Do you follow up and find out later what the real diagnosis was?
[00:15:20.680 --> 00:15:26.320]   And also, how would you train a system to be much better than the average doctor?
[00:15:26.320 --> 00:15:31.840]   Do you somehow have a way of finding more accurate doctors and then using that for training?
[00:15:31.840 --> 00:15:32.840]   How does that even work?
[00:15:32.840 --> 00:15:35.760]   Yeah, this is a great question.
[00:15:35.760 --> 00:15:40.200]   So when I said that, I specifically added in the lab, right?
[00:15:40.200 --> 00:15:42.560]   We are better than the average physician in the lab.
[00:15:42.560 --> 00:15:49.640]   And that's because the only real ground truth that we have to evaluate this are our so-called
[00:15:49.640 --> 00:15:55.880]   clinical vignettes, which are basically cases that are documented and they're agreed upon
[00:15:55.880 --> 00:15:58.360]   and they've been published.
[00:15:58.360 --> 00:16:01.000]   And there's not many of those, unfortunately.
[00:16:01.000 --> 00:16:03.240]   So that's something that is lacking.
[00:16:03.240 --> 00:16:09.000]   But when we are making diagnosis on those vignettes, we kind of agree that that's a
[00:16:09.000 --> 00:16:13.600]   ground truth that's been published, and that's the one that we use as the measuring bar.
[00:16:13.600 --> 00:16:17.880]   There's a public data set, which is pretty small, but we also have our own internal one
[00:16:17.880 --> 00:16:21.360]   that we keep using for development.
[00:16:21.360 --> 00:16:27.600]   And we even use synthetic data and all kinds of different data that we can get into.
[00:16:27.600 --> 00:16:34.000]   Now, unfortunately, the generation of ground truth in medicine is extremely hard.
[00:16:34.000 --> 00:16:41.040]   And there are a lot of studies out there that with doctors, for example, there's a famous,
[00:16:41.040 --> 00:16:48.400]   well famous in our field, but well-known publication by the Human DX Project, where they found
[00:16:48.400 --> 00:16:54.640]   out that the average accuracy of a single doctor on similar vignettes to the ones that
[00:16:54.640 --> 00:17:03.000]   I'm saying, so medical cases, it was roughly, I think, around 60%, between 50 and 60%.
[00:17:03.000 --> 00:17:10.920]   And in order to get past a reasonable 80% accuracy, you had to aggregate the opinion
[00:17:10.920 --> 00:17:12.440]   of six to eight doctors.
[00:17:12.440 --> 00:17:16.960]   So basically, the only way you have to really increase that accuracy and say, "Okay, I'm
[00:17:16.960 --> 00:17:22.400]   going to ask eight different doctors and then take the opinion of the ones that agree the
[00:17:22.400 --> 00:17:27.640]   most and use that as my ground truth," which is honestly what many of us do in the lab
[00:17:27.640 --> 00:17:32.560]   to generate those vignettes, is not trust one single doctor, but ask many, and then
[00:17:32.560 --> 00:17:38.200]   sort of have quality processes to understand who is right, and then take that as the ground
[00:17:38.200 --> 00:17:39.360]   truth.
[00:17:39.360 --> 00:17:46.160]   But in order to have a learning healthcare system and sort of have this system improve,
[00:17:46.160 --> 00:17:51.040]   the only thing you can do is sort of establish those mechanisms in which the system is actually
[00:17:51.040 --> 00:17:52.880]   learning and improving from itself.
[00:17:52.880 --> 00:17:58.120]   And you do have sort of humans in the loop having the follow-up and saying, "Okay, we
[00:17:58.120 --> 00:17:59.360]   diagnosed this first.
[00:17:59.360 --> 00:18:00.960]   Was this correct?"
[00:18:00.960 --> 00:18:07.960]   And very importantly, you also have the ability to have follow-ups and very constant follow-ups
[00:18:07.960 --> 00:18:12.320]   to understand if you got it right or if you missed something.
[00:18:12.320 --> 00:18:20.280]   One of the nice things about the system that we have, which is all virtual and chat-based
[00:18:20.280 --> 00:18:25.400]   and message-based, is that we can follow up and we can automate follow-ups with the patients
[00:18:25.400 --> 00:18:27.640]   at a very little cost or almost no cost.
[00:18:27.640 --> 00:18:34.520]   So we can literally sort of have the patient come back every hour and check on the patient
[00:18:34.520 --> 00:18:35.960]   as like, "Hey, did the fever go up?
[00:18:35.960 --> 00:18:36.960]   Did it go down?
[00:18:36.960 --> 00:18:38.280]   Did we get it right or not?"
[00:18:38.280 --> 00:18:42.120]   Which is usually not the case in a normal medical situation, right?
[00:18:42.120 --> 00:18:46.120]   You go see the doctor and then if you're lucky, you see them in two weeks.
[00:18:46.120 --> 00:18:52.240]   And that's the sampling time between sort of different data points is much coarser than
[00:18:52.240 --> 00:18:53.240]   what we have.
[00:18:53.240 --> 00:18:54.240]   Yeah, it's funny.
[00:18:55.240 --> 00:18:58.000]   I'm thinking about my own interactions with doctors.
[00:18:58.000 --> 00:19:07.120]   And I was thinking, when I kind of call a hospital or call my doctor to ask them what
[00:19:07.120 --> 00:19:11.080]   to do, I feel like I can almost guarantee that they're going to ask me to come in and
[00:19:11.080 --> 00:19:12.560]   get more tests.
[00:19:12.560 --> 00:19:14.080]   And my little sister is also a doctor.
[00:19:14.080 --> 00:19:17.960]   And I feel like when I call her, I can almost guarantee that she's going to tell me, "Lucas,
[00:19:17.960 --> 00:19:18.960]   you're fine.
[00:19:18.960 --> 00:19:19.960]   You're being ridiculous.
[00:19:19.960 --> 00:19:24.480]   Just drink some water and get some rest."
[00:19:24.480 --> 00:19:29.440]   And so they're clearly optimizing, my sister and a professional that I call, optimizing
[00:19:29.440 --> 00:19:32.400]   for kind of different things.
[00:19:32.400 --> 00:19:34.160]   I guess, how do you think about that?
[00:19:34.160 --> 00:19:36.720]   What do you kind of optimize for in your interactions?
[00:19:36.720 --> 00:19:41.420]   I would imagine that missing a serious condition would be so bad that you would really want
[00:19:41.420 --> 00:19:47.080]   to err on the side of caution with your suggestions to patients.
[00:19:47.080 --> 00:19:50.640]   And how would you, I mean, how do you know if you're doing a good job there?
[00:19:50.640 --> 00:19:57.000]   Yeah, I mean, definitely patient safety is uttermost concern and what that is very critical.
[00:19:57.000 --> 00:20:04.280]   And our care team is very much fixated on patient safety first.
[00:20:04.280 --> 00:20:09.600]   And we do things that even go against sort of like what would be good for the business
[00:20:09.600 --> 00:20:11.080]   because of patient safety.
[00:20:11.080 --> 00:20:14.600]   And that's understood and it's the right thing to do.
[00:20:14.600 --> 00:20:21.960]   However, one of the important pieces here around patient safety and around sort of like
[00:20:21.960 --> 00:20:30.380]   not erring on the side of being extremely conservative is one, the population that we
[00:20:30.380 --> 00:20:37.720]   are dealing with is population that doesn't generally have good access to healthcare.
[00:20:37.720 --> 00:20:44.680]   So if our response to their concerns was always, "Hey, go and get a blood test and you need
[00:20:44.680 --> 00:20:51.400]   to go through this super expensive procedure and good luck with it and come back to us."
[00:20:51.400 --> 00:20:56.680]   And that would be the kind of service we would be providing, these people would not come
[00:20:56.680 --> 00:20:57.680]   back, right?
[00:20:57.680 --> 00:21:02.620]   Because they literally cannot afford it and it's not something that it's optimized in
[00:21:02.620 --> 00:21:03.620]   any way.
[00:21:03.800 --> 00:21:10.560]   We need to provide the best possible care with optimizing also the cost side of the
[00:21:10.560 --> 00:21:15.480]   equation for them and for the overall system.
[00:21:15.480 --> 00:21:21.800]   And the reason we can do that is because we have this high level of access and accessibility.
[00:21:21.800 --> 00:21:29.400]   So we can play it safe because we can always tell them, "Hey, come back in two hours if
[00:21:29.400 --> 00:21:34.800]   your fever gets past this, or if you start coughing tonight, come back."
[00:21:34.800 --> 00:21:40.600]   That's something that most doctors, one of the reasons they are on the side of the safety,
[00:21:40.600 --> 00:21:45.520]   on the safety side, sometimes excessively is one is liability, but the other one is
[00:21:45.520 --> 00:21:51.600]   because they can't assume that they're going to be in touch with you for the next few weeks.
[00:21:51.600 --> 00:21:56.560]   So it's like, "Gosh, I need to just make sure that this doesn't happen in the next two weeks."
[00:21:56.560 --> 00:22:01.520]   If they had the ability to say, "Hey, you're going to be calling me in every two hours
[00:22:01.520 --> 00:22:08.680]   if there's something happening," they could take a little bit more of a less aggressive
[00:22:08.680 --> 00:22:11.940]   approach, but that's usually not possible.
[00:22:11.940 --> 00:22:16.780]   But in a system like this, where there's a lot of automation and a lot of accessibility
[00:22:16.780 --> 00:22:22.360]   through a virtual and through an application, through a phone, you can actually do that
[00:22:22.360 --> 00:22:24.240]   and it's much more efficient.
[00:22:24.240 --> 00:22:31.400]   And more importantly, it's more efficient also long-term for the health of the patient,
[00:22:31.400 --> 00:22:35.600]   because you're catching things right when they happen and you're not letting it get
[00:22:35.600 --> 00:22:37.760]   to a point that it's like, "Oh gosh, now it's too late.
[00:22:37.760 --> 00:22:39.960]   Now we need to do this surgery."
[00:22:39.960 --> 00:22:43.920]   So can you tell me a little bit about your tech stack behind the scenes?
[00:22:43.920 --> 00:22:48.440]   I mean, you're actually really deploying, it sounds like, multiple models into production
[00:22:48.440 --> 00:22:49.920]   and running them live.
[00:22:49.920 --> 00:22:53.640]   Are you continuously updating these models?
[00:22:53.640 --> 00:22:55.600]   How do you think about that?
[00:22:55.600 --> 00:23:00.160]   Are you retraining them constantly on the feedback that you're getting from the human
[00:23:00.160 --> 00:23:01.160]   operators?
[00:23:01.160 --> 00:23:02.160]   Yeah.
[00:23:02.160 --> 00:23:07.080]   There's a combination of different models and each one has its own cycle.
[00:23:07.080 --> 00:23:13.720]   We do have what we call the learning loop, which is sort of like the ability to inject
[00:23:13.720 --> 00:23:16.940]   data back into the models and retrain them.
[00:23:16.940 --> 00:23:22.960]   But there's a combination of different models that have different levels of, I would say,
[00:23:22.960 --> 00:23:28.920]   complexity in the way that they can be retrained and they can be redeployed.
[00:23:28.920 --> 00:23:33.620]   And in my experience, that is not any different than any other company.
[00:23:33.620 --> 00:23:35.440]   When I was at Netflix, we had the same.
[00:23:35.440 --> 00:23:39.720]   We had some that had a lot of data and were retrained daily, and there were others that
[00:23:39.720 --> 00:23:45.200]   it honestly, they needed longer windows of data and more data to be retrained.
[00:23:45.200 --> 00:23:48.000]   And you didn't need to retrain that that often.
[00:23:48.000 --> 00:23:49.960]   So we're in the same place.
[00:23:49.960 --> 00:23:57.320]   Particularly, for example, things like diagnosis model, we don't get that much good quality
[00:23:57.320 --> 00:23:59.960]   granular data on diagnosis daily.
[00:23:59.960 --> 00:24:01.560]   So it doesn't make sense.
[00:24:01.560 --> 00:24:04.300]   And we need to make sure that that data is high quality.
[00:24:04.300 --> 00:24:08.400]   And we combine it with synthetic data that we generate from a simulator.
[00:24:08.400 --> 00:24:14.840]   And there's a lot of data cooking, going behind the scenes for making sure that those diagnosis
[00:24:14.840 --> 00:24:16.660]   models are good.
[00:24:16.660 --> 00:24:19.640]   So that's a model that is not going to be updated that frequently.
[00:24:19.640 --> 00:24:25.080]   Now there are others that are around, say entity recognition or intent classification
[00:24:25.080 --> 00:24:30.480]   or things like that, that we do gather more constant data and those can be updated more
[00:24:30.480 --> 00:24:33.040]   often.
[00:24:33.040 --> 00:24:38.720]   And I will say, just to clarify for everyone who's listening, our modeling and even our
[00:24:38.720 --> 00:24:45.320]   research is the intersection of natural language on one side and then medical decision-making
[00:24:45.320 --> 00:24:46.320]   on the other.
[00:24:46.320 --> 00:24:47.760]   And they both intersect.
[00:24:47.760 --> 00:24:54.960]   So there's an intersection of both, but we kind of go all the way from using GPT-3 and
[00:24:54.960 --> 00:25:04.200]   language models to using synthetic data from expert systems to train diagnosis models.
[00:25:04.200 --> 00:25:10.120]   And there's a very cool kind of intersection of both things, where it's the purely knowledge
[00:25:10.120 --> 00:25:17.740]   based, knowledge intensive approach of traditional AI systems in medicine and all the way to
[00:25:17.740 --> 00:25:21.240]   language models and very much deep learning approaches.
[00:25:21.240 --> 00:25:26.800]   And we have different models that are in the intersection of those, some of which, as you
[00:25:26.800 --> 00:25:32.820]   can tell, the ones that are more on the data intensive language side, we do get more constant
[00:25:32.820 --> 00:25:34.440]   data and we can retrain.
[00:25:34.440 --> 00:25:41.600]   The ones that are more knowledge intensive, we have to sort of do intermediate processes,
[00:25:41.600 --> 00:25:42.600]   so to speak.
[00:25:42.600 --> 00:25:43.600]   That makes sense.
[00:25:43.600 --> 00:25:45.640]   Do you literally use GPT-3?
[00:25:45.640 --> 00:25:46.640]   We do.
[00:25:46.640 --> 00:25:50.480]   In fact, we just published a paper about it.
[00:25:50.480 --> 00:25:54.640]   We won the best paper award at one of the works of ACL.
[00:25:54.640 --> 00:26:02.280]   In that particular case, we were using GPT-3 for generating training data for language
[00:26:02.280 --> 00:26:03.760]   summarization.
[00:26:03.760 --> 00:26:07.360]   So that's an interesting approach.
[00:26:07.360 --> 00:26:12.160]   I think one that I know several people are following in different domains, but instead
[00:26:12.160 --> 00:26:20.860]   of using GPT-3 directly at inference time to use it as a way to enhance and generate
[00:26:20.860 --> 00:26:27.320]   high volumes of training data with different priming mechanisms, it's a very interesting
[00:26:27.320 --> 00:26:33.720]   approach and one that we showed in our publication that it's actually better than just having
[00:26:33.720 --> 00:26:36.640]   a lot of humans generating training data.
[00:26:36.640 --> 00:26:37.640]   So that's an interesting-
[00:26:37.640 --> 00:26:39.640]   Wait, can you tell me more about how this works?
[00:26:39.640 --> 00:26:45.080]   So how do you exactly generate the data and what's the summarization task?
[00:26:45.080 --> 00:26:48.760]   Yeah, it is a summarization task.
[00:26:48.760 --> 00:26:57.880]   And summarization of medical conversations is pretty hard because you need to generate
[00:26:57.880 --> 00:27:07.920]   the data, but also you need to have the original data, but then generate summaries.
[00:27:07.920 --> 00:27:13.320]   And you need to generate summaries and examples of summaries, which are mostly correct, but
[00:27:13.320 --> 00:27:19.920]   some that might be incorrect to also make decisions on where you're training the model.
[00:27:19.920 --> 00:27:26.120]   It has to learn what is a good medical summarization and what's a bad medical summarization.
[00:27:26.120 --> 00:27:35.080]   So in the case of this project, what we did is prime GPT-3 with a number of examples of
[00:27:35.080 --> 00:27:44.800]   both positive and negative summaries to conversations, and then have it generate thousands of different
[00:27:44.800 --> 00:27:50.560]   training examples that we use to train our own offline model.
[00:27:50.560 --> 00:27:58.400]   And interestingly, the availability of more data, but also more nuanced variabilities
[00:27:58.400 --> 00:28:05.200]   that GPT-3 was generating itself, made the final model that we were training was better
[00:28:05.200 --> 00:28:10.520]   than anything that we could have trained with our own data and our own human labelers.
[00:28:10.520 --> 00:28:15.440]   It's so interesting because you would think that the generation task would be so much
[00:28:15.440 --> 00:28:19.360]   harder than the decision task, if something's a good summary or not.
[00:28:19.360 --> 00:28:24.400]   It's kind of amazing to me that that works so well.
[00:28:24.400 --> 00:28:37.040]   Yeah, I mean, to be clear, we could have tried to use GPT-3 directly for the task at hand
[00:28:37.040 --> 00:28:44.480]   if we had had access to sort of unlimited resources and fine tuning.
[00:28:44.480 --> 00:28:50.600]   And also, which by the way, I know that OpenAI is going to open the API for fine tuning soon,
[00:28:50.600 --> 00:28:52.840]   but we didn't have it that time.
[00:28:52.840 --> 00:28:59.120]   And also very importantly, there's a tricky aspect here with the privacy aspect of the
[00:28:59.120 --> 00:29:01.360]   data that we're dealing with.
[00:29:01.360 --> 00:29:07.600]   We don't want to be in a situation where we are sending GPT-3 data that is private from
[00:29:07.600 --> 00:29:13.720]   our patient, unless there are some guarantees of very strict compliance and privacy.
[00:29:13.720 --> 00:29:20.300]   So if all those things were met, you could use GPT-3 directly and you would probably
[00:29:20.300 --> 00:29:26.880]   get a summarization that is as good as the one that we were generating.
[00:29:26.880 --> 00:29:37.120]   However, because that did not exist, it's a very interesting intermediate step to prime
[00:29:37.120 --> 00:29:42.440]   GPT-3 with some knowledge and some examples, and then let it generate all these other training
[00:29:42.440 --> 00:29:46.640]   examples that you can then use to train your own.
[00:29:46.640 --> 00:29:49.860]   I mean, you're not going to train a GPT-3, but you don't need to, right?
[00:29:49.860 --> 00:29:54.740]   Because the complexity of the model and the number of parameters that GPT-3 has is because
[00:29:54.740 --> 00:29:59.340]   it's a language model and it's a universal language model, right?
[00:29:59.340 --> 00:30:04.660]   But the model that you're training, which is very much focused on summarization and
[00:30:04.660 --> 00:30:08.820]   summarization in a particular domain, you can train a much smaller model, much more
[00:30:08.820 --> 00:30:15.300]   efficient with the right data, and you're going to get the same...
[00:30:15.300 --> 00:30:16.300]   I mean, I'd be interested.
[00:30:16.300 --> 00:30:20.420]   I don't know if it's exactly the same accuracy or it's even better, because again, there's
[00:30:20.420 --> 00:30:27.120]   the question of how much a universal language model can be as good as a smaller model on
[00:30:27.120 --> 00:30:28.800]   a very specific task, right?
[00:30:28.800 --> 00:30:32.160]   Which is what we train.
[00:30:32.160 --> 00:30:33.160]   That makes total sense.
[00:30:33.160 --> 00:30:34.160]   That's really cool.
[00:30:34.160 --> 00:30:42.640]   I guess I wonder, do you worry about the data training on the conversations that you have?
[00:30:42.640 --> 00:30:46.840]   I imagine those are incredibly sensitive conversations with patients.
[00:30:46.840 --> 00:30:51.320]   If you use that data to train models, is it possible that some of the information could
[00:30:51.320 --> 00:30:53.120]   kind of bleed through into the models?
[00:30:53.120 --> 00:30:59.040]   Do you take precautions somehow to try to remove personally identifying information
[00:30:59.040 --> 00:31:01.680]   before you train a model on the data?
[00:31:01.680 --> 00:31:04.200]   Yes, we do.
[00:31:04.200 --> 00:31:10.800]   Our models, sorry, all our data sets that are used for our models go through a de-identification
[00:31:10.800 --> 00:31:21.640]   process, and we do make sure that the identifiers that our models, our original data set have,
[00:31:21.640 --> 00:31:24.280]   are actually extracted.
[00:31:24.280 --> 00:31:30.040]   That being said, you can never guarantee 100% perfection, right?
[00:31:30.040 --> 00:31:34.640]   On those de-identification of text is in itself a research task.
[00:31:34.640 --> 00:31:39.200]   So there's different approaches to it, and there's different things that can be done.
[00:31:39.200 --> 00:31:46.360]   But even with that, you'll get as far as a particular percentage of accuracy.
[00:31:46.360 --> 00:31:52.320]   We're pretty confident that most of our data sets that we use to train the models are pretty
[00:31:52.320 --> 00:31:58.140]   well de-identified, which then in turn means that the likelihood that then something even
[00:31:58.140 --> 00:32:01.620]   bleeds into the model is very, very small, right?
[00:32:01.620 --> 00:32:05.960]   Because it would need to be the combination of something makes it through the de-identification
[00:32:05.960 --> 00:32:10.040]   step plus something gets picked up in the model that then can be retrieved.
[00:32:10.040 --> 00:32:14.600]   But that otherwise, sure, it would be a concern, right?
[00:32:14.600 --> 00:32:18.640]   Do you have systems to evaluate the quality of the models before you deploy a new one
[00:32:18.640 --> 00:32:19.640]   to do production?
[00:32:19.640 --> 00:32:24.380]   Or do you do live production monitoring on the quality of models as they run?
[00:32:24.380 --> 00:32:34.100]   We do have systems and we do have a process in place.
[00:32:34.100 --> 00:32:41.680]   We have different data sets, different metrics, and different processes to make sure that
[00:32:41.680 --> 00:32:45.280]   we detect any anomaly.
[00:32:45.280 --> 00:32:52.500]   And it's interesting because in fact, I was talking today to Francois, who is leading
[00:32:52.500 --> 00:32:59.200]   my AI engineering team, and they're building a tool now that we're going to be using that
[00:32:59.200 --> 00:33:05.160]   basically automatically enables you to analyze the anomalies that we detect when we change
[00:33:05.160 --> 00:33:06.480]   a model.
[00:33:06.480 --> 00:33:12.640]   But by seeing actual examples of what is the actual case, I was talking about the vignettes
[00:33:12.640 --> 00:33:16.800]   that we have, for example, for diagnosis, right?
[00:33:16.800 --> 00:33:21.520]   So if you train a new model and all of a sudden you see a difference, like, "Hey, this metric
[00:33:21.520 --> 00:33:25.360]   is lower than in the previous version of the model," that's okay.
[00:33:25.360 --> 00:33:32.880]   But in this case, you really want to understand, is it being unfair to a particular demographic?
[00:33:32.880 --> 00:33:39.000]   Is it worse for older people or for women, or can I actually go and see where it made
[00:33:39.000 --> 00:33:40.340]   the error?
[00:33:40.340 --> 00:33:47.640]   And then interestingly, now you need the collaboration of a physician or a doctor to sit with you
[00:33:47.640 --> 00:33:52.760]   and say, "Hey, this new version of the model decided that this thing instead of the flu
[00:33:52.760 --> 00:33:54.140]   was a cold.
[00:33:54.140 --> 00:33:56.300]   Is this correct or what's going on?"
[00:33:56.300 --> 00:33:57.840]   And then you need to debug.
[00:33:57.840 --> 00:34:02.760]   And in most cases, and I know this is something that in other companies, people have this
[00:34:02.760 --> 00:34:11.120]   kind of debugging tools, but they are usually debugging tools that a layman or a lay person
[00:34:11.120 --> 00:34:12.480]   can understand, right?
[00:34:12.480 --> 00:34:16.920]   Like when I was on Netflix, we did have a similar tool that you would see the shows
[00:34:16.920 --> 00:34:19.160]   and like, "Whoa, this ranking doesn't make sense."
[00:34:19.160 --> 00:34:24.460]   But if you're dealing with a highly knowledge-intensive domain like medicine, you actually need that
[00:34:24.460 --> 00:34:26.800]   collaboration with the doctors.
[00:34:26.800 --> 00:34:31.680]   And we do have doctors in the development team, and we do have experts that are kind
[00:34:31.680 --> 00:34:37.560]   of like sitting hand in hand with the engineers and the researchers to do those kind of iterations
[00:34:37.560 --> 00:34:41.520]   and debugging and QA of the medical models.
[00:34:41.520 --> 00:34:42.520]   That's cool.
[00:34:42.520 --> 00:34:43.520]   So what does the interface look like?
[00:34:43.520 --> 00:34:49.120]   Does it show somehow the explanation behind why the model made the choice that it made?
[00:34:49.120 --> 00:34:50.120]   Yes.
[00:34:50.120 --> 00:34:51.120]   Yes.
[00:34:51.120 --> 00:34:58.320]   It shows the overall difference between the previous model and the current model.
[00:34:58.320 --> 00:35:03.680]   And then you can click and see sort of like, okay, what are the ones that it got right
[00:35:03.680 --> 00:35:07.680]   and the ones that it got wrong compared the two models.
[00:35:07.680 --> 00:35:10.720]   And you can kind of like see the dip with a color code.
[00:35:10.720 --> 00:35:17.240]   So you can actually dig and say, okay, well, yeah, this one that got wrong, it's very wrong.
[00:35:17.240 --> 00:35:20.400]   So we should not move forward.
[00:35:20.400 --> 00:35:21.680]   Do you try to build models?
[00:35:21.680 --> 00:35:26.640]   I mean, GPT would be kind of the furthest from explainability you could go to, but do
[00:35:26.640 --> 00:35:32.000]   some of your models, do you try to build them in ways that they maintain explainability?
[00:35:32.000 --> 00:35:34.480]   Yeah.
[00:35:34.480 --> 00:35:35.480]   It's a great question.
[00:35:35.480 --> 00:35:42.440]   I think explainability is, it's important, but it's also kind of tricky in the case of
[00:35:42.440 --> 00:35:52.280]   medicine in the sense that not even doctors many times have an explanation for their decisions.
[00:35:52.280 --> 00:35:56.960]   In fact, something that it's kind of a little bit nuanced, but I think it might be interesting
[00:35:56.960 --> 00:36:06.880]   is many times doctors will go all the way to prescribing without having a clear diagnosis.
[00:36:06.880 --> 00:36:09.120]   That's called symptomatic treatment, right?
[00:36:09.120 --> 00:36:15.280]   So it's like, oh gosh, I don't know if this is flu or a cold, but no matter what, I'm
[00:36:15.280 --> 00:36:20.200]   going to prescribe you this particular thing because it's going to be good for both things
[00:36:20.200 --> 00:36:22.920]   and they don't really have a clear diagnosis.
[00:36:22.920 --> 00:36:24.360]   And that's not bad.
[00:36:24.360 --> 00:36:26.600]   I mean, it's like, it's okay.
[00:36:26.600 --> 00:36:29.040]   It's better to do that than to do nothing.
[00:36:29.040 --> 00:36:33.640]   In fact, the good thing is to be doing some symptomatic treatment and then following up
[00:36:33.640 --> 00:36:35.800]   and understanding like, what's the evolution?
[00:36:35.800 --> 00:36:37.620]   Did I get it right or not?
[00:36:37.620 --> 00:36:40.800]   So as long as you have a possibility to follow up.
[00:36:40.800 --> 00:36:47.680]   So explanation is not always possible and it's not always available in an imperfect
[00:36:47.680 --> 00:36:49.960]   information situation, right?
[00:36:49.960 --> 00:36:54.400]   Now that being said, if you do have it, it's good to provide it.
[00:36:54.400 --> 00:37:02.000]   And it's something that we have definitely worked on, on sort of like providing explanations.
[00:37:02.000 --> 00:37:06.360]   I'm actually a fan of providing explainability.
[00:37:06.360 --> 00:37:12.920]   So adding explainability as a post hoc process to the model, I think it's something that
[00:37:12.920 --> 00:37:20.760]   has a lot of value and does not necessarily require the model itself to be explainable,
[00:37:20.760 --> 00:37:27.520]   but you need to go after the fact and understand like, okay, this is why the model picked this.
[00:37:27.520 --> 00:37:35.560]   And is there an explanation that can explain in a simple way, why did the model pick this
[00:37:35.560 --> 00:37:38.520]   particular option or this particular class?
[00:37:38.520 --> 00:37:39.520]   So how do you do that?
[00:37:39.520 --> 00:37:44.900]   So if you have a really complicated model, like too complicated to inspect, what kinds
[00:37:44.900 --> 00:37:50.520]   of methods do you like to use to sort of get at some explainability of why the model did
[00:37:50.520 --> 00:37:51.520]   what it did?
[00:37:51.520 --> 00:37:52.520]   Yeah.
[00:37:52.520 --> 00:37:59.080]   I mean, there's different approaches to adding explainability, right?
[00:37:59.080 --> 00:38:04.960]   I mean, the simplest one is you approximate the decision boundaries of your model, no
[00:38:04.960 --> 00:38:09.880]   matter how complex, no matter whether it's a deep model or not, by a simple or linear
[00:38:09.880 --> 00:38:14.440]   model and then use that to build the explanations, right?
[00:38:14.440 --> 00:38:19.760]   That is a typical approach that many of the explainability solutions take.
[00:38:19.760 --> 00:38:25.880]   And that's one that can actually work pretty well.
[00:38:25.880 --> 00:38:29.680]   And it's one that we have experimented with and even implemented.
[00:38:29.680 --> 00:38:35.560]   I will say that's not really implemented in the product yet, but it's been implemented
[00:38:35.560 --> 00:38:36.960]   sort of like as a prototype.
[00:38:36.960 --> 00:38:40.240]   And I think we even wrote about it in one of our blog posts.
[00:38:40.240 --> 00:38:47.140]   So that's, I think, one of the easiest, but also the same time, more effective ways to
[00:38:47.140 --> 00:38:54.880]   explain things that have sort of like a complex nonlinear decision boundary and cannot be
[00:38:54.880 --> 00:38:57.860]   explained in sort of easy terms.
[00:38:57.860 --> 00:39:04.060]   I will again say that in many cases in medicine, those decisions do exist, right?
[00:39:04.060 --> 00:39:12.740]   And even though as much as we try to infer causality from the decisions, those are hard
[00:39:12.740 --> 00:39:19.660]   to come by because there's a lot of nuances in the ways that the information is being
[00:39:19.660 --> 00:39:24.900]   processed and the decision boundaries of the models are being constructed.
[00:39:24.900 --> 00:39:26.900]   Interesting.
[00:39:26.900 --> 00:39:32.660]   Well, we're getting close to running out of time and we always have two questions that
[00:39:32.660 --> 00:39:35.940]   we end on and I want to make sure that I cover them with you.
[00:39:35.940 --> 00:39:40.900]   The second to last question that we always ask is, what's an underrated aspect of machine
[00:39:40.900 --> 00:39:41.900]   learning?
[00:39:41.900 --> 00:39:47.300]   I know that you've played across your career at Netflix and Quora and Curie.
[00:39:47.300 --> 00:39:53.100]   What's been a topic in ML that's been maybe particularly useful to you or important that
[00:39:53.100 --> 00:39:56.660]   you feel like research doesn't cover as much as it should?
[00:39:56.660 --> 00:40:03.620]   I think an important topic that is not covered in research enough, despite the fact that
[00:40:03.620 --> 00:40:08.140]   I've tried to put in myself because I was a researcher back in the days before going
[00:40:08.140 --> 00:40:18.140]   into industry, is what actually happens to models in the wild, right?
[00:40:18.140 --> 00:40:26.260]   It's a different thing that you build a model with perfect data that has been cooked in
[00:40:26.260 --> 00:40:33.380]   the lab and you know what it is and you can have sort of like control over the boundaries
[00:40:33.380 --> 00:40:44.140]   and even understand the distribution of the noise and all the different variables than
[00:40:44.140 --> 00:40:51.580]   to deploy a product in the wild and to then be faced with sort of like all kinds of different
[00:40:51.580 --> 00:40:55.180]   drifts in the data distribution, noise and whatnot.
[00:40:55.180 --> 00:41:04.340]   I think that is something that is not usually researched enough, understandably so, because
[00:41:04.340 --> 00:41:11.860]   you can't ... I mean, most research is done with datasets that are available and are distributed
[00:41:11.860 --> 00:41:16.940]   in a way that they're kind of artificial, right?
[00:41:16.940 --> 00:41:22.780]   I went into Netflix through the Netflix prize, so I know that that was a dataset that was
[00:41:22.780 --> 00:41:28.500]   very, very good and very exciting to make progress in recommendations and the recommender
[00:41:28.500 --> 00:41:29.820]   systems arena.
[00:41:29.820 --> 00:41:36.300]   However, it was very different from then the data that I found out we had at Netflix when
[00:41:36.300 --> 00:41:40.540]   I was there and there were kind of all these other things happening, right?
[00:41:40.540 --> 00:41:41.540]   Right.
[00:41:41.540 --> 00:41:42.540]   Yeah.
[00:41:42.540 --> 00:41:47.260]   I guess it's also kind of a hard problem to formalize, right?
[00:41:47.260 --> 00:41:49.460]   Like there's so many variations on it.
[00:41:49.460 --> 00:41:52.260]   I mean, I know a lot of people talk about it, but it's hard to ... I guess it's sort
[00:41:52.260 --> 00:41:55.220]   of like about robustness or something, right?
[00:41:55.220 --> 00:41:56.220]   Yeah.
[00:41:56.220 --> 00:41:59.540]   I think, yeah, robustness is one.
[00:41:59.540 --> 00:42:06.380]   I think another one that is very interesting, we have done some work on it, is out of band
[00:42:06.380 --> 00:42:08.900]   classification or prediction, right?
[00:42:08.900 --> 00:42:18.560]   It's like building models that can react to classes that have not been seen during training.
[00:42:18.560 --> 00:42:23.580]   For example, that's a very important aspect and one that gets a little bit of attention
[00:42:23.580 --> 00:42:25.660]   in research, but not so much.
[00:42:25.660 --> 00:42:30.500]   I will say, for example, that particular problem is one that it's relatively easy to replicate
[00:42:30.500 --> 00:42:31.500]   in a lab, right?
[00:42:31.500 --> 00:42:37.380]   So basically you can build models that say, "Hey, I have a hundred classes, but I'm
[00:42:37.380 --> 00:42:41.860]   only going to let the model see 50 during training, see what it does with the other
[00:42:41.860 --> 00:42:42.860]   50, right?"
[00:42:42.860 --> 00:42:45.580]   And the model needs to understand, "Hey, I haven't seen this class.
[00:42:45.580 --> 00:42:47.260]   Sorry, I don't know what to say."
[00:42:47.260 --> 00:42:53.580]   So that's an example of something that out of band classification is one that kind of
[00:42:53.580 --> 00:42:56.660]   mimics some of the problems you see in real life, right?
[00:42:56.660 --> 00:43:01.940]   Because in real life, you will deploy a model and it will see something that is very different
[00:43:01.940 --> 00:43:07.780]   from the things they've been trained on and having the model be able to raise their hand
[00:43:07.780 --> 00:43:11.380]   and say, "Hey, I don't know what this is because I've never seen it before."
[00:43:11.380 --> 00:43:16.540]   That's a very interesting, for example, it's a specific concrete case, but it's one that
[00:43:16.540 --> 00:43:25.140]   relates very much to sort of like having this models in real life and being able to replicate
[00:43:25.140 --> 00:43:29.780]   this kind of situations in a lab.
[00:43:29.780 --> 00:43:36.620]   For example, another one that we've worked on is on introducing artificial noise on the
[00:43:36.620 --> 00:43:40.700]   training and testing data by using some domain knowledge, right?
[00:43:40.700 --> 00:43:50.780]   For example, in medicine, you know the prevalence of some symptoms and you can say, "I know
[00:43:50.780 --> 00:43:55.900]   that if I ask people if they have a headache, many people are going to say yes because most
[00:43:55.900 --> 00:43:57.780]   people have a headache, right?"
[00:43:57.780 --> 00:44:02.060]   It might not even be related to the current situation and the current condition, but people
[00:44:02.060 --> 00:44:03.060]   are going to say yes.
[00:44:03.060 --> 00:44:08.900]   Well, you can play around with those knobs and introduce artificial noise in your training
[00:44:08.900 --> 00:44:15.660]   data sets to then anticipate some of the noise you're going to be finding in the wild in
[00:44:15.660 --> 00:44:16.660]   real life.
[00:44:16.660 --> 00:44:17.660]   So that's another example.
[00:44:17.660 --> 00:44:22.420]   So yeah, it is hard to recreate the exact situation you're going to find out there,
[00:44:22.420 --> 00:44:27.580]   but I think there are some interesting ways to mimic at least some of those situations
[00:44:27.580 --> 00:44:32.580]   that probably deserve more attention than they usually get.
[00:44:32.580 --> 00:44:38.260]   It's fine if not, but do you have a favorite paper on the topic that you'd like us to point
[00:44:38.260 --> 00:44:41.900]   to or any research you could send people to, want to learn more?
[00:44:41.900 --> 00:44:46.500]   Well, I have our papers that I could point you to.
[00:44:46.500 --> 00:44:53.140]   I mean, the two things that I mentioned, we did dermatology image classification with
[00:44:53.140 --> 00:44:55.020]   out-of-band distribution.
[00:44:55.020 --> 00:44:58.340]   That's one that refers to the first thing that I was talking about.
[00:44:58.340 --> 00:45:03.700]   And the artificial noise that we introduced in synthetic data, that's a paper we wrote
[00:45:03.700 --> 00:45:06.700]   on diagnosis and diagnosis training.
[00:45:06.700 --> 00:45:10.860]   And of course those papers cite a lot of other papers that could be interesting.
[00:45:10.860 --> 00:45:13.060]   So I could definitely point you to those.
[00:45:13.060 --> 00:45:14.060]   Cool.
[00:45:14.060 --> 00:45:15.060]   That's perfect.
[00:45:15.060 --> 00:45:16.060]   Having a good starting place.
[00:45:16.060 --> 00:45:17.060]   Yeah.
[00:45:17.060 --> 00:45:23.100]   By the way, if people are interested, there's in our tech blog at Curai, we have sort of
[00:45:23.100 --> 00:45:25.140]   like a full list of our publications.
[00:45:25.140 --> 00:45:30.500]   We probably have now the order of 15 or 20 publications.
[00:45:30.500 --> 00:45:34.500]   And I like to be very open about the research we do.
[00:45:34.500 --> 00:45:38.660]   And I think that comes from my old times as a researcher.
[00:45:38.660 --> 00:45:44.420]   I'm very much in favor of sort of like open publications and sharing knowledge.
[00:45:44.420 --> 00:45:48.060]   And you will find most of that in our blog post.
[00:45:48.060 --> 00:45:49.060]   Cool.
[00:45:49.060 --> 00:45:50.060]   Awesome.
[00:45:50.060 --> 00:45:53.420]   We'll definitely put a link to that.
[00:45:53.420 --> 00:45:58.180]   And my final question is kind of broad, but I'm really curious in your case.
[00:45:58.180 --> 00:46:03.340]   And the question is basically, what's been the hardest part or maybe the most unexpectedly
[00:46:03.340 --> 00:46:07.540]   challenging part of getting ML models deployed in production?
[00:46:07.540 --> 00:46:11.540]   Just kind of going from my conception of like, this is the thing we want to do to like, here's
[00:46:11.540 --> 00:46:13.460]   a working model.
[00:46:13.460 --> 00:46:15.060]   Where are the big bottlenecks?
[00:46:15.060 --> 00:46:18.540]   Well, yeah, that is a broad question.
[00:46:18.540 --> 00:46:24.860]   And I think there's a number of things that come to mind.
[00:46:24.860 --> 00:46:32.500]   I think at the highest level, one of the very difficult things to get right is how do you
[00:46:32.500 --> 00:46:39.620]   connect the offline metrics that you have in anything you're doing in any model in the
[00:46:39.620 --> 00:46:46.820]   lab to what's going to be the real impact that that model has on your product.
[00:46:46.820 --> 00:46:54.140]   And we would love for that to be a clean thing and say, "Hey, if I get my precision and recall
[00:46:54.140 --> 00:47:00.420]   up and my F1 measure increases, I know that that's going to work in production and that's
[00:47:00.420 --> 00:47:05.580]   going to generate this much lift and whatever people are either going to click more or be
[00:47:05.580 --> 00:47:09.220]   happier or love the product more."
[00:47:09.220 --> 00:47:17.140]   That's usually that road from what you see in your model in the lab to the model in production
[00:47:17.140 --> 00:47:19.180]   is not that straight.
[00:47:19.180 --> 00:47:23.740]   And there's a lot of issues that get in the way and a lot of questions and a lot of things
[00:47:23.740 --> 00:47:29.220]   that are really, really important to get right.
[00:47:29.220 --> 00:47:34.580]   And some of them relate to mundane things like the UX, right?
[00:47:34.580 --> 00:47:36.460]   How is the user experience?
[00:47:36.460 --> 00:47:42.940]   How are you presenting things to, in our case, the patient or the doctor and how are they
[00:47:42.940 --> 00:47:43.940]   reacting?
[00:47:43.940 --> 00:47:49.220]   Your model might be awesome, but if they're not seeing it or it's confusing or you're
[00:47:49.220 --> 00:47:53.980]   not explaining it right, that's not going to help in any way or it might be worse, right?
[00:47:53.980 --> 00:47:55.540]   It might be confusing.
[00:47:55.540 --> 00:48:08.700]   So I think the connection between research and modeling and the actual user experience
[00:48:08.700 --> 00:48:14.780]   and the interface and how that's actually introduced into the product is an aspect that
[00:48:14.780 --> 00:48:17.180]   I find fascinating myself.
[00:48:17.180 --> 00:48:24.020]   And it's very hard to have people that actually understand the end to end, right?
[00:48:24.020 --> 00:48:29.340]   Because you need to have a very broad experience that goes all the way from the modeling to
[00:48:29.340 --> 00:48:34.780]   the metrics, to the product, to the user understanding, the user research.
[00:48:34.780 --> 00:48:37.060]   That's really hard to cover end to end.
[00:48:37.060 --> 00:48:42.740]   And then you need to build all that through collaboration and through teams that have
[00:48:42.740 --> 00:48:44.860]   the ability to collaborate.
[00:48:44.860 --> 00:48:51.300]   And in medicine where that's even harder because you throw in the domain knowledge, that even
[00:48:51.300 --> 00:48:53.220]   becomes more tricky, right?
[00:48:53.220 --> 00:48:57.620]   It's like something that you might see in the lab and it's like, "Whoa, this is fantastic.
[00:48:57.620 --> 00:48:58.780]   This is getting the metric.
[00:48:58.780 --> 00:49:02.700]   This is actually going to be a killer feature."
[00:49:02.700 --> 00:49:05.380]   It might turn out that it's a killer feature in the wrong way.
[00:49:05.380 --> 00:49:12.380]   Sorry, I shouldn't have used that metaphor probably in this context, but it's important
[00:49:12.380 --> 00:49:20.060]   to understand that the results that you get in your experiments are mediated by many things
[00:49:20.060 --> 00:49:24.940]   before they can be evaluated in an A/B test, for example.
[00:49:24.940 --> 00:49:25.940]   Awesome.
[00:49:25.940 --> 00:49:26.940]   Well, thanks so much for your time.
[00:49:26.940 --> 00:49:27.940]   I really appreciate this conversation.
[00:49:27.940 --> 00:49:28.940]   It was super interesting.
[00:49:28.940 --> 00:49:29.940]   Thank you.
[00:49:29.940 --> 00:49:30.940]   Yeah.
[00:49:30.940 --> 00:49:31.940]   Thank you.
[00:49:31.940 --> 00:49:37.940]   If you're enjoying Gradient Descent, I'd really love for you to check out Fully Connected,
[00:49:37.940 --> 00:49:43.240]   which is an inclusive machine learning community that we're building to let everyone know about
[00:49:43.240 --> 00:49:47.400]   all the stuff going on in ML and all the new research coming out.
[00:49:47.400 --> 00:49:52.780]   If you go to wmv.ai/fc, you can see all the different stuff that we do, including Gradient
[00:49:52.780 --> 00:49:58.460]   Descent, but also salons where we talk about new research and folks share insights, AMAs
[00:49:58.460 --> 00:50:02.780]   where you can directly connect with members of our community, and a Slack channel where
[00:50:02.780 --> 00:50:08.500]   you can get answers to everything from very basic questions about ML to bug reports on
[00:50:08.500 --> 00:50:12.340]   weights and biases to how to hire an ML team.
[00:50:12.340 --> 00:50:13.460]   We're looking forward to meeting you.

