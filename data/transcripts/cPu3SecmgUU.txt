
[00:00:00.000 --> 00:00:02.460]   I'm curious how you explain what's happened.
[00:00:02.460 --> 00:00:04.140]   Like why in a year, year and a half,
[00:00:04.140 --> 00:00:07.440]   have you guys been, you know,
[00:00:07.440 --> 00:00:09.560]   made important contributions to your field?
[00:00:09.560 --> 00:00:11.480]   - It goes without saying luck, obviously.
[00:00:11.480 --> 00:00:14.400]   And I feel like I've been very lucky in like the timing
[00:00:14.400 --> 00:00:18.620]   of different progressions has been just like really good
[00:00:18.620 --> 00:00:21.120]   in terms of advancing to the next level of growth.
[00:00:21.120 --> 00:00:25.080]   I feel like for the interpretability team specifically,
[00:00:25.080 --> 00:00:27.080]   I joined when we were five people.
[00:00:27.080 --> 00:00:28.500]   We've now grown quite a lot.
[00:00:29.880 --> 00:00:31.640]   But there were so many ideas floating around
[00:00:31.640 --> 00:00:34.980]   and we just needed to like really execute on them
[00:00:34.980 --> 00:00:36.640]   and have like quick feedback loops
[00:00:36.640 --> 00:00:38.680]   and like do careful experimentation
[00:00:38.680 --> 00:00:41.820]   that led to like signs of life
[00:00:41.820 --> 00:00:44.040]   and have now allowed us to like really scale.
[00:00:44.040 --> 00:00:46.240]   And I feel like that's kind of been my biggest value
[00:00:46.240 --> 00:00:49.440]   add to the team, which it's not all engineering,
[00:00:49.440 --> 00:00:52.040]   but quite a lot of it has been.
[00:00:52.040 --> 00:00:52.860]   - Interesting.
[00:00:52.860 --> 00:00:54.540]   So you're saying like you came at a point where
[00:00:54.540 --> 00:00:56.740]   like there had been a lot of science done
[00:00:56.740 --> 00:00:58.560]   and there was a lot of like good research floating around,
[00:00:58.560 --> 00:01:00.000]   but they needed someone to like just take that
[00:01:00.000 --> 00:01:02.100]   and like maniacally execute on it.
[00:01:02.100 --> 00:01:02.940]   - Yeah, yeah.
[00:01:02.940 --> 00:01:05.100]   And this is why it's not all engineering
[00:01:05.100 --> 00:01:07.240]   'cause it's like running different experiments
[00:01:07.240 --> 00:01:09.520]   and like having a hunch for why it might not be working
[00:01:09.520 --> 00:01:10.740]   and then like opening up the model
[00:01:10.740 --> 00:01:12.400]   or opening up the weights and like, what is it learning?
[00:01:12.400 --> 00:01:13.920]   Okay, well, let me try and do this instead
[00:01:13.920 --> 00:01:14.740]   and that sort of thing.
[00:01:14.740 --> 00:01:18.120]   But a lot of it has just been being able to do
[00:01:18.120 --> 00:01:22.120]   like very careful, thorough, but quick investigation
[00:01:22.120 --> 00:01:23.320]   of different ideas.
[00:01:23.320 --> 00:01:25.200]   I just don't get blocked very often.
[00:01:25.200 --> 00:01:26.760]   Like if I'm trying to write some code
[00:01:26.760 --> 00:01:28.000]   and like something isn't working,
[00:01:28.000 --> 00:01:30.000]   even if it's like in another part of the code base,
[00:01:30.000 --> 00:01:32.400]   I'll often just go in and fix that thing
[00:01:32.400 --> 00:01:34.460]   or at least hack it together to be able to get results.
[00:01:34.460 --> 00:01:37.280]   And I've seen other people where they're just like, help.
[00:01:37.280 --> 00:01:38.120]   I can't.
[00:01:38.120 --> 00:01:39.880]   And it's like, no, that's not a good enough excuse.
[00:01:39.880 --> 00:01:41.000]   Like go all the way down.
[00:01:41.000 --> 00:01:41.880]   - I've definitely heard like people
[00:01:41.880 --> 00:01:43.600]   in management type positions
[00:01:43.600 --> 00:01:45.800]   talk about the lack of such people
[00:01:45.800 --> 00:01:47.840]   where they will check in on somebody
[00:01:47.840 --> 00:01:49.240]   a month after they gave them a task
[00:01:49.240 --> 00:01:50.080]   or a week after they gave them a task
[00:01:50.080 --> 00:01:51.560]   and like, how's it going?
[00:01:51.560 --> 00:01:54.200]   And they say, well, you know, we need to do this thing
[00:01:54.200 --> 00:01:56.000]   which requires lawyers
[00:01:56.080 --> 00:01:58.480]   'cause it requires talking about this regulation.
[00:01:58.480 --> 00:01:59.600]   It's like, how's that going?
[00:01:59.600 --> 00:02:01.360]   I was like, well, we need lawyers.
[00:02:01.360 --> 00:02:03.360]   I'm like, why didn't you get lawyers?
[00:02:03.360 --> 00:02:07.200]   - I think that's arguably the most important quality
[00:02:07.200 --> 00:02:08.240]   in like almost anything.
[00:02:08.240 --> 00:02:10.240]   It's just pursuing it to like the end of the earth
[00:02:10.240 --> 00:02:11.960]   and like whatever you need to do to make it happen,
[00:02:11.960 --> 00:02:12.920]   you'll make it happen.
[00:02:12.920 --> 00:02:13.840]   - If you do everything you want.
[00:02:13.840 --> 00:02:15.520]   - If you do everything you want, exactly.
[00:02:15.520 --> 00:02:16.760]   I think from my side,
[00:02:16.760 --> 00:02:19.760]   definitely that quality has been important.
[00:02:19.760 --> 00:02:20.680]   Like agency and work.
[00:02:20.680 --> 00:02:22.320]   There are thousands,
[00:02:22.320 --> 00:02:23.920]   I would even like probably tens of thousands
[00:02:23.920 --> 00:02:26.840]   of engineers at Google who are like, you know, basically
[00:02:26.840 --> 00:02:28.080]   like we're all like equivalent
[00:02:28.080 --> 00:02:30.280]   like software engineering ability, let's say.
[00:02:30.280 --> 00:02:33.960]   Like, you know, if you gave us like a very well-defined task
[00:02:33.960 --> 00:02:35.600]   then we'd probably do it like equivalently well.
[00:02:35.600 --> 00:02:37.600]   Maybe a bunch of them will do it a lot better than me,
[00:02:37.600 --> 00:02:39.000]   you know, in all likelihood.
[00:02:39.000 --> 00:02:42.520]   But what I've been, like one of the reasons
[00:02:42.520 --> 00:02:46.680]   that I've been impactful so far is I've been very good
[00:02:46.680 --> 00:02:50.840]   at picking extremely high leverage problems.
[00:02:50.840 --> 00:02:51.920]   So problems that haven't been
[00:02:51.920 --> 00:02:56.160]   like particularly well solved so far, perhaps as a result
[00:02:56.160 --> 00:02:59.280]   of like frustrating structural factors, like the ones
[00:02:59.280 --> 00:03:01.320]   that you pointed out in like that scenario before
[00:03:01.320 --> 00:03:02.440]   where they're like, oh, we can't do X
[00:03:02.440 --> 00:03:04.560]   'cause this team won't do Y.
[00:03:04.560 --> 00:03:06.440]   Or like, and then going, okay, well, I'm just gonna
[00:03:06.440 --> 00:03:08.280]   like vertically solve the entire thing.
[00:03:08.280 --> 00:03:10.760]   (all laughing)
[00:03:10.760 --> 00:03:14.560]   - We should talk about how you guys got hired.
[00:03:14.560 --> 00:03:16.000]   'Cause I think that's a really interesting story.
[00:03:16.000 --> 00:03:17.960]   - So like the TL;DR of this is I studied robotics
[00:03:17.960 --> 00:03:18.800]   in undergrad.
[00:03:18.800 --> 00:03:20.280]   And in the meantime, on nights and weekends
[00:03:20.280 --> 00:03:23.080]   basically every night from 10 p.m. till 2 a.m.
[00:03:23.080 --> 00:03:25.240]   I would do my own like research.
[00:03:25.240 --> 00:03:28.160]   And every weekend for like at least six to eight hours
[00:03:28.160 --> 00:03:30.360]   each day, I would do my own like research
[00:03:30.360 --> 00:03:32.080]   and coding projects and this kind of stuff.
[00:03:32.080 --> 00:03:33.640]   That sort of switched in part
[00:03:33.640 --> 00:03:36.600]   from like quite robotics specific work
[00:03:36.600 --> 00:03:39.840]   to after reading Gwen's scaling hypothesis post
[00:03:39.840 --> 00:03:41.880]   I got completely scaling pills.
[00:03:41.880 --> 00:03:43.080]   And I was like, okay, but clearly the way
[00:03:43.080 --> 00:03:44.720]   that you solve robotics is by like scaling
[00:03:44.720 --> 00:03:46.360]   large multimodal models.
[00:03:46.360 --> 00:03:48.360]   I was trying to work out how to scale that effectively.
[00:03:48.360 --> 00:03:52.280]   And James Bradbury, who at the time was at Google
[00:03:52.280 --> 00:03:57.280]   and is now at Anthropic, saw some of my questions online
[00:03:57.280 --> 00:03:59.280]   where I was trying to work out how to do this properly.
[00:03:59.280 --> 00:04:00.640]   And he was like, I thought I knew all the people
[00:04:00.640 --> 00:04:02.640]   in the world who were like asking these questions.
[00:04:02.640 --> 00:04:03.680]   Who on earth are you?
[00:04:03.680 --> 00:04:08.840]   And he looked at that and he looked at some
[00:04:08.840 --> 00:04:10.160]   of the like the robotic stuff that had been putting
[00:04:10.160 --> 00:04:11.320]   up on my blog and that kind of thing.
[00:04:11.320 --> 00:04:12.720]   And he reached out and said, hey, do you wanna have a chat?
[00:04:12.720 --> 00:04:15.360]   And do you wanna like explore working with us here?
[00:04:16.640 --> 00:04:19.800]   And I was hired, as I understand it later
[00:04:19.800 --> 00:04:22.000]   as an experiment in trying to take someone
[00:04:22.000 --> 00:04:24.640]   with extremely high enthusiasm and agency
[00:04:24.640 --> 00:04:27.600]   and pairing them with some of the best engineers
[00:04:27.600 --> 00:04:28.600]   that he knew.
[00:04:28.600 --> 00:04:30.800]   And so one, another one of the reasons I could say
[00:04:30.800 --> 00:04:33.200]   like I've been impactful is I had this like dedicated
[00:04:33.200 --> 00:04:35.600]   mentorship from utterly wonderful people.
[00:04:35.600 --> 00:04:39.040]   - What you mentioned about being bootstrapped immediately
[00:04:39.040 --> 00:04:41.400]   by these people might've meant that since you're getting
[00:04:41.400 --> 00:04:43.520]   up to speed on everything at the same time
[00:04:43.520 --> 00:04:45.920]   rather than spending grad school going deep
[00:04:45.920 --> 00:04:47.800]   on like one specific way of being RL,
[00:04:47.800 --> 00:04:49.400]   you actually can take the global view
[00:04:49.400 --> 00:04:51.200]   and aren't like totally bought in on one thing.
[00:04:51.200 --> 00:04:53.040]   So not only is it something that's possible
[00:04:53.040 --> 00:04:55.680]   but like has greater returns than just hiring somebody
[00:04:55.680 --> 00:04:56.800]   at a grad school potentially.
[00:04:56.800 --> 00:04:59.160]   - Yeah, you come at everything with fresh eyes
[00:04:59.160 --> 00:05:02.560]   and you don't come in locked to any particular field.
[00:05:02.560 --> 00:05:05.360]   Now, what like one caveat to that is that before
[00:05:05.360 --> 00:05:07.240]   like during my self-experimentation and stuff
[00:05:07.240 --> 00:05:08.640]   I was reading everything I could.
[00:05:08.640 --> 00:05:11.360]   I was like obsessively reading papers every night
[00:05:11.400 --> 00:05:16.400]   and like actually funnily enough, I like read much less
[00:05:16.400 --> 00:05:19.920]   widely now that I like my day is occupied
[00:05:19.920 --> 00:05:21.440]   by working on things.
[00:05:21.440 --> 00:05:23.840]   And in some respect, I had like this very broad perspective
[00:05:23.840 --> 00:05:27.440]   before where not that many people, even like in a PhD
[00:05:27.440 --> 00:05:29.560]   you're probably gonna go like focus on a particular area.
[00:05:29.560 --> 00:05:31.360]   If you just like read all the NLP work
[00:05:31.360 --> 00:05:32.440]   and all the computer vision work
[00:05:32.440 --> 00:05:34.000]   and like all the robotics work, you like see
[00:05:34.000 --> 00:05:36.880]   all these patterns that start to emerge across subfields
[00:05:36.880 --> 00:05:40.200]   in a way that I guess like foreshadowed
[00:05:40.200 --> 00:05:42.360]   some of the work that I would like to do.
[00:05:42.360 --> 00:05:44.600]   - And Trenton, does this map onto any of your experience?
[00:05:44.600 --> 00:05:46.760]   - I think Sholto's story is more exciting.
[00:05:46.760 --> 00:05:49.960]   Mine was just very serendipitous
[00:05:49.960 --> 00:05:52.560]   in that I got into computational and neuroscience,
[00:05:52.560 --> 00:05:54.920]   didn't have much business being there.
[00:05:54.920 --> 00:05:57.440]   My first paper was mapping the cerebellum
[00:05:57.440 --> 00:05:59.680]   to the attention operation and transformers.
[00:05:59.680 --> 00:06:01.120]   My next ones were looking at like--
[00:06:01.120 --> 00:06:02.880]   - How old were you when you wrote that?
[00:06:02.880 --> 00:06:05.000]   - It was my first year of grad school.
[00:06:05.000 --> 00:06:06.960]   So 22.
[00:06:06.960 --> 00:06:08.080]   - Oh yeah.
[00:06:08.080 --> 00:06:12.800]   - But yeah, my next work was on sparsity in networks,
[00:06:12.800 --> 00:06:14.840]   like inspired by sparsity in the brain,
[00:06:14.840 --> 00:06:17.480]   which was when I met Tristan Hume
[00:06:17.480 --> 00:06:18.960]   and Anthropic was doing the SOLU,
[00:06:18.960 --> 00:06:21.080]   the Softmax Linear Output Unit work,
[00:06:21.080 --> 00:06:23.480]   which was very related in quite a few ways of like,
[00:06:23.480 --> 00:06:25.680]   let's make the activation of neurons
[00:06:25.680 --> 00:06:27.320]   across a layer really sparse.
[00:06:27.320 --> 00:06:29.400]   And if we do that, then we can get some interpretability
[00:06:29.400 --> 00:06:30.560]   of what the neuron's doing.
[00:06:30.560 --> 00:06:31.560]   That started the conversation.
[00:06:31.560 --> 00:06:33.040]   I shared drafts of that paper with Tristan.
[00:06:33.040 --> 00:06:34.440]   He was excited about it.
[00:06:34.440 --> 00:06:36.680]   And then, and that was basically what led me
[00:06:36.680 --> 00:06:40.680]   to become Tristan's resident and then convert to full-time.
[00:06:40.680 --> 00:06:42.480]   But during that period,
[00:06:42.480 --> 00:06:46.040]   I also moved as a visiting researcher to Berkeley
[00:06:46.040 --> 00:06:47.960]   and started working with Bruno Olshausen.
[00:06:47.960 --> 00:06:51.320]   And Bruno Olshausen basically invented sparse coding
[00:06:51.320 --> 00:06:52.360]   back in 1997.
[00:06:52.360 --> 00:06:55.560]   And so it was like, my research agenda
[00:06:55.560 --> 00:06:56.720]   and the interpretability team
[00:06:56.720 --> 00:06:58.800]   seemed to just be running in parallel
[00:06:58.800 --> 00:07:02.360]   with just research tastes.
[00:07:02.360 --> 00:07:04.560]   And so it, yeah, it made a lot of sense
[00:07:04.560 --> 00:07:06.560]   for me to work with the team.
[00:07:06.560 --> 00:07:09.120]   And it's been a dream since.
[00:07:09.120 --> 00:07:11.360]   - One thing I've noticed when people tell stories
[00:07:11.360 --> 00:07:14.000]   about their careers or their successes,
[00:07:14.000 --> 00:07:16.000]   they ascribe it way more to contingency,
[00:07:16.000 --> 00:07:17.640]   but when they hear about other people's stories,
[00:07:17.640 --> 00:07:19.360]   they're like, of course it wasn't contingent.
[00:07:19.360 --> 00:07:20.200]   You know what I mean?
[00:07:20.200 --> 00:07:21.520]   It's like, if that didn't happen,
[00:07:21.520 --> 00:07:22.480]   something else would have happened.
[00:07:22.480 --> 00:07:23.480]   - Yeah, but I mean, like,
[00:07:23.480 --> 00:07:25.400]   I literally met Tristan at a conference
[00:07:25.400 --> 00:07:28.400]   and like, didn't have a scheduled meeting with her,
[00:07:28.400 --> 00:07:31.160]   anything, just like joined a little group of people chatting
[00:07:31.160 --> 00:07:32.240]   and he happened to be standing there
[00:07:32.240 --> 00:07:34.320]   and I happened to mention what I was working on.
[00:07:34.320 --> 00:07:35.760]   And that led to more conversations.
[00:07:35.760 --> 00:07:37.240]   And I think I probably would have applied
[00:07:37.240 --> 00:07:38.840]   to Anthropic at some point anyways,
[00:07:38.840 --> 00:07:41.480]   but I would have waited at least another year.
[00:07:41.480 --> 00:07:44.720]   I, yeah, it's still crazy to me
[00:07:44.720 --> 00:07:47.280]   that I can like actually contribute to interpretability
[00:07:47.280 --> 00:07:48.240]   in a meaningful way.
[00:07:48.240 --> 00:07:50.120]   - I think there's a big important aspect
[00:07:50.120 --> 00:07:52.480]   of like shots on goal there, so to speak, right?
[00:07:52.480 --> 00:07:53.960]   Where like, you're even just going to,
[00:07:53.960 --> 00:07:55.720]   choosing to go to conferences itself
[00:07:55.720 --> 00:07:57.320]   is like putting yourself in a position
[00:07:57.320 --> 00:08:00.440]   where luck is more likely to happen.
[00:08:00.440 --> 00:08:01.640]   And like conversely, in my own situation,
[00:08:01.640 --> 00:08:04.400]   it was like doing all of this work independently
[00:08:04.400 --> 00:08:06.760]   and trying to produce and do interesting things
[00:08:06.760 --> 00:08:09.440]   was my own way of like trying to manufacture luck,
[00:08:09.440 --> 00:08:10.280]   so to speak.
[00:08:10.280 --> 00:08:13.040]   And like try and do something meaningful enough
[00:08:13.040 --> 00:08:14.360]   that it got noticed.
[00:08:14.360 --> 00:08:16.000]   - For the people who are like,
[00:08:16.000 --> 00:08:18.400]   assuming that the other end of the job board
[00:08:18.400 --> 00:08:20.760]   is like just like super legible and mechanical.
[00:08:20.760 --> 00:08:21.680]   This is not how it works.
[00:08:21.680 --> 00:08:23.320]   And in fact, like people are looking
[00:08:23.320 --> 00:08:25.200]   for the sort of different way,
[00:08:25.200 --> 00:08:26.520]   different kind of person who's agentic
[00:08:26.520 --> 00:08:27.360]   and putting stuff out there.
[00:08:27.360 --> 00:08:29.520]   - And I think specifically what people are looking
[00:08:29.520 --> 00:08:30.640]   for there is two things.
[00:08:30.640 --> 00:08:33.600]   One is agency and like putting yourself out there.
[00:08:33.600 --> 00:08:37.320]   And the second is the ability to do world class something.
[00:08:37.320 --> 00:08:38.160]   - Yeah.
[00:08:38.160 --> 00:08:42.600]   - Andy Jones from Anthropic did an amazing paper
[00:08:42.600 --> 00:08:44.400]   on scaling laws as applied to board games.
[00:08:44.400 --> 00:08:45.520]   It didn't require much resources.
[00:08:45.520 --> 00:08:46.960]   It demonstrated incredible engineering skill,
[00:08:46.960 --> 00:08:48.480]   which demonstrated incredible understanding
[00:08:48.480 --> 00:08:50.560]   of like the most topical problem of the time.
[00:08:50.560 --> 00:08:53.600]   And he didn't come from like typical academic background
[00:08:53.600 --> 00:08:55.120]   or whatever, as I understand it basically,
[00:08:55.120 --> 00:08:56.760]   like as soon as he came out with that paper,
[00:08:56.760 --> 00:08:58.200]   both Anthropic and OpenAI were like,
[00:08:58.200 --> 00:08:59.960]   we would desperately like to hire you.
[00:08:59.960 --> 00:09:02.120]   There's this line, the system is not your friend.
[00:09:02.120 --> 00:09:03.120]   - Right.
[00:09:03.120 --> 00:09:04.360]   - And it's not necessarily to say
[00:09:04.360 --> 00:09:08.120]   it's actively against you or it's your sworn enemy.
[00:09:08.120 --> 00:09:11.240]   It's just not looking out for you.
[00:09:11.240 --> 00:09:12.080]   - Right.
[00:09:12.080 --> 00:09:13.560]   - And so I think that's where a lot
[00:09:13.560 --> 00:09:15.600]   of the proactiveness comes in of like,
[00:09:15.600 --> 00:09:17.840]   there are no adults in the room or like,
[00:09:17.840 --> 00:09:21.400]   and like you have to come to some decision
[00:09:21.400 --> 00:09:23.720]   for what you want your life to look like and execute on it.
[00:09:23.720 --> 00:09:26.760]   And yeah, hopefully you can then update later
[00:09:26.760 --> 00:09:28.600]   if you're too headstrong in the wrong way.
[00:09:28.600 --> 00:09:31.280]   But I think you almost have to just kind of charge
[00:09:31.280 --> 00:09:32.120]   at certain things.
[00:09:32.120 --> 00:09:32.960]   - Right.
[00:09:32.960 --> 00:09:34.400]   - To get much of anything done,
[00:09:34.400 --> 00:09:35.600]   not be swept up in the tide
[00:09:35.600 --> 00:09:37.560]   of whatever the expectations are.
[00:09:37.560 --> 00:09:39.360]   - There's like one final thing I want to add,
[00:09:39.360 --> 00:09:40.800]   which is like, we talked a lot about agency
[00:09:40.800 --> 00:09:41.640]   and this kind of stuff,
[00:09:41.640 --> 00:09:43.640]   but I think actually like surprisingly enough,
[00:09:43.640 --> 00:09:44.680]   one of the most important things
[00:09:44.680 --> 00:09:48.640]   is just caring an unbelievable amount.
[00:09:48.640 --> 00:09:50.640]   And when you care an unbelievable amount,
[00:09:50.640 --> 00:09:52.200]   you'd like, you check all the details
[00:09:52.200 --> 00:09:53.720]   and you have like this understanding
[00:09:53.720 --> 00:09:54.800]   of like what could have gone wrong.
[00:09:54.800 --> 00:09:59.280]   And you'd like, it matters more than you think
[00:09:59.280 --> 00:10:02.160]   because people end up not caring.
[00:10:02.160 --> 00:10:03.000]   - Sure.
[00:10:03.000 --> 00:10:04.440]   - Or not caring enough.
[00:10:04.440 --> 00:10:05.800]   This is like LeBron quote,
[00:10:05.800 --> 00:10:08.640]   where he talks about how when he sort of,
[00:10:08.640 --> 00:10:09.920]   before he started in the league,
[00:10:09.920 --> 00:10:11.240]   he was like worried that everyone would be like
[00:10:11.240 --> 00:10:12.080]   incredibly good.
[00:10:12.080 --> 00:10:13.720]   And then he gets there and he like realized
[00:10:13.720 --> 00:10:15.480]   that actually once people hit financial stability,
[00:10:15.480 --> 00:10:17.720]   then they like, they relax a bit.
[00:10:17.720 --> 00:10:19.800]   And he's like, oh, this is going to be easy.
[00:10:19.800 --> 00:10:21.000]   I don't think that's quite true,
[00:10:21.000 --> 00:10:22.400]   because I think in like AI research,
[00:10:22.400 --> 00:10:24.920]   'cause most people actually care quite deeply,
[00:10:24.920 --> 00:10:27.320]   but there's caring about your problem.
[00:10:27.320 --> 00:10:29.080]   And there's also just caring about the entire stack
[00:10:29.080 --> 00:10:30.160]   and everything that goes up and down,
[00:10:30.160 --> 00:10:32.200]   like going explicitly going and fixing things
[00:10:32.200 --> 00:10:33.840]   that aren't your responsibility to fix,
[00:10:33.840 --> 00:10:37.160]   because overall it makes like the stack better.
[00:10:37.160 --> 00:10:38.520]   - Something that a friend said to me a while back,
[00:10:38.520 --> 00:10:40.440]   but I think it's stuck is like,
[00:10:40.440 --> 00:10:42.880]   it's amazing how quickly you can become world-class
[00:10:42.880 --> 00:10:45.720]   at something just because most people aren't trying that hard
[00:10:45.720 --> 00:10:47.760]   and like are only working like, I don't know,
[00:10:47.760 --> 00:10:50.440]   the actual like 20 hours that they're actually spending
[00:10:50.440 --> 00:10:52.040]   on this thing or something.
[00:10:52.040 --> 00:10:54.360]   And so, yeah, if you just go ham,
[00:10:54.360 --> 00:10:57.120]   then like you can get really far pretty fast.

