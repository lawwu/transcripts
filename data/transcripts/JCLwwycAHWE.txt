
[00:00:00.000 --> 00:00:05.840]   The ML team has to take more chances.
[00:00:05.840 --> 00:00:12.160]   And you cannot have the ML team work on a schedule and have clear times for when something
[00:00:12.160 --> 00:00:13.160]   is done.
[00:00:13.160 --> 00:00:14.160]   Something might never be done.
[00:00:14.160 --> 00:00:15.440]   It's also okay to fail.
[00:00:15.440 --> 00:00:20.960]   If someone starts a project today at your iPad and there is no result, but they do the
[00:00:20.960 --> 00:00:25.600]   right thing and you learn from that, that's a good project.
[00:00:25.600 --> 00:00:29.160]   Sometimes you have to spend some time to learn that something doesn't work.
[00:00:29.160 --> 00:00:33.560]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:33.560 --> 00:00:36.120]   And I'm your host, Lukas Biewald.
[00:00:36.120 --> 00:00:43.160]   Today I'm talking with Mircea NagyviÄ‡, who is the VP of AI and Research at UiPath.
[00:00:43.160 --> 00:00:49.600]   UiPath is a company you might not have heard of, but they're a leader in the space of RPA,
[00:00:49.600 --> 00:00:53.880]   which is essentially a way of automating a lot of the tasks that companies do.
[00:00:53.880 --> 00:00:59.640]   So Mircea is an expert on real world machine learning, getting something working for tasks
[00:00:59.640 --> 00:01:01.340]   that actually matter to businesses.
[00:01:01.340 --> 00:01:05.120]   This is a very practical, very interesting interview.
[00:01:05.120 --> 00:01:10.700]   I thought a good place to start would be your current company, UiPath, because I think the
[00:01:10.700 --> 00:01:17.960]   applications there are things that a lot of our audience might not know is an issue for
[00:01:17.960 --> 00:01:18.960]   businesses.
[00:01:18.960 --> 00:01:21.920]   So I thought maybe you could describe what UiPath does and then get into how machine
[00:01:21.920 --> 00:01:23.920]   learning fits into that.
[00:01:23.920 --> 00:01:27.960]   Yeah, UiPath is in the RPA business.
[00:01:27.960 --> 00:01:31.740]   This means robotic process automation.
[00:01:31.740 --> 00:01:39.600]   What it exactly means is basically programs that can do repetitive tasks that humans don't
[00:01:39.600 --> 00:01:41.960]   want to do.
[00:01:41.960 --> 00:01:44.320]   Simple tasks.
[00:01:44.320 --> 00:01:50.760]   It's been a good business for the company since about 2015 or so.
[00:01:50.760 --> 00:01:56.840]   But if you make those robots smarter and if you put machine learning into them, then I
[00:01:56.840 --> 00:01:59.600]   think we can take this company to a new level.
[00:01:59.600 --> 00:02:05.600]   So I think RPA has a lot of potential, but RPA plus AI has a lot more potential.
[00:02:05.600 --> 00:02:07.140]   I totally agree.
[00:02:07.140 --> 00:02:12.120]   But before I get into the AI, could you give a few examples of where RPA might affect someone
[00:02:12.120 --> 00:02:13.640]   in their day to day life?
[00:02:13.640 --> 00:02:21.240]   I think in all areas you see people doing repetitive tasks, like opening an email, opening
[00:02:21.240 --> 00:02:28.320]   an attachment, look at some data, copy a number into a form, then go back to that email, take
[00:02:28.320 --> 00:02:30.640]   another number, put it in the form.
[00:02:30.640 --> 00:02:33.440]   These kinds of very simple things.
[00:02:33.440 --> 00:02:37.600]   But they take time and they can actually fail.
[00:02:37.600 --> 00:02:41.840]   Robots once they get started, they are more reliable.
[00:02:41.840 --> 00:02:46.240]   And how do you actually set up today an RPA task?
[00:02:46.240 --> 00:02:49.240]   Is this a programmer does it or can anyone do it?
[00:02:49.240 --> 00:02:51.800]   We have a concept of RPA developer.
[00:02:51.800 --> 00:02:55.840]   RPA developer is our target for most of our products.
[00:02:55.840 --> 00:03:03.240]   An RPA developer is not a software developer, is maybe more like a basic developer from
[00:03:03.240 --> 00:03:04.240]   20 years ago.
[00:03:04.240 --> 00:03:10.360]   They understand data, they understand processes, they know what has to be automated, and then
[00:03:10.360 --> 00:03:11.920]   they create the workflows.
[00:03:11.920 --> 00:03:15.760]   There is a separate question about what do you want to automate?
[00:03:15.760 --> 00:03:19.960]   And I think we'll probably cover that a bit later when we talk about the project.
[00:03:19.960 --> 00:03:24.880]   It's not always clear what to automate, especially in a big company.
[00:03:24.880 --> 00:03:29.600]   But once you know what to automate, the RPA developer job is to take a process and make
[00:03:29.600 --> 00:03:31.280]   a workflow out of it.
[00:03:31.280 --> 00:03:32.280]   Got it.
[00:03:32.280 --> 00:03:36.200]   So UiPath has been one of those really phenomenally successful companies that a lot of people
[00:03:36.200 --> 00:03:38.960]   might not have heard of.
[00:03:38.960 --> 00:03:45.160]   What's the killer use case for UiPath that's made it successful so far?
[00:03:45.160 --> 00:03:47.880]   I think it's the broad usage.
[00:03:47.880 --> 00:03:55.200]   I don't know if we have a killer scenario, but we are able to save costs and we are able
[00:03:55.200 --> 00:04:01.120]   to have those repetitive processes taken care of by the robot, which allows people to do
[00:04:01.120 --> 00:04:03.760]   more of other things.
[00:04:03.760 --> 00:04:09.320]   I think we all have experiences when we have to do something that we don't exactly like
[00:04:09.320 --> 00:04:15.360]   to do, like move the data from one place to another, from Excel to a form, fill the form.
[00:04:15.360 --> 00:04:21.120]   This is, I think, the power of RPA, being able to do a lot of those processes.
[00:04:21.120 --> 00:04:29.840]   What do you think is the current level of use of ML in UiPath today?
[00:04:29.840 --> 00:04:35.080]   How much ML is actually working in the product right now as opposed to in the future?
[00:04:35.080 --> 00:04:41.080]   We have been starting putting ML into RPA about four years ago.
[00:04:41.080 --> 00:04:44.480]   Our first project was a computer vision project.
[00:04:44.480 --> 00:04:48.720]   Our robots usually work because they know the Windows APIs and they know what's on the
[00:04:48.720 --> 00:04:52.120]   screen, where to click and where to type.
[00:04:52.120 --> 00:04:53.520]   But this is not always the case.
[00:04:53.520 --> 00:04:57.040]   If you run in a remote desktop, there is no Windows API available.
[00:04:57.040 --> 00:04:58.040]   You only see a picture.
[00:04:58.040 --> 00:05:03.280]   If you are in an operating system other than Windows, the same thing.
[00:05:03.280 --> 00:05:07.600]   For us, falling back on the picture and making our robots work with the picture, without
[00:05:07.600 --> 00:05:11.600]   the APIs, was our first project.
[00:05:11.600 --> 00:05:15.400]   It is a competitive advantage for us, as far as I can tell.
[00:05:15.400 --> 00:05:20.240]   Our competitors don't have this computer vision feature.
[00:05:20.240 --> 00:05:21.680]   What is the act that the feature does?
[00:05:21.680 --> 00:05:25.440]   It finds the button to click on based on the screen?
[00:05:25.440 --> 00:05:30.640]   It finds all the controls on the screen, which are available to you if you are on Windows
[00:05:30.640 --> 00:05:33.720]   and we can actually use the Windows APIs.
[00:05:33.720 --> 00:05:37.760]   But if we have the picture, we can find everything in the picture.
[00:05:37.760 --> 00:05:40.000]   Then we have a design time and the runtime.
[00:05:40.000 --> 00:05:46.040]   In the design time, we detect all the controls and people can design their workflow.
[00:05:46.040 --> 00:05:49.880]   And then at the runtime, we have a picture that's different, different resolution.
[00:05:49.880 --> 00:05:53.920]   It's not the exact same picture, but it looks the same.
[00:05:53.920 --> 00:05:57.760]   Then we find the controls and we know where to click on the type, given what we have done
[00:05:57.760 --> 00:06:00.280]   at the design time.
[00:06:00.280 --> 00:06:04.960]   You started with this vision task, which is a really interesting task.
[00:06:04.960 --> 00:06:09.240]   And then what were the follow-on tasks that you made work with ML at UiPath?
[00:06:09.240 --> 00:06:14.600]   Then the next thing we realized is that we have now the controls on the screen and we
[00:06:14.600 --> 00:06:15.600]   want to do OCR.
[00:06:15.600 --> 00:06:20.280]   There are many cases when we want to do a screen OCR.
[00:06:20.280 --> 00:06:27.600]   We were using at the time Google and others, but we thought nobody did optimize the OCR
[00:06:27.600 --> 00:06:28.600]   for the screens.
[00:06:28.600 --> 00:06:33.880]   And we thought we have an opportunity to do a better OCR for our use case.
[00:06:33.880 --> 00:06:37.680]   And then we implemented an OCR around three years ago.
[00:06:37.680 --> 00:06:40.560]   It was not different than others do.
[00:06:40.560 --> 00:06:42.240]   I mean, it's still the same idea.
[00:06:42.240 --> 00:06:47.120]   You do detection first, you find where the text is, and then you do recognition.
[00:06:47.120 --> 00:06:56.080]   We didn't invent a new OCR, but we did train on our own data and our own use case.
[00:06:56.080 --> 00:06:58.960]   And we built a significantly better OCR in the process.
[00:06:58.960 --> 00:07:01.960]   The same thing for documents OCR later.
[00:07:01.960 --> 00:07:06.000]   We don't have such a big advantage in performance, but we have more flexibility.
[00:07:06.000 --> 00:07:12.680]   We can put it on device, we can put it in a services, we can ship it in any way we want.
[00:07:12.680 --> 00:07:18.000]   So OCR for screens and OCR for documents was another project.
[00:07:18.000 --> 00:07:26.640]   And then also during 2018-19, we were hearing from customers that they want to do document
[00:07:26.640 --> 00:07:31.560]   processing, emails, semi-structured content, unstructured.
[00:07:31.560 --> 00:07:37.280]   There is very many scenarios from very many customers we've seen.
[00:07:37.280 --> 00:07:44.000]   I think it was quite clear that the number one thing in document processing is doing
[00:07:44.000 --> 00:07:49.840]   information extraction from semi-structured content, invoices, receipts, purchase orders.
[00:07:49.840 --> 00:07:53.760]   And then we made some models that actually can read those documents and extract what
[00:07:53.760 --> 00:07:58.640]   we really care about, including, like I said, receipts, invoices, purchase orders.
[00:07:58.640 --> 00:08:06.200]   And now we have like 15 or 18 document types, W2s, W9s, and so on.
[00:08:06.200 --> 00:08:08.800]   We do some classification for those documents.
[00:08:08.800 --> 00:08:14.200]   We have models that do information extraction from unstructured content, like legal contracts,
[00:08:14.200 --> 00:08:17.200]   lease contracts.
[00:08:17.200 --> 00:08:20.320]   We've put quite a lot of effort into document understanding.
[00:08:20.320 --> 00:08:26.880]   So you have a lot of custom models running in production.
[00:08:26.880 --> 00:08:33.080]   I feel like compared to a lot of companies, you probably have a more advanced operational
[00:08:33.080 --> 00:08:34.080]   setup than others.
[00:08:34.080 --> 00:08:39.720]   I'm kind of curious what the structure looks like.
[00:08:39.720 --> 00:08:45.560]   We made the decision in 2018 to make a framework for hosting those models.
[00:08:45.560 --> 00:08:50.680]   The interesting thing is that we don't only want to do hosting.
[00:08:50.680 --> 00:08:56.080]   We also want to allow customers to fine tune our models or to train our models.
[00:08:56.080 --> 00:09:02.960]   And then at the time, we didn't see anyone to partner with this.
[00:09:02.960 --> 00:09:05.920]   There were some solutions in the cloud, but nothing on-prem.
[00:09:05.920 --> 00:09:13.040]   A lot of our customers have more trouble to move from on-prem to cloud for our scenarios.
[00:09:13.040 --> 00:09:16.880]   People now accept that email and documents are okay in cloud, but when it comes to processes
[00:09:16.880 --> 00:09:21.320]   and invoices and stuff, there is more reluctance.
[00:09:21.320 --> 00:09:25.240]   So we put a lot of time and effort into this.
[00:09:25.240 --> 00:09:26.240]   It's a big engineering project.
[00:09:26.240 --> 00:09:33.360]   This was not an ML project, but a big engineering project to build a framework that can host
[00:09:33.360 --> 00:09:40.360]   and train the models on-prem, online, and there are multiple configurations online.
[00:09:40.360 --> 00:09:43.400]   We call it now AI Center.
[00:09:43.400 --> 00:09:50.160]   And everything we do is hosted or trained in AI Center.
[00:09:50.160 --> 00:09:52.440]   It is a very big project.
[00:09:52.440 --> 00:09:56.160]   And all these models can be fine-tuned, so you have to have kind of separate instances?
[00:09:56.160 --> 00:09:58.560]   Not all, but many of them.
[00:09:58.560 --> 00:10:05.560]   We don't allow the computer vision model to be trained by customers or the OCR.
[00:10:05.560 --> 00:10:11.840]   Although for the OCR, we have to get the feedback from the customers and improve.
[00:10:11.840 --> 00:10:17.280]   But in document understanding, most of our models are retrainable, and this is why.
[00:10:17.280 --> 00:10:22.520]   We have a model for receipts, a model for invoices, and basically we have one big model
[00:10:22.520 --> 00:10:24.240]   with multiple tasks.
[00:10:24.240 --> 00:10:31.640]   But then when customers start to use this out-of-the-box model, either they want to
[00:10:31.640 --> 00:10:36.960]   fine-tune or dent on their data, which basically means overfitting on their data, but this
[00:10:36.960 --> 00:10:44.380]   is a good thing for them, or they have a bit of a different schema, or they have a totally
[00:10:44.380 --> 00:10:45.800]   different schema.
[00:10:45.800 --> 00:10:50.920]   So for all those cases, they have to fine-tune our models.
[00:10:50.920 --> 00:10:56.780]   So 2018 isn't that long ago by the calendar, but I feel like in terms of ML frameworks,
[00:10:56.780 --> 00:10:59.120]   it's kind of ancient history.
[00:10:59.120 --> 00:11:01.800]   What did you end up choosing for your ML framework?
[00:11:01.800 --> 00:11:07.600]   What are these models training in and how do you actually deploy them?
[00:11:07.600 --> 00:11:12.200]   We started with a mix of PyTorch and TensorFlow.
[00:11:12.200 --> 00:11:14.920]   Oh, a mix of PyTorch and TensorFlow?
[00:11:14.920 --> 00:11:15.920]   Wow.
[00:11:15.920 --> 00:11:18.200]   We didn't mix them on purpose.
[00:11:18.200 --> 00:11:25.280]   We preferred PyTorch from the very beginning, but our computer vision models, at the time,
[00:11:25.280 --> 00:11:27.520]   it was a lot easier to do this in TensorFlow.
[00:11:27.520 --> 00:11:32.600]   Google had a research repo, we implemented faster R-CNN.
[00:11:32.600 --> 00:11:36.360]   It was exactly what we needed.
[00:11:36.360 --> 00:11:40.440]   We took that model and trained with our own data.
[00:11:40.440 --> 00:11:41.880]   So we used both.
[00:11:41.880 --> 00:11:45.000]   In Dockman Understanding, we used PyTorch from the very beginning.
[00:11:45.000 --> 00:11:48.200]   And then later, it became easier with PyTorch.
[00:11:48.200 --> 00:11:53.560]   And actually, we also got a bit of a performance boost with PyTorch, I mean, quality performance.
[00:11:53.560 --> 00:11:57.320]   So at this point, everything we do is in PyTorch.
[00:11:57.320 --> 00:12:03.160]   We trained the models and then we shipped these models in AI Center.
[00:12:03.160 --> 00:12:04.720]   For the customer, they only see AI Center.
[00:12:04.720 --> 00:12:06.120]   We kind of do auto ML.
[00:12:06.120 --> 00:12:08.280]   We don't expose that many hyperparameters.
[00:12:08.280 --> 00:12:11.080]   There's very few things to expose.
[00:12:11.080 --> 00:12:18.080]   The other thing I want to mention that was very tricky is for people to train our models,
[00:12:18.080 --> 00:12:21.200]   they can fine tune our models in two ways.
[00:12:21.200 --> 00:12:27.640]   One is if they label data, and this is what they do before they deploy, they label 100,
[00:12:27.640 --> 00:12:34.800]   200, 500 documents, and we gave them our own tools to label.
[00:12:34.800 --> 00:12:40.600]   Or they can fine tune on the data.
[00:12:40.600 --> 00:12:46.160]   Once we deploy, we have a human in the loop concept and the validation station, and someone
[00:12:46.160 --> 00:12:49.660]   does fix our mistakes for the workflow to continue.
[00:12:49.660 --> 00:12:52.620]   And from those mistakes, we close the loop and we do learning.
[00:12:52.620 --> 00:12:56.520]   So those are the two types of data used for learning.
[00:12:56.520 --> 00:13:03.120]   So to come back, when we do a release, we make a branch, we train our models, we basically
[00:13:03.120 --> 00:13:07.440]   ship containers with code and models.
[00:13:07.440 --> 00:13:13.720]   And then in AI Center, they are hosted and trainable also.
[00:13:13.720 --> 00:13:18.680]   And I guess back in 2018, there was definitely a sense that PyTorch is kind of the framework
[00:13:18.680 --> 00:13:24.560]   for research where TensorFlow is kind of more for production deployment.
[00:13:24.560 --> 00:13:25.560]   How did you think about that?
[00:13:25.560 --> 00:13:30.840]   What was the key feature that made you prefer PyTorch to TensorFlow and choose to standardize
[00:13:30.840 --> 00:13:31.840]   on it?
[00:13:31.840 --> 00:13:35.360]   What were some models in TensorFlow that felt like they solved your needs really well?
[00:13:35.360 --> 00:13:37.200]   It was how debuggable PyTorch is.
[00:13:37.200 --> 00:13:41.880]   It was really optimized for developing faster.
[00:13:41.880 --> 00:13:45.600]   I mean, TensorFlow is a good framework, but it is really hard to use.
[00:13:45.600 --> 00:13:49.400]   It's been always hard to use even after they did the TensorFlow 2.0.
[00:13:49.400 --> 00:13:53.400]   We didn't have such a big issue with performance.
[00:13:53.400 --> 00:14:01.960]   Our computer vision model runs on GPU and it runs in a sub-second.
[00:14:01.960 --> 00:14:05.960]   So basically 0.5, 0.6 seconds, you cannot really see.
[00:14:05.960 --> 00:14:11.400]   Human can only see, notice things that take more than 0.7 or 0.8 seconds.
[00:14:11.400 --> 00:14:14.360]   So we didn't have an issue with CV.
[00:14:14.360 --> 00:14:19.480]   But also when we moved from TensorFlow to PyTorch, our PyTorch inference was a bit faster.
[00:14:19.480 --> 00:14:22.640]   We didn't exactly understand why.
[00:14:22.640 --> 00:14:24.120]   But it was definitely not slower.
[00:14:24.120 --> 00:14:26.040]   But in any case, we didn't have a big performance thing.
[00:14:26.040 --> 00:14:31.520]   And then most of our document understanding models actually run on CPU.
[00:14:31.520 --> 00:14:36.080]   And the request takes a second and a half, two seconds, something in this range.
[00:14:36.080 --> 00:14:39.080]   And for document processing by the robot, this is fine.
[00:14:39.080 --> 00:14:45.800]   So clearly in some scenarios, TensorFlow was faster than PyTorch was too slow.
[00:14:45.800 --> 00:14:47.880]   But that didn't happen for us.
[00:14:47.880 --> 00:14:50.480]   We just didn't have a performance issue back then.
[00:14:50.480 --> 00:14:53.080]   Do you do any kind of performance monitoring?
[00:14:53.080 --> 00:14:57.720]   I guess you have this human loop system to sort of catch issues with the models feeling
[00:14:57.720 --> 00:14:58.720]   uncertain.
[00:14:58.720 --> 00:15:04.400]   But are things like concept drift and data drift stuff that you actually watch in production?
[00:15:04.400 --> 00:15:07.240]   We have to do a lot more here.
[00:15:07.240 --> 00:15:11.360]   Drifting is a concept that we have to be concerned about.
[00:15:11.360 --> 00:15:18.880]   But for us, even before that, before the drifting, we have a hard time telling to a customer
[00:15:18.880 --> 00:15:21.280]   if their data is good enough or not for training.
[00:15:21.280 --> 00:15:27.000]   And then if we don't say anything, and then they start a very expensive labeling process
[00:15:27.000 --> 00:15:31.040]   and training process, and then we say, "Ah, your model didn't work because the data didn't
[00:15:31.040 --> 00:15:32.040]   work.
[00:15:32.040 --> 00:15:33.820]   But why didn't you say something before?"
[00:15:33.820 --> 00:15:39.560]   We don't have a good visual way to tell people, "You have to be labeled this much.
[00:15:39.560 --> 00:15:41.920]   You are now 50% done, 70% done.
[00:15:41.920 --> 00:15:43.920]   Or label more of this, label more of that."
[00:15:43.920 --> 00:15:45.240]   This is an issue for us.
[00:15:45.240 --> 00:15:48.440]   And then, of course, the drifting.
[00:15:48.440 --> 00:15:50.160]   But we didn't solve the first problem.
[00:15:50.160 --> 00:15:54.200]   Do you do any kind of active learning in the labeling?
[00:15:54.200 --> 00:15:57.640]   Do you try to pick examples that are going to help the model the most?
[00:15:57.640 --> 00:15:58.640]   How do you think about that?
[00:15:58.640 --> 00:16:04.680]   That's another thing that is kind of a debt we have to do more active learning.
[00:16:04.680 --> 00:16:06.880]   We train on...
[00:16:06.880 --> 00:16:09.520]   We do mostly supervised learning.
[00:16:09.520 --> 00:16:16.920]   We now know how to also do pre-training on unsupervised learning for document understanding
[00:16:16.920 --> 00:16:20.760]   and for CV.
[00:16:20.760 --> 00:16:24.200]   Active learning is something that we are now thinking about.
[00:16:24.200 --> 00:16:27.800]   It's not something that we shipped.
[00:16:27.800 --> 00:16:32.560]   But clearly, it is our way forward.
[00:16:32.560 --> 00:16:33.560]   Where do you think this goes?
[00:16:33.560 --> 00:16:38.440]   What are applications that you're really excited about building new models to do?
[00:16:38.440 --> 00:16:42.360]   And what would that help you at UiPath do that it can't do now?
[00:16:42.360 --> 00:16:47.520]   We have a very interesting project called Task Mining.
[00:16:47.520 --> 00:16:58.280]   Task Mining is a product that runs on people's desktops, records what people do, and then
[00:16:58.280 --> 00:17:04.000]   has a nice, interesting algorithm to find what are the most common processes.
[00:17:04.000 --> 00:17:11.000]   If you ask a CIO what to automate, they have a hard time to say exactly what people are
[00:17:11.000 --> 00:17:13.840]   doing, especially in larger work.
[00:17:13.840 --> 00:17:20.400]   So we built this Task Mining product that instead of having analysts and a lot of people
[00:17:20.400 --> 00:17:27.120]   talking and figuring out what has to be automated, we tried to discover this thing ourselves.
[00:17:27.120 --> 00:17:28.520]   It is a very interesting project.
[00:17:28.520 --> 00:17:30.720]   It has a lot of potential for us.
[00:17:30.720 --> 00:17:35.440]   Basically, we start with pictures.
[00:17:35.440 --> 00:17:41.320]   We have a recorder that knows when something relevant happens, like a click or a type.
[00:17:41.320 --> 00:17:46.440]   And we end up with two weeks of recording for, let's say, 10-15 users.
[00:17:46.440 --> 00:17:47.960]   And then we have to find some processes.
[00:17:47.960 --> 00:17:52.440]   It is a very, very interesting process, product.
[00:17:52.440 --> 00:17:56.960]   And we have a lot less research from the big companies or from the universities.
[00:17:56.960 --> 00:17:59.000]   Nobody's really doing research on this.
[00:17:59.000 --> 00:18:02.040]   In CV, in DU, you can just read a paper.
[00:18:02.040 --> 00:18:04.240]   You know what's going on.
[00:18:04.240 --> 00:18:05.240]   Not so much here.
[00:18:05.240 --> 00:18:06.240]   We have to do our own research.
[00:18:06.240 --> 00:18:09.840]   That's one project that we are very excited about.
[00:18:09.840 --> 00:18:14.080]   So the idea here is you could look at what people do over and over, where you're confident
[00:18:14.080 --> 00:18:17.200]   they're going to click on something or type something.
[00:18:17.200 --> 00:18:18.200]   Not that.
[00:18:18.200 --> 00:18:23.640]   We think about that one too, like recording people and believe what's the most likely
[00:18:23.640 --> 00:18:26.520]   thing they're going to do, like a language model for actions.
[00:18:26.520 --> 00:18:30.000]   But Task Mining is actually different.
[00:18:30.000 --> 00:18:33.780]   We look at the recording after two weeks, let's say, for 15 people.
[00:18:33.780 --> 00:18:36.520]   And then we find the processes.
[00:18:36.520 --> 00:18:41.560]   We found that the best process to automate is, for example, invoice processing.
[00:18:41.560 --> 00:18:46.480]   Or the best process to automate is some lookup that starts in some browser, does a lookup,
[00:18:46.480 --> 00:18:49.880]   goes to Excel, this kind of process.
[00:18:49.880 --> 00:18:52.840]   We just find the better candidates for automation.
[00:18:52.840 --> 00:18:55.640]   This is just a short summary.
[00:18:55.640 --> 00:18:57.880]   In reality, it's a little more complicated.
[00:18:57.880 --> 00:19:00.720]   And we don't exactly find the process.
[00:19:00.720 --> 00:19:04.400]   We build an explorer for the customer to find it.
[00:19:04.400 --> 00:19:10.080]   But still, we believe we take a process that takes on average 50 days down to maybe two
[00:19:10.080 --> 00:19:11.680]   days or something like this.
[00:19:11.680 --> 00:19:12.680]   Have you tried this on yourself?
[00:19:12.680 --> 00:19:17.480]   I'm imagining, I wonder what it would see me doing all day long.
[00:19:17.480 --> 00:19:22.880]   Well, us developers and engineers are not really good.
[00:19:22.880 --> 00:19:28.660]   If we record us, we'll see probably a lot of random stuff and coding and more watching
[00:19:28.660 --> 00:19:29.660]   and debugging.
[00:19:29.660 --> 00:19:34.960]   I cannot see myself recording something that brings any value to the product.
[00:19:34.960 --> 00:19:39.200]   I'd be just scared to, I mean, I'd be interested, maybe afraid to know how much time I spend
[00:19:39.200 --> 00:19:43.520]   sort of like moving around meetings or sending emails.
[00:19:43.520 --> 00:19:47.840]   We don't do like a Big Brother kind of thing that tells you what to do and how you waste
[00:19:47.840 --> 00:19:48.840]   your time.
[00:19:48.840 --> 00:19:52.960]   We don't make people feel bad about it.
[00:19:52.960 --> 00:19:58.240]   We just try to find the real processes and not all the overhead and the distractions.
[00:19:58.240 --> 00:20:01.400]   I see.
[00:20:01.400 --> 00:20:05.760]   It does seem interesting though to predict where somebody is going to click or what they're
[00:20:05.760 --> 00:20:06.760]   going to type.
[00:20:06.760 --> 00:20:10.920]   I can imagine you can make interesting UI changes to help somebody if you can sort of
[00:20:10.920 --> 00:20:12.880]   know what they're likely to do next.
[00:20:12.880 --> 00:20:16.920]   This is for us one of the things we want to look at in the future.
[00:20:16.920 --> 00:20:19.160]   Can we tell what people are going to do?
[00:20:19.160 --> 00:20:22.040]   And assuming we can, what do we do with that information?
[00:20:22.040 --> 00:20:27.080]   Let's suppose you click in three edit boxes and now we know we are going to click in the
[00:20:27.080 --> 00:20:28.080]   fourth.
[00:20:28.080 --> 00:20:29.080]   What do we do?
[00:20:29.080 --> 00:20:31.560]   We cannot take the mouse from you and start without telling you.
[00:20:31.560 --> 00:20:33.800]   So it's like the autopilot.
[00:20:33.800 --> 00:20:35.440]   So we don't know the experience.
[00:20:35.440 --> 00:20:37.920]   We don't know what the good experience is.
[00:20:37.920 --> 00:20:40.960]   But so far we don't even know how to do that.
[00:20:40.960 --> 00:20:46.360]   The other thing we can do that's probably a bit easier is we can see when you create
[00:20:46.360 --> 00:20:53.280]   a workflow and then we can tell you that we see you doing like a few clicks and a few
[00:20:53.280 --> 00:20:57.980]   types and we recognize that this is actually an action that we know.
[00:20:57.980 --> 00:21:01.560]   So we have those simple activities when we create workflows like click and type and those
[00:21:01.560 --> 00:21:02.720]   kinds of things.
[00:21:02.720 --> 00:21:08.020]   But we can also have more complicated activities like create a user in Salesforce.
[00:21:08.020 --> 00:21:13.960]   We can tell after we do three or four things, we can maybe tell that you are going to do
[00:21:13.960 --> 00:21:19.560]   10 more and all those 15 steps in the end are just one activity, which is create user.
[00:21:19.560 --> 00:21:23.400]   This is the kind of thing I think is a bit closer to us.
[00:21:23.400 --> 00:21:30.160]   But yeah, the ultimate goal is to just have the computer do the human work without minimal
[00:21:30.160 --> 00:21:31.360]   intervention from the human.
[00:21:31.360 --> 00:21:34.160]   But I don't think we are that close.
[00:21:34.160 --> 00:21:35.160]   Interesting.
[00:21:35.160 --> 00:21:40.040]   Have advances in language models, I feel like since 2018, languages models have gotten kind
[00:21:40.040 --> 00:21:45.200]   of much, much bigger and kind of better at predicting words.
[00:21:45.200 --> 00:21:46.760]   Has that affected you at all?
[00:21:46.760 --> 00:21:53.240]   Do you use these sort of like kind of modern gigantic language models in your product?
[00:21:53.240 --> 00:21:55.360]   We use the bird models.
[00:21:55.360 --> 00:21:58.460]   We use those big models.
[00:21:58.460 --> 00:22:03.760]   We don't do a GPT-3 kind of thing, although we did some experiments with it.
[00:22:03.760 --> 00:22:07.240]   We don't do zero-shot learning just yet.
[00:22:07.240 --> 00:22:13.840]   So we don't use a language model for this kind of predicting the next words thing.
[00:22:13.840 --> 00:22:19.680]   But we do use the large models trained with mass language models.
[00:22:19.680 --> 00:22:25.120]   We use them in unstructured documents and we use them in semi-structured documents.
[00:22:25.120 --> 00:22:32.880]   There is a model called LayoutLM built by Microsoft, and that's a transformer in 2D.
[00:22:32.880 --> 00:22:36.800]   And that one is useful for us for the semi-structured content.
[00:22:36.800 --> 00:22:37.800]   Cool.
[00:22:37.800 --> 00:22:41.400]   It's funny, going into this conversation, I was prepared to ask a lot of questions around
[00:22:41.400 --> 00:22:48.160]   the mix of traditional ML and deep learning, but you seem like very much more than I thought
[00:22:48.160 --> 00:22:49.720]   using primarily deep learning models.
[00:22:49.720 --> 00:22:54.520]   Is that accurate or do you do any kind of traditional machine learning as well?
[00:22:54.520 --> 00:22:58.640]   We try to use the best tool we know for a task.
[00:22:58.640 --> 00:23:03.040]   We don't say it's not your network, it's out.
[00:23:03.040 --> 00:23:10.040]   We have all sorts of smaller things, all sorts of smaller classifiers that just use a bag
[00:23:10.040 --> 00:23:14.540]   of words and trees and those kinds of things.
[00:23:14.540 --> 00:23:19.080]   We have reasons for classification to use simpler models because they are more explainable
[00:23:19.080 --> 00:23:23.120]   or easily explainable.
[00:23:23.120 --> 00:23:25.280]   We usually offer choice.
[00:23:25.280 --> 00:23:29.480]   In computer vision, we don't have, in OCR, we don't have a simpler model.
[00:23:29.480 --> 00:23:32.040]   We have to use the neural networks.
[00:23:32.040 --> 00:23:36.960]   But in document understanding and especially in classification, we have other methods as
[00:23:36.960 --> 00:23:37.960]   well.
[00:23:37.960 --> 00:23:38.960]   Interesting.
[00:23:38.960 --> 00:23:43.160]   Can you give an example of how a model might give you more explainability and you would
[00:23:43.160 --> 00:23:44.160]   pick it?
[00:23:44.160 --> 00:23:47.780]   I mean, we can talk about that, but it's hard to get real case studies.
[00:23:47.780 --> 00:23:56.700]   We had a customer who wanted to classify documents and they wanted to do two things.
[00:23:56.700 --> 00:24:02.800]   After the model is trained, they want to see which words or which features define each
[00:24:02.800 --> 00:24:03.800]   class.
[00:24:03.800 --> 00:24:10.900]   But then they also want, at inference time, to tell which words were the main contributors
[00:24:10.900 --> 00:24:12.760]   to a prediction.
[00:24:12.760 --> 00:24:17.960]   It was a very interesting conversation we had with the customer.
[00:24:17.960 --> 00:24:22.560]   Before that, we were talking about explainability in more abstract terms, but this was a real
[00:24:22.560 --> 00:24:23.560]   use case.
[00:24:23.560 --> 00:24:30.820]   And at the predict time, they want to see those words who actually contribute to a prediction
[00:24:30.820 --> 00:24:32.960]   in the evaluation phase.
[00:24:32.960 --> 00:24:38.560]   But I'm pretty sure they also want it when the model is deployed.
[00:24:38.560 --> 00:24:42.360]   Not everybody will look at those words, but they want to have the option when the model
[00:24:42.360 --> 00:24:48.080]   is deployed to see the weights on those words.
[00:24:48.080 --> 00:24:51.400]   And you can do the same thing with a BERT model, but it's more complicated.
[00:24:51.400 --> 00:24:55.920]   You have to get the tokens and it is definitely simpler.
[00:24:55.920 --> 00:25:00.920]   And also, the other thing I want to say is that we are not going to train for a customer
[00:25:00.920 --> 00:25:05.200]   a BERT model that takes eight hours to train.
[00:25:05.200 --> 00:25:09.320]   We're fine when we can train a Bag of Fours model in five seconds with similar or better
[00:25:09.320 --> 00:25:10.320]   performance.
[00:25:10.320 --> 00:25:14.320]   Where do you think the cutoff is?
[00:25:14.320 --> 00:25:19.640]   At what point would you switch from a Bag of Fours to a more complicated model?
[00:25:19.640 --> 00:25:22.880]   I think this is really hard to say.
[00:25:22.880 --> 00:25:25.220]   In some cases, you have to try both to know.
[00:25:25.220 --> 00:25:29.200]   We have some guidance, maybe, but we cannot really tell.
[00:25:29.200 --> 00:25:32.600]   I think it depends more on the content than the size.
[00:25:32.600 --> 00:25:36.020]   It is a mix of a number of documents.
[00:25:36.020 --> 00:25:41.480]   Most of our customers have very few documents and they expect us to learn from a very, very
[00:25:41.480 --> 00:25:42.480]   small number of documents.
[00:25:42.480 --> 00:25:47.720]   For example, they believe if they give us two forms, if they have two templates and
[00:25:47.720 --> 00:25:51.880]   they give us two forms for each, we should be able to do something.
[00:25:51.880 --> 00:25:55.200]   And that's a reasonable expectation.
[00:25:55.200 --> 00:26:01.480]   We have some more traditional models, no deep methods involved, that actually do just that.
[00:26:01.480 --> 00:26:04.240]   You give us a document, we look at it and we remember it.
[00:26:04.240 --> 00:26:08.360]   You have a second document that we believe is the same and then we are able to match
[00:26:08.360 --> 00:26:09.360]   them.
[00:26:09.360 --> 00:26:13.800]   We call this Forms AI, it's our newest feature.
[00:26:13.800 --> 00:26:19.200]   And this one doesn't use neural networks, it's just matching and searching and more
[00:26:19.200 --> 00:26:21.000]   traditional techniques.
[00:26:21.000 --> 00:26:27.040]   But I think what we are going to do is, when people have documents, we don't want to ask
[00:26:27.040 --> 00:26:30.520]   them to start with a thousand documents or even five hundred, that's too much.
[00:26:30.520 --> 00:26:34.520]   There are cases when the documents are very much the same and then we should start document
[00:26:34.520 --> 00:26:38.160]   by document and use simple techniques internally.
[00:26:38.160 --> 00:26:40.000]   We should not even tell the customer what to do.
[00:26:40.000 --> 00:26:43.600]   But if the documents are kind of the same or actually the documents are very much the
[00:26:43.600 --> 00:26:48.700]   same, then we can deal with them without neural networks.
[00:26:48.700 --> 00:26:54.400]   But if they keep giving us documents and we keep making mistakes after five, eight, ten,
[00:26:54.400 --> 00:26:59.360]   fifteen documents, there is a cutoff point where we say this template is just too complex
[00:26:59.360 --> 00:27:01.040]   for our simplest method.
[00:27:01.040 --> 00:27:05.000]   Our simplest method is more like a vehicle to get you started.
[00:27:05.000 --> 00:27:07.280]   Where we end depends on the content.
[00:27:07.280 --> 00:27:08.280]   Interesting.
[00:27:08.280 --> 00:27:13.120]   That kind of reminds me, do you do any kind of auto ML?
[00:27:13.120 --> 00:27:18.200]   Is hyperparameter search something that you do all the time or in certain cases?
[00:27:18.200 --> 00:27:19.200]   How do you think about that?
[00:27:19.200 --> 00:27:21.480]   We implicitly do auto ML.
[00:27:21.480 --> 00:27:26.920]   You cannot at this point in 2022, you cannot tell a customer that we give you a classification,
[00:27:26.920 --> 00:27:30.280]   that you have to change the learning rate, you have to change the bed size.
[00:27:30.280 --> 00:27:31.520]   You cannot do that.
[00:27:31.520 --> 00:27:33.840]   You have to find a way to do auto.
[00:27:33.840 --> 00:27:39.480]   Whether you like the term or not, you still do some sort of auto ML internally.
[00:27:39.480 --> 00:27:45.400]   And there are models that are kind of easier to generalize and you don't have to change
[00:27:45.400 --> 00:27:49.080]   as many hyperparameters and some of them are harder.
[00:27:49.080 --> 00:27:55.000]   But the ideas that you had before, like if you remember the Azure ML products where you
[00:27:55.000 --> 00:27:58.160]   give people 50 choices and 50...
[00:27:58.160 --> 00:28:02.520]   That's I think we are past that and people expect you just figure out what to do.
[00:28:02.520 --> 00:28:06.720]   But if you internally want to train one model or 50 and choose the best one, I think that's
[00:28:06.720 --> 00:28:09.800]   up to us.
[00:28:09.800 --> 00:28:13.640]   But it's interesting because it seems like from a lot of the examples you gave, sometimes
[00:28:13.640 --> 00:28:18.160]   your goal is not to make the most accurate model, but the model that will kind of fine
[00:28:18.160 --> 00:28:21.160]   tune the best on customers' data.
[00:28:21.160 --> 00:28:24.820]   Does that mean that you're optimizing something special?
[00:28:24.820 --> 00:28:29.360]   How do you know if the model is good in that kind of situation?
[00:28:29.360 --> 00:28:32.840]   We have some evaluation framework.
[00:28:32.840 --> 00:28:35.640]   But you're right.
[00:28:35.640 --> 00:28:36.640]   We don't necessarily...
[00:28:36.640 --> 00:28:40.400]   Let me give you an example.
[00:28:40.400 --> 00:28:46.780]   If you train a model for too long, you might end up with a slightly better model.
[00:28:46.780 --> 00:28:52.080]   But the confidence scores are worse because the way that the overfit works and the way
[00:28:52.080 --> 00:28:56.840]   that the numbers get too close to one and basically you are very confident for wrong
[00:28:56.840 --> 00:28:57.920]   predictions.
[00:28:57.920 --> 00:29:02.280]   You get most predictions right, but the ones you get wrong, you are very confident.
[00:29:02.280 --> 00:29:04.240]   And this is a thing that we have to figure out.
[00:29:04.240 --> 00:29:11.760]   What is the trade-off between overall model performance and other things?
[00:29:11.760 --> 00:29:14.000]   Fine tuning is an aspect.
[00:29:14.000 --> 00:29:18.040]   One thing that our customers really care a lot about is our confidence scores.
[00:29:18.040 --> 00:29:24.160]   Somebody will take a model that's like three points worse in terms of quality if the confidence
[00:29:24.160 --> 00:29:25.160]   scores are perfect.
[00:29:25.160 --> 00:29:30.520]   Because the confidence scores will tell them when to get a human involved.
[00:29:30.520 --> 00:29:38.520]   So yeah, it's not only about getting the absolute best model like a paper kind of goal for us.
[00:29:38.520 --> 00:29:42.480]   The goal is to kind of make the product work, not necessarily just have the highest score
[00:29:42.480 --> 00:29:43.480]   for the model.
[00:29:43.480 --> 00:29:46.480]   I really appreciate that perspective.
[00:29:46.480 --> 00:29:51.000]   I guess switching gears a little bit, but something I really wanted to cover is looking
[00:29:51.000 --> 00:29:55.920]   at your background, it looks like you've kind of gone from more traditional software engineering
[00:29:55.920 --> 00:29:58.760]   to running a machine learning organization.
[00:29:58.760 --> 00:30:04.600]   And I know from talking with people that enjoy these interviews, that's the perspective of
[00:30:04.600 --> 00:30:06.760]   a lot of people watching this.
[00:30:06.760 --> 00:30:12.240]   So I'm kind of curious if that's actually true, if you kind of learned machine learning
[00:30:12.240 --> 00:30:14.080]   mid-career.
[00:30:14.080 --> 00:30:17.160]   And either way, if you have any advice for someone that's trying to do the same type
[00:30:17.160 --> 00:30:19.480]   of thing.
[00:30:19.480 --> 00:30:26.840]   I was at Microsoft for a very long time doing software engineering.
[00:30:26.840 --> 00:30:32.280]   And then after like 12, 13, 14 years, something like this, I wanted to do something new and
[00:30:32.280 --> 00:30:35.000]   I didn't exactly know what to do.
[00:30:35.000 --> 00:30:40.760]   And I was very lucky to talk to a few people at Microsoft that actually made me see there
[00:30:40.760 --> 00:30:45.560]   is this machine learning opportunity.
[00:30:45.560 --> 00:30:51.040]   And then I started to learn and I was really fascinated to kind of go back into learning
[00:30:51.040 --> 00:30:52.040]   mode.
[00:30:52.040 --> 00:30:57.280]   Now that I look back at the last 20 years, I had the gap.
[00:30:57.280 --> 00:31:02.520]   I kind of thought you joined Microsoft, you learn on the job and this is true to some
[00:31:02.520 --> 00:31:03.840]   extent, but I don't think it's enough.
[00:31:03.840 --> 00:31:13.480]   So then I went back into more learning mode and did some math and some statistics that
[00:31:13.480 --> 00:31:19.120]   I have not done for the previous, I don't know how many years.
[00:31:19.120 --> 00:31:27.120]   And then after about 18 months or so of doing this for maybe four or five hours a day, nights
[00:31:27.120 --> 00:31:31.080]   and weekends and so on, then I thought I was ready to change jobs.
[00:31:31.080 --> 00:31:40.160]   And then I moved from my previous engineering role to a Microsoft research team.
[00:31:40.160 --> 00:31:42.820]   And that was a very good move for me.
[00:31:42.820 --> 00:31:45.480]   I was just learning these things.
[00:31:45.480 --> 00:31:49.240]   So they hired me to help them more through the engineering, but also they understood
[00:31:49.240 --> 00:31:53.540]   that I want to do more machine learning.
[00:31:53.540 --> 00:31:58.680]   But then I thought, actually, I also want to go back to school.
[00:31:58.680 --> 00:32:04.520]   And I started the master's program in computer science at UW.
[00:32:04.520 --> 00:32:11.320]   So yeah, so basically what happened is that I spent like about four years or so learning
[00:32:11.320 --> 00:32:15.240]   online Coursera at the beginning and then this master's.
[00:32:15.240 --> 00:32:24.680]   And then I was able to transition from a software engineer role to this ML thing.
[00:32:24.680 --> 00:32:31.340]   Do you have any advice for your younger self or someone that wants to make this transition?
[00:32:31.340 --> 00:32:35.400]   I think they have to be motivated.
[00:32:35.400 --> 00:32:37.440]   And this is a long journey.
[00:32:37.440 --> 00:32:42.120]   I think if you believe you do this in two months, I think that's not setting the right
[00:32:42.120 --> 00:32:43.120]   expectations.
[00:32:43.120 --> 00:32:46.040]   You have to be prepared for a longer transition.
[00:32:46.040 --> 00:32:50.200]   And I think you have to go back and do some math.
[00:32:50.200 --> 00:32:53.240]   It depends after how many years you want to transition.
[00:32:53.240 --> 00:32:56.000]   It's a lot easier to transition early.
[00:32:56.000 --> 00:32:59.960]   And also for younger people now go to those good universities.
[00:32:59.960 --> 00:33:08.760]   They have good knowledge about math that's fresh in their mind and they have good ML
[00:33:08.760 --> 00:33:10.320]   courses if they are interested.
[00:33:10.320 --> 00:33:14.800]   So I think what I can say is more for people who actually spend like 10, 15 years in software
[00:33:14.800 --> 00:33:21.400]   engineering, just prepare for a longer journey and try to learn the fundamentals.
[00:33:21.400 --> 00:33:27.360]   If you rush into it and it's not enough to be able to say model.fit and put some parameters
[00:33:27.360 --> 00:33:30.480]   in there, they're not going to do it.
[00:33:30.480 --> 00:33:33.720]   And I strongly recommend those master's programs.
[00:33:33.720 --> 00:33:41.040]   I think they are good programs and they kind of force you to put more time and you have
[00:33:41.040 --> 00:33:45.000]   to do a lot of projects and homework.
[00:33:45.000 --> 00:33:49.320]   The other thing I thought was a good resource is to do Kaggle competitions.
[00:33:49.320 --> 00:33:53.560]   I was in three of them and it was just a great experience.
[00:33:53.560 --> 00:33:58.200]   But the second part of it was very intense.
[00:33:58.200 --> 00:34:02.040]   But overall, Kaggle is a great resource.
[00:34:02.040 --> 00:34:03.920]   I love that answer.
[00:34:03.920 --> 00:34:10.320]   Do you think that your background in software engineering makes you approach machine learning
[00:34:10.320 --> 00:34:12.880]   differently in any way?
[00:34:12.880 --> 00:34:14.200]   I don't know what to say about that one.
[00:34:14.200 --> 00:34:19.240]   I think it's good to have some software engineering experience.
[00:34:19.240 --> 00:34:20.760]   And so a few things happen.
[00:34:20.760 --> 00:34:23.760]   If you don't do software engineering for a few years, like I didn't do software engineering
[00:34:23.760 --> 00:34:27.640]   for five years now, you are not current anymore.
[00:34:27.640 --> 00:34:29.680]   Things happen you don't exactly understand.
[00:34:29.680 --> 00:34:34.400]   I hear people talking and more and more it happens to me that I don't understand the
[00:34:34.400 --> 00:34:36.960]   details of what they're talking.
[00:34:36.960 --> 00:34:46.600]   I think it's very hard to do ML and do engineering, basically both at a good level.
[00:34:46.600 --> 00:34:51.520]   This is why at UiPath we have a separation between more science ML team and the engineering
[00:34:51.520 --> 00:34:52.800]   team.
[00:34:52.800 --> 00:34:56.680]   But I think it's good to have the background.
[00:34:56.680 --> 00:35:01.640]   It's good to understand memory and processors and threads.
[00:35:01.640 --> 00:35:08.100]   Are there differences in the way that you think teams should approach an ML problem versus
[00:35:08.100 --> 00:35:09.440]   an engineering problem?
[00:35:09.440 --> 00:35:15.400]   Is even the cadence of shipping different?
[00:35:15.400 --> 00:35:22.360]   The ML team has to take more chances and you cannot have the ML team work on a schedule
[00:35:22.360 --> 00:35:26.080]   and have clear times for when something is done.
[00:35:26.080 --> 00:35:27.080]   Something might never be done.
[00:35:27.080 --> 00:35:29.640]   It is also okay to fail.
[00:35:29.640 --> 00:35:35.360]   If someone starts a project today at UiPath and there is no result, but they do the right
[00:35:35.360 --> 00:35:39.760]   thing and you learn from that, that's a good project.
[00:35:39.760 --> 00:35:43.440]   Sometimes you have to spend some time to learn that something doesn't work.
[00:35:43.440 --> 00:35:45.240]   It's harder to do this in engineering.
[00:35:45.240 --> 00:35:53.840]   In engineering you have more strict schedules, more products and all those huddles and sprints.
[00:35:53.840 --> 00:35:57.480]   So yeah, I think you have to organize somehow different.
[00:35:57.480 --> 00:36:02.080]   We are a more hacker kind of org than engineering.
[00:36:02.080 --> 00:36:07.160]   And we are also more flexible, easier to move people from one project to another.
[00:36:07.160 --> 00:36:12.160]   For us now, our CV model, our DU model, our task mining model, they have a lot of things
[00:36:12.160 --> 00:36:14.480]   in common.
[00:36:14.480 --> 00:36:20.760]   It's funny, we were talking to Jeremy Howard, the Fast.ai founder, and he was saying that
[00:36:20.760 --> 00:36:26.440]   he thinks that engineering software is more fun because you make incremental progress
[00:36:26.440 --> 00:36:27.800]   that you can really see.
[00:36:27.800 --> 00:36:29.200]   And I was reflecting on that.
[00:36:29.200 --> 00:36:33.960]   I think my background is more in ML, but actually adding features to the Wasted Biases project
[00:36:33.960 --> 00:36:37.960]   is definitely more satisfying for me than training ML models.
[00:36:37.960 --> 00:36:43.920]   Because I feel like ML models, mostly they don't work and the debugging cycles are way
[00:36:43.920 --> 00:36:45.600]   longer and harder.
[00:36:45.600 --> 00:36:47.560]   Is that consistent with your experience?
[00:36:47.560 --> 00:36:50.320]   There must be something about ML that you love.
[00:36:50.320 --> 00:36:56.080]   Yes, but we do new features, although we don't...
[00:36:56.080 --> 00:36:58.560]   It depends how you define engineering.
[00:36:58.560 --> 00:37:00.040]   I think the way I look at it is this.
[00:37:00.040 --> 00:37:05.040]   You have people who do research science and they write papers, they create new knowledge.
[00:37:05.040 --> 00:37:06.720]   We don't do much of that.
[00:37:06.720 --> 00:37:12.000]   We have one researcher and we want to hire a second one, but for the most part, we don't
[00:37:12.000 --> 00:37:13.080]   do research.
[00:37:13.080 --> 00:37:14.960]   We do apply science though.
[00:37:14.960 --> 00:37:17.940]   Most of our team is an applied science team.
[00:37:17.940 --> 00:37:25.120]   So we do build new features and our work is, I think, maybe 10% in training the models
[00:37:25.120 --> 00:37:33.080]   and the rest is to just make something happen, make them work somehow, put them together.
[00:37:33.080 --> 00:37:38.720]   But then it's the engineering team who actually puts those things in production and create
[00:37:38.720 --> 00:37:45.200]   the containers and deploy in all data centers and takes care of scale and availability and
[00:37:45.200 --> 00:37:47.520]   all networking and all the other things.
[00:37:47.520 --> 00:37:51.320]   So that's why I'm saying it depends where we draw the line.
[00:37:51.320 --> 00:37:55.320]   We in this team don't just train models and then tell others, "Okay, take the models."
[00:37:55.320 --> 00:37:58.040]   We do the post-processing.
[00:37:58.040 --> 00:38:02.420]   In most cases, there is more post-processing than the model itself.
[00:38:02.420 --> 00:38:05.120]   We do pre-processing, we do data manipulation.
[00:38:05.120 --> 00:38:10.760]   So we build some feature, not just a model that doesn't do anything.
[00:38:10.760 --> 00:38:12.560]   It's just nice and shiny.
[00:38:12.560 --> 00:38:15.600]   But I know what you're saying.
[00:38:15.600 --> 00:38:18.760]   We also like to build features.
[00:38:18.760 --> 00:38:24.200]   Are there different ways that your team collaborates together?
[00:38:24.200 --> 00:38:27.360]   Is it a different kind of collaboration than an engineering team?
[00:38:27.360 --> 00:38:31.600]   Even though I know you're applied science, it's still kind of a different thing than
[00:38:31.600 --> 00:38:33.420]   software engineering, I think.
[00:38:33.420 --> 00:38:41.360]   So are there different ways to do code reviews and things like that on your team?
[00:38:41.360 --> 00:38:47.240]   We have less process than engineering teams I'm aware of.
[00:38:47.240 --> 00:38:53.960]   I don't know in detail how an engineering team functions now.
[00:38:53.960 --> 00:38:55.440]   But I think many things are in common.
[00:38:55.440 --> 00:38:59.100]   We want people to write good codes.
[00:38:59.100 --> 00:39:03.640]   We want people to write the simplest code possible and not complicate things to the
[00:39:03.640 --> 00:39:04.640]   point that nobody understands.
[00:39:04.640 --> 00:39:07.480]   So there are some things that are similar.
[00:39:07.480 --> 00:39:09.600]   But there are also some things that are different.
[00:39:09.600 --> 00:39:16.180]   When we merge a PR, we don't ask people questions like, "Have you seen this one in production?
[00:39:16.180 --> 00:39:17.680]   What is the impact?
[00:39:17.680 --> 00:39:19.560]   What is the latency difference?
[00:39:19.560 --> 00:39:22.560]   We don't do, "Where is the telemetry?"
[00:39:22.560 --> 00:39:26.480]   Although we want to have telemetry.
[00:39:26.480 --> 00:39:31.900]   I think the coding part is quite similar to engineering.
[00:39:31.900 --> 00:39:37.320]   But the way we change our mind and the way we choose the project and what to do and what
[00:39:37.320 --> 00:39:44.160]   to not do and the flexibility I think is the main difference.
[00:39:44.160 --> 00:39:48.640]   And then the testing is also something that we are very...
[00:39:48.640 --> 00:39:49.640]   Testing is very important.
[00:39:49.640 --> 00:39:54.140]   You cannot ship a good product if you don't have unit tests, automated tests, and so on.
[00:39:54.140 --> 00:39:59.760]   Some of those end-to-end testing are run by engineering, but we also do significant testing.
[00:39:59.760 --> 00:40:03.560]   So that's another thing that's similar between us and engineering.
[00:40:03.560 --> 00:40:07.800]   I think it's really the flexibility that's different.
[00:40:07.800 --> 00:40:16.560]   If we now believe a project is really important, we can easier move people around.
[00:40:16.560 --> 00:40:21.920]   We now have a semantic automation project that tries to make the robots understand better
[00:40:21.920 --> 00:40:24.720]   what's going on, not just click and type.
[00:40:24.720 --> 00:40:30.320]   And this is a mix of CV and document understanding, and we can apply the same knowledge.
[00:40:30.320 --> 00:40:33.320]   We can use the same graphs.
[00:40:33.320 --> 00:40:40.520]   Yeah, there are many, many things that are similar between our team and engineering.
[00:40:40.520 --> 00:40:41.520]   Interesting.
[00:40:41.520 --> 00:40:46.200]   Well, we always end with two questions, and I want to make sure that we give you some
[00:40:46.200 --> 00:40:47.240]   time to answer them.
[00:40:47.240 --> 00:40:53.240]   So what's an underrated aspect of machine learning or deep learning that you think people
[00:40:53.240 --> 00:40:55.240]   should pay more attention to?
[00:40:55.240 --> 00:40:59.120]   Or maybe what's something that if you could go back to school or had more time to look
[00:40:59.120 --> 00:41:02.480]   into, you'd spend some time engaging with?
[00:41:02.480 --> 00:41:05.920]   I think there are two ways to answer the question.
[00:41:05.920 --> 00:41:12.200]   I think people spend a lot of time on models, and I think people should spend more time
[00:41:12.200 --> 00:41:13.800]   on data.
[00:41:13.800 --> 00:41:15.840]   And this is changing in the last year or so.
[00:41:15.840 --> 00:41:19.200]   You see this more and more.
[00:41:19.200 --> 00:41:26.400]   If you want to improve the product, you look at the data more than you look at the models.
[00:41:26.400 --> 00:41:29.760]   So that's something that people are talking about.
[00:41:29.760 --> 00:41:38.680]   To me, I would also like to see more effort into more business kind of data.
[00:41:38.680 --> 00:41:46.320]   All those nice models are trained on Wikipedia, but customers have very small data sets with
[00:41:46.320 --> 00:41:48.080]   all those semi-structured things.
[00:41:48.080 --> 00:41:51.640]   There are no paragraphs, no sentences.
[00:41:51.640 --> 00:42:00.240]   It's quite hard to take a good BERT model or all those NLP models and apply them on
[00:42:00.240 --> 00:42:03.720]   the documents that you see in the enterprise.
[00:42:03.720 --> 00:42:07.000]   There is a lot less context.
[00:42:07.000 --> 00:42:11.400]   The graphs are less connected and so on.
[00:42:11.400 --> 00:42:16.680]   So this is about data sets and about customer data and business data.
[00:42:16.680 --> 00:42:21.000]   I totally agree with actually all the points that you just made, but I guess I want to
[00:42:21.000 --> 00:42:23.880]   ask about the data thing.
[00:42:23.880 --> 00:42:28.400]   People have been noticing that it's a better use of time to spend more time with data for
[00:42:28.400 --> 00:42:32.960]   20 years at least, as long as I've been kind of watching it.
[00:42:32.960 --> 00:42:38.480]   And yet it seems so hard to get teams to look at the data as much as they should by teams'
[00:42:38.480 --> 00:42:39.480]   own admission.
[00:42:39.480 --> 00:42:41.800]   What do you think is going on there?
[00:42:41.800 --> 00:42:46.720]   Why is it so hard to orient more towards the data than the models?
[00:42:46.720 --> 00:42:51.640]   It's not clear who is motivated by the job.
[00:42:51.640 --> 00:42:56.160]   People have been talking about it in theory, not really do anything about it.
[00:42:56.160 --> 00:43:01.320]   And if you look at what people really love to train models, even before the neural networks,
[00:43:01.320 --> 00:43:05.280]   people love to train the trees and everything.
[00:43:05.280 --> 00:43:09.080]   But not many people are passionate about the data in itself.
[00:43:09.080 --> 00:43:14.440]   All our good people do the data manipulation and the cleanup of the data just to build
[00:43:14.440 --> 00:43:15.440]   better models.
[00:43:15.440 --> 00:43:22.160]   There are now companies who help with the data, including what you guys do.
[00:43:22.160 --> 00:43:31.160]   But what is the profile of a person for us to hire to actually really focus on the data?
[00:43:31.160 --> 00:43:32.160]   It's unclear.
[00:43:32.160 --> 00:43:35.960]   Do you want to have software engineers, you have data engineers?
[00:43:35.960 --> 00:43:38.000]   It's unclear.
[00:43:38.000 --> 00:43:44.200]   I think this job really belongs to the applied scientists, but they rather do something else
[00:43:44.200 --> 00:43:45.200]   with their time.
[00:43:45.200 --> 00:43:50.600]   So I think this is why everybody says we should do more progress, but actually nobody really
[00:43:50.600 --> 00:43:51.600]   does.
[00:43:51.600 --> 00:43:52.600]   Right, right.
[00:43:52.600 --> 00:43:55.600]   Okay, that makes sense.
[00:43:55.600 --> 00:44:00.120]   My final question for you, and this is an interesting one because you've put probably
[00:44:00.120 --> 00:44:07.440]   more models into production than most people in the world, most people on this show.
[00:44:07.440 --> 00:44:16.200]   What's the hardest part about getting a model from conception to running live in production?
[00:44:16.200 --> 00:44:20.320]   When we build something, we start the ML part.
[00:44:20.320 --> 00:44:22.760]   So we see if a project has legs.
[00:44:22.760 --> 00:44:26.760]   But then to ship it, you need a big machinery in place.
[00:44:26.760 --> 00:44:31.680]   You need testing and you need engineering and you need product and you need alignment
[00:44:31.680 --> 00:44:37.000]   and people to sell to customers the real thing, not to oversell or undersell.
[00:44:37.000 --> 00:44:40.920]   So I think building this whole machinery is to me the biggest part.
[00:44:40.920 --> 00:44:46.880]   In the end, when you are done, you realize that the ML part that we love so much is just
[00:44:46.880 --> 00:44:47.880]   a small thing.
[00:44:47.880 --> 00:44:56.280]   Whether it's 10% or 15%, I'm not sure, but there is a lot more work on top of that.
[00:44:56.280 --> 00:45:03.560]   People in ML should give more credit to engineering and product managers and pre-sales because
[00:45:03.560 --> 00:45:08.600]   without those people, there is no ML in production.
[00:45:08.600 --> 00:45:16.480]   So having everybody aligned and kind of see the same, go in the same direction, this is
[00:45:16.480 --> 00:45:17.480]   tricky.
[00:45:17.480 --> 00:45:23.360]   The other thing that's tricky is to have more experimentation in the product.
[00:45:23.360 --> 00:45:30.200]   We struggle with convincing our product managers and our engineering to do more experiments,
[00:45:30.200 --> 00:45:36.200]   put more stuff into their code so we can experiment and maybe ship a better product.
[00:45:36.200 --> 00:45:43.360]   It is very hard to take time off their schedule for something that has the potential to give
[00:45:43.360 --> 00:45:44.360]   you nothing.
[00:45:44.360 --> 00:45:50.080]   On the other hand, if you don't do this and you only exploit and don't explore, it's not
[00:45:50.080 --> 00:45:51.080]   good.
[00:45:51.080 --> 00:45:56.480]   So this is another tricky thing, how you convince the whole world to have the right mix between
[00:45:56.480 --> 00:45:57.480]   exploring and exploiting.
[00:45:57.480 --> 00:45:58.480]   Awesome.
[00:45:58.480 --> 00:45:59.480]   Well, that's a great answer.
[00:45:59.480 --> 00:46:00.480]   Thank you very much.
[00:46:00.480 --> 00:46:01.480]   Thank you.
[00:46:01.480 --> 00:46:02.480]   This was super fun.
[00:46:02.480 --> 00:46:03.480]   I appreciate it.
[00:46:03.480 --> 00:46:04.480]   Very nice.
[00:46:04.480 --> 00:46:05.480]   Very nice talking to you, Lukas.
[00:46:05.480 --> 00:46:06.480]   Thanks for having me.
[00:46:06.480 --> 00:46:09.780]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:46:09.780 --> 00:46:14.460]   to the show notes in the description where you can find links to all the papers that
[00:46:14.460 --> 00:46:18.760]   are mentioned, supplemental material and a transcription that we work really hard to
[00:46:18.760 --> 00:46:19.760]   produce.
[00:46:19.760 --> 00:46:20.060]   So check it out.
[00:46:20.060 --> 00:46:22.300]   (upbeat music)

