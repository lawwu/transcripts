
[00:00:00.000 --> 00:00:02.480]   The following is a conversation with George Hotz,
[00:00:02.480 --> 00:00:06.560]   AKA GeoHot, his second time on the podcast.
[00:00:06.560 --> 00:00:09.360]   He's the founder of Kama AI,
[00:00:09.360 --> 00:00:12.920]   an autonomous and semi-autonomous vehicle technology company
[00:00:12.920 --> 00:00:15.840]   that seeks to be, to Tesla Autopilot,
[00:00:15.840 --> 00:00:18.960]   what Android is to the iOS.
[00:00:18.960 --> 00:00:22.800]   They sell the Kama 2 device for $1,000
[00:00:22.800 --> 00:00:25.640]   that when installed in many of their supported cars
[00:00:25.640 --> 00:00:28.000]   can keep the vehicle centered in the lane
[00:00:28.000 --> 00:00:30.800]   even when there are no lane markings.
[00:00:30.800 --> 00:00:33.280]   It includes driver sensing that ensures
[00:00:33.280 --> 00:00:35.680]   that the driver's eyes are on the road.
[00:00:35.680 --> 00:00:38.440]   As you may know, I'm a big fan of driver sensing.
[00:00:38.440 --> 00:00:40.560]   I do believe Tesla Autopilot and others
[00:00:40.560 --> 00:00:43.520]   should definitely include it in their sensor suite.
[00:00:43.520 --> 00:00:47.120]   Also, I'm a fan of Android and a big fan of George
[00:00:47.120 --> 00:00:50.080]   for many reasons, including his nonlinear
[00:00:50.080 --> 00:00:51.680]   out of the box brilliance,
[00:00:51.680 --> 00:00:55.160]   and the fact that he's a superstar programmer
[00:00:55.160 --> 00:00:57.400]   of a very different style than myself.
[00:00:57.400 --> 00:01:01.160]   Styles make fights and styles make conversations.
[00:01:01.160 --> 00:01:02.920]   So I really enjoyed this chat.
[00:01:02.920 --> 00:01:06.280]   I'm sure we'll talk many more times on this podcast.
[00:01:06.280 --> 00:01:07.720]   Quick mention of a sponsor,
[00:01:07.720 --> 00:01:10.160]   followed by some thoughts related to the episode.
[00:01:10.160 --> 00:01:12.240]   First is Four Sigmatic,
[00:01:12.240 --> 00:01:15.360]   the maker of delicious mushroom coffee.
[00:01:15.360 --> 00:01:17.520]   Second is Decoding Digital,
[00:01:17.520 --> 00:01:19.760]   a podcast on tech and entrepreneurship
[00:01:19.760 --> 00:01:22.120]   that I listen to and enjoy.
[00:01:22.120 --> 00:01:24.520]   And finally, ExpressVPN,
[00:01:24.520 --> 00:01:26.600]   the VPN I've used for many years
[00:01:26.600 --> 00:01:29.320]   to protect my privacy on the internet.
[00:01:29.320 --> 00:01:31.280]   Please check out the sponsors in the description
[00:01:31.280 --> 00:01:34.840]   to get a discount and to support this podcast.
[00:01:34.840 --> 00:01:38.080]   As a side note, let me say that my work at MIT
[00:01:38.080 --> 00:01:40.520]   on autonomous and semi-autonomous vehicles
[00:01:40.520 --> 00:01:43.080]   led me to study the human side of autonomy
[00:01:43.080 --> 00:01:46.600]   enough to understand that it's a beautifully complicated
[00:01:46.600 --> 00:01:48.600]   and interesting problem space,
[00:01:48.600 --> 00:01:51.800]   much richer than what can be studied in the lab.
[00:01:51.800 --> 00:01:54.120]   In that sense, the data that Kama AI,
[00:01:54.120 --> 00:01:56.320]   Tesla Autopilot, and perhaps others
[00:01:56.320 --> 00:01:58.480]   like Cadillac Super Cruiser are collecting
[00:01:58.480 --> 00:02:00.560]   gives us a chance to understand
[00:02:00.560 --> 00:02:03.760]   how we can design safe semi-autonomous vehicles
[00:02:03.760 --> 00:02:07.560]   for real human beings in real world conditions.
[00:02:07.560 --> 00:02:09.880]   I think this requires bold innovation
[00:02:09.880 --> 00:02:12.960]   and a serious exploration of the first principles
[00:02:12.960 --> 00:02:15.600]   of the driving task itself.
[00:02:15.600 --> 00:02:17.880]   If you enjoyed this thing, subscribe on YouTube,
[00:02:17.880 --> 00:02:20.160]   review it with Five Stars and Apple Podcast,
[00:02:20.160 --> 00:02:22.760]   follow on Spotify, support on Patreon,
[00:02:22.760 --> 00:02:26.280]   or connect with me on Twitter @LexFriedman.
[00:02:26.280 --> 00:02:30.360]   And now, here's my conversation with George Hotz.
[00:02:30.360 --> 00:02:34.040]   So last time we started talking about the simulation.
[00:02:34.040 --> 00:02:35.640]   This time, let me ask you,
[00:02:35.640 --> 00:02:37.000]   do you think there's intelligent life
[00:02:37.000 --> 00:02:38.600]   out there in the universe?
[00:02:38.600 --> 00:02:41.640]   - I've always maintained my answer to the Fermi paradox.
[00:02:41.640 --> 00:02:44.440]   I think there has been intelligent life
[00:02:44.440 --> 00:02:45.880]   elsewhere in the universe.
[00:02:45.880 --> 00:02:47.920]   - So intelligent civilizations existed,
[00:02:47.920 --> 00:02:49.240]   but they've blown themselves up.
[00:02:49.240 --> 00:02:50.720]   So your general intuition is that
[00:02:50.720 --> 00:02:54.520]   intelligent civilizations quickly,
[00:02:54.520 --> 00:02:57.720]   like there's that parameter in the Drake equation,
[00:02:57.720 --> 00:02:59.640]   your sense is they don't last very long.
[00:02:59.640 --> 00:03:00.480]   - Yeah.
[00:03:00.480 --> 00:03:01.520]   - How are we doing on that?
[00:03:01.520 --> 00:03:03.640]   Like, have we lasted pretty good?
[00:03:03.640 --> 00:03:04.840]   - Oh, no. - Are we due?
[00:03:04.840 --> 00:03:06.080]   - Oh, yeah.
[00:03:06.080 --> 00:03:08.080]   I mean, not quite yet.
[00:03:08.080 --> 00:03:10.560]   Well, it's like I was telling you,
[00:03:10.560 --> 00:03:13.480]   Kowski, the IQ required to destroy the world
[00:03:13.480 --> 00:03:15.440]   falls by one point every year.
[00:03:15.440 --> 00:03:18.840]   - Okay, so technology democratizes
[00:03:18.840 --> 00:03:21.120]   the destruction of the world.
[00:03:21.120 --> 00:03:22.920]   - When can a meme destroy the world?
[00:03:22.920 --> 00:03:27.280]   - It kind of is already, right?
[00:03:27.280 --> 00:03:28.480]   - Somewhat.
[00:03:28.480 --> 00:03:32.240]   I don't think we've seen anywhere near the worst of it yet.
[00:03:32.240 --> 00:03:34.000]   World's gonna get weird.
[00:03:34.000 --> 00:03:36.480]   - Well, maybe a meme can save the world.
[00:03:36.480 --> 00:03:37.480]   You thought about that?
[00:03:37.480 --> 00:03:40.800]   The meme lord, Elon Musk, fighting on the side of good
[00:03:40.800 --> 00:03:44.560]   versus the meme lord of the darkness,
[00:03:44.560 --> 00:03:48.280]   which is not saying anything bad about Donald Trump,
[00:03:48.280 --> 00:03:51.720]   but he is the lord of the meme on the dark side.
[00:03:51.720 --> 00:03:53.760]   He's a Darth Vader of memes.
[00:03:53.760 --> 00:03:57.240]   - I think in every fairy tale,
[00:03:57.240 --> 00:03:59.920]   they always end it with, "And they lived happily ever after."
[00:03:59.920 --> 00:04:00.960]   And I'm like, please tell me more
[00:04:00.960 --> 00:04:02.440]   about this happily ever after.
[00:04:02.440 --> 00:04:05.880]   I've heard 50% of marriages end in divorce.
[00:04:05.880 --> 00:04:07.840]   Why doesn't your marriage end up there?
[00:04:07.840 --> 00:04:09.240]   You can't just say happily ever after.
[00:04:09.240 --> 00:04:12.280]   So the thing about destruction
[00:04:12.280 --> 00:04:14.840]   is it's over after the destruction.
[00:04:14.840 --> 00:04:18.160]   We have to do everything right in order to avoid it.
[00:04:18.160 --> 00:04:21.080]   And one thing wrong, I mean,
[00:04:21.080 --> 00:04:22.960]   actually this is what I really like about cryptography.
[00:04:22.960 --> 00:04:24.640]   Cryptography, it seems like we live in a world
[00:04:24.640 --> 00:04:26.040]   where the defense wins
[00:04:26.040 --> 00:04:30.920]   versus like nuclear weapons, the opposite is true.
[00:04:30.920 --> 00:04:32.960]   It is much easier to build a warhead
[00:04:32.960 --> 00:04:34.520]   that splits into 100 little warheads
[00:04:34.520 --> 00:04:37.000]   than to build something that can take out
[00:04:37.000 --> 00:04:38.880]   100 little warheads.
[00:04:38.880 --> 00:04:41.400]   The offense has the advantage there.
[00:04:41.400 --> 00:04:44.520]   So maybe our future is in crypto, but.
[00:04:44.520 --> 00:04:45.720]   - So cryptography, right.
[00:04:45.720 --> 00:04:49.800]   The Goliath is the defense.
[00:04:49.800 --> 00:04:54.320]   And then all the different hackers are the Davids.
[00:04:54.320 --> 00:04:56.960]   And that equation is flipped for nuclear war.
[00:04:56.960 --> 00:04:58.840]   'Cause there's so many,
[00:04:58.840 --> 00:05:02.000]   like one nuclear weapon destroys everything, essentially.
[00:05:02.000 --> 00:05:06.160]   - Yeah, and it is much easier to attack
[00:05:06.160 --> 00:05:07.840]   with a nuclear weapon than it is to like,
[00:05:07.840 --> 00:05:10.920]   the technology required to intercept and destroy a rocket
[00:05:10.920 --> 00:05:13.120]   is much more complicated than the technology required
[00:05:13.120 --> 00:05:15.160]   to just, you know, orbital trajectory,
[00:05:15.160 --> 00:05:16.520]   send a rocket to somebody.
[00:05:16.520 --> 00:05:19.840]   - So, okay, your intuition that
[00:05:19.840 --> 00:05:22.960]   there were intelligent civilizations out there,
[00:05:22.960 --> 00:05:26.240]   but it's very possible that they're no longer there.
[00:05:26.240 --> 00:05:27.680]   It's kind of a sad picture.
[00:05:27.680 --> 00:05:29.520]   - They enter some steady state.
[00:05:29.520 --> 00:05:31.520]   They all wirehead themselves.
[00:05:31.520 --> 00:05:32.440]   - What's wirehead?
[00:05:32.440 --> 00:05:35.400]   - Stimulate their pleasure centers.
[00:05:35.400 --> 00:05:39.720]   And just, you know, live forever in this kind of stasis.
[00:05:39.720 --> 00:05:42.640]   They become, well, I mean,
[00:05:42.640 --> 00:05:46.320]   I think the reason I believe this is because where are they?
[00:05:46.320 --> 00:05:49.800]   If there's some reason they stopped expanding,
[00:05:49.800 --> 00:05:52.160]   'cause otherwise they would have taken over the universe.
[00:05:52.160 --> 00:05:53.440]   The universe isn't that big,
[00:05:53.440 --> 00:05:54.280]   or at least, you know,
[00:05:54.280 --> 00:05:56.120]   let's just talk about the galaxy, right?
[00:05:56.120 --> 00:05:57.800]   70,000 light years across.
[00:05:57.800 --> 00:05:59.920]   I took that number from Star Trek Voyager.
[00:05:59.920 --> 00:06:00.920]   I don't know how true it is.
[00:06:00.920 --> 00:06:04.960]   But yeah, that's not big, right?
[00:06:04.960 --> 00:06:07.320]   70,000 light years is nothing.
[00:06:07.320 --> 00:06:10.040]   - For some possible technology that you can imagine
[00:06:10.040 --> 00:06:12.320]   that can leverage like wormholes or something like that.
[00:06:12.320 --> 00:06:13.320]   - No, you don't even need wormholes.
[00:06:13.320 --> 00:06:15.120]   Just a von Neumann probe is enough.
[00:06:15.120 --> 00:06:18.440]   A von Neumann probe and a million years of sublight travel,
[00:06:18.440 --> 00:06:20.440]   and you'd have taken over the whole universe.
[00:06:20.440 --> 00:06:22.480]   That clearly didn't happen.
[00:06:22.480 --> 00:06:24.000]   So something stopped it.
[00:06:24.000 --> 00:06:25.400]   - So you mean if you, right,
[00:06:25.400 --> 00:06:27.040]   for like a few million years,
[00:06:27.040 --> 00:06:29.880]   if you sent out probes that travel close,
[00:06:29.880 --> 00:06:30.720]   what's sublight?
[00:06:30.720 --> 00:06:32.240]   You mean close to the speed of light?
[00:06:32.240 --> 00:06:33.720]   - Let's say 0.1c.
[00:06:33.720 --> 00:06:34.800]   - And it just spreads.
[00:06:34.800 --> 00:06:35.640]   Interesting.
[00:06:35.640 --> 00:06:38.120]   Actually, that's an interesting calculation.
[00:06:38.120 --> 00:06:38.960]   Huh.
[00:06:38.960 --> 00:06:40.640]   So what makes you think that we'd be able
[00:06:40.640 --> 00:06:42.280]   to communicate with them?
[00:06:42.280 --> 00:06:45.160]   Like, yeah, what's,
[00:06:45.160 --> 00:06:48.920]   why do you think we would be able to comprehend
[00:06:48.920 --> 00:06:50.760]   intelligent lives that are out there?
[00:06:50.760 --> 00:06:54.960]   Like even if they were among us kind of thing,
[00:06:54.960 --> 00:06:57.600]   like, or even just flying around?
[00:06:57.600 --> 00:07:01.200]   - Well, I mean, that's possible.
[00:07:01.200 --> 00:07:04.640]   It's possible that there is some sort of prime directive.
[00:07:04.640 --> 00:07:07.040]   That'd be a really cool universe to live in.
[00:07:07.040 --> 00:07:09.040]   And there's some reason they're not making themselves
[00:07:09.040 --> 00:07:10.920]   visible to us.
[00:07:10.920 --> 00:07:15.200]   But it makes sense that they would use the same,
[00:07:15.200 --> 00:07:16.960]   well, at least the same entropy.
[00:07:16.960 --> 00:07:18.800]   - Well, you're implying the same laws of physics.
[00:07:18.800 --> 00:07:20.800]   I don't know what you mean by entropy in this case.
[00:07:20.800 --> 00:07:21.920]   - Oh, yeah.
[00:07:21.920 --> 00:07:25.000]   I mean, if entropy is the scarce resource in the universe.
[00:07:25.000 --> 00:07:26.960]   - So what do you think about like Stephen Wolfram
[00:07:26.960 --> 00:07:28.840]   and everything is a computation,
[00:07:28.840 --> 00:07:31.080]   and then what if they are traveling
[00:07:31.080 --> 00:07:32.640]   through this world of computation?
[00:07:32.640 --> 00:07:34.240]   So if you think of the universe
[00:07:34.240 --> 00:07:36.600]   as just information processing,
[00:07:36.600 --> 00:07:40.840]   then what you're referring to with entropy,
[00:07:40.840 --> 00:07:42.960]   and then these pockets of interesting,
[00:07:42.960 --> 00:07:45.480]   complex computation swimming around,
[00:07:45.480 --> 00:07:47.480]   how do we know they're not already here?
[00:07:47.480 --> 00:07:51.040]   How do we know that this,
[00:07:51.040 --> 00:07:53.040]   like all the different amazing things
[00:07:53.040 --> 00:07:55.040]   that are full of mystery on earth
[00:07:55.040 --> 00:07:58.640]   are just like little footprints of intelligence
[00:07:58.640 --> 00:08:01.160]   from light years away?
[00:08:01.160 --> 00:08:02.800]   - Maybe.
[00:08:02.800 --> 00:08:05.720]   I mean, I tend to think that as civilizations expand,
[00:08:05.720 --> 00:08:07.800]   they use more and more energy,
[00:08:07.800 --> 00:08:10.200]   and you can never overcome the problem of waste heat.
[00:08:10.200 --> 00:08:11.880]   So where is their waste heat?
[00:08:11.880 --> 00:08:12.720]   - So we'd be able to,
[00:08:12.720 --> 00:08:13.560]   with our crude methods,
[00:08:13.560 --> 00:08:18.560]   be able to see like there's a whole lot of energy here,
[00:08:18.560 --> 00:08:20.560]   but it could be something we're not,
[00:08:20.560 --> 00:08:22.480]   I mean, we don't understand dark energy, right?
[00:08:22.480 --> 00:08:23.560]   Dark matter.
[00:08:23.560 --> 00:08:26.120]   It could be just stuff we don't understand at all.
[00:08:26.120 --> 00:08:29.040]   Or they can have a fundamentally different physics,
[00:08:29.040 --> 00:08:32.480]   you know, like that we just don't even comprehend.
[00:08:32.480 --> 00:08:33.440]   - I think, okay.
[00:08:33.440 --> 00:08:35.080]   I mean, it depends how far out you want to go.
[00:08:35.080 --> 00:08:36.800]   I don't think physics is very different
[00:08:36.800 --> 00:08:38.400]   on the other side of the galaxy.
[00:08:39.720 --> 00:08:41.920]   I would suspect that they have,
[00:08:41.920 --> 00:08:43.680]   I mean, if they're in our universe,
[00:08:43.680 --> 00:08:45.760]   they have the same physics.
[00:08:45.760 --> 00:08:47.600]   - Well, yeah, that's the assumption we have,
[00:08:47.600 --> 00:08:50.000]   but there could be like super trippy things
[00:08:50.000 --> 00:08:55.000]   like our cognition only gets to a slice,
[00:08:55.000 --> 00:08:59.440]   and all the possible instruments that we can design
[00:08:59.440 --> 00:09:01.560]   only get to a particular slice of the universe.
[00:09:01.560 --> 00:09:04.080]   And there's something much like weirder.
[00:09:04.080 --> 00:09:06.880]   - Maybe we can try a thought experiment.
[00:09:06.880 --> 00:09:11.880]   Would people from the past be able to detect
[00:09:11.880 --> 00:09:14.000]   the remnants of our,
[00:09:14.000 --> 00:09:16.600]   would we be able to detect our modern civilization?
[00:09:16.600 --> 00:09:18.840]   I think the answer is obviously yes.
[00:09:18.840 --> 00:09:20.680]   - You mean past from 100 years ago?
[00:09:20.680 --> 00:09:22.120]   - Well, let's even go back further.
[00:09:22.120 --> 00:09:24.440]   Let's go to a million years ago.
[00:09:24.440 --> 00:09:26.560]   The humans who were lying around in the desert
[00:09:26.560 --> 00:09:27.760]   probably didn't even have,
[00:09:27.760 --> 00:09:29.360]   maybe they just barely had fire.
[00:09:29.360 --> 00:09:34.140]   They would understand if a 747 flew overhead.
[00:09:35.560 --> 00:09:40.560]   - In this vicinity, but not if a 747 flew on Mars,
[00:09:40.560 --> 00:09:45.080]   'cause they wouldn't be able to see far,
[00:09:45.080 --> 00:09:47.240]   'cause we're not actually communicating that well
[00:09:47.240 --> 00:09:48.920]   with the rest of the universe.
[00:09:48.920 --> 00:09:50.160]   We're doing okay.
[00:09:50.160 --> 00:09:54.200]   Just sending out random like 50s tracks of music.
[00:09:54.200 --> 00:09:57.000]   - True, and yeah, I mean, they'd have to,
[00:09:57.000 --> 00:09:59.600]   you know, we've only been broadcasting radio waves
[00:09:59.600 --> 00:10:04.600]   for 150 years, and well, there's your light cone.
[00:10:04.600 --> 00:10:05.840]   So.
[00:10:05.840 --> 00:10:06.960]   - Yeah, okay.
[00:10:06.960 --> 00:10:08.840]   What do you make about all the,
[00:10:08.840 --> 00:10:11.020]   I recently came across this,
[00:10:11.020 --> 00:10:14.760]   having talked to David Fravor.
[00:10:14.760 --> 00:10:17.000]   I don't know if you caught what the videos
[00:10:17.000 --> 00:10:18.880]   that Pentagon released,
[00:10:18.880 --> 00:10:23.520]   and the New York Times reporting of the UFO sightings.
[00:10:23.520 --> 00:10:26.080]   So I kind of looked into it, quote unquote,
[00:10:26.080 --> 00:10:31.080]   and there's actually been like hundreds of thousands
[00:10:31.080 --> 00:10:33.840]   of UFO sightings, right?
[00:10:33.840 --> 00:10:36.000]   And a lot of it, you can explain away
[00:10:36.000 --> 00:10:37.120]   in different kinds of ways.
[00:10:37.120 --> 00:10:40.220]   So one is it could be interesting physical phenomena.
[00:10:40.220 --> 00:10:44.680]   Two, it could be people wanting to believe,
[00:10:44.680 --> 00:10:46.800]   and therefore they conjure up a lot of different things
[00:10:46.800 --> 00:10:49.040]   that just, you know, when you see different kinds of lights,
[00:10:49.040 --> 00:10:50.760]   some basic physics phenomena,
[00:10:50.760 --> 00:10:54.680]   and then you just conjure up ideas of possible,
[00:10:54.680 --> 00:10:56.720]   out there mysterious worlds.
[00:10:56.720 --> 00:10:59.000]   But, you know, it's also possible,
[00:10:59.000 --> 00:11:02.520]   like you have a case of David Fravor,
[00:11:02.520 --> 00:11:05.220]   who is a Navy pilot, who's, you know,
[00:11:05.220 --> 00:11:08.880]   as legit as it gets in terms of humans
[00:11:08.880 --> 00:11:13.480]   who are able to perceive things in the environment
[00:11:13.480 --> 00:11:15.360]   and make conclusions,
[00:11:15.360 --> 00:11:17.640]   whether those things are a threat or not.
[00:11:17.640 --> 00:11:22.040]   And he and several other pilots saw a thing,
[00:11:22.040 --> 00:11:23.480]   I don't know if you follow this,
[00:11:23.480 --> 00:11:26.840]   but they saw a thing that they've since then called TikTok
[00:11:26.840 --> 00:11:29.540]   that moved in all kinds of weird ways.
[00:11:29.540 --> 00:11:30.640]   They don't know what it is.
[00:11:30.640 --> 00:11:35.640]   It could be technology developed by the United States,
[00:11:35.640 --> 00:11:38.080]   and they're just not aware of it
[00:11:38.080 --> 00:11:40.000]   in the surface level from the Navy, right?
[00:11:40.000 --> 00:11:42.280]   It could be different kind of lighting technology
[00:11:42.280 --> 00:11:45.040]   or drone technology, all that kind of stuff.
[00:11:45.040 --> 00:11:46.600]   It could be the Russians and the Chinese,
[00:11:46.600 --> 00:11:48.040]   all that kind of stuff.
[00:11:48.040 --> 00:11:51.420]   And of course their mind, our mind,
[00:11:51.420 --> 00:11:54.200]   can also venture into the possibility
[00:11:54.200 --> 00:11:56.360]   that it's from another world.
[00:11:56.360 --> 00:11:58.200]   Have you looked into this at all?
[00:11:58.200 --> 00:11:59.680]   What do you think about it?
[00:11:59.680 --> 00:12:01.380]   - I think all the news is a psyop.
[00:12:01.380 --> 00:12:05.200]   I think that the most plausible--
[00:12:05.200 --> 00:12:06.480]   - Nothing is real.
[00:12:06.480 --> 00:12:10.040]   - Yeah, I listened to the, I think it was Bob Lazar
[00:12:10.040 --> 00:12:12.400]   on Joe Rogan.
[00:12:12.400 --> 00:12:15.880]   And I believe everything this guy is saying.
[00:12:15.880 --> 00:12:17.320]   And then I think that it's probably
[00:12:17.320 --> 00:12:19.280]   just some MKUltra kind of thing.
[00:12:19.280 --> 00:12:21.560]   - What do you mean?
[00:12:21.560 --> 00:12:24.760]   - Like they made some weird thing
[00:12:24.760 --> 00:12:26.360]   and they called it an alien spaceship.
[00:12:26.360 --> 00:12:29.600]   Maybe it was just to stimulate young physicists' minds.
[00:12:29.600 --> 00:12:31.080]   We'll tell them it's alien technology
[00:12:31.080 --> 00:12:33.640]   and we'll see what they come up with, right?
[00:12:33.640 --> 00:12:36.120]   - Do you find any conspiracy theories compelling?
[00:12:36.120 --> 00:12:38.360]   Like have you pulled at the string
[00:12:38.360 --> 00:12:42.460]   of the rich, complex world of conspiracy theories
[00:12:42.460 --> 00:12:43.920]   that's out there?
[00:12:43.920 --> 00:12:46.520]   - I think that I've heard a conspiracy theory
[00:12:46.520 --> 00:12:48.960]   that conspiracy theories were invented by the CIA
[00:12:48.960 --> 00:12:52.080]   in the '60s to discredit true things.
[00:12:53.880 --> 00:12:58.520]   - So you can go to ridiculous conspiracy theories
[00:12:58.520 --> 00:13:01.000]   like Flat Earth and Pizzagate.
[00:13:01.000 --> 00:13:06.000]   And these things are almost to hide conspiracy theories
[00:13:06.000 --> 00:13:10.120]   that like, remember when the Chinese locked up the doctors
[00:13:10.120 --> 00:13:11.360]   who discovered coronavirus?
[00:13:11.360 --> 00:13:12.840]   Like I tell people this and I'm like,
[00:13:12.840 --> 00:13:14.400]   no, no, no, that's not a conspiracy theory.
[00:13:14.400 --> 00:13:15.900]   That actually happened.
[00:13:15.900 --> 00:13:17.320]   Do you remember the time that the money
[00:13:17.320 --> 00:13:20.060]   used to be backed by gold and now it's backed by nothing?
[00:13:20.060 --> 00:13:21.660]   This is not a conspiracy theory.
[00:13:21.660 --> 00:13:23.800]   This actually happened.
[00:13:23.800 --> 00:13:26.360]   - Well, that's one of my worries today
[00:13:26.360 --> 00:13:31.360]   with the idea of fake news is that when nothing is real,
[00:13:31.360 --> 00:13:37.600]   then like you dilute the possibility of anything being true
[00:13:37.600 --> 00:13:41.000]   by conjuring up all kinds of conspiracy theories.
[00:13:41.000 --> 00:13:42.400]   And then you don't know what to believe.
[00:13:42.400 --> 00:13:45.240]   And then like the idea of truth,
[00:13:45.240 --> 00:13:47.840]   of objectivity is lost completely.
[00:13:47.840 --> 00:13:50.120]   Everybody has their own truth.
[00:13:50.120 --> 00:13:53.600]   - So you used to control information by censoring it.
[00:13:53.600 --> 00:13:55.880]   - And then the internet happened and governments are like,
[00:13:55.880 --> 00:13:58.360]   oh shit, we can't censor things anymore.
[00:13:58.360 --> 00:13:59.580]   I know what we'll do.
[00:13:59.580 --> 00:14:05.000]   You know, it's the old story of like tying a flag
[00:14:05.000 --> 00:14:07.080]   where the leprechaun tells you his gold is buried
[00:14:07.080 --> 00:14:09.120]   and you tie one flag and you make the leprechaun swear
[00:14:09.120 --> 00:14:11.240]   to not remove the flag and you come back to the field later
[00:14:11.240 --> 00:14:13.280]   with a shovel and there's flags everywhere.
[00:14:13.280 --> 00:14:16.360]   - That's one way to maintain privacy, right?
[00:14:16.360 --> 00:14:20.320]   Is like in order to protect the contents
[00:14:20.320 --> 00:14:21.840]   of this conversation, for example,
[00:14:21.840 --> 00:14:24.760]   we could just generate like millions
[00:14:24.760 --> 00:14:27.600]   of deep fake conversations where you and I talk
[00:14:27.600 --> 00:14:29.160]   and say random things.
[00:14:29.160 --> 00:14:31.160]   So this is just one of them and nobody knows
[00:14:31.160 --> 00:14:32.760]   which one was the real one.
[00:14:32.760 --> 00:14:34.480]   This could be fake right now.
[00:14:34.480 --> 00:14:36.240]   - Classic steganography technique.
[00:14:36.240 --> 00:14:39.960]   - Okay, another absurd question about intelligent life.
[00:14:39.960 --> 00:14:44.000]   'Cause you're an incredible programmer
[00:14:44.000 --> 00:14:45.560]   outside of everything else we'll talk about
[00:14:45.560 --> 00:14:46.820]   just as a programmer.
[00:14:49.360 --> 00:14:53.000]   Do you think intelligent beings out there,
[00:14:53.000 --> 00:14:54.560]   the civilizations that were out there
[00:14:54.560 --> 00:14:57.440]   had computers and programming?
[00:14:57.440 --> 00:15:01.480]   Do we naturally have to develop something
[00:15:01.480 --> 00:15:05.520]   where we engineer machines and are able to encode
[00:15:05.520 --> 00:15:08.720]   both knowledge into those machines
[00:15:08.720 --> 00:15:11.760]   and instructions that process that knowledge,
[00:15:11.760 --> 00:15:14.300]   process that information to make decisions
[00:15:14.300 --> 00:15:15.700]   and actions and so on?
[00:15:15.700 --> 00:15:18.360]   And would those programming languages,
[00:15:18.360 --> 00:15:21.360]   if you think they exist, be at all similar
[00:15:21.360 --> 00:15:22.800]   to anything we've developed?
[00:15:22.800 --> 00:15:26.640]   - So I don't see that much of a difference
[00:15:26.640 --> 00:15:29.440]   between quote unquote natural languages
[00:15:29.440 --> 00:15:30.760]   and programming languages.
[00:15:30.760 --> 00:15:36.720]   I think there's so many similarities.
[00:15:36.720 --> 00:15:39.960]   So when asked the question,
[00:15:39.960 --> 00:15:42.400]   what do alien languages look like?
[00:15:42.400 --> 00:15:46.520]   I imagine they're not all that dissimilar from ours.
[00:15:46.520 --> 00:15:49.400]   And I think translating in and out of them
[00:15:49.400 --> 00:15:53.040]   wouldn't be that crazy.
[00:15:53.040 --> 00:15:57.640]   - Well, it's difficult to compile like DNA to Python
[00:15:57.640 --> 00:15:59.280]   and then to see.
[00:15:59.280 --> 00:16:02.120]   There is a little bit of a gap in the kind of languages
[00:16:02.120 --> 00:16:06.920]   we use for touring machines
[00:16:06.920 --> 00:16:10.240]   and the kind of languages nature seems to use a little bit.
[00:16:10.240 --> 00:16:13.920]   Maybe that's just, we just haven't understood
[00:16:13.920 --> 00:16:16.440]   the kind of language that nature uses well yet.
[00:16:16.440 --> 00:16:17.900]   - DNA is a CAD model.
[00:16:17.900 --> 00:16:21.180]   It's not quite a programming language.
[00:16:21.180 --> 00:16:25.280]   It has no sort of a serial execution.
[00:16:25.280 --> 00:16:29.460]   It's not quite a, yeah, it's a CAD model.
[00:16:29.460 --> 00:16:30.880]   So I think in that sense,
[00:16:30.880 --> 00:16:32.480]   we actually completely understand it.
[00:16:32.480 --> 00:16:37.280]   The problem is, well, simulating on these CAD models.
[00:16:37.280 --> 00:16:38.360]   I played with it a bit this year,
[00:16:38.360 --> 00:16:41.080]   is super computationally intensive.
[00:16:41.080 --> 00:16:43.680]   If you want to go down to like the molecular level,
[00:16:43.680 --> 00:16:45.840]   where you need to go to see a lot of these phenomenon
[00:16:45.840 --> 00:16:46.880]   like protein folding.
[00:16:46.880 --> 00:16:52.160]   So yeah, it's not that we don't understand it.
[00:16:52.160 --> 00:16:55.080]   It just requires a whole lot of compute to kind of compile it.
[00:16:55.080 --> 00:16:56.640]   - For our human minds, it's inefficient
[00:16:56.640 --> 00:17:00.480]   both for the data representation and for the programming.
[00:17:00.480 --> 00:17:02.760]   - Yeah, it runs well on raw nature.
[00:17:02.760 --> 00:17:03.840]   It runs well on raw nature.
[00:17:03.840 --> 00:17:06.800]   And when we try to build emulators or simulators for that,
[00:17:06.800 --> 00:17:10.600]   well, they're mad slow and I've tried it.
[00:17:10.600 --> 00:17:14.240]   It runs in, yeah, you've commented elsewhere.
[00:17:14.240 --> 00:17:15.760]   I don't remember where,
[00:17:15.760 --> 00:17:20.760]   that one of the problems is simulating nature is tough.
[00:17:20.760 --> 00:17:24.240]   And if you want to sort of deploy a prototype,
[00:17:24.240 --> 00:17:28.000]   I forgot how you put it, but it made me laugh,
[00:17:28.000 --> 00:17:30.400]   but animals or humans would need to be involved
[00:17:30.400 --> 00:17:36.200]   in order to try to run some prototype code.
[00:17:38.120 --> 00:17:41.160]   - Like if we're talking about COVID and viruses and so on,
[00:17:41.160 --> 00:17:42.880]   if you were trying to engineer
[00:17:42.880 --> 00:17:45.000]   some kind of defense mechanisms,
[00:17:45.000 --> 00:17:49.560]   like a vaccine against COVID or all that kind of stuff,
[00:17:49.560 --> 00:17:52.040]   that doing any kind of experimentation,
[00:17:52.040 --> 00:17:54.000]   like you can with like autonomous vehicles
[00:17:54.000 --> 00:17:59.000]   would be very technically and ethically costly.
[00:17:59.000 --> 00:18:00.920]   - I'm not sure about that.
[00:18:00.920 --> 00:18:05.080]   I think you can do tons of crazy biology in test tubes.
[00:18:05.080 --> 00:18:08.480]   I think my bigger complaint is more,
[00:18:08.480 --> 00:18:10.120]   all the tools are so bad.
[00:18:10.120 --> 00:18:14.560]   - Like literally, you mean like libraries and--
[00:18:14.560 --> 00:18:16.120]   - I'm not pipetting shit.
[00:18:16.120 --> 00:18:20.120]   Like you're handing me a, I gotta, no, no, no, no.
[00:18:20.120 --> 00:18:21.300]   There has to be some.
[00:18:21.300 --> 00:18:26.120]   - Like automating stuff and like the,
[00:18:26.120 --> 00:18:28.320]   yeah, but human biology is messy.
[00:18:28.320 --> 00:18:29.160]   Like it seems--
[00:18:29.160 --> 00:18:31.240]   - But like, look at those Duranos videos.
[00:18:31.240 --> 00:18:32.080]   They were a joke.
[00:18:32.080 --> 00:18:33.440]   It's like a little gantry.
[00:18:33.440 --> 00:18:35.960]   It's like a little XY gantry high school science project
[00:18:35.960 --> 00:18:36.800]   with the pipette.
[00:18:36.800 --> 00:18:38.280]   I'm like, really?
[00:18:38.280 --> 00:18:39.240]   - Gotta be something better.
[00:18:39.240 --> 00:18:41.440]   - You can't build like nice microfluidics
[00:18:41.440 --> 00:18:45.320]   and I can program the computation to biointerface.
[00:18:45.320 --> 00:18:47.200]   I mean, this is gonna happen.
[00:18:47.200 --> 00:18:50.040]   But like right now, if you are asking me
[00:18:50.040 --> 00:18:53.620]   to pipette 50 milliliters of solution, I'm out.
[00:18:53.620 --> 00:18:55.520]   This is so crude.
[00:18:55.520 --> 00:18:56.720]   - Yeah.
[00:18:56.720 --> 00:19:00.000]   Okay, let's get all the crazy out of the way.
[00:19:00.000 --> 00:19:02.280]   So a bunch of people asked me,
[00:19:02.280 --> 00:19:05.120]   since we talked about the simulation last time,
[00:19:05.120 --> 00:19:06.920]   we talked about hacking the simulation.
[00:19:06.920 --> 00:19:09.920]   Do you have any updates, any insights
[00:19:09.920 --> 00:19:13.780]   about how we might be able to go about hacking simulation
[00:19:13.780 --> 00:19:16.300]   if we indeed do live in a simulation?
[00:19:16.300 --> 00:19:19.960]   - I think a lot of people misinterpreted
[00:19:19.960 --> 00:19:22.440]   the point of that South by talk.
[00:19:22.440 --> 00:19:23.560]   The point of the South by talk
[00:19:23.560 --> 00:19:25.640]   was not literally to hack the simulation.
[00:19:25.640 --> 00:19:27.940]   I think that this,
[00:19:31.800 --> 00:19:33.560]   this as an idea is literally just,
[00:19:33.560 --> 00:19:34.600]   I think, theoretical physics.
[00:19:34.600 --> 00:19:39.600]   I think that's the whole goal, right?
[00:19:39.600 --> 00:19:41.400]   You want your grand unified theory,
[00:19:41.400 --> 00:19:43.240]   but then, okay, build a grand unified theory,
[00:19:43.240 --> 00:19:45.160]   search for exploits, right?
[00:19:45.160 --> 00:19:47.680]   I think we're nowhere near actually there yet.
[00:19:47.680 --> 00:19:50.200]   My hope with that was just more to like,
[00:19:50.200 --> 00:19:52.760]   are you people kidding me
[00:19:52.760 --> 00:19:55.040]   with the things you spend time thinking about?
[00:19:55.040 --> 00:19:58.080]   Do you understand like kind of how small you are?
[00:19:58.520 --> 00:20:02.560]   You are bytes and God's computer, really?
[00:20:02.560 --> 00:20:04.920]   And the things that people get worked up about.
[00:20:04.920 --> 00:20:10.080]   - So basically, it was more a message
[00:20:10.080 --> 00:20:12.600]   of we should humble ourselves,
[00:20:12.600 --> 00:20:13.880]   that we get to,
[00:20:13.880 --> 00:20:19.520]   like what are we humans in this byte code?
[00:20:19.520 --> 00:20:22.460]   - Yeah, and not just humble ourselves,
[00:20:22.460 --> 00:20:25.000]   but like I'm not trying to like make people guilty
[00:20:25.000 --> 00:20:25.840]   or anything like that.
[00:20:25.840 --> 00:20:27.360]   I'm trying to say like literally,
[00:20:27.360 --> 00:20:30.280]   look at what you are spending time on, right?
[00:20:30.280 --> 00:20:31.120]   - What are you referring to?
[00:20:31.120 --> 00:20:32.520]   You're referring to the Kardashians?
[00:20:32.520 --> 00:20:34.240]   What are we talking about?
[00:20:34.240 --> 00:20:35.160]   Twitter? - I'm referring to,
[00:20:35.160 --> 00:20:38.120]   no, the Kardashians, everyone knows that's kind of fun.
[00:20:38.120 --> 00:20:41.880]   I'm referring more to like the economy,
[00:20:41.880 --> 00:20:48.040]   this idea that we gotta up our stock price.
[00:20:48.040 --> 00:20:55.440]   Or what is the goal function of humanity?
[00:20:55.440 --> 00:20:57.640]   - You don't like the game of capitalism?
[00:20:57.640 --> 00:20:59.400]   Like you don't like the games we've constructed
[00:20:59.400 --> 00:21:00.720]   for ourselves as humans?
[00:21:00.720 --> 00:21:02.880]   - I'm a big fan of capitalism.
[00:21:02.880 --> 00:21:04.320]   I don't think that's really the game
[00:21:04.320 --> 00:21:05.160]   we're playing right now.
[00:21:05.160 --> 00:21:07.280]   I think we're playing a different game
[00:21:07.280 --> 00:21:08.700]   where the rules are rigged.
[00:21:08.700 --> 00:21:12.600]   - Okay, which games are interesting to you
[00:21:12.600 --> 00:21:14.800]   that we humans have constructed and which aren't?
[00:21:14.800 --> 00:21:18.440]   Which are productive and which are not?
[00:21:18.440 --> 00:21:21.920]   - Actually, maybe that's the real point of the talk.
[00:21:21.920 --> 00:21:25.120]   It's like, stop playing these fake human games.
[00:21:25.120 --> 00:21:26.840]   There's a real game here.
[00:21:26.840 --> 00:21:28.720]   We can play the real game.
[00:21:28.720 --> 00:21:31.320]   The real game is, you know, nature wrote the rules.
[00:21:31.320 --> 00:21:32.600]   This is a real game.
[00:21:32.600 --> 00:21:35.240]   There still is a game to play.
[00:21:35.240 --> 00:21:37.000]   - But if you look at, sorry to interrupt,
[00:21:37.000 --> 00:21:38.440]   I don't know if you've seen the Instagram account,
[00:21:38.440 --> 00:21:40.280]   Nature Is Metal.
[00:21:40.280 --> 00:21:42.880]   The game that nature seems to be playing
[00:21:42.880 --> 00:21:47.400]   is a lot more cruel than we humans want to put up with.
[00:21:47.400 --> 00:21:49.600]   Or at least we see it as cruel.
[00:21:49.600 --> 00:21:52.840]   It's like the bigger thing eats the smaller thing
[00:21:53.720 --> 00:21:58.240]   and does it to impress another big thing
[00:21:58.240 --> 00:22:00.520]   so it can mate with that thing.
[00:22:00.520 --> 00:22:01.360]   And that's it.
[00:22:01.360 --> 00:22:04.120]   That seems to be the entirety of it.
[00:22:04.120 --> 00:22:05.240]   - Well.
[00:22:05.240 --> 00:22:07.320]   - There's no art, there's no music,
[00:22:07.320 --> 00:22:09.960]   there's no comma AI,
[00:22:09.960 --> 00:22:11.760]   there's no comma one, no comma two,
[00:22:11.760 --> 00:22:15.000]   no George Hotz with his brilliant talks
[00:22:15.000 --> 00:22:17.080]   at South by Southwest.
[00:22:17.080 --> 00:22:17.960]   - I disagree though.
[00:22:17.960 --> 00:22:19.680]   I disagree that this is what nature is.
[00:22:19.680 --> 00:22:22.480]   I think nature just provided
[00:22:22.480 --> 00:22:26.840]   basically a open world MMORPG.
[00:22:26.840 --> 00:22:29.920]   And you know, here it's open world.
[00:22:29.920 --> 00:22:31.160]   I mean, if that's the game you want to play,
[00:22:31.160 --> 00:22:32.360]   you can play that game.
[00:22:32.360 --> 00:22:33.840]   - But isn't that beautiful?
[00:22:33.840 --> 00:22:35.600]   I don't know if you played Diablo.
[00:22:35.600 --> 00:22:39.280]   They used to have a, I think, cow level where it's,
[00:22:39.280 --> 00:22:42.960]   so everybody will go, just,
[00:22:42.960 --> 00:22:44.480]   they figured out this,
[00:22:44.480 --> 00:22:48.440]   like the best way to gain like experience points
[00:22:48.440 --> 00:22:51.040]   is to just slaughter cows over and over and over.
[00:22:52.240 --> 00:22:55.920]   And so they figured out this little sub game
[00:22:55.920 --> 00:22:56.960]   within the bigger game
[00:22:56.960 --> 00:22:58.840]   that this is the most efficient way
[00:22:58.840 --> 00:22:59.920]   to get experience points.
[00:22:59.920 --> 00:23:01.880]   And everybody somehow agreed
[00:23:01.880 --> 00:23:04.480]   that getting experience points in RPG context,
[00:23:04.480 --> 00:23:06.520]   where you always want to be getting more stuff,
[00:23:06.520 --> 00:23:09.160]   more skills, more levels, keep advancing,
[00:23:09.160 --> 00:23:10.480]   that seems to be good.
[00:23:10.480 --> 00:23:12.680]   So might as well spend,
[00:23:12.680 --> 00:23:16.200]   sacrifice actual enjoyment of playing a game,
[00:23:16.200 --> 00:23:17.680]   exploring a world,
[00:23:17.680 --> 00:23:21.600]   and spending like hundreds of hours of your time
[00:23:21.600 --> 00:23:22.440]   in cow level.
[00:23:22.440 --> 00:23:25.560]   I mean, the number of hours I spent in cow level,
[00:23:25.560 --> 00:23:28.160]   I'm not like the most impressive person
[00:23:28.160 --> 00:23:30.440]   'cause people have probably thousands of hours there,
[00:23:30.440 --> 00:23:31.600]   but it's ridiculous.
[00:23:31.600 --> 00:23:33.960]   So that's a little absurd game
[00:23:33.960 --> 00:23:37.520]   that brought me joy in some weird dopamine drug kind of way.
[00:23:37.520 --> 00:23:38.520]   - Yeah.
[00:23:38.520 --> 00:23:40.080]   - So you don't like those games.
[00:23:40.080 --> 00:23:45.080]   You don't think that's us humans feeling the nature.
[00:23:45.080 --> 00:23:47.320]   - I think so.
[00:23:47.320 --> 00:23:49.640]   - And that was the point of the talk.
[00:23:49.640 --> 00:23:50.480]   - Yeah.
[00:23:50.480 --> 00:23:51.440]   - So how do we hack it then?
[00:23:51.440 --> 00:23:53.800]   - Well, I want to live forever and--
[00:23:53.800 --> 00:23:54.640]   - Wait.
[00:23:54.640 --> 00:23:55.840]   - I want to live forever.
[00:23:55.840 --> 00:23:56.680]   And this is--
[00:23:56.680 --> 00:23:57.520]   - That's the goal.
[00:23:57.520 --> 00:23:59.240]   - Well, that's a game against nature.
[00:23:59.240 --> 00:24:00.080]   - Yeah.
[00:24:00.080 --> 00:24:02.600]   Immortality is the good objective function to you?
[00:24:02.600 --> 00:24:05.080]   - I mean, start there and then you can do whatever else
[00:24:05.080 --> 00:24:07.400]   you want 'cause you got a long time.
[00:24:07.400 --> 00:24:10.760]   - What if immortality makes the game just totally not fun?
[00:24:10.760 --> 00:24:15.760]   I mean, like, why do you assume immortality is somehow--
[00:24:15.760 --> 00:24:16.960]   - It's not--
[00:24:16.960 --> 00:24:18.160]   - A good objective function?
[00:24:18.160 --> 00:24:19.920]   - It's not immortality that I want.
[00:24:19.920 --> 00:24:22.560]   A true immortality where I could not die,
[00:24:22.560 --> 00:24:25.000]   I would prefer what we have right now.
[00:24:25.000 --> 00:24:28.120]   But I want to choose my own death, of course.
[00:24:28.120 --> 00:24:29.840]   I don't want nature to decide when I die,
[00:24:29.840 --> 00:24:31.720]   I'm going to win, I'm going to be you.
[00:24:31.720 --> 00:24:36.920]   - And then at some point, if you choose, commit suicide.
[00:24:36.920 --> 00:24:40.400]   Like, how long do you think you'd live?
[00:24:40.400 --> 00:24:43.120]   - Until I get bored.
[00:24:43.120 --> 00:24:48.040]   - See, I don't think people, like, brilliant people like you
[00:24:48.040 --> 00:24:52.400]   that really ponder living a long time
[00:24:52.400 --> 00:24:57.400]   are really considering how meaningless life becomes.
[00:24:57.400 --> 00:24:59.640]   - Well, I want to know everything
[00:24:59.640 --> 00:25:00.960]   and then I'm ready to die.
[00:25:00.960 --> 00:25:04.480]   As long as there's--
[00:25:04.480 --> 00:25:05.800]   - Yeah, but why do you want, isn't it possible
[00:25:05.800 --> 00:25:09.720]   that you want to know everything because it's finite?
[00:25:09.720 --> 00:25:11.760]   Like, the reason you want to know, quote unquote,
[00:25:11.760 --> 00:25:14.440]   everything is because you don't have enough time
[00:25:14.440 --> 00:25:16.400]   to know everything.
[00:25:16.400 --> 00:25:18.760]   And once you have unlimited time,
[00:25:18.760 --> 00:25:22.200]   then you realize, like, why do anything?
[00:25:22.200 --> 00:25:24.040]   Like, why learn anything?
[00:25:24.040 --> 00:25:27.080]   - I want to know everything and then I'm ready to die.
[00:25:27.080 --> 00:25:28.440]   - So you have, yeah, okay.
[00:25:28.440 --> 00:25:30.960]   - It's not a, like, it's a terminal value.
[00:25:30.960 --> 00:25:34.760]   It's not in service of anything else.
[00:25:34.760 --> 00:25:36.360]   - I'm conscious of the possibility,
[00:25:36.360 --> 00:25:39.000]   this is not a certainty, but the possibility
[00:25:39.000 --> 00:25:43.160]   of that engine of curiosity that you're speaking to
[00:25:43.160 --> 00:25:48.160]   is actually a symptom of the finiteness of life.
[00:25:48.160 --> 00:25:51.920]   Like, without that finiteness,
[00:25:51.920 --> 00:25:57.000]   your curiosity would vanish, like a morning fog.
[00:25:57.000 --> 00:25:57.840]   - All right, cool.
[00:25:57.840 --> 00:25:59.280]   - Pulkowski talked about love like that.
[00:25:59.280 --> 00:26:01.320]   - Then let me solve immortality
[00:26:01.320 --> 00:26:02.880]   and let me change the thing in my brain
[00:26:02.880 --> 00:26:04.680]   that reminds me of the fact that I'm immortal,
[00:26:04.680 --> 00:26:06.240]   tells me that life is finite shit.
[00:26:06.240 --> 00:26:09.480]   Maybe I'll have it tell me that life ends next week, right?
[00:26:09.480 --> 00:26:12.680]   I'm okay with some self-manipulation like that.
[00:26:12.680 --> 00:26:14.440]   I'm okay with deceiving myself.
[00:26:14.440 --> 00:26:17.040]   - Oh, changing the code.
[00:26:17.040 --> 00:26:18.320]   - Yeah, if that's the problem, right?
[00:26:18.320 --> 00:26:21.840]   If the problem is that I will no longer have that curiosity,
[00:26:21.840 --> 00:26:24.920]   I'd like to have backup copies of myself, which I--
[00:26:24.920 --> 00:26:25.760]   - Revert, yeah.
[00:26:25.760 --> 00:26:27.480]   - Well, which I check in with occasionally
[00:26:27.480 --> 00:26:29.240]   to make sure they're okay with the trajectory
[00:26:29.240 --> 00:26:31.000]   and they can kind of override it.
[00:26:31.000 --> 00:26:33.160]   Maybe a nice, like, I think of like those wave nets,
[00:26:33.160 --> 00:26:35.200]   those like logarithmic, go back to the copies.
[00:26:35.200 --> 00:26:36.720]   - But sometimes it's not reversible.
[00:26:36.720 --> 00:26:39.680]   Like, I've done this with video games.
[00:26:39.680 --> 00:26:41.600]   Once you figure out the cheat code
[00:26:41.600 --> 00:26:43.960]   or like you look up how to cheat old school,
[00:26:43.960 --> 00:26:46.880]   like single player, it ruins the game for you.
[00:26:46.880 --> 00:26:48.160]   - Absolutely, I know that feeling.
[00:26:48.160 --> 00:26:51.880]   But again, that just means our brain manipulation
[00:26:51.880 --> 00:26:53.260]   technology is not good enough yet.
[00:26:53.260 --> 00:26:54.720]   Remove that cheat code from your brain.
[00:26:54.720 --> 00:26:57.040]   - But what if we, so it's also possible
[00:26:57.040 --> 00:26:59.380]   that if we figure out immortality,
[00:26:59.380 --> 00:27:03.440]   that all of us will kill ourselves
[00:27:03.440 --> 00:27:08.440]   before we advance far enough to be able to revert the change.
[00:27:08.440 --> 00:27:11.560]   - I'm not killing myself till I know everything, so.
[00:27:11.560 --> 00:27:15.020]   - That's what you say now because your life is finite.
[00:27:15.020 --> 00:27:18.400]   - You know, I think, yeah, self-modifying systems
[00:27:18.400 --> 00:27:21.040]   gets, comes up with all these hairy complexities.
[00:27:21.040 --> 00:27:23.000]   And can I promise that I'll do it perfectly?
[00:27:23.000 --> 00:27:25.960]   No, but I think I can put good safety structures in place.
[00:27:25.960 --> 00:27:31.000]   - So that talk in your thinking here is not literally
[00:27:31.000 --> 00:27:38.000]   referring to a simulation in that our universe
[00:27:38.600 --> 00:27:42.240]   is a kind of computer program running on a computer.
[00:27:42.240 --> 00:27:44.200]   That's more of a thought experiment.
[00:27:44.200 --> 00:27:49.240]   Do you also think of the potential of the sort of
[00:27:49.240 --> 00:27:55.200]   Bostrom, Elon Musk, and others that talk about
[00:27:55.200 --> 00:27:59.680]   an actual program that simulates our universe?
[00:27:59.680 --> 00:28:01.960]   - Oh, I don't doubt that we're in a simulation.
[00:28:01.960 --> 00:28:05.300]   I just think that it's not quite that important.
[00:28:05.300 --> 00:28:06.920]   I mean, I'm interested only in simulation theory
[00:28:06.920 --> 00:28:09.720]   as far as like it gives me power over nature.
[00:28:09.720 --> 00:28:13.120]   If it's totally unfalsifiable, then who cares?
[00:28:13.120 --> 00:28:15.280]   - I mean, what do you think that experiment would look like?
[00:28:15.280 --> 00:28:17.720]   Like somebody on Twitter asked,
[00:28:17.720 --> 00:28:20.760]   ask George what signs we would look for
[00:28:20.760 --> 00:28:22.960]   to know whether or not we're in a simulation,
[00:28:22.960 --> 00:28:25.160]   which is exactly what you're asking is like,
[00:28:25.160 --> 00:28:29.480]   the step that precedes the step of knowing
[00:28:29.480 --> 00:28:32.200]   how to get more power from this knowledge
[00:28:32.200 --> 00:28:35.200]   is to get an indication that there's some power to be gained.
[00:28:35.200 --> 00:28:39.560]   So get an indication that you can discover
[00:28:39.560 --> 00:28:42.120]   and exploit cracks in the simulation.
[00:28:42.120 --> 00:28:45.360]   Or it doesn't have to be in the physics of the universe.
[00:28:45.360 --> 00:28:46.720]   - Yeah.
[00:28:46.720 --> 00:28:50.640]   Show me, I mean, like a memory leak would be cool.
[00:28:50.640 --> 00:28:54.000]   Some scrying technology, you know?
[00:28:54.000 --> 00:28:55.240]   - What kind of technology?
[00:28:55.240 --> 00:28:56.240]   - Scrying?
[00:28:56.240 --> 00:28:57.080]   - What's that?
[00:28:57.080 --> 00:29:01.760]   - Oh, that's a weird, scrying is the paranormal ability
[00:29:01.760 --> 00:29:04.440]   to like remote viewing,
[00:29:04.440 --> 00:29:07.000]   like being able to see somewhere where you're not.
[00:29:07.000 --> 00:29:10.080]   So, you know, I don't think you can do it
[00:29:10.080 --> 00:29:13.240]   by chanting in a room, but if we could find,
[00:29:13.240 --> 00:29:14.760]   it's a memory leak, basically.
[00:29:14.760 --> 00:29:17.320]   - It's a memory leak.
[00:29:17.320 --> 00:29:19.960]   Yeah, you're able to access parts you're not supposed to.
[00:29:19.960 --> 00:29:20.800]   - Yeah, yeah, yeah.
[00:29:20.800 --> 00:29:22.120]   - And thereby discover a shortcut.
[00:29:22.120 --> 00:29:24.720]   - Yeah, maybe memory leak means the other thing as well,
[00:29:24.720 --> 00:29:25.560]   but I mean like, yeah,
[00:29:25.560 --> 00:29:28.120]   like an ability to read arbitrary memory, right?
[00:29:28.120 --> 00:29:29.840]   And that one's not that horrifying.
[00:29:29.840 --> 00:29:31.400]   The right ones start to be horrifying.
[00:29:31.400 --> 00:29:32.240]   - Read, right.
[00:29:32.240 --> 00:29:34.880]   So the reading is not the problem.
[00:29:34.880 --> 00:29:37.240]   - Yeah, it's like "Heartfleet" for the universe.
[00:29:37.240 --> 00:29:40.760]   - Oh boy, the writing is a big, big problem.
[00:29:40.760 --> 00:29:42.080]   It's a big problem.
[00:29:42.080 --> 00:29:44.640]   It's the moment you can write anything,
[00:29:44.640 --> 00:29:46.500]   even if it's just random noise.
[00:29:46.500 --> 00:29:49.200]   That's terrifying.
[00:29:49.200 --> 00:29:51.600]   - I mean, even without that,
[00:29:51.600 --> 00:29:54.200]   like even some of the nanotech stuff that's coming,
[00:29:54.200 --> 00:29:55.240]   I think is...
[00:29:55.240 --> 00:29:58.360]   - I don't know if you're paying attention,
[00:29:58.360 --> 00:30:00.520]   but actually Eric Weinstein came out
[00:30:00.520 --> 00:30:02.240]   with the theory of everything.
[00:30:02.240 --> 00:30:03.560]   I mean, that came out.
[00:30:03.560 --> 00:30:05.440]   He's been working on a theory of everything
[00:30:05.440 --> 00:30:08.040]   in the physics world called geometric unity.
[00:30:08.040 --> 00:30:11.640]   And then for me, from a computer science person like you,
[00:30:11.640 --> 00:30:14.200]   Stephen Wolfram's theory of everything,
[00:30:14.200 --> 00:30:17.640]   of like hypergraphs is super interesting and beautiful,
[00:30:17.640 --> 00:30:19.400]   but not from a physics perspective,
[00:30:19.400 --> 00:30:20.920]   but from a computational perspective.
[00:30:20.920 --> 00:30:23.000]   I don't know, have you paid attention to any of that?
[00:30:23.000 --> 00:30:26.400]   - So again, like what would make me pay attention
[00:30:26.400 --> 00:30:29.520]   and like why like I hate string theory is,
[00:30:29.520 --> 00:30:31.760]   okay, make a testable prediction, right?
[00:30:31.760 --> 00:30:33.680]   I'm only interested in,
[00:30:33.680 --> 00:30:36.040]   I'm not interested in theories for their intrinsic beauty.
[00:30:36.040 --> 00:30:37.080]   I'm interested in theories
[00:30:37.080 --> 00:30:38.920]   that give me power over the universe.
[00:30:38.920 --> 00:30:42.360]   So if these theories do, I'm very interested.
[00:30:42.360 --> 00:30:45.120]   - Can I just say how beautiful that is?
[00:30:45.120 --> 00:30:47.160]   Because a lot of physicists say,
[00:30:47.160 --> 00:30:50.000]   I'm interested in experimental validation,
[00:30:50.000 --> 00:30:52.940]   and they skip out the part where they say,
[00:30:52.940 --> 00:30:55.520]   to give me more power in the universe.
[00:30:55.520 --> 00:30:57.520]   I just love the...
[00:30:57.520 --> 00:30:58.760]   - No, I want, I want, I want...
[00:30:58.760 --> 00:30:59.760]   - The clarity of that.
[00:30:59.760 --> 00:31:02.040]   - I want a hundred gigahertz processors.
[00:31:02.040 --> 00:31:04.120]   I want transistors that are smaller than atoms.
[00:31:04.120 --> 00:31:05.620]   I want like power.
[00:31:05.620 --> 00:31:10.560]   - That's, that's true.
[00:31:10.560 --> 00:31:12.440]   And that's where people from aliens
[00:31:12.440 --> 00:31:13.520]   to this kind of technology,
[00:31:13.520 --> 00:31:16.920]   where people are worried that governments,
[00:31:16.920 --> 00:31:19.320]   like who owns that power?
[00:31:19.320 --> 00:31:20.760]   Is it a George Hartz?
[00:31:20.760 --> 00:31:25.000]   Is it thousands of distributed hackers across the world?
[00:31:25.000 --> 00:31:26.640]   Is it governments?
[00:31:26.640 --> 00:31:28.700]   You know, is it Mark Zuckerberg?
[00:31:28.700 --> 00:31:32.520]   There's a lot of people that,
[00:31:32.520 --> 00:31:35.560]   I don't know if anyone trusts any one individual with power.
[00:31:35.560 --> 00:31:37.440]   So they're always worried.
[00:31:37.440 --> 00:31:39.400]   - It's the beauty of blockchains.
[00:31:39.400 --> 00:31:43.200]   - That's the beauty of blockchains, which we'll talk about.
[00:31:43.200 --> 00:31:46.280]   On Twitter, somebody pointed me to a story,
[00:31:46.280 --> 00:31:49.320]   a bunch of people pointed me to a story a few months ago,
[00:31:49.320 --> 00:31:51.680]   where you went into a restaurant in New York,
[00:31:51.680 --> 00:31:53.720]   and you can correct me if any of this is wrong,
[00:31:53.720 --> 00:31:55.840]   and ran into a bunch of folks
[00:31:55.840 --> 00:31:58.820]   from a company, a crypto company,
[00:31:58.820 --> 00:32:00.620]   who are trying to scale up Ethereum.
[00:32:00.620 --> 00:32:03.300]   And they had a technical deadline
[00:32:03.300 --> 00:32:07.380]   related to a solidity to OVM compiler.
[00:32:07.380 --> 00:32:09.660]   So these are all Ethereum technologies.
[00:32:09.660 --> 00:32:13.440]   So you stepped in, they recognized you,
[00:32:13.440 --> 00:32:16.220]   pulled you aside, explained their problem,
[00:32:16.220 --> 00:32:19.580]   and you stepped in and helped them solve the problem,
[00:32:19.580 --> 00:32:22.940]   thereby creating legend status story.
[00:32:24.820 --> 00:32:28.960]   - Can you tell me the story in a little more detail?
[00:32:28.960 --> 00:32:30.600]   It seems kind of incredible.
[00:32:30.600 --> 00:32:32.400]   Did this happen?
[00:32:32.400 --> 00:32:34.080]   - Yeah, yeah, it's a true story, it's a true story.
[00:32:34.080 --> 00:32:36.600]   I mean, they wrote a very flattering account of it.
[00:32:36.600 --> 00:32:43.800]   So Optimism is the, the company's called Optimism,
[00:32:43.800 --> 00:32:45.440]   spin-off of Plasma.
[00:32:45.440 --> 00:32:47.840]   They're trying to build L2 solutions on Ethereum.
[00:32:47.840 --> 00:32:52.600]   So right now, every Ethereum node
[00:32:52.600 --> 00:32:56.420]   has to run every transaction on the Ethereum network.
[00:32:56.420 --> 00:32:58.540]   And this kind of doesn't scale, right?
[00:32:58.540 --> 00:32:59.980]   Because if you have N computers,
[00:32:59.980 --> 00:33:02.300]   well, you know, if that becomes two N computers,
[00:33:02.300 --> 00:33:05.140]   you actually still get the same amount of compute.
[00:33:05.140 --> 00:33:07.820]   Right, this is like O of one scaling,
[00:33:07.820 --> 00:33:10.140]   because they all have to run it.
[00:33:10.140 --> 00:33:12.820]   Okay, fine, you get more blockchain security,
[00:33:12.820 --> 00:33:15.340]   but like, blockchain's already so secure.
[00:33:15.340 --> 00:33:17.740]   Can we trade some of that off for speed?
[00:33:17.740 --> 00:33:20.460]   So that's kind of what these L2 solutions are.
[00:33:20.460 --> 00:33:23.000]   They built this thing, which kind of,
[00:33:23.000 --> 00:33:26.280]   kind of sandbox for Ethereum contracts,
[00:33:26.280 --> 00:33:28.200]   so they can run it in this L2 world,
[00:33:28.200 --> 00:33:30.920]   and it can't do certain things in L world, in L1.
[00:33:30.920 --> 00:33:32.400]   - Can I ask you for some definitions?
[00:33:32.400 --> 00:33:33.400]   What's L2?
[00:33:33.400 --> 00:33:34.840]   - Oh, L2 is layer two.
[00:33:34.840 --> 00:33:37.160]   So L1 is like the base Ethereum chain,
[00:33:37.160 --> 00:33:40.920]   and then layer two is like a computational layer
[00:33:40.920 --> 00:33:44.440]   that runs elsewhere,
[00:33:44.440 --> 00:33:47.680]   but still is kind of secured by layer one.
[00:33:47.680 --> 00:33:49.720]   - And I'm sure a lot of people know,
[00:33:49.720 --> 00:33:51.960]   but Ethereum is a cryptocurrency,
[00:33:51.960 --> 00:33:53.720]   probably one of the most popular cryptocurrencies,
[00:33:53.720 --> 00:33:55.320]   second to Bitcoin,
[00:33:55.320 --> 00:33:58.800]   and a lot of interesting technological innovations there.
[00:33:58.800 --> 00:34:01.880]   Maybe you could also slip in,
[00:34:01.880 --> 00:34:03.240]   whenever you talk about this,
[00:34:03.240 --> 00:34:06.400]   any things that are exciting to you in the Ethereum space,
[00:34:06.400 --> 00:34:07.800]   and why Ethereum?
[00:34:07.800 --> 00:34:11.120]   - Well, I mean, Bitcoin is not Turing complete.
[00:34:11.120 --> 00:34:13.800]   Well, Ethereum is not technically Turing complete
[00:34:13.800 --> 00:34:16.040]   with a gas limit, but close enough.
[00:34:16.040 --> 00:34:16.880]   - With a gas limit?
[00:34:16.880 --> 00:34:18.200]   What's the gas limit?
[00:34:18.200 --> 00:34:19.080]   Resources?
[00:34:19.080 --> 00:34:21.160]   - Yeah, I mean, no computer is actually Turing complete.
[00:34:21.160 --> 00:34:22.000]   - Right.
[00:34:22.000 --> 00:34:24.360]   - They're your finite RAM, you know?
[00:34:24.360 --> 00:34:25.200]   I can actually solve the whole problem.
[00:34:25.200 --> 00:34:26.720]   - What's the word gas limit?
[00:34:26.720 --> 00:34:28.640]   You have so many brilliant words.
[00:34:28.640 --> 00:34:29.480]   I'm not even gonna ask.
[00:34:29.480 --> 00:34:30.920]   - No, that's not my word.
[00:34:30.920 --> 00:34:32.200]   That's Ethereum's word.
[00:34:32.200 --> 00:34:33.040]   - Gas limit.
[00:34:33.040 --> 00:34:35.320]   - Ethereum, you have to spend gas per instruction.
[00:34:35.320 --> 00:34:37.800]   So like different op codes use different amounts of gas,
[00:34:37.800 --> 00:34:40.640]   and you buy gas with ether to prevent people
[00:34:40.640 --> 00:34:42.720]   from basically DDoSing the network.
[00:34:42.720 --> 00:34:45.400]   - So Bitcoin is proof of work,
[00:34:45.400 --> 00:34:47.120]   and then what's Ethereum?
[00:34:47.120 --> 00:34:48.320]   - It's also proof of work.
[00:34:48.320 --> 00:34:51.000]   They're working on some proof of stake Ethereum 2.0 stuff,
[00:34:51.000 --> 00:34:52.680]   but right now it's proof of work.
[00:34:52.680 --> 00:34:54.640]   It uses a different hash function from Bitcoin
[00:34:54.640 --> 00:34:57.280]   that's more ASIC resistance 'cause you need RAM.
[00:34:57.280 --> 00:34:59.920]   - So we're all talking about Ethereum 1.0.
[00:34:59.920 --> 00:35:03.840]   So what were they trying to do to scale this whole process?
[00:35:03.840 --> 00:35:04.800]   - So they were like, well,
[00:35:04.800 --> 00:35:07.720]   if we could run contracts elsewhere,
[00:35:07.720 --> 00:35:10.360]   and then only save the results of that computation,
[00:35:10.360 --> 00:35:14.240]   you know, well, we don't actually have to do the compute
[00:35:14.240 --> 00:35:15.080]   on the chain.
[00:35:15.080 --> 00:35:15.920]   We can do the compute off chain
[00:35:15.920 --> 00:35:17.360]   and just post what the results are.
[00:35:17.360 --> 00:35:18.720]   Now, the problem with that is,
[00:35:18.720 --> 00:35:21.040]   well, somebody could lie about what the results are.
[00:35:21.040 --> 00:35:23.200]   So you need a resolution mechanism,
[00:35:23.200 --> 00:35:26.440]   and the resolution mechanism can be really expensive
[00:35:26.440 --> 00:35:28.920]   because, you know, you just have to make sure
[00:35:28.920 --> 00:35:31.040]   that like the person who is saying,
[00:35:31.040 --> 00:35:33.720]   look, I swear that this is the real computation.
[00:35:33.720 --> 00:35:36.600]   I'm staking $10,000 on that fact,
[00:35:36.600 --> 00:35:38.920]   and if you prove it wrong,
[00:35:38.920 --> 00:35:42.760]   yeah, it might cost you $3,000 in gas fees to prove wrong,
[00:35:42.760 --> 00:35:44.720]   but you'll get the $10,000 bounty.
[00:35:44.720 --> 00:35:47.200]   So you can secure using those kinds of systems.
[00:35:47.200 --> 00:35:52.880]   So it's effectively a sandbox, which runs contracts,
[00:35:52.880 --> 00:35:55.560]   and like, just like any kind of normal sandbox,
[00:35:55.560 --> 00:35:59.360]   you have to like replace syscalls with, you know,
[00:35:59.360 --> 00:36:00.680]   calls into the hypervisor.
[00:36:00.680 --> 00:36:05.040]   - Sandbox, syscalls, hypervisor.
[00:36:05.040 --> 00:36:06.360]   What do these things mean?
[00:36:06.360 --> 00:36:09.160]   As long as it's interesting to talk about.
[00:36:09.160 --> 00:36:11.240]   - Yeah, I mean, you can take like the Chrome sandbox
[00:36:11.240 --> 00:36:12.680]   is maybe the one to think about, right?
[00:36:12.680 --> 00:36:15.960]   So the Chrome process that's doing a rendering
[00:36:15.960 --> 00:36:18.760]   can't, for example, read a file from the file system.
[00:36:18.760 --> 00:36:21.760]   It has, if it tries to make an open syscall in Linux,
[00:36:21.760 --> 00:36:23.680]   the open syscall, you can't make an open syscall,
[00:36:23.680 --> 00:36:24.720]   no, no, no.
[00:36:24.720 --> 00:36:29.120]   You have to request from the kind of hypervisor process,
[00:36:29.120 --> 00:36:31.640]   or like, I don't know what's called in Chrome,
[00:36:31.640 --> 00:36:36.360]   but the, hey, could you open this file for me?
[00:36:36.360 --> 00:36:37.480]   And then it does all these checks,
[00:36:37.480 --> 00:36:39.160]   and then it passes the file handle back in
[00:36:39.160 --> 00:36:40.000]   if it's approved.
[00:36:40.000 --> 00:36:41.120]   - Got it.
[00:36:41.120 --> 00:36:42.600]   - So that's, yeah.
[00:36:42.600 --> 00:36:45.280]   - So what's the, in the context of Ethereum,
[00:36:45.280 --> 00:36:47.160]   what are the boundaries of the sandbox
[00:36:47.160 --> 00:36:48.360]   that we're talking about?
[00:36:48.360 --> 00:36:50.760]   - Well, like one of the calls that you,
[00:36:50.760 --> 00:36:53.920]   actually reading and writing any state
[00:36:53.920 --> 00:36:56.920]   to the Ethereum contract, to the Ethereum blockchain.
[00:36:56.920 --> 00:37:01.000]   Writing state is one of those calls
[00:37:01.000 --> 00:37:04.440]   that you're going to have to sandbox in layer two,
[00:37:04.440 --> 00:37:08.080]   because if you let layer two just arbitrarily write
[00:37:08.080 --> 00:37:09.440]   to the Ethereum blockchain.
[00:37:10.440 --> 00:37:15.120]   - So layer two is really sitting on top of layer one.
[00:37:15.120 --> 00:37:17.160]   So you're going to have a lot of different kinds of ideas
[00:37:17.160 --> 00:37:18.200]   that you can play with.
[00:37:18.200 --> 00:37:19.040]   - Yeah.
[00:37:19.040 --> 00:37:21.360]   - And they're all, they're not fundamentally changing
[00:37:21.360 --> 00:37:25.120]   the source code level of Ethereum.
[00:37:25.120 --> 00:37:28.880]   - Well, you have to replace a bunch of calls
[00:37:28.880 --> 00:37:31.120]   with calls into the hypervisor.
[00:37:31.120 --> 00:37:33.840]   So instead of doing the syscall directly,
[00:37:33.840 --> 00:37:37.360]   you replace it with a call to the hypervisor.
[00:37:37.360 --> 00:37:41.680]   So originally they were doing this by first running the,
[00:37:41.680 --> 00:37:43.560]   so Solidity is the language
[00:37:43.560 --> 00:37:45.480]   that most Ethereum contracts are written in.
[00:37:45.480 --> 00:37:47.440]   It compiles to a bytecode.
[00:37:47.440 --> 00:37:50.080]   And then they wrote this thing they call the transpiler.
[00:37:50.080 --> 00:37:52.480]   And the transpiler took the bytecode
[00:37:52.480 --> 00:37:56.040]   and it transpiled it into OVM safe bytecode.
[00:37:56.040 --> 00:37:57.360]   Basically bytecode that didn't make
[00:37:57.360 --> 00:37:58.840]   any of those restricted syscalls
[00:37:58.840 --> 00:38:00.740]   and added the calls to the hypervisor.
[00:38:00.740 --> 00:38:04.680]   This transpiler was a 3000 line mess.
[00:38:04.680 --> 00:38:07.120]   And it's hard to do.
[00:38:07.120 --> 00:38:09.080]   It's hard to do if you're trying to do it like that
[00:38:09.080 --> 00:38:12.200]   because you have to kind of like deconstruct the bytecode,
[00:38:12.200 --> 00:38:15.400]   change things about it and then reconstruct it.
[00:38:15.400 --> 00:38:17.760]   And I mean, as soon as I hear this, I'm like,
[00:38:17.760 --> 00:38:19.960]   well, why don't you just change the compiler?
[00:38:19.960 --> 00:38:20.800]   Right?
[00:38:20.800 --> 00:38:22.360]   Why not the first place you build the bytecode,
[00:38:22.360 --> 00:38:23.720]   just do it in the compiler?
[00:38:23.720 --> 00:38:29.160]   So yeah, you know, I asked them how much they wanted it.
[00:38:29.160 --> 00:38:32.200]   Of course, measured in dollars and I'm like, well, okay.
[00:38:32.200 --> 00:38:34.600]   And yeah.
[00:38:34.600 --> 00:38:35.920]   - And you wrote the compiler.
[00:38:35.920 --> 00:38:39.280]   - Yeah, I modified, I wrote a 300 line diff to the compiler.
[00:38:39.280 --> 00:38:40.840]   It's open source, you can look at it.
[00:38:40.840 --> 00:38:42.880]   - Yeah, I looked at the code last night.
[00:38:42.880 --> 00:38:44.560]   (laughing)
[00:38:44.560 --> 00:38:45.480]   - It's cute.
[00:38:45.480 --> 00:38:46.640]   - Yeah, exactly.
[00:38:46.640 --> 00:38:49.440]   Cute is a good word for it.
[00:38:49.440 --> 00:38:52.000]   And it's C++.
[00:38:52.000 --> 00:38:52.840]   - C++, yeah.
[00:38:52.840 --> 00:38:57.040]   - So when asked how you were able to do it,
[00:38:57.040 --> 00:39:01.920]   you said you just gotta think and then do it right.
[00:39:01.920 --> 00:39:04.680]   So can you break that apart a little bit?
[00:39:04.680 --> 00:39:07.600]   What's your process of one, thinking,
[00:39:07.600 --> 00:39:09.160]   and two, doing it right?
[00:39:09.160 --> 00:39:12.560]   - You know, the people I was working for
[00:39:12.560 --> 00:39:13.520]   were amused that I said that.
[00:39:13.520 --> 00:39:14.840]   It doesn't really mean anything.
[00:39:14.840 --> 00:39:15.680]   - Okay.
[00:39:15.680 --> 00:39:16.520]   (laughing)
[00:39:16.520 --> 00:39:19.640]   I mean, is there some deep, profound insights
[00:39:19.640 --> 00:39:23.640]   to draw from like how you problem solve from that?
[00:39:23.640 --> 00:39:24.680]   - This is always what I say.
[00:39:24.680 --> 00:39:26.080]   I'm like, do you want to be a good programmer?
[00:39:26.080 --> 00:39:27.880]   Do it for 20 years.
[00:39:27.880 --> 00:39:29.640]   - Yeah, there's no shortcuts.
[00:39:29.640 --> 00:39:30.480]   - No.
[00:39:30.480 --> 00:39:33.480]   - What are your thoughts on crypto in general?
[00:39:33.480 --> 00:39:38.160]   So what parts technically or philosophically
[00:39:38.160 --> 00:39:40.080]   do you find especially beautiful maybe?
[00:39:40.080 --> 00:39:42.840]   - Oh, I'm extremely bullish on crypto long-term.
[00:39:42.840 --> 00:39:45.120]   Not any specific crypto project,
[00:39:45.120 --> 00:39:50.120]   but this idea of, well, two ideas.
[00:39:50.120 --> 00:39:54.280]   One, the Nakamoto consensus algorithm
[00:39:54.280 --> 00:39:57.360]   is I think one of the greatest innovations
[00:39:57.360 --> 00:39:58.640]   of the 21st century.
[00:39:58.640 --> 00:40:01.280]   This idea that people can reach consensus.
[00:40:01.280 --> 00:40:03.440]   You can reach a group consensus.
[00:40:03.440 --> 00:40:08.280]   Using a relatively straightforward algorithm is wild.
[00:40:08.280 --> 00:40:13.280]   And like, you know, Satoshi Nakamoto,
[00:40:13.280 --> 00:40:15.640]   people always ask me who I look up to.
[00:40:15.640 --> 00:40:17.840]   It's like, whoever that is.
[00:40:17.840 --> 00:40:19.160]   - Who do you think it is?
[00:40:19.160 --> 00:40:20.000]   - I mean, I--
[00:40:20.000 --> 00:40:20.840]   - Elon Musk?
[00:40:20.840 --> 00:40:22.360]   Is it you?
[00:40:22.360 --> 00:40:24.080]   - It is definitely not me.
[00:40:24.080 --> 00:40:26.120]   And I do not think it's Elon Musk.
[00:40:26.120 --> 00:40:31.120]   But yeah, this idea of groups reaching consensus
[00:40:31.720 --> 00:40:34.960]   in a decentralized yet formulaic way
[00:40:34.960 --> 00:40:37.720]   is one extremely powerful idea from crypto.
[00:40:37.720 --> 00:40:45.560]   Maybe the second idea is this idea of smart contracts.
[00:40:45.560 --> 00:40:49.160]   When you write a contract between two parties,
[00:40:49.160 --> 00:40:53.760]   any contract, this contract, if there are disputes,
[00:40:53.760 --> 00:40:56.160]   it's interpreted by lawyers.
[00:40:56.160 --> 00:41:00.040]   Lawyers are just really shitty overpaid interpreters.
[00:41:00.040 --> 00:41:01.480]   Imagine you had, let's talk about them
[00:41:01.480 --> 00:41:05.280]   in terms of like, let's compare a lawyer to Python, right?
[00:41:05.280 --> 00:41:07.120]   So, well, okay.
[00:41:07.120 --> 00:41:10.040]   - That's really, oh, I never thought of it that way.
[00:41:10.040 --> 00:41:11.640]   It's hilarious.
[00:41:11.640 --> 00:41:15.960]   - So Python, I'm paying even 10 cents an hour.
[00:41:15.960 --> 00:41:17.240]   I'll use the nice Azure machine.
[00:41:17.240 --> 00:41:19.720]   I can run Python for 10 cents an hour.
[00:41:19.720 --> 00:41:21.520]   Lawyers cost $1,000 an hour.
[00:41:21.520 --> 00:41:25.920]   So Python is 10,000x better on that axis.
[00:41:25.920 --> 00:41:29.560]   Lawyers don't always return the same answer.
[00:41:31.080 --> 00:41:32.480]   Python almost always does.
[00:41:32.480 --> 00:41:37.760]   - Cost.
[00:41:37.760 --> 00:41:40.960]   - Yeah, I mean, just cost, reliability,
[00:41:40.960 --> 00:41:43.840]   everything about Python is so much better than lawyers.
[00:41:43.840 --> 00:41:46.080]   So if you can make smart contracts,
[00:41:46.080 --> 00:41:49.080]   this whole concept of code is law,
[00:41:49.080 --> 00:41:53.200]   I love and I would love to live in a world
[00:41:53.200 --> 00:41:55.600]   where everybody accepted that fact.
[00:41:55.600 --> 00:42:00.600]   - So maybe you can talk about what smart contracts are.
[00:42:01.160 --> 00:42:05.920]   - So let's say, let's say, you know, we have a,
[00:42:05.920 --> 00:42:11.680]   even something as simple as a safety deposit box, right?
[00:42:11.680 --> 00:42:14.800]   A safety deposit box that holds a million dollars.
[00:42:14.800 --> 00:42:17.040]   I have a contract with the bank that says
[00:42:17.040 --> 00:42:22.040]   two out of these three parties must be present
[00:42:22.040 --> 00:42:25.280]   to open the safety deposit box and get the money out.
[00:42:25.280 --> 00:42:26.320]   So that's a contract with the bank
[00:42:26.320 --> 00:42:29.000]   and it's only as good as the bank and the lawyers, right?
[00:42:29.000 --> 00:42:32.800]   Let's say, you know, somebody dies and now,
[00:42:32.800 --> 00:42:34.520]   oh, we're gonna go through a big legal dispute
[00:42:34.520 --> 00:42:36.080]   about whether, oh, well, was it in the will?
[00:42:36.080 --> 00:42:37.480]   Was it not in the will?
[00:42:37.480 --> 00:42:39.760]   What, what, like, it's just so messy
[00:42:39.760 --> 00:42:44.560]   and the cost to determine truth is so expensive
[00:42:44.560 --> 00:42:47.160]   versus a smart contract, which just uses cryptography
[00:42:47.160 --> 00:42:50.040]   to check if two out of three keys are present.
[00:42:50.040 --> 00:42:53.840]   Well, I can look at that and I can have certainty
[00:42:53.840 --> 00:42:55.960]   in the answer that it's going to return.
[00:42:55.960 --> 00:42:58.200]   And that's what all businesses want is certainty.
[00:42:58.200 --> 00:42:59.600]   You know, they say businesses don't care.
[00:42:59.600 --> 00:43:02.000]   Viacom YouTube, YouTube's like,
[00:43:02.000 --> 00:43:04.080]   look, we don't care which way this lawsuit goes.
[00:43:04.080 --> 00:43:07.200]   Just please tell us so we can have certainty.
[00:43:07.200 --> 00:43:09.080]   - Yeah, I wonder how many agreements in this,
[00:43:09.080 --> 00:43:12.680]   'cause we're talking about financial transactions only
[00:43:12.680 --> 00:43:13.840]   in this case, correct?
[00:43:13.840 --> 00:43:15.560]   The smart contracts.
[00:43:15.560 --> 00:43:17.320]   - Oh, you can go to anything.
[00:43:17.320 --> 00:43:20.120]   You can go, you can put a prenup in the Ethereum blockchain.
[00:43:20.120 --> 00:43:21.800]   (laughing)
[00:43:21.800 --> 00:43:22.960]   - Married smart contract.
[00:43:22.960 --> 00:43:24.800]   - Sorry, divorce lawyers, sorry.
[00:43:24.800 --> 00:43:26.760]   You're gonna be replaced by Python.
[00:43:26.960 --> 00:43:29.560]   (laughing)
[00:43:29.560 --> 00:43:34.560]   - Okay, so that's another beautiful idea.
[00:43:34.560 --> 00:43:37.680]   Do you think there's something that's appealing to you
[00:43:37.680 --> 00:43:40.240]   about any one specific implementation?
[00:43:40.240 --> 00:43:45.080]   So if you look 10, 20, 50 years down the line,
[00:43:45.080 --> 00:43:48.120]   do you see any like Bitcoin, Ethereum,
[00:43:48.120 --> 00:43:51.120]   any of the other hundreds of cryptocurrencies winning out?
[00:43:51.120 --> 00:43:53.320]   Is there, like, what's your intuition about the space?
[00:43:53.320 --> 00:43:55.320]   Are you just sitting back and watching the chaos
[00:43:55.320 --> 00:43:57.320]   and look who cares what emerges?
[00:43:57.320 --> 00:43:58.920]   - Oh, I don't speculate.
[00:43:58.920 --> 00:43:59.840]   I don't really care.
[00:43:59.840 --> 00:44:02.560]   I don't really care which one of these projects wins.
[00:44:02.560 --> 00:44:05.480]   I'm kind of in the Bitcoin is a meme coin camp.
[00:44:05.480 --> 00:44:06.960]   I mean, why does Bitcoin have value?
[00:44:06.960 --> 00:44:11.960]   It's technically kind of, you know, not great.
[00:44:11.960 --> 00:44:14.320]   Like the block size debate,
[00:44:14.320 --> 00:44:16.080]   when I found out what the block size debate was,
[00:44:16.080 --> 00:44:17.960]   I'm like, are you guys kidding?
[00:44:17.960 --> 00:44:19.520]   - What's the block size debate?
[00:44:19.520 --> 00:44:21.960]   - You know what?
[00:44:21.960 --> 00:44:23.760]   It's really, it's too stupid to even talk about.
[00:44:23.760 --> 00:44:27.080]   People can look it up, but I'm like, wow.
[00:44:27.080 --> 00:44:28.400]   You know, Ethereum seems,
[00:44:28.400 --> 00:44:31.280]   the governance of Ethereum seems much better.
[00:44:31.280 --> 00:44:34.440]   I've come around a bit on proof of stake ideas.
[00:44:34.440 --> 00:44:37.680]   You know, very smart people thinking about some things.
[00:44:37.680 --> 00:44:38.680]   - Yeah.
[00:44:38.680 --> 00:44:40.320]   You know, governance is interesting.
[00:44:40.320 --> 00:44:43.200]   It does feel like Vitalik,
[00:44:43.200 --> 00:44:46.080]   like it does feel like an open,
[00:44:46.080 --> 00:44:48.000]   even in these distributed systems,
[00:44:48.000 --> 00:44:50.040]   leaders are helpful
[00:44:51.200 --> 00:44:54.560]   because they kind of help you drive the mission
[00:44:54.560 --> 00:44:58.120]   and the vision and they put a face to a project.
[00:44:58.120 --> 00:45:00.120]   It's a weird thing about us humans.
[00:45:00.120 --> 00:45:02.080]   - Geniuses are helpful, like Vitalik.
[00:45:02.080 --> 00:45:02.920]   - Right.
[00:45:02.920 --> 00:45:03.760]   Yeah.
[00:45:03.760 --> 00:45:04.600]   Brilliant.
[00:45:04.600 --> 00:45:06.520]   (laughing)
[00:45:06.520 --> 00:45:08.880]   - Leaders are not necessary.
[00:45:08.880 --> 00:45:10.320]   - Yeah.
[00:45:10.320 --> 00:45:15.320]   So you think the reason he's the face of Ethereum
[00:45:15.320 --> 00:45:17.120]   is because he's a genius.
[00:45:17.120 --> 00:45:18.080]   That's interesting.
[00:45:18.080 --> 00:45:19.680]   I mean, that was,
[00:45:20.680 --> 00:45:22.920]   it's interesting to think about
[00:45:22.920 --> 00:45:24.840]   that we need to create systems
[00:45:24.840 --> 00:45:30.280]   in which the quote unquote leaders that emerge
[00:45:30.280 --> 00:45:32.000]   are the geniuses in the system.
[00:45:32.000 --> 00:45:34.960]   I mean, that's arguably why
[00:45:34.960 --> 00:45:36.920]   the current state of democracy is broken
[00:45:36.920 --> 00:45:39.360]   is the people who are emerging as the leaders
[00:45:39.360 --> 00:45:40.960]   are not the most competent,
[00:45:40.960 --> 00:45:43.280]   are not the superstars of the system.
[00:45:43.280 --> 00:45:46.040]   And it seems like at least for now in the crypto world,
[00:45:46.040 --> 00:45:49.320]   oftentimes the leaders are the superstars.
[00:45:49.320 --> 00:45:51.680]   Imagine at the debate, they asked,
[00:45:51.680 --> 00:45:53.600]   what's the sixth amendment?
[00:45:53.600 --> 00:45:56.240]   What are the four fundamental forces in the universe?
[00:45:56.240 --> 00:45:58.720]   What's the integral of two to the X?
[00:45:58.720 --> 00:45:59.920]   - Yeah.
[00:45:59.920 --> 00:46:01.680]   - I'd love to see those questions asked.
[00:46:01.680 --> 00:46:03.560]   And that's what I want as our leader.
[00:46:03.560 --> 00:46:04.400]   - It's a little bit--
[00:46:04.400 --> 00:46:05.240]   - What's Bayes' rule?
[00:46:05.240 --> 00:46:07.960]   - Yeah.
[00:46:07.960 --> 00:46:10.840]   I mean, even, oh wow, you're hurting my brain.
[00:46:10.840 --> 00:46:14.960]   My standard was even lower,
[00:46:14.960 --> 00:46:17.640]   but I would have loved to see
[00:46:17.640 --> 00:46:20.600]   just this basic brilliance.
[00:46:20.600 --> 00:46:22.160]   Like I've talked to historians.
[00:46:22.160 --> 00:46:23.880]   There's just these, they're not even like,
[00:46:23.880 --> 00:46:26.720]   they don't have a PhD or even education history.
[00:46:26.720 --> 00:46:30.040]   They just like a Dan Carlin type character
[00:46:30.040 --> 00:46:32.760]   who just like, holy shit,
[00:46:32.760 --> 00:46:35.400]   how did all this information get into your head?
[00:46:35.400 --> 00:46:38.440]   They're able to just connect Genghis Khan
[00:46:38.440 --> 00:46:41.840]   to the entirety of the history of the 20th century.
[00:46:41.840 --> 00:46:46.160]   They know everything about every single battle that happened
[00:46:46.160 --> 00:46:51.040]   and they know the like the Game of Thrones
[00:46:51.040 --> 00:46:55.120]   of the different power plays and all that happened there.
[00:46:55.120 --> 00:46:56.720]   And they know like the individuals
[00:46:56.720 --> 00:46:58.560]   and all the documents involved.
[00:46:58.560 --> 00:47:02.080]   And they integrate that into their regular life.
[00:47:02.080 --> 00:47:03.960]   It's not like they're ultra history nerds.
[00:47:03.960 --> 00:47:06.360]   They're just, they know this information.
[00:47:06.360 --> 00:47:08.040]   That's what competence looks like.
[00:47:08.040 --> 00:47:09.080]   - Yeah.
[00:47:09.080 --> 00:47:10.680]   - 'Cause I've seen that with programmers too, right?
[00:47:10.680 --> 00:47:12.600]   That's what great programmers do.
[00:47:12.600 --> 00:47:14.760]   But yeah, it would be,
[00:47:14.760 --> 00:47:16.680]   it's really unfortunate that those kinds of people
[00:47:16.680 --> 00:47:19.360]   aren't emerging as our leaders.
[00:47:19.360 --> 00:47:21.880]   But for now, at least in the crypto world,
[00:47:21.880 --> 00:47:23.320]   that seems to be the case.
[00:47:23.320 --> 00:47:25.120]   I don't know if that always,
[00:47:25.120 --> 00:47:27.720]   you could imagine that in a hundred years,
[00:47:27.720 --> 00:47:28.840]   it's not the case, right?
[00:47:28.840 --> 00:47:31.960]   - Crypto world has one very powerful idea going for it.
[00:47:31.960 --> 00:47:35.040]   And that's the idea of forks, right?
[00:47:35.040 --> 00:47:38.440]   I mean, you know,
[00:47:38.440 --> 00:47:42.960]   imagine we'll use a less controversial example.
[00:47:42.960 --> 00:47:47.200]   This was actually in my joke app in 2012.
[00:47:47.200 --> 00:47:49.480]   I was like, Barack Obama, Mitt Romney,
[00:47:49.480 --> 00:47:51.080]   let's let them both be president, right?
[00:47:51.080 --> 00:47:53.000]   Like imagine we could fork America
[00:47:53.000 --> 00:47:54.520]   and just let them both be president.
[00:47:54.520 --> 00:47:56.120]   And then the Americas could compete.
[00:47:56.120 --> 00:47:58.200]   And you know, people could invest in one,
[00:47:58.200 --> 00:48:00.520]   pull their liquidity out of one, put it in the other.
[00:48:00.520 --> 00:48:02.480]   You have this in the crypto world.
[00:48:02.480 --> 00:48:05.600]   Ethereum forks into Ethereum and Ethereum classic.
[00:48:05.600 --> 00:48:07.400]   And you can pull your liquidity out of one
[00:48:07.400 --> 00:48:09.040]   and put it in another.
[00:48:09.040 --> 00:48:10.800]   And people vote with their dollars,
[00:48:12.000 --> 00:48:16.480]   which forks companies should be able to fork.
[00:48:16.480 --> 00:48:18.240]   I'd love to fork Nvidia, you know?
[00:48:18.240 --> 00:48:22.800]   - Yeah, like different business strategies
[00:48:22.800 --> 00:48:26.200]   and then try them out and see what works.
[00:48:26.200 --> 00:48:27.360]   Like even take,
[00:48:27.360 --> 00:48:34.800]   yeah, take CalmAI that closes its source
[00:48:34.800 --> 00:48:38.360]   and then take one that's open source and see what works.
[00:48:38.360 --> 00:48:41.080]   Take one that's purchased by GM
[00:48:41.080 --> 00:48:43.560]   and one that remains Android Renegade
[00:48:43.560 --> 00:48:45.320]   and all these different versions and see.
[00:48:45.320 --> 00:48:47.960]   - The beauty of CalmAI is someone can actually do that.
[00:48:47.960 --> 00:48:49.680]   Please take CalmAI and fork it.
[00:48:49.680 --> 00:48:51.520]   - That's right.
[00:48:51.520 --> 00:48:53.080]   That's the beauty of open source.
[00:48:53.080 --> 00:48:56.480]   So you're, I mean, we'll talk about autonomous vehicle space,
[00:48:56.480 --> 00:49:01.480]   but it does seem that you're really knowledgeable
[00:49:01.480 --> 00:49:03.960]   about a lot of different topics.
[00:49:03.960 --> 00:49:06.120]   So the natural question, a bunch of people ask this,
[00:49:06.120 --> 00:49:10.000]   which is how do you keep learning new things?
[00:49:10.000 --> 00:49:12.440]   Do you have like practical advice?
[00:49:12.440 --> 00:49:17.440]   If you were to introspect, like taking notes, allocate time,
[00:49:17.440 --> 00:49:19.240]   or do you just mess around
[00:49:19.240 --> 00:49:21.240]   and just allow your curiosity to drive?
[00:49:21.240 --> 00:49:23.040]   - I'll write these people a self-help book
[00:49:23.040 --> 00:49:25.020]   and I'll charge $67 for it.
[00:49:25.020 --> 00:49:26.320]   (laughing)
[00:49:26.320 --> 00:49:27.520]   I will write--
[00:49:27.520 --> 00:49:28.520]   - What's chapter one?
[00:49:28.520 --> 00:49:30.320]   - I will write on the cover of the self-help book,
[00:49:30.320 --> 00:49:32.560]   all of this advice is completely meaningless.
[00:49:32.560 --> 00:49:34.760]   You're gonna be a sucker and buy this book anyway.
[00:49:34.760 --> 00:49:38.840]   And the one lesson that I hope they take away from the book
[00:49:38.840 --> 00:49:42.600]   is that I can't give you a meaningful answer to that.
[00:49:42.600 --> 00:49:44.040]   - That's interesting.
[00:49:44.040 --> 00:49:45.460]   Let me translate that.
[00:49:45.460 --> 00:49:50.360]   Is you haven't really thought about what it is you do
[00:49:50.360 --> 00:49:53.920]   systematically, because you could reduce it.
[00:49:53.920 --> 00:49:57.000]   And there's some people, I mean, I've met brilliant people
[00:49:57.000 --> 00:50:00.200]   that this is really clear with athletes.
[00:50:00.200 --> 00:50:02.440]   Some are just, you know,
[00:50:02.440 --> 00:50:05.160]   the best in the world at something.
[00:50:05.160 --> 00:50:07.600]   And they have zero interest in writing
[00:50:07.600 --> 00:50:11.600]   like a self-help book or how to master this game.
[00:50:11.600 --> 00:50:15.600]   And then there's some athletes who become great coaches
[00:50:15.600 --> 00:50:18.800]   and they love the analysis, perhaps the over analysis.
[00:50:18.800 --> 00:50:20.720]   And you right now, at least at your age,
[00:50:20.720 --> 00:50:23.160]   which is an interesting, you're in the middle of the battle.
[00:50:23.160 --> 00:50:25.600]   You're like the warriors that have zero interest
[00:50:25.600 --> 00:50:26.640]   in writing books.
[00:50:26.640 --> 00:50:29.080]   So you're in the middle of the battle.
[00:50:29.080 --> 00:50:30.560]   So you have, yeah.
[00:50:30.560 --> 00:50:31.700]   - This is a fair point.
[00:50:31.700 --> 00:50:34.040]   I do think I have a certain aversion
[00:50:34.040 --> 00:50:39.040]   to this kind of deliberate, intentional way of living life.
[00:50:39.040 --> 00:50:43.440]   - You're eventually, the hilarity of this,
[00:50:43.440 --> 00:50:45.440]   especially since this is recorded,
[00:50:45.440 --> 00:50:49.120]   it will reveal beautifully the absurdity
[00:50:49.120 --> 00:50:51.360]   when you finally do publish this book.
[00:50:51.360 --> 00:50:52.840]   I guarantee you, you will.
[00:50:52.840 --> 00:50:55.640]   The story of Kama AI,
[00:50:55.640 --> 00:50:58.800]   it'll be, maybe it'll be a biography written about you.
[00:50:58.800 --> 00:51:00.520]   That'll be better, I guess.
[00:51:00.520 --> 00:51:02.480]   - And you might be able to learn some cute lessons
[00:51:02.480 --> 00:51:05.480]   if you're starting a company like Kama AI from that book.
[00:51:05.480 --> 00:51:07.360]   But if you're asking generic questions,
[00:51:07.360 --> 00:51:10.160]   like how do I be good at things?
[00:51:10.160 --> 00:51:11.800]   Dude, I don't know.
[00:51:11.800 --> 00:51:13.240]   - Well, I mean, the interesting--
[00:51:13.240 --> 00:51:14.280]   - Do them a lot.
[00:51:14.280 --> 00:51:15.120]   - Do them a lot.
[00:51:15.120 --> 00:51:18.480]   But the interesting thing here is learning things
[00:51:18.480 --> 00:51:21.920]   outside of your current trajectory,
[00:51:21.920 --> 00:51:24.040]   which is what it feels like from an outsider's perspective.
[00:51:24.040 --> 00:51:29.040]   I mean, that, I don't know if there's advice on that,
[00:51:30.780 --> 00:51:33.220]   but it is an interesting curiosity.
[00:51:33.220 --> 00:51:36.060]   When you become really busy, you're running a company.
[00:51:36.060 --> 00:51:38.780]   - Part time.
[00:51:38.780 --> 00:51:41.540]   - Yeah.
[00:51:41.540 --> 00:51:45.740]   But like there's a natural inclination and trend,
[00:51:45.740 --> 00:51:48.940]   like just the momentum of life carries you
[00:51:48.940 --> 00:51:51.020]   into a particular direction of wanting to focus.
[00:51:51.020 --> 00:51:55.140]   And this kind of dispersion that curiosity can lead to
[00:51:55.140 --> 00:51:58.180]   gets harder and harder with time.
[00:51:58.180 --> 00:52:00.900]   'Cause you get really good at certain things
[00:52:00.900 --> 00:52:03.780]   and it sucks trying things that you're not good at,
[00:52:03.780 --> 00:52:05.300]   like trying to figure them out.
[00:52:05.300 --> 00:52:07.340]   When you do this with your live streams,
[00:52:07.340 --> 00:52:10.020]   you're on the fly figuring stuff out.
[00:52:10.020 --> 00:52:11.660]   You don't mind looking dumb.
[00:52:11.660 --> 00:52:16.620]   You just figure it out pretty quickly.
[00:52:16.620 --> 00:52:19.140]   - Sometimes I try things and I don't figure them out.
[00:52:19.140 --> 00:52:20.940]   My chess rating is like a 1400
[00:52:20.940 --> 00:52:23.260]   despite putting like a couple hundred hours in,
[00:52:23.260 --> 00:52:24.300]   it's pathetic.
[00:52:24.300 --> 00:52:26.820]   I mean, to be fair, I know that I could do it better
[00:52:26.820 --> 00:52:29.820]   if I did it better, like don't play five minute games,
[00:52:29.820 --> 00:52:31.380]   play 15 minute games at least.
[00:52:31.380 --> 00:52:34.220]   Like I know these things, but it just doesn't,
[00:52:34.220 --> 00:52:37.220]   it doesn't stick nicely in my knowledge stream.
[00:52:37.220 --> 00:52:39.260]   - All right, let's talk about ComAI.
[00:52:39.260 --> 00:52:42.100]   What's the mission of the company?
[00:52:42.100 --> 00:52:44.740]   Let's like look at the biggest picture.
[00:52:44.740 --> 00:52:46.780]   - Oh, I have an exact statement.
[00:52:46.780 --> 00:52:48.620]   Solve self-driving cars
[00:52:48.620 --> 00:52:51.500]   while delivering shippable intermediaries.
[00:52:51.500 --> 00:52:56.260]   - So long-term vision is have fully autonomous vehicles
[00:52:56.260 --> 00:52:59.020]   and make sure you're making money along the way.
[00:52:59.020 --> 00:53:00.180]   - I think it doesn't really speak to money,
[00:53:00.180 --> 00:53:03.220]   but I can talk about what solve self-driving cars means.
[00:53:03.220 --> 00:53:05.220]   Solve self-driving cars of course means
[00:53:05.220 --> 00:53:07.980]   you're not building a new car,
[00:53:07.980 --> 00:53:10.420]   you're building a person replacement.
[00:53:10.420 --> 00:53:12.300]   That person can sit in the driver's seat
[00:53:12.300 --> 00:53:14.540]   and drive you anywhere a person can drive
[00:53:14.540 --> 00:53:17.940]   with a human or better level of safety,
[00:53:17.940 --> 00:53:19.460]   speed, quality, comfort.
[00:53:19.460 --> 00:53:23.220]   - And what's the second part of that?
[00:53:23.220 --> 00:53:26.100]   - Delivering shippable intermediaries
[00:53:26.100 --> 00:53:28.140]   is well, it's a way to fund the company, that's true.
[00:53:28.140 --> 00:53:29.980]   But it's also a way to keep us honest.
[00:53:29.980 --> 00:53:33.580]   If you don't have that,
[00:53:33.580 --> 00:53:36.980]   it is very easy with this technology
[00:53:36.980 --> 00:53:40.140]   to think you're making progress when you're not.
[00:53:40.140 --> 00:53:41.940]   I've heard it best described on Hacker News
[00:53:41.940 --> 00:53:46.620]   as you can set any arbitrary milestone,
[00:53:46.620 --> 00:53:48.100]   meet that milestone,
[00:53:48.100 --> 00:53:49.460]   and still be infinitely far away
[00:53:49.460 --> 00:53:51.740]   from solving self-driving cars.
[00:53:51.740 --> 00:53:53.780]   - So it's hard to have like real deadlines
[00:53:53.780 --> 00:53:56.740]   when you're like Cruz or Waymo
[00:53:56.740 --> 00:54:00.700]   when you don't have revenue.
[00:54:00.700 --> 00:54:04.460]   Is that, I mean is revenue essentially
[00:54:04.460 --> 00:54:07.780]   the thing we're talking about here?
[00:54:07.780 --> 00:54:11.420]   - Revenue is, capitalism is based around consent.
[00:54:11.420 --> 00:54:13.540]   Capitalism, the way that you get revenue is,
[00:54:13.540 --> 00:54:16.300]   real capitalism comes in the real capitalism camp.
[00:54:16.300 --> 00:54:17.300]   There's definitely scams out there,
[00:54:17.300 --> 00:54:19.540]   but real capitalism is based around consent.
[00:54:19.540 --> 00:54:20.700]   It's based around this idea that like
[00:54:20.700 --> 00:54:21.900]   if we're getting revenue,
[00:54:21.900 --> 00:54:22.940]   it's because we're providing
[00:54:22.940 --> 00:54:24.780]   at least that much value to another person.
[00:54:24.780 --> 00:54:27.460]   When someone buys $1,000 Comma 2 from us,
[00:54:27.460 --> 00:54:29.100]   we're providing them at least $1,000 of value
[00:54:29.100 --> 00:54:30.140]   or they wouldn't buy it.
[00:54:30.140 --> 00:54:31.020]   - Brilliant.
[00:54:31.020 --> 00:54:32.900]   So can you give a whirlwind overview
[00:54:32.900 --> 00:54:34.900]   of the products that CommAI provides
[00:54:34.900 --> 00:54:38.140]   like throughout its history and today?
[00:54:38.140 --> 00:54:40.740]   - I mean, yeah, the past ones aren't really that interesting.
[00:54:40.740 --> 00:54:43.860]   It's kind of just been refinement of the same idea.
[00:54:43.860 --> 00:54:48.260]   The real only product we sell today is the Comma 2.
[00:54:48.260 --> 00:54:50.460]   - Which is a piece of hardware with cameras.
[00:54:52.340 --> 00:54:54.100]   - So the Comma 2, I mean,
[00:54:54.100 --> 00:54:56.820]   you can think about it kind of like a person.
[00:54:56.820 --> 00:54:57.660]   You know, in future hardware,
[00:54:57.660 --> 00:55:00.260]   it will probably be even more and more person-like.
[00:55:00.260 --> 00:55:05.260]   So it has, you know, eyes, ears, a mouth, a brain,
[00:55:05.260 --> 00:55:09.500]   and a way to interface with the car.
[00:55:09.500 --> 00:55:10.940]   - Does it have consciousness?
[00:55:10.940 --> 00:55:13.460]   Just kidding, that was a trick question.
[00:55:13.460 --> 00:55:15.100]   - I don't have consciousness either.
[00:55:15.100 --> 00:55:16.380]   Me and the Comma 2 are the same.
[00:55:16.380 --> 00:55:17.220]   - You're the same?
[00:55:17.220 --> 00:55:18.620]   - I have a little more compute than it.
[00:55:18.620 --> 00:55:21.420]   It only has like the same compute as a B.
[00:55:22.420 --> 00:55:23.260]   You know?
[00:55:23.260 --> 00:55:25.300]   - You're more efficient energy-wise
[00:55:25.300 --> 00:55:26.460]   for the compute you're doing.
[00:55:26.460 --> 00:55:28.100]   - Far more efficient energy-wise.
[00:55:28.100 --> 00:55:30.620]   20 petaflops, 20 watts, crazy.
[00:55:30.620 --> 00:55:32.220]   - Do you lack consciousness?
[00:55:32.220 --> 00:55:33.060]   - Sure.
[00:55:33.060 --> 00:55:33.900]   - Do you fear death?
[00:55:33.900 --> 00:55:35.500]   You do, you want immortality.
[00:55:35.500 --> 00:55:36.340]   - Of course I fear death.
[00:55:36.340 --> 00:55:38.100]   - Does Comma AI fear death?
[00:55:38.100 --> 00:55:39.580]   I don't think so.
[00:55:39.580 --> 00:55:40.500]   - Of course it does.
[00:55:40.500 --> 00:55:42.980]   It very much fears, well, it fears negative loss.
[00:55:42.980 --> 00:55:43.820]   Oh yeah.
[00:55:43.820 --> 00:55:49.260]   - Okay, so Comma 2, when did that come out?
[00:55:49.260 --> 00:55:50.540]   That was a year ago?
[00:55:50.540 --> 00:55:52.140]   Not two?
[00:55:52.140 --> 00:55:53.580]   - Early this year.
[00:55:53.580 --> 00:55:55.940]   - Wow, time, it feels like, yeah.
[00:55:55.940 --> 00:56:00.860]   2020 feels like it's taken 10 years to get to the end.
[00:56:00.860 --> 00:56:01.860]   - It's a long year.
[00:56:01.860 --> 00:56:03.180]   - It's a long year.
[00:56:03.180 --> 00:56:08.180]   So what's the sexiest thing about Comma 2, feature-wise?
[00:56:08.180 --> 00:56:14.380]   So, I mean, maybe you can also linger on like, what is it?
[00:56:14.380 --> 00:56:15.780]   Like what's its purpose?
[00:56:15.780 --> 00:56:18.620]   'Cause there's a hardware, there's a software component.
[00:56:18.620 --> 00:56:20.060]   You've mentioned the sensors,
[00:56:20.060 --> 00:56:23.140]   but also like what is it's features and capabilities?
[00:56:23.140 --> 00:56:25.420]   - I think our slogan summarizes it well.
[00:56:25.420 --> 00:56:27.300]   Comma slogan is make driving chill.
[00:56:27.300 --> 00:56:30.740]   - I love it, okay.
[00:56:30.740 --> 00:56:34.380]   - Yeah, I mean, it is, you know, if you like cruise control,
[00:56:34.380 --> 00:56:36.780]   imagine cruise control, but much, much more.
[00:56:36.780 --> 00:56:41.060]   - So it can do adaptive cruise control things,
[00:56:41.060 --> 00:56:43.020]   which is like slow down for cars in front of it,
[00:56:43.020 --> 00:56:44.220]   maintain a certain speed,
[00:56:44.220 --> 00:56:46.300]   and it can also do lane keeping,
[00:56:46.300 --> 00:56:48.980]   so staying in the lane and doing it better and better
[00:56:48.980 --> 00:56:50.020]   and better over time.
[00:56:50.020 --> 00:56:52.020]   It's very much machine learning based.
[00:56:52.020 --> 00:56:56.660]   So there's cameras, there's a driver facing camera too.
[00:56:56.660 --> 00:57:02.100]   What else is there?
[00:57:02.100 --> 00:57:02.940]   What am I thinking?
[00:57:02.940 --> 00:57:04.380]   So the hardware versus software.
[00:57:04.380 --> 00:57:09.020]   So open pilot versus the actual hardware of the device.
[00:57:09.020 --> 00:57:10.540]   What's, can you draw that distinction?
[00:57:10.540 --> 00:57:11.580]   What's one, what's the other?
[00:57:11.580 --> 00:57:13.780]   - I mean, the hardware is pretty much a cell phone
[00:57:13.780 --> 00:57:14.700]   with a few additions.
[00:57:14.700 --> 00:57:16.940]   A cell phone with a cooling system
[00:57:16.940 --> 00:57:20.980]   and with a car interface connected to it.
[00:57:20.980 --> 00:57:24.500]   - And by cell phone, you mean like Qualcomm Snapdragon?
[00:57:24.500 --> 00:57:28.060]   - Yeah, the current hardware is a Snapdragon 821.
[00:57:28.060 --> 00:57:32.300]   It has a wifi radio, it has an LTE radio, it has a screen.
[00:57:32.300 --> 00:57:35.980]   We use every part of the cell phone.
[00:57:35.980 --> 00:57:37.500]   - And then the interface with the car
[00:57:37.500 --> 00:57:38.460]   is specific to the car,
[00:57:38.460 --> 00:57:40.580]   so you keep supporting more and more cars.
[00:57:40.580 --> 00:57:42.620]   - Yeah, so the interface to the car,
[00:57:42.620 --> 00:57:45.340]   I mean, the device itself just has four CAN buses,
[00:57:45.340 --> 00:57:46.860]   has four CAN interfaces on it
[00:57:46.860 --> 00:57:49.420]   that are connected through the USB port to the phone.
[00:57:49.420 --> 00:57:53.260]   And then, yeah, on those four CAN buses,
[00:57:53.260 --> 00:57:54.420]   you connect it to the car
[00:57:54.420 --> 00:57:56.420]   and there's a little harness to do this.
[00:57:56.420 --> 00:57:58.460]   Cars are actually surprisingly similar.
[00:57:58.460 --> 00:58:01.500]   - So CAN is the protocol by which cars communicate
[00:58:01.500 --> 00:58:04.260]   and then you're able to read stuff and write stuff
[00:58:04.260 --> 00:58:06.900]   to be able to control the car depending on the car.
[00:58:06.900 --> 00:58:08.260]   So what's the software side?
[00:58:08.260 --> 00:58:09.300]   What's open pilot?
[00:58:10.340 --> 00:58:11.740]   - So, I mean, open pilot is,
[00:58:11.740 --> 00:58:13.740]   the hardware is pretty simple compared to open pilot.
[00:58:13.740 --> 00:58:14.980]   Open pilot is,
[00:58:14.980 --> 00:58:21.020]   well, so you have a machine learning model,
[00:58:21.020 --> 00:58:24.540]   which it's in open pilot, it's a blob.
[00:58:24.540 --> 00:58:25.620]   It's just a blob of weights.
[00:58:25.620 --> 00:58:27.420]   It's not like people are like, "Oh, it's closed source."
[00:58:27.420 --> 00:58:30.220]   I'm like, "It's a blob of weights, what do you expect?"
[00:58:30.220 --> 00:58:33.380]   - So it's primarily neural network based?
[00:58:33.380 --> 00:58:36.020]   - You, well, open pilot is all the software
[00:58:36.020 --> 00:58:37.660]   kind of around that neural network.
[00:58:37.660 --> 00:58:39.020]   That if you have a neural network that says,
[00:58:39.020 --> 00:58:40.860]   "Here's where you want to send the car,"
[00:58:40.860 --> 00:58:43.660]   open pilot actually goes and executes all of that.
[00:58:43.660 --> 00:58:46.940]   - It cleans up the input to the neural network,
[00:58:46.940 --> 00:58:49.100]   it cleans up the output and executes on it.
[00:58:49.100 --> 00:58:50.300]   So it connects,
[00:58:50.300 --> 00:58:51.900]   it's the glue that connects everything together.
[00:58:51.900 --> 00:58:53.060]   - Runs the sensors,
[00:58:53.060 --> 00:58:55.660]   does a bunch of calibration for the neural network,
[00:58:55.660 --> 00:58:58.100]   does, you know, deals with like, you know,
[00:58:58.100 --> 00:59:00.140]   if the car is on a banked road,
[00:59:00.140 --> 00:59:02.060]   you have to counter steer against that.
[00:59:02.060 --> 00:59:03.820]   And the neural network can't necessarily know that
[00:59:03.820 --> 00:59:05.120]   by looking at the picture.
[00:59:05.120 --> 00:59:08.580]   So you do that with other sensors and fusion
[00:59:08.580 --> 00:59:09.700]   and localizer.
[00:59:09.700 --> 00:59:13.380]   Open pilot also is responsible for sending the data
[00:59:13.380 --> 00:59:16.700]   up to our servers so we can learn from it,
[00:59:16.700 --> 00:59:19.100]   logging it, recording it, running the cameras,
[00:59:19.100 --> 00:59:21.460]   thermally managing the device,
[00:59:21.460 --> 00:59:23.140]   managing the disk space on the device,
[00:59:23.140 --> 00:59:24.740]   managing all the resources on the device.
[00:59:24.740 --> 00:59:27.660]   - So what, since we last spoke, I don't remember when,
[00:59:27.660 --> 00:59:30.100]   maybe a year ago, maybe a little bit longer,
[00:59:30.100 --> 00:59:33.060]   how has open pilot improved?
[00:59:33.060 --> 00:59:34.820]   - We did exactly what I promised you.
[00:59:34.820 --> 00:59:36.740]   I promised you that by the end of the year,
[00:59:36.740 --> 00:59:38.820]   we would be able to remove the lanes.
[00:59:38.820 --> 00:59:45.460]   The lateral policy is now almost completely end to end.
[00:59:45.460 --> 00:59:48.580]   You can turn the lanes off and it will drive,
[00:59:48.580 --> 00:59:49.940]   drive slightly worse on the highway
[00:59:49.940 --> 00:59:51.020]   if you turn the lanes off,
[00:59:51.020 --> 00:59:54.220]   but you can turn the lanes off and it will drive well,
[00:59:54.220 --> 00:59:57.180]   trained completely end to end on user data.
[00:59:57.180 --> 00:59:58.660]   And this year we hope to do the same
[00:59:58.660 --> 01:00:00.100]   for the longitudinal policy.
[01:00:00.100 --> 01:00:03.500]   - So that's the interesting thing is you're not doing,
[01:00:03.500 --> 01:00:05.340]   you don't appear to be, maybe you can correct me,
[01:00:05.340 --> 01:00:08.740]   but you don't appear to be doing lane detection
[01:00:08.740 --> 01:00:12.460]   or lane marking detection or kind of the segmentation task
[01:00:12.460 --> 01:00:15.180]   or any kind of object detection task.
[01:00:15.180 --> 01:00:17.620]   You're doing what's traditionally more called
[01:00:17.620 --> 01:00:19.500]   like end to end learning.
[01:00:19.500 --> 01:00:24.260]   So, and trained on actual behavior of drivers
[01:00:24.260 --> 01:00:26.180]   when they're driving the car manually.
[01:00:26.180 --> 01:00:29.580]   - And this is hard to do.
[01:00:29.580 --> 01:00:32.300]   You know, it's not supervised learning.
[01:00:32.300 --> 01:00:34.780]   - Yeah, but the, so the nice thing is there's a lot of data.
[01:00:34.780 --> 01:00:37.100]   So it's hard and easy, right?
[01:00:37.100 --> 01:00:37.940]   It's-
[01:00:37.940 --> 01:00:40.060]   - We have a lot of high quality data, yeah.
[01:00:40.060 --> 01:00:41.700]   - Like more than you need in a sense.
[01:00:41.700 --> 01:00:42.740]   Well-
[01:00:42.740 --> 01:00:43.580]   - We have way more than we do.
[01:00:43.580 --> 01:00:45.020]   We have way more data than we need.
[01:00:45.020 --> 01:00:47.060]   - I mean, it's an interesting question actually,
[01:00:47.060 --> 01:00:50.460]   because in terms of amount, you have more than you need,
[01:00:50.460 --> 01:00:54.300]   but the, you know, driving is full of edge cases.
[01:00:54.300 --> 01:00:57.260]   So how do you select the data you train on?
[01:00:57.260 --> 01:01:00.620]   I think this is an interesting open question.
[01:01:00.620 --> 01:01:04.260]   Like what's the cleverest way to select data?
[01:01:04.260 --> 01:01:06.700]   That's the question Tesla is probably working on.
[01:01:06.700 --> 01:01:09.940]   That's, I mean, the entirety of machine learning can be,
[01:01:09.940 --> 01:01:11.100]   they don't seem to really care.
[01:01:11.100 --> 01:01:12.300]   They just kind of select data.
[01:01:12.300 --> 01:01:14.900]   But I feel like that if you want to solve,
[01:01:14.900 --> 01:01:16.260]   if you want to create intelligent systems,
[01:01:16.260 --> 01:01:18.940]   you have to pick data well, right?
[01:01:18.940 --> 01:01:22.940]   And so do you have any hints, ideas of how to do it well?
[01:01:22.940 --> 01:01:26.420]   - So in some ways that is the definition I like
[01:01:26.420 --> 01:01:29.340]   of reinforcement learning versus supervised learning.
[01:01:29.340 --> 01:01:32.740]   In supervised learning, the weights depend on the data.
[01:01:32.740 --> 01:01:33.580]   Right?
[01:01:34.580 --> 01:01:36.980]   And this is obviously true, but the,
[01:01:36.980 --> 01:01:40.460]   in reinforcement learning, the data depends on the weights.
[01:01:40.460 --> 01:01:41.380]   - Yeah.
[01:01:41.380 --> 01:01:42.940]   - And actually both ways.
[01:01:42.940 --> 01:01:43.980]   - That's poetry.
[01:01:43.980 --> 01:01:44.900]   - So. - That's brilliant.
[01:01:44.900 --> 01:01:46.300]   - How does it know what data to train on?
[01:01:46.300 --> 01:01:47.540]   Well, let it pick.
[01:01:47.540 --> 01:01:49.460]   We're not there yet, but that's the eventual.
[01:01:49.460 --> 01:01:51.140]   - So you're thinking this almost like
[01:01:51.140 --> 01:01:53.340]   a reinforcement learning framework.
[01:01:53.340 --> 01:01:55.380]   - We're going to do RL on the world.
[01:01:55.380 --> 01:01:58.140]   Every time a car makes a mistake, user disengages,
[01:01:58.140 --> 01:02:00.140]   we train on that and do RL on the world.
[01:02:00.140 --> 01:02:02.380]   Ship out a new model, that's an epoch, right?
[01:02:03.260 --> 01:02:08.260]   - And for now you're not doing the Elon style promising
[01:02:08.260 --> 01:02:09.700]   that it's going to be fully autonomous.
[01:02:09.700 --> 01:02:12.420]   You really are sticking to level two.
[01:02:12.420 --> 01:02:15.060]   And like, it's supposed to be supervised.
[01:02:15.060 --> 01:02:16.900]   - Oh, it is definitely supposed to be supervised
[01:02:16.900 --> 01:02:19.700]   and we enforce the fact that it's supervised.
[01:02:19.700 --> 01:02:23.620]   We look at our rate of improvement in disengagements.
[01:02:23.620 --> 01:02:25.660]   OpenPilot now has an unplanned disengagement
[01:02:25.660 --> 01:02:27.420]   about every hundred miles.
[01:02:27.420 --> 01:02:31.580]   This is up from 10 miles, like maybe,
[01:02:32.540 --> 01:02:36.540]   maybe a year ago.
[01:02:36.540 --> 01:02:38.460]   Yeah, so maybe we've seen 10X improvement in a year,
[01:02:38.460 --> 01:02:41.700]   but a hundred miles is still a far cry
[01:02:41.700 --> 01:02:43.860]   from the hundred thousand you're going to need.
[01:02:43.860 --> 01:02:45.740]   So you're going to somehow need to get
[01:02:45.740 --> 01:02:48.540]   three more 10Xs in there.
[01:02:48.540 --> 01:02:52.260]   - And what's your intuition?
[01:02:52.260 --> 01:02:54.900]   You're basically hoping that there's exponential improvement
[01:02:54.900 --> 01:02:56.900]   built into the, baked into the cake somewhere.
[01:02:56.900 --> 01:02:58.380]   - Well, that's even, I mean, 10X improvement,
[01:02:58.380 --> 01:03:00.500]   that's already assuming exponential, right?
[01:03:00.500 --> 01:03:02.580]   There's definitely exponential improvement.
[01:03:02.580 --> 01:03:04.300]   And I think when Elon talks about exponential,
[01:03:04.300 --> 01:03:06.180]   like these things, these systems are going
[01:03:06.180 --> 01:03:07.860]   to exponentially improve.
[01:03:07.860 --> 01:03:10.220]   Just exponential doesn't mean you're getting
[01:03:10.220 --> 01:03:13.300]   a hundred gigahertz processors tomorrow, right?
[01:03:13.300 --> 01:03:15.060]   Like it's going to still take a while
[01:03:15.060 --> 01:03:18.340]   because the gap between even our best system
[01:03:18.340 --> 01:03:20.300]   and humans is still large.
[01:03:20.300 --> 01:03:22.340]   - So that's an interesting distinction to draw.
[01:03:22.340 --> 01:03:25.260]   So if you look at the way Tesla's approaching the problem
[01:03:25.260 --> 01:03:28.340]   and the way you're approaching the problem,
[01:03:28.340 --> 01:03:30.060]   which is very different than the rest
[01:03:30.060 --> 01:03:32.700]   of the self-driving car world.
[01:03:32.700 --> 01:03:33.860]   So let's put them aside.
[01:03:33.860 --> 01:03:36.140]   Is you're treating most of the driving tasks
[01:03:36.140 --> 01:03:37.540]   as a machine learning problem.
[01:03:37.540 --> 01:03:39.100]   And the way Tesla is approaching it
[01:03:39.100 --> 01:03:41.060]   is with a multitask learning,
[01:03:41.060 --> 01:03:43.140]   where you break the task of driving
[01:03:43.140 --> 01:03:45.380]   into hundreds of different tasks.
[01:03:45.380 --> 01:03:48.340]   And you have this multi-headed neural network
[01:03:48.340 --> 01:03:51.660]   that's very good at performing each task.
[01:03:51.660 --> 01:03:54.340]   And there's presumably something on top
[01:03:54.340 --> 01:03:56.380]   that's stitching stuff together
[01:03:56.380 --> 01:04:00.620]   in order to make control decisions, policy decisions
[01:04:00.620 --> 01:04:02.180]   about how you move the car.
[01:04:02.180 --> 01:04:04.420]   But what that allows you, there's a brilliance to this
[01:04:04.420 --> 01:04:08.380]   because it allows you to master each task,
[01:04:08.380 --> 01:04:13.380]   like lane detection, stop sign detection,
[01:04:13.380 --> 01:04:17.620]   traffic light detection, drivable area segmentation,
[01:04:17.620 --> 01:04:21.860]   you know, vehicle, bicycle, pedestrian detection.
[01:04:21.860 --> 01:04:25.420]   There's some localization tasks in there.
[01:04:25.420 --> 01:04:30.420]   Also predicting, like yeah,
[01:04:30.420 --> 01:04:34.060]   predicting how the entities in the scene are gonna move.
[01:04:34.060 --> 01:04:36.340]   Like everything is basically a machine learning task
[01:04:36.340 --> 01:04:40.340]   where there's a classification, segmentation, prediction.
[01:04:40.340 --> 01:04:44.420]   And it's nice because you can have this entire engine,
[01:04:44.420 --> 01:04:47.980]   data engine that's mining for edge cases
[01:04:47.980 --> 01:04:49.460]   for each one of these tasks.
[01:04:49.460 --> 01:04:51.540]   And you can have people like engineers
[01:04:51.540 --> 01:04:53.780]   that are basically masters of that task,
[01:04:53.780 --> 01:04:56.580]   like become the best person in the world at,
[01:04:56.580 --> 01:04:59.820]   as you talk about the cone guy for Waymo.
[01:04:59.820 --> 01:05:01.700]   - Yeah, the good old cone guy.
[01:05:01.700 --> 01:05:06.300]   - Become the best person in the world at cone detection.
[01:05:06.300 --> 01:05:08.860]   So that's a compelling notion
[01:05:08.860 --> 01:05:10.940]   from a supervised learning perspective,
[01:05:10.940 --> 01:05:16.060]   automating much of the process of edge case discovery
[01:05:16.060 --> 01:05:17.340]   and retraining neural network
[01:05:17.340 --> 01:05:19.740]   for each of the individual perception tasks.
[01:05:19.740 --> 01:05:22.100]   And then you're looking at the machine learning
[01:05:22.100 --> 01:05:26.980]   in a more holistic way, basically doing end-to-end learning
[01:05:26.980 --> 01:05:29.820]   on the driving tasks, supervised,
[01:05:29.820 --> 01:05:33.620]   trained on the data of the actual driving
[01:05:33.620 --> 01:05:36.460]   of people they use, comma, AI.
[01:05:36.460 --> 01:05:38.740]   Like actual human drivers doing manual control,
[01:05:38.740 --> 01:05:42.620]   plus the moments of disengagement
[01:05:42.620 --> 01:05:45.540]   that maybe with some labeling
[01:05:45.540 --> 01:05:47.340]   could indicate the failure of the system.
[01:05:47.340 --> 01:05:50.180]   So you have a huge amount of data
[01:05:50.180 --> 01:05:52.780]   for positive control of the vehicle,
[01:05:52.780 --> 01:05:55.580]   like successful control of the vehicle,
[01:05:55.580 --> 01:05:58.060]   both maintaining the lane,
[01:05:58.060 --> 01:06:00.020]   as I think you're also working on
[01:06:00.020 --> 01:06:01.980]   longitudinal control of the vehicle,
[01:06:01.980 --> 01:06:05.420]   and then failure cases where the vehicle does something wrong
[01:06:05.420 --> 01:06:08.420]   that needs disengagement.
[01:06:08.420 --> 01:06:11.420]   So like, why do you think you're right
[01:06:11.420 --> 01:06:14.260]   and Tesla is wrong on this?
[01:06:14.260 --> 01:06:17.620]   And do you think you'll come around the Tesla way?
[01:06:17.620 --> 01:06:20.180]   Do you think Tesla will come around to your way?
[01:06:20.180 --> 01:06:24.020]   - If you were to start a chess engine company,
[01:06:24.020 --> 01:06:26.180]   would you hire a Bishop guy?
[01:06:26.180 --> 01:06:29.140]   - See, we have, this is Monday morning,
[01:06:29.140 --> 01:06:34.140]   quarterbacking is, yes, probably.
[01:06:34.140 --> 01:06:35.980]   (laughing)
[01:06:35.980 --> 01:06:36.820]   So-
[01:06:36.820 --> 01:06:37.660]   - Oh, our rook guy.
[01:06:37.660 --> 01:06:39.500]   Oh, we stole the rook guy from that company.
[01:06:39.500 --> 01:06:40.900]   Oh, we're going to have real good rooks.
[01:06:40.900 --> 01:06:43.060]   - Well, there's not many pieces, right?
[01:06:46.300 --> 01:06:48.940]   There's not many guys and gals to hire.
[01:06:48.940 --> 01:06:51.180]   You just have a few that work on the Bishop,
[01:06:51.180 --> 01:06:52.820]   a few that work on the rook.
[01:06:52.820 --> 01:06:55.220]   - But is that not ludicrous today to think about
[01:06:55.220 --> 01:06:57.620]   in a world of AlphaZero?
[01:06:57.620 --> 01:06:58.940]   - But AlphaZero is a chess game,
[01:06:58.940 --> 01:07:01.860]   so the fundamental question is
[01:07:01.860 --> 01:07:04.460]   how hard is driving compared to chess?
[01:07:04.460 --> 01:07:09.060]   Because, so long-term, end-to-end
[01:07:09.060 --> 01:07:10.700]   will be the right solution.
[01:07:10.700 --> 01:07:13.580]   The question is, how many years away is that?
[01:07:13.580 --> 01:07:15.900]   - End-to-end's going to be the only solution for level five.
[01:07:15.900 --> 01:07:17.380]   That's going to be the only way we get there.
[01:07:17.380 --> 01:07:18.500]   - Of course, and of course Tesla's
[01:07:18.500 --> 01:07:19.700]   going to come around to my way.
[01:07:19.700 --> 01:07:22.000]   And if you're a rook guy out there, I'm sorry.
[01:07:22.000 --> 01:07:23.940]   - The cone guy.
[01:07:23.940 --> 01:07:25.820]   I don't know.
[01:07:25.820 --> 01:07:26.940]   - We're going to specialize each task.
[01:07:26.940 --> 01:07:30.540]   We're going to really understand rook placement, yeah.
[01:07:30.540 --> 01:07:32.060]   - I understand the intuition you have.
[01:07:32.060 --> 01:07:36.820]   I mean, that is a very compelling notion
[01:07:36.820 --> 01:07:39.140]   that we can learn the task end-to-end,
[01:07:39.140 --> 01:07:40.820]   like the same compelling notion you might have
[01:07:40.820 --> 01:07:42.620]   for natural language conversation.
[01:07:42.620 --> 01:07:45.700]   But I'm not sure,
[01:07:45.700 --> 01:07:48.940]   'cause one thing you sneaked in there
[01:07:48.940 --> 01:07:51.620]   is the assertion that it's impossible
[01:07:51.620 --> 01:07:55.420]   to get to level five without this kind of approach.
[01:07:55.420 --> 01:07:57.140]   I don't know if that's obvious.
[01:07:57.140 --> 01:07:58.260]   - I don't know if that's obvious either.
[01:07:58.260 --> 01:08:01.320]   I don't actually mean that.
[01:08:01.320 --> 01:08:04.420]   I think that it is much easier to get to level five
[01:08:04.420 --> 01:08:05.660]   with an end-to-end approach.
[01:08:05.660 --> 01:08:08.860]   I think that the other approach is doable,
[01:08:08.860 --> 01:08:11.360]   but the magnitude of the engineering challenge
[01:08:11.360 --> 01:08:13.740]   may exceed what humanity is capable of.
[01:08:13.740 --> 01:08:17.820]   - So, but what do you think of the Tesla data engine
[01:08:17.820 --> 01:08:21.060]   approach, which to me is an active learning task,
[01:08:21.060 --> 01:08:22.460]   is kind of fascinating.
[01:08:22.460 --> 01:08:25.660]   It's breaking it down into these multiple tasks
[01:08:25.660 --> 01:08:29.480]   and mining their data constantly for like edge cases
[01:08:29.480 --> 01:08:30.420]   for these different tasks.
[01:08:30.420 --> 01:08:32.420]   - Yeah, but the tasks themselves are not being learned.
[01:08:32.420 --> 01:08:33.820]   This is feature engineering.
[01:08:33.820 --> 01:08:40.700]   - I mean, it's a higher abstraction level
[01:08:40.700 --> 01:08:43.300]   of feature engineering for the different tasks.
[01:08:43.300 --> 01:08:44.900]   It's task engineering in a sense.
[01:08:44.900 --> 01:08:46.900]   - It's slightly better feature engineering,
[01:08:46.900 --> 01:08:49.380]   but it's still fundamentally is feature engineering.
[01:08:49.380 --> 01:08:51.420]   And if anything about the history of AI
[01:08:51.420 --> 01:08:52.740]   has taught us anything,
[01:08:52.740 --> 01:08:54.660]   it's that feature engineering approaches
[01:08:54.660 --> 01:08:57.800]   will always be replaced and lose to end-to-end.
[01:08:57.800 --> 01:09:02.180]   Now, to be fair, I cannot really make promises on timelines,
[01:09:02.180 --> 01:09:05.780]   but I can say that when you look at the code for Stockfish
[01:09:05.780 --> 01:09:07.060]   and the code for AlphaZero,
[01:09:07.060 --> 01:09:09.180]   one is a lot shorter than the other,
[01:09:09.180 --> 01:09:10.820]   a lot more elegant, required a lot less
[01:09:10.820 --> 01:09:12.700]   programmer hours to write.
[01:09:12.700 --> 01:09:17.020]   - Yeah, but there was a lot more murder
[01:09:17.020 --> 01:09:22.020]   of bad agents on the AlphaZero side.
[01:09:22.020 --> 01:09:29.380]   By murder, I mean agents that played a game
[01:09:29.380 --> 01:09:30.660]   and failed miserably.
[01:09:30.660 --> 01:09:31.660]   - Yeah.
[01:09:31.660 --> 01:09:32.500]   Oh, oh.
[01:09:32.500 --> 01:09:34.980]   - In simulation, that failure is less costly.
[01:09:34.980 --> 01:09:35.820]   - Yeah.
[01:09:35.820 --> 01:09:37.460]   - In real world, it's--
[01:09:37.460 --> 01:09:38.420]   - Do you mean in practice?
[01:09:38.420 --> 01:09:40.780]   Like AlphaZero has lost games miserably?
[01:09:40.780 --> 01:09:41.620]   - No.
[01:09:41.620 --> 01:09:42.460]   - Oh.
[01:09:42.460 --> 01:09:43.280]   - Well.
[01:09:43.280 --> 01:09:44.120]   - I haven't seen that.
[01:09:44.120 --> 01:09:47.380]   - No, but I know, but the requirement for AlphaZero is--
[01:09:47.380 --> 01:09:48.220]   - A simulator.
[01:09:48.220 --> 01:09:51.500]   - To be able to like evolution, human evolution,
[01:09:51.500 --> 01:09:54.420]   not human evolution, biological evolution of life on earth
[01:09:54.420 --> 01:09:58.700]   from the origin of life has murdered trillions
[01:09:58.700 --> 01:10:02.260]   upon trillions of organisms on the path to us humans.
[01:10:02.260 --> 01:10:03.100]   - Yeah.
[01:10:03.100 --> 01:10:05.860]   - So the question is, can we stitch together
[01:10:05.860 --> 01:10:07.960]   a human-like object without having to go
[01:10:07.960 --> 01:10:09.900]   through the entirety process of evolution?
[01:10:09.900 --> 01:10:11.940]   - Well, no, but do the evolution in simulation.
[01:10:11.940 --> 01:10:12.860]   - Yeah, that's the question.
[01:10:12.860 --> 01:10:13.700]   Can we simulate?
[01:10:13.700 --> 01:10:15.060]   So do you have a sense that it's possible
[01:10:15.060 --> 01:10:15.900]   to simulate some aspect of--
[01:10:15.900 --> 01:10:18.220]   - MuZero is exactly this.
[01:10:18.220 --> 01:10:21.300]   MuZero is the solution to this.
[01:10:21.300 --> 01:10:23.980]   MuZero, I think, is going to be looked back
[01:10:23.980 --> 01:10:25.220]   as the canonical paper.
[01:10:25.220 --> 01:10:26.860]   And I don't think deep learning is everything.
[01:10:26.860 --> 01:10:28.720]   I think that there's still a bunch of things missing
[01:10:28.720 --> 01:10:30.420]   to get there, but MuZero, I think,
[01:10:30.420 --> 01:10:34.260]   is going to be looked back as the kind of cornerstone paper
[01:10:34.260 --> 01:10:37.100]   of this whole deep learning era.
[01:10:37.100 --> 01:10:39.600]   And MuZero is the solution to self-driving cars.
[01:10:39.600 --> 01:10:41.240]   You have to make a few tweaks to it,
[01:10:41.240 --> 01:10:42.840]   but MuZero does effectively that.
[01:10:42.840 --> 01:10:45.540]   It does those rollouts and those murdering
[01:10:45.540 --> 01:10:48.780]   in a learned simulator, in a learned dynamics model.
[01:10:48.780 --> 01:10:51.640]   - It's interesting, it doesn't get enough love.
[01:10:51.640 --> 01:10:54.260]   - I was blown away when I read that paper.
[01:10:54.260 --> 01:10:57.120]   I'm like, okay, I've always said a comma.
[01:10:57.120 --> 01:10:57.960]   I'm gonna sit and I'm gonna wait
[01:10:57.960 --> 01:11:00.400]   for the solution to self-driving cars to come along.
[01:11:00.400 --> 01:11:01.760]   This year I saw it, it's MuZero.
[01:11:01.760 --> 01:11:02.600]   - Yeah.
[01:11:02.600 --> 01:11:05.940]   So.
[01:11:06.920 --> 01:11:09.240]   - Sit back and let the winning roll in.
[01:11:09.240 --> 01:11:12.360]   - So your sense, just to elaborate a little bit,
[01:11:12.360 --> 01:11:15.160]   to linger on the topic, your sense is neural networks
[01:11:15.160 --> 01:11:16.360]   will solve driving.
[01:11:16.360 --> 01:11:17.200]   - Yes.
[01:11:17.200 --> 01:11:18.880]   - Like we don't need anything else.
[01:11:18.880 --> 01:11:20.600]   - I think the same way chess was maybe,
[01:11:20.600 --> 01:11:22.960]   chess and maybe Google are the pinnacle
[01:11:22.960 --> 01:11:25.440]   of like search algorithms and things
[01:11:25.440 --> 01:11:26.640]   that look kind of like A*.
[01:11:26.640 --> 01:11:32.820]   The pinnacle of this era is going to be self-driving cars.
[01:11:34.760 --> 01:11:38.160]   - But on the path to that, you have to deliver products.
[01:11:38.160 --> 01:11:40.280]   And it's possible that the path
[01:11:40.280 --> 01:11:44.440]   to full self-driving cars will take decades.
[01:11:44.440 --> 01:11:45.280]   - I doubt it.
[01:11:45.280 --> 01:11:46.940]   - How long would you put on it?
[01:11:46.940 --> 01:11:52.420]   Like what are we, you're chasing it, Tesla's chasing it.
[01:11:52.420 --> 01:11:56.160]   What are we talking about, five years, 10 years, 50 years?
[01:11:56.160 --> 01:11:58.060]   - In the 2020s.
[01:11:58.060 --> 01:11:59.560]   - In the 2020s?
[01:11:59.560 --> 01:12:01.200]   - The later part of the 2020s.
[01:12:03.600 --> 01:12:05.520]   - With the neural network.
[01:12:05.520 --> 01:12:06.560]   That would be nice to see.
[01:12:06.560 --> 01:12:09.160]   And on the path to that, you're delivering products,
[01:12:09.160 --> 01:12:10.600]   which is a nice L2 system.
[01:12:10.600 --> 01:12:13.000]   That's what Tesla's doing, a nice L2 system.
[01:12:13.000 --> 01:12:14.400]   - It just gets better every time.
[01:12:14.400 --> 01:12:16.680]   L2, the only difference between L2 and the other levels
[01:12:16.680 --> 01:12:17.640]   is who takes liability.
[01:12:17.640 --> 01:12:20.120]   And I'm not a liability guy, I don't want to take liability.
[01:12:20.120 --> 01:12:21.560]   I'm going to level two forever.
[01:12:21.560 --> 01:12:25.760]   - Now on that little transition,
[01:12:25.760 --> 01:12:29.600]   I mean, how do you make the transition work?
[01:12:29.600 --> 01:12:32.800]   Is this where driver sensing comes in?
[01:12:32.800 --> 01:12:35.920]   Like how do you make the, 'cause you said a hundred miles,
[01:12:35.920 --> 01:12:40.440]   like is there some sort of human factors,
[01:12:40.440 --> 01:12:43.160]   psychology thing where people start to overtrust the system,
[01:12:43.160 --> 01:12:45.000]   all those kinds of effects.
[01:12:45.000 --> 01:12:46.800]   Once it gets better and better and better and better,
[01:12:46.800 --> 01:12:49.360]   they get lazier and lazier and lazier.
[01:12:49.360 --> 01:12:52.440]   Is that, like how do you get that transition right?
[01:12:52.440 --> 01:12:54.520]   - First off, our monitoring is already adaptive.
[01:12:54.520 --> 01:12:56.640]   Our monitoring is already seen adaptive.
[01:12:56.640 --> 01:12:58.920]   - Driver monitoring, is this the camera
[01:12:58.920 --> 01:13:00.080]   that's looking at the driver?
[01:13:00.080 --> 01:13:02.080]   You have an infrared camera in the?
[01:13:02.080 --> 01:13:06.360]   - Our policy for how we enforce the driver monitoring
[01:13:06.360 --> 01:13:07.960]   is seen adaptive.
[01:13:07.960 --> 01:13:08.800]   - What's that mean?
[01:13:08.800 --> 01:13:11.460]   - Well, for example, in one of the extreme cases,
[01:13:11.460 --> 01:13:14.920]   if the car is not moving,
[01:13:14.920 --> 01:13:17.640]   we do not actively enforce driver monitoring.
[01:13:17.640 --> 01:13:24.520]   If you are going through a 45 mile an hour road with lights
[01:13:24.520 --> 01:13:28.040]   and stop signs and potentially pedestrians,
[01:13:28.040 --> 01:13:30.840]   we enforce a very tight driver monitoring policy.
[01:13:30.840 --> 01:13:33.960]   If you are alone on a perfectly straight highway,
[01:13:33.960 --> 01:13:35.680]   and this is, it's all machine learning,
[01:13:35.680 --> 01:13:37.000]   none of that is hand-coded.
[01:13:37.000 --> 01:13:39.160]   Actually, the stop is hand-coded, but.
[01:13:39.160 --> 01:13:42.400]   - So there's some kind of machine learning estimation of risk.
[01:13:42.400 --> 01:13:43.720]   - Yes.
[01:13:43.720 --> 01:13:45.920]   - Yeah, I mean, I've always been a huge fan of that.
[01:13:45.920 --> 01:13:49.720]   That's, it's difficult to do.
[01:13:49.720 --> 01:13:55.160]   Every step into that direction is a worthwhile step to take.
[01:13:55.160 --> 01:13:56.640]   It might be difficult to do really well.
[01:13:56.640 --> 01:14:00.080]   Like us humans are able to estimate risk pretty damn well,
[01:14:00.080 --> 01:14:01.600]   whatever the hell that is.
[01:14:01.600 --> 01:14:05.640]   That feels like one of the nice features of us humans.
[01:14:05.640 --> 01:14:09.080]   'Cause like we humans are really good drivers
[01:14:09.080 --> 01:14:11.160]   when we're really like tuned in.
[01:14:11.160 --> 01:14:13.040]   And we're good at estimating risk,
[01:14:13.040 --> 01:14:15.000]   like when are we supposed to be tuned in?
[01:14:15.000 --> 01:14:16.120]   - Yeah.
[01:14:16.120 --> 01:14:18.320]   And you know, people are like, oh, well, you know,
[01:14:18.320 --> 01:14:20.560]   why would you ever make the driver monitoring policy
[01:14:20.560 --> 01:14:21.400]   less aggressive?
[01:14:21.400 --> 01:14:24.120]   Why would you always not keep it at its most aggressive?
[01:14:24.120 --> 01:14:25.880]   Because then people are just gonna get fatigue from it.
[01:14:25.880 --> 01:14:27.320]   - Yeah, well, and they get annoyed.
[01:14:27.320 --> 01:14:30.720]   You want them, you want the experience to be pleasant.
[01:14:30.720 --> 01:14:32.520]   - Obviously I want the experience to be pleasant,
[01:14:32.520 --> 01:14:36.000]   but even just from a straight up safety perspective,
[01:14:36.000 --> 01:14:39.760]   if you alert people when they look around and they're like,
[01:14:39.760 --> 01:14:41.040]   why is this thing alerting me?
[01:14:41.040 --> 01:14:43.020]   There's nothing I could possibly hit right now.
[01:14:43.020 --> 01:14:45.240]   People will just learn to tune it out.
[01:14:45.240 --> 01:14:46.940]   People will just learn to tune it out,
[01:14:46.940 --> 01:14:48.080]   to put weights on the steering wheel,
[01:14:48.080 --> 01:14:49.840]   to do whatever to overcome it.
[01:14:49.840 --> 01:14:53.600]   And remember that you're always part of this adaptive system.
[01:14:53.600 --> 01:14:55.680]   So all I can really say about, you know,
[01:14:55.680 --> 01:14:57.200]   how this scales going forward is yeah,
[01:14:57.200 --> 01:14:59.440]   something we have to monitor for.
[01:14:59.440 --> 01:15:00.280]   We don't know.
[01:15:00.280 --> 01:15:02.080]   This is a great psychology experiment at scale.
[01:15:02.080 --> 01:15:03.240]   Like, we'll see.
[01:15:03.240 --> 01:15:04.080]   - Yeah, it's fascinating.
[01:15:04.080 --> 01:15:07.640]   - Track it and making sure you have a good understanding
[01:15:07.640 --> 01:15:11.400]   of attention is a very key part of that psychology problem.
[01:15:11.400 --> 01:15:14.120]   - Yeah, I think, I mean, you and I probably have a different
[01:15:14.120 --> 01:15:16.680]   come to it differently, but to me,
[01:15:16.680 --> 01:15:19.700]   it's a fascinating psychology problem
[01:15:19.700 --> 01:15:22.080]   to explore something much deeper than just driving.
[01:15:22.080 --> 01:15:26.720]   It's such a nice way to explore human attention
[01:15:26.720 --> 01:15:30.200]   and human behavior, which is why, again,
[01:15:30.200 --> 01:15:34.120]   we've probably both criticized Mr. Elon Musk
[01:15:34.120 --> 01:15:38.240]   on this one topic from different avenues.
[01:15:38.240 --> 01:15:41.280]   So both offline and online, I had little chats with Elon,
[01:15:41.280 --> 01:15:46.040]   and like, I love human beings.
[01:15:46.040 --> 01:15:49.560]   As a computer vision problem, as an AI problem,
[01:15:49.560 --> 01:15:51.080]   it's fascinating.
[01:15:51.080 --> 01:15:53.320]   He wasn't so much interested in that problem.
[01:15:53.320 --> 01:15:56.800]   It's like, in order to solve driving,
[01:15:56.800 --> 01:15:58.800]   the whole point is you want to remove the human
[01:15:58.800 --> 01:15:59.780]   from the picture.
[01:15:59.780 --> 01:16:04.140]   And it seems like you can't do that quite yet.
[01:16:04.140 --> 01:16:07.920]   Eventually, yes, but you can't quite do that yet.
[01:16:07.920 --> 01:16:12.320]   So this is the moment where you can't yet say,
[01:16:12.320 --> 01:16:15.160]   I told you so, to Tesla.
[01:16:15.160 --> 01:16:18.400]   But it's getting there because I don't know
[01:16:18.400 --> 01:16:19.240]   if you've seen this.
[01:16:19.240 --> 01:16:21.480]   There's some reporting that they're in fact starting
[01:16:21.480 --> 01:16:23.200]   to do driver monitoring.
[01:16:23.200 --> 01:16:25.040]   - Yeah, they shipped the model in shadow mode.
[01:16:25.040 --> 01:16:29.600]   - With, I believe, only a visible light camera.
[01:16:29.600 --> 01:16:31.760]   It might even be fisheye.
[01:16:31.760 --> 01:16:33.240]   It's like a low resolution.
[01:16:33.240 --> 01:16:34.840]   - Low resolution, visible light.
[01:16:34.840 --> 01:16:37.120]   I mean, to be fair, that's what we have in the Eon as well.
[01:16:37.120 --> 01:16:38.820]   Our last generation product.
[01:16:38.820 --> 01:16:41.080]   This is the one area where I can say
[01:16:41.080 --> 01:16:42.200]   our hardware is ahead of Tesla.
[01:16:42.200 --> 01:16:43.820]   The rest of our hardware, way, way behind,
[01:16:43.820 --> 01:16:46.000]   but our driver monitoring camera.
[01:16:46.000 --> 01:16:51.000]   - So you think, I think on the third row Tesla podcast,
[01:16:51.000 --> 01:16:53.840]   somewhere else, I've heard you say that,
[01:16:53.840 --> 01:16:55.520]   obviously, eventually, they're going to have
[01:16:55.520 --> 01:16:57.240]   driver monitoring.
[01:16:57.240 --> 01:16:59.660]   - I think what I've said is Elon will definitely ship
[01:16:59.660 --> 01:17:01.840]   driver monitoring before he ships level five.
[01:17:01.840 --> 01:17:02.680]   - Before level five.
[01:17:02.680 --> 01:17:04.640]   - And I'm willing to bet 10 grand on that.
[01:17:04.640 --> 01:17:06.200]   - And you bet 10 grand on that.
[01:17:06.200 --> 01:17:08.260]   - I mean, now I know where to take the bet,
[01:17:08.260 --> 01:17:09.560]   but before, maybe someone would have.
[01:17:09.560 --> 01:17:10.520]   I should have got my money in.
[01:17:10.520 --> 01:17:11.920]   - Yeah.
[01:17:11.920 --> 01:17:12.960]   It's an interesting bet.
[01:17:12.960 --> 01:17:16.480]   I think you're right.
[01:17:16.480 --> 01:17:19.200]   I'm actually, on a human level,
[01:17:19.200 --> 01:17:23.600]   because he's been, he's made the decision,
[01:17:23.600 --> 01:17:27.700]   like he said that driver monitoring is the wrong way to go,
[01:17:27.700 --> 01:17:31.280]   but you have to think of, as a human, as a CEO,
[01:17:31.280 --> 01:17:35.020]   I think that's the right thing to say when,
[01:17:35.020 --> 01:17:40.160]   sometimes you have to say things publicly
[01:17:40.160 --> 01:17:41.800]   that are different than what you actually believe,
[01:17:41.800 --> 01:17:45.480]   because when you're producing a large number of vehicles,
[01:17:45.480 --> 01:17:47.880]   and the decision was made not to include the camera,
[01:17:47.880 --> 01:17:49.480]   like what are you supposed to say?
[01:17:49.480 --> 01:17:51.800]   Like, our cars don't have the thing
[01:17:51.800 --> 01:17:54.040]   that I think is right to have.
[01:17:54.040 --> 01:17:57.520]   It's an interesting thing, but on the other side,
[01:17:57.520 --> 01:17:59.800]   as a CEO, I mean, something you could probably speak to
[01:17:59.800 --> 01:18:03.900]   as a leader, I think about me as a human,
[01:18:03.900 --> 01:18:07.040]   to publicly change your mind on something.
[01:18:07.040 --> 01:18:08.400]   How hard is that?
[01:18:08.400 --> 01:18:10.560]   Especially when assholes like George Haas say,
[01:18:10.560 --> 01:18:12.360]   "I told you so."
[01:18:12.360 --> 01:18:14.720]   - All I will say is I am not a leader,
[01:18:14.720 --> 01:18:17.060]   and I am happy to change my mind.
[01:18:17.060 --> 01:18:17.900]   And I will--
[01:18:17.900 --> 01:18:18.720]   - You think Elon will?
[01:18:18.720 --> 01:18:21.440]   - Yeah, I do.
[01:18:21.440 --> 01:18:24.300]   I think he'll come up with a good way
[01:18:24.300 --> 01:18:27.440]   to make it psychologically okay for him.
[01:18:27.440 --> 01:18:29.720]   - Well, it's such an important thing, man,
[01:18:29.720 --> 01:18:31.480]   especially for a first principles thinker,
[01:18:31.480 --> 01:18:34.800]   'cause he made a decision that driver monitoring
[01:18:34.800 --> 01:18:35.800]   is not the right way to go.
[01:18:35.800 --> 01:18:37.680]   And I could see that decision,
[01:18:37.680 --> 01:18:39.280]   and I could even make that decision.
[01:18:39.440 --> 01:18:41.840]   I was on the fence, too.
[01:18:41.840 --> 01:18:47.280]   Driver monitoring is such an obvious,
[01:18:47.280 --> 01:18:50.080]   simple solution to the problem of attention.
[01:18:50.080 --> 01:18:52.920]   It's not obvious to me that just by putting a camera there,
[01:18:52.920 --> 01:18:54.400]   you solve things.
[01:18:54.400 --> 01:18:59.160]   You have to create an incredible, compelling experience,
[01:18:59.160 --> 01:19:01.160]   just like you're talking about.
[01:19:01.160 --> 01:19:03.400]   I don't know if it's easy to do that.
[01:19:03.400 --> 01:19:06.100]   It's not at all easy to do that, in fact, I think.
[01:19:06.100 --> 01:19:11.100]   So as a creator of a car that's trying to create a product
[01:19:11.100 --> 01:19:14.500]   that people love, which is what Tesla tries to do, right?
[01:19:14.500 --> 01:19:17.460]   It's not obvious to me that, you know,
[01:19:17.460 --> 01:19:18.660]   as a design decision,
[01:19:18.660 --> 01:19:21.260]   whether adding a camera is a good idea.
[01:19:21.260 --> 01:19:22.740]   From a safety perspective either,
[01:19:22.740 --> 01:19:25.420]   like in the human factors community,
[01:19:25.420 --> 01:19:27.700]   everybody says that you should obviously
[01:19:27.700 --> 01:19:30.820]   have driver sensing, driver monitoring.
[01:19:30.820 --> 01:19:35.820]   But like, that's like saying it's obvious
[01:19:36.540 --> 01:19:39.980]   as parents you shouldn't let your kids go out at night.
[01:19:39.980 --> 01:19:41.260]   But okay.
[01:19:41.260 --> 01:19:45.820]   But like, they're still gonna find ways to do drugs.
[01:19:45.820 --> 01:19:48.500]   - Yeah.
[01:19:48.500 --> 01:19:50.060]   - You have to also be good parents.
[01:19:50.060 --> 01:19:51.820]   So like, it's much more complicated
[01:19:51.820 --> 01:19:54.380]   than just you need to have driver monitoring.
[01:19:54.380 --> 01:19:58.980]   - I totally disagree on, okay, if you have a camera there
[01:19:58.980 --> 01:20:00.540]   and the camera's watching the person
[01:20:00.540 --> 01:20:03.740]   but never throws an alert, they'll never think about it.
[01:20:03.740 --> 01:20:04.580]   Right?
[01:20:04.580 --> 01:20:08.620]   The driver monitoring policy that you choose to,
[01:20:08.620 --> 01:20:10.420]   how you choose to communicate with the user
[01:20:10.420 --> 01:20:14.500]   is entirely separate from the data collection perspective.
[01:20:14.500 --> 01:20:15.340]   - Right.
[01:20:15.340 --> 01:20:16.180]   - Right?
[01:20:16.180 --> 01:20:21.180]   So, you know, like, there's one thing to say, like,
[01:20:21.180 --> 01:20:24.740]   you know, tell your teenager they can't do something.
[01:20:24.740 --> 01:20:27.260]   There's another thing to like, you know, gather the data.
[01:20:27.260 --> 01:20:28.740]   - So you can make informed decisions.
[01:20:28.740 --> 01:20:29.580]   That's really interesting.
[01:20:29.580 --> 01:20:31.220]   But you have to make that,
[01:20:31.220 --> 01:20:33.620]   that's the interesting thing about cars.
[01:20:33.620 --> 01:20:35.220]   But even true with CalmAI,
[01:20:35.220 --> 01:20:39.020]   like, you don't have to manufacture the thing into the car.
[01:20:39.020 --> 01:20:40.260]   Is you have to make a decision
[01:20:40.260 --> 01:20:44.380]   that anticipates the right strategy long-term.
[01:20:44.380 --> 01:20:46.820]   So like, you have to start collecting the data
[01:20:46.820 --> 01:20:47.940]   and start making decisions.
[01:20:47.940 --> 01:20:50.100]   - Started it three years ago.
[01:20:50.100 --> 01:20:52.820]   I believe that we have the best driver monitoring solution
[01:20:52.820 --> 01:20:53.660]   in the world.
[01:20:53.660 --> 01:20:56.860]   I think that when you compare it to,
[01:20:56.860 --> 01:20:58.100]   well, Super Cruise is the only other one
[01:20:58.100 --> 01:21:01.620]   that I really know that shipped and ours is better.
[01:21:01.620 --> 01:21:06.620]   What do you like and not like about Super Cruise?
[01:21:06.620 --> 01:21:08.780]   - I mean, I had a few Super Cruise,
[01:21:08.780 --> 01:21:12.060]   the sun would be shining through the window,
[01:21:12.060 --> 01:21:13.220]   would blind the camera,
[01:21:13.220 --> 01:21:14.580]   and it would say I wasn't paying attention
[01:21:14.580 --> 01:21:16.100]   when I was looking completely straight.
[01:21:16.100 --> 01:21:19.180]   I couldn't reset the attention with a steering wheel touch,
[01:21:19.180 --> 01:21:21.060]   and Super Cruise would disengage.
[01:21:21.060 --> 01:21:22.300]   Like, I was communicating to the car,
[01:21:22.300 --> 01:21:24.420]   I'm like, look, I am here, I am paying attention.
[01:21:24.420 --> 01:21:26.300]   Why are you really gonna force me to disengage?
[01:21:26.300 --> 01:21:27.380]   And it did.
[01:21:28.660 --> 01:21:32.100]   So it's a constant conversation with the user,
[01:21:32.100 --> 01:21:33.980]   and yeah, there's no way to ship a system like this
[01:21:33.980 --> 01:21:34.900]   if you can't OTA.
[01:21:34.900 --> 01:21:37.180]   We're shipping a new one every month.
[01:21:37.180 --> 01:21:40.580]   Sometimes we balance it with our users on Discord.
[01:21:40.580 --> 01:21:41.980]   Sometimes we make the driver monitoring
[01:21:41.980 --> 01:21:43.820]   a little more aggressive and people complain.
[01:21:43.820 --> 01:21:45.500]   Sometimes they don't.
[01:21:45.500 --> 01:21:47.060]   We want it to be as aggressive as possible
[01:21:47.060 --> 01:21:49.180]   where people don't complain and it doesn't feel intrusive.
[01:21:49.180 --> 01:21:51.100]   - So being able to update the system over the air
[01:21:51.100 --> 01:21:52.460]   is an essential component.
[01:21:52.460 --> 01:21:55.300]   I mean, that's probably, to me, you mentioned,
[01:21:56.700 --> 01:22:01.100]   I mean, to me, that is the biggest innovation of Tesla,
[01:22:01.100 --> 01:22:04.900]   that it made people realize that over the air updates
[01:22:04.900 --> 01:22:05.740]   is essential.
[01:22:05.740 --> 01:22:07.420]   - Yeah.
[01:22:07.420 --> 01:22:10.180]   Was that not obvious from the iPhone?
[01:22:10.180 --> 01:22:13.100]   The iPhone was the first real product that OTA'd, I think.
[01:22:13.100 --> 01:22:13.940]   - Was it?
[01:22:13.940 --> 01:22:14.780]   Actually, that's brilliant.
[01:22:14.780 --> 01:22:15.620]   You're right.
[01:22:15.620 --> 01:22:17.100]   - I mean, the game consoles used to not, right?
[01:22:17.100 --> 01:22:19.020]   The game consoles were maybe the second thing that did.
[01:22:19.020 --> 01:22:20.500]   - Well, I didn't really think about it.
[01:22:20.500 --> 01:22:23.540]   Well, one of the amazing features of a smartphone
[01:22:24.500 --> 01:22:27.740]   isn't just, like the touchscreen isn't the thing.
[01:22:27.740 --> 01:22:30.780]   It's the ability to constantly update.
[01:22:30.780 --> 01:22:32.020]   - Yeah, it gets better.
[01:22:32.020 --> 01:22:32.900]   - It gets better.
[01:22:32.900 --> 01:22:36.860]   - Love my iOS 14.
[01:22:36.860 --> 01:22:38.340]   - Yeah.
[01:22:38.340 --> 01:22:41.580]   Well, one thing that I probably disagree with you
[01:22:41.580 --> 01:22:45.620]   on driver monitoring is you've said that it's easy.
[01:22:45.620 --> 01:22:48.980]   I mean, you tend to say stuff is easy.
[01:22:48.980 --> 01:22:49.820]   I'm sure.
[01:22:51.580 --> 01:22:53.260]   I guess you said it's easy relative
[01:22:53.260 --> 01:22:55.820]   to the external perception problem though.
[01:22:55.820 --> 01:23:00.980]   Can you elaborate why you think it's easy?
[01:23:00.980 --> 01:23:03.580]   - Feature engineering works for driver monitoring.
[01:23:03.580 --> 01:23:05.940]   Feature engineering does not work for the external.
[01:23:05.940 --> 01:23:08.900]   - So human faces are not,
[01:23:08.900 --> 01:23:11.780]   human faces and the movement of human faces
[01:23:11.780 --> 01:23:14.820]   and head and body is not as variable
[01:23:14.820 --> 01:23:16.140]   as the external environment?
[01:23:16.140 --> 01:23:17.220]   - Yeah. - Is your intuition?
[01:23:17.220 --> 01:23:20.220]   - Yes, and there's another big difference as well.
[01:23:20.220 --> 01:23:22.580]   Your reliability of a driver monitoring system
[01:23:22.580 --> 01:23:24.460]   doesn't actually need to be that high.
[01:23:24.460 --> 01:23:27.300]   The uncertainty, if you have something that's detecting
[01:23:27.300 --> 01:23:28.540]   whether the human's paying attention
[01:23:28.540 --> 01:23:30.620]   and only works 92% of the time,
[01:23:30.620 --> 01:23:32.820]   you're still getting almost all the benefit of that
[01:23:32.820 --> 01:23:35.420]   because the human, you're training the human.
[01:23:35.420 --> 01:23:39.180]   You're dealing with a system that's really helping you out.
[01:23:39.180 --> 01:23:40.260]   It's a conversation.
[01:23:40.260 --> 01:23:43.540]   It's not like the external thing where, guess what?
[01:23:43.540 --> 01:23:46.540]   If you swerve into a tree, you swerve into a tree.
[01:23:46.540 --> 01:23:48.420]   You get no margin for error there.
[01:23:48.420 --> 01:23:49.700]   - Yeah, I think that's really well put.
[01:23:49.700 --> 01:23:54.020]   I think that's the right, exactly the place
[01:23:54.020 --> 01:23:58.620]   where comparing to the external perception
[01:23:58.620 --> 01:24:00.300]   and the control problem,
[01:24:00.300 --> 01:24:02.220]   driver monitoring is easier because you don't,
[01:24:02.220 --> 01:24:04.400]   the bar for success is much lower.
[01:24:04.400 --> 01:24:09.140]   Yeah, but I still think like the human face
[01:24:09.140 --> 01:24:12.140]   is more complicated actually than the external environment,
[01:24:12.140 --> 01:24:14.380]   but for driving, you don't give a damn.
[01:24:14.380 --> 01:24:17.100]   - I don't need something that complicated
[01:24:18.740 --> 01:24:22.260]   to have to communicate the idea to the human
[01:24:22.260 --> 01:24:24.020]   that I want to communicate, which is,
[01:24:24.020 --> 01:24:25.780]   yo, system might mess up here.
[01:24:25.780 --> 01:24:27.820]   You got to pay attention.
[01:24:27.820 --> 01:24:31.300]   - Yeah, see, that's my love and fascination
[01:24:31.300 --> 01:24:32.620]   is the human face.
[01:24:32.620 --> 01:24:36.460]   And it feels like this is a nice place
[01:24:36.460 --> 01:24:40.100]   to create products that create an experience in the car.
[01:24:40.100 --> 01:24:42.660]   So like, it feels like there should be
[01:24:42.660 --> 01:24:46.160]   more richer experiences in the car.
[01:24:47.240 --> 01:24:51.540]   You know, that's an opportunity for something like CalmAI
[01:24:51.540 --> 01:24:53.900]   or just any kind of system like a Tesla
[01:24:53.900 --> 01:24:56.260]   or any of the autonomous vehicle companies
[01:24:56.260 --> 01:24:59.260]   is because software is, and there's much more sensors
[01:24:59.260 --> 01:25:00.680]   and so much is on a software
[01:25:00.680 --> 01:25:02.960]   and you're doing machine learning anyway,
[01:25:02.960 --> 01:25:06.340]   there's an opportunity to create totally new experiences
[01:25:06.340 --> 01:25:08.300]   that we're not even anticipating.
[01:25:08.300 --> 01:25:09.300]   You don't think so?
[01:25:09.300 --> 01:25:11.020]   - Nah.
[01:25:11.020 --> 01:25:12.940]   - You think it's a box that gets you from A to B
[01:25:12.940 --> 01:25:14.940]   and you want to do it, chill.
[01:25:14.940 --> 01:25:16.460]   - Yeah, I mean, I think as soon as we get
[01:25:16.460 --> 01:25:19.300]   to level three on highways, okay, enjoy your Candy Crush,
[01:25:19.300 --> 01:25:23.660]   enjoy your Hulu, enjoy your, you know, whatever, whatever.
[01:25:23.660 --> 01:25:24.480]   Sure, you get this.
[01:25:24.480 --> 01:25:26.260]   You can look at screens basically.
[01:25:26.260 --> 01:25:27.580]   Versus right now, what do you have?
[01:25:27.580 --> 01:25:28.700]   Music and audio books.
[01:25:28.700 --> 01:25:30.740]   - So level three is where you can kind of disengage
[01:25:30.740 --> 01:25:32.300]   in stretches of time.
[01:25:32.300 --> 01:25:37.460]   Well, you think level three is possible?
[01:25:37.460 --> 01:25:39.180]   Like on the highway going for a hundred miles
[01:25:39.180 --> 01:25:40.500]   and you can just go to sleep?
[01:25:40.500 --> 01:25:42.100]   - Oh yeah.
[01:25:42.100 --> 01:25:43.600]   Sleep.
[01:25:43.640 --> 01:25:47.400]   So again, I think it's really all on a spectrum.
[01:25:47.400 --> 01:25:50.120]   I think that being able to use your phone
[01:25:50.120 --> 01:25:53.560]   while you're on the highway and like this all being okay
[01:25:53.560 --> 01:25:55.440]   and being aware that the car might alert you
[01:25:55.440 --> 01:25:57.240]   and you have five seconds to basically.
[01:25:57.240 --> 01:25:59.120]   - So the five second thing you think is possible.
[01:25:59.120 --> 01:25:59.960]   - Yeah, I think it is.
[01:25:59.960 --> 01:26:00.780]   Oh yeah.
[01:26:00.780 --> 01:26:02.280]   Not in all scenarios.
[01:26:02.280 --> 01:26:04.000]   Some scenarios it's not.
[01:26:04.000 --> 01:26:06.400]   - It's the whole risk thing that you mentioned is nice.
[01:26:06.400 --> 01:26:10.720]   It's to be able to estimate like how risky is this situation.
[01:26:10.720 --> 01:26:12.700]   That's really important to understand.
[01:26:12.700 --> 01:26:17.380]   One other thing you mentioned comparing Kama and Autopilot
[01:26:17.380 --> 01:26:22.380]   is that something about the haptic feel
[01:26:22.380 --> 01:26:24.400]   of the way Kama controls the car
[01:26:24.400 --> 01:26:25.960]   when things are uncertain.
[01:26:25.960 --> 01:26:27.800]   Like it behaves a little bit more uncertain
[01:26:27.800 --> 01:26:29.440]   when things are uncertain.
[01:26:29.440 --> 01:26:31.120]   That's kind of an interesting point.
[01:26:31.120 --> 01:26:34.120]   And then Autopilot is much more confident always,
[01:26:34.120 --> 01:26:37.500]   even when it's uncertain until it runs into trouble.
[01:26:37.500 --> 01:26:40.960]   That's a funny thing.
[01:26:40.960 --> 01:26:42.740]   I actually mentioned that to Elon, I think.
[01:26:42.740 --> 01:26:46.340]   And then the first time we talked, he wasn't biting.
[01:26:46.340 --> 01:26:48.940]   It's like communicating uncertainty.
[01:26:48.940 --> 01:26:51.900]   I guess Kama doesn't really communicate uncertainty
[01:26:51.900 --> 01:26:55.060]   explicitly, it communicates it through haptic feel.
[01:26:55.060 --> 01:26:57.460]   Like what's the role of communicating uncertainty
[01:26:57.460 --> 01:26:58.280]   do you think?
[01:26:58.280 --> 01:26:59.900]   - Oh, we do some stuff explicitly.
[01:26:59.900 --> 01:27:01.860]   Like we do detect the lanes when you're on the highway
[01:27:01.860 --> 01:27:04.420]   and we'll show you how many lanes we're using to drive with.
[01:27:04.420 --> 01:27:06.260]   You can look at where it thinks the lanes are.
[01:27:06.260 --> 01:27:10.460]   You can look at the path and we want to be better about this
[01:27:10.460 --> 01:27:12.920]   and we're actually hiring, want to hire some new UI people.
[01:27:12.920 --> 01:27:14.320]   - UI people, you mentioned this.
[01:27:14.320 --> 01:27:16.660]   'Cause it's such an, it's a UI problem too, right?
[01:27:16.660 --> 01:27:17.500]   It's--
[01:27:17.500 --> 01:27:19.720]   - We have a great designer now, but you know,
[01:27:19.720 --> 01:27:21.160]   we need people who are just going to like build this
[01:27:21.160 --> 01:27:23.800]   and debug these UIs, Qt people.
[01:27:23.800 --> 01:27:26.440]   - Qt, is that what the UI is done with, is Qt?
[01:27:26.440 --> 01:27:28.000]   - Moving, the new UI is in Qt.
[01:27:28.000 --> 01:27:30.160]   - C++ Qt?
[01:27:30.160 --> 01:27:33.280]   - Tesla uses it too.
[01:27:33.280 --> 01:27:34.200]   - Yeah.
[01:27:34.200 --> 01:27:36.040]   - We had some React stuff in there.
[01:27:36.040 --> 01:27:39.440]   - React.js or just React?
[01:27:39.440 --> 01:27:41.160]   React is its own language, right?
[01:27:41.160 --> 01:27:44.480]   - React native, React is a JavaScript framework.
[01:27:44.480 --> 01:27:45.320]   - Yeah.
[01:27:45.320 --> 01:27:48.080]   - It's all based on JavaScript, but it's, you know,
[01:27:48.080 --> 01:27:49.780]   I like C++.
[01:27:49.780 --> 01:27:55.040]   - What do you think about Dojo with Tesla
[01:27:55.040 --> 01:27:58.720]   and their foray into what appears to be
[01:27:58.720 --> 01:28:03.320]   specialized hardware for training your nets?
[01:28:03.320 --> 01:28:07.340]   I guess it's something, maybe you can correct me,
[01:28:07.340 --> 01:28:09.960]   from my shallow looking at it,
[01:28:09.960 --> 01:28:12.080]   it seems like something that Google did with TPUs,
[01:28:12.080 --> 01:28:15.680]   but specialized for driving data.
[01:28:15.680 --> 01:28:18.360]   - I don't think it's specialized for driving data.
[01:28:18.360 --> 01:28:20.120]   - It's just legit, just TPU.
[01:28:20.120 --> 01:28:22.160]   They want to go the Apple way,
[01:28:22.160 --> 01:28:25.640]   basically everything required in the chain is done in-house.
[01:28:25.640 --> 01:28:27.840]   - Well, so you have a problem right now,
[01:28:27.840 --> 01:28:31.720]   and this is one of my concerns.
[01:28:31.720 --> 01:28:33.800]   I really would like to see somebody deal with this,
[01:28:33.800 --> 01:28:35.280]   if anyone out there is doing it,
[01:28:35.280 --> 01:28:37.140]   I'd like to help them if I can.
[01:28:37.140 --> 01:28:40.580]   You basically have two options right now to train.
[01:28:40.580 --> 01:28:43.800]   Your options are NVIDIA or Google.
[01:28:43.800 --> 01:28:48.700]   So Google is not even an option.
[01:28:48.700 --> 01:28:53.100]   Their TPUs are only available in Google Cloud.
[01:28:53.100 --> 01:28:55.060]   Google has absolutely onerous
[01:28:55.060 --> 01:28:56.700]   terms of service restrictions.
[01:28:56.700 --> 01:28:59.200]   They may have changed it,
[01:28:59.200 --> 01:29:00.540]   but back in Google's terms of service,
[01:29:00.540 --> 01:29:03.660]   it said explicitly you are not allowed to use Google Cloud ML
[01:29:03.660 --> 01:29:05.280]   for training autonomous vehicles,
[01:29:05.280 --> 01:29:07.280]   or for doing anything that competes with Google
[01:29:07.280 --> 01:29:09.200]   without Google's prior written permission.
[01:29:09.200 --> 01:29:10.040]   - Wow, okay.
[01:29:10.040 --> 01:29:12.360]   - I mean, Google is not a platform company.
[01:29:12.360 --> 01:29:16.680]   I wouldn't touch TPUs at the 10-foot pole.
[01:29:16.680 --> 01:29:19.200]   So that leaves you with the monopoly.
[01:29:19.200 --> 01:29:20.040]   - NVIDIA?
[01:29:20.040 --> 01:29:20.960]   - NVIDIA.
[01:29:20.960 --> 01:29:22.400]   So, I mean--
[01:29:22.400 --> 01:29:23.840]   - That you're not a fan of.
[01:29:23.840 --> 01:29:28.480]   - Well, look, I was a huge fan of in 2016 NVIDIA.
[01:29:28.480 --> 01:29:29.920]   Jensen Kane sat in the car.
[01:29:31.860 --> 01:29:34.580]   Cool guy, when the stock was $30 a share.
[01:29:34.580 --> 01:29:37.300]   NVIDIA's stock has skyrocketed.
[01:29:37.300 --> 01:29:41.020]   I witnessed a real change in who was in management
[01:29:41.020 --> 01:29:43.540]   over there in like 2018.
[01:29:43.540 --> 01:29:46.660]   And now they are, let's exploit,
[01:29:46.660 --> 01:29:48.380]   let's take every dollar we possibly can
[01:29:48.380 --> 01:29:49.540]   out of this ecosystem.
[01:29:49.540 --> 01:29:51.700]   Let's charge $10,000 for A100s
[01:29:51.700 --> 01:29:54.140]   because we know we got the best shit in the game.
[01:29:54.140 --> 01:29:57.880]   And let's charge $10,000 for an A100
[01:29:57.880 --> 01:30:00.060]   when it's really not that different from a 3080,
[01:30:00.060 --> 01:30:01.460]   which is $699.
[01:30:02.360 --> 01:30:05.040]   The margins that they are making
[01:30:05.040 --> 01:30:08.600]   off of those high-end chips are so high
[01:30:08.600 --> 01:30:10.240]   that, I mean, I think they're shooting themselves
[01:30:10.240 --> 01:30:12.160]   in the foot, just from a business perspective.
[01:30:12.160 --> 01:30:14.920]   Because there's a lot of people talking like me now
[01:30:14.920 --> 01:30:17.520]   who are like, "Somebody's got to take NVIDIA down."
[01:30:17.520 --> 01:30:19.880]   - Yeah.
[01:30:19.880 --> 01:30:21.000]   - Where they could dominate it.
[01:30:21.000 --> 01:30:22.440]   NVIDIA could be the new Intel.
[01:30:22.440 --> 01:30:26.920]   - Yeah, to be inside everything, essentially.
[01:30:26.920 --> 01:30:30.640]   And yet the winners in certain spaces,
[01:30:30.640 --> 01:30:33.760]   like in autonomous driving, the winners,
[01:30:33.760 --> 01:30:36.560]   only the people who are like desperately falling back
[01:30:36.560 --> 01:30:38.500]   and trying to catch up and have a ton of money,
[01:30:38.500 --> 01:30:40.140]   like the big automakers,
[01:30:40.140 --> 01:30:43.200]   are the ones interested in partnering with NVIDIA.
[01:30:43.200 --> 01:30:44.800]   - Oh, and I think a lot of those things
[01:30:44.800 --> 01:30:45.900]   are going to fall through.
[01:30:45.900 --> 01:30:49.280]   If I were NVIDIA, sell chips.
[01:30:49.280 --> 01:30:52.240]   Sell chips at a reasonable markup.
[01:30:52.240 --> 01:30:53.080]   - To everybody.
[01:30:53.080 --> 01:30:53.900]   - To everybody.
[01:30:53.900 --> 01:30:54.920]   - Without any restrictions.
[01:30:54.920 --> 01:30:56.080]   - Without any restrictions.
[01:30:56.080 --> 01:30:57.320]   Intel did this.
[01:30:57.320 --> 01:30:58.240]   Look at Intel.
[01:30:58.240 --> 01:30:59.880]   They had a great long run.
[01:30:59.880 --> 01:31:01.520]   NVIDIA is trying to turn their,
[01:31:01.520 --> 01:31:05.600]   they're like trying to productize their chips way too much.
[01:31:05.600 --> 01:31:07.840]   They're trying to extract way more value
[01:31:07.840 --> 01:31:09.360]   than they can sustainably.
[01:31:09.360 --> 01:31:10.760]   Sure, you can do it tomorrow.
[01:31:10.760 --> 01:31:12.120]   Is it going to up your share price?
[01:31:12.120 --> 01:31:13.700]   Sure, if you're one of those CEOs who's like,
[01:31:13.700 --> 01:31:15.280]   "How much can I strip mine this company?"
[01:31:15.280 --> 01:31:17.840]   And I think, you know, and that's what's weird about it too.
[01:31:17.840 --> 01:31:19.360]   Like the CEO is the founder.
[01:31:19.360 --> 01:31:20.520]   It's the same guy.
[01:31:20.520 --> 01:31:22.320]   I mean, I still think Jensen's a great guy.
[01:31:22.320 --> 01:31:23.280]   - He is great.
[01:31:23.280 --> 01:31:25.160]   - Why do this?
[01:31:25.160 --> 01:31:26.680]   You have a choice.
[01:31:26.680 --> 01:31:27.960]   You have a choice right now.
[01:31:27.960 --> 01:31:28.840]   Are you trying to cash out?
[01:31:28.840 --> 01:31:30.680]   Are you trying to buy a yacht?
[01:31:30.680 --> 01:31:32.120]   If you are, fine.
[01:31:32.120 --> 01:31:34.240]   But if you're trying to be
[01:31:34.240 --> 01:31:37.280]   the next huge semiconductor company, sell chips.
[01:31:37.280 --> 01:31:40.160]   - Well, the interesting thing about Jensen
[01:31:40.160 --> 01:31:42.080]   is he is a big vision guy.
[01:31:42.080 --> 01:31:47.080]   So he has a plan like for 50 years down the road.
[01:31:47.080 --> 01:31:50.560]   So it makes me wonder like--
[01:31:50.560 --> 01:31:51.840]   - How does price gouging fit into it?
[01:31:51.840 --> 01:31:53.160]   - Yeah, how does that fit?
[01:31:53.160 --> 01:31:57.080]   Like it doesn't seem to make sense as a plan.
[01:31:57.080 --> 01:31:59.320]   - I worry that he's listening to the wrong people.
[01:31:59.320 --> 01:32:02.560]   - Yeah, that's the sense I have too sometimes.
[01:32:02.560 --> 01:32:05.280]   'Cause I, despite everything,
[01:32:05.280 --> 01:32:09.040]   I think Nvidia is an incredible company.
[01:32:09.040 --> 01:32:12.440]   Well, one, I'm deeply grateful to Nvidia
[01:32:12.440 --> 01:32:14.680]   for the products they've created in the past, right?
[01:32:14.680 --> 01:32:16.240]   And so--
[01:32:16.240 --> 01:32:18.040]   - The 1080 Ti was a great GPU.
[01:32:18.040 --> 01:32:18.880]   Still have a lot of them.
[01:32:18.880 --> 01:32:20.200]   - Still is, yeah.
[01:32:20.200 --> 01:32:24.580]   But at the same time, it just feels like,
[01:32:26.920 --> 01:32:28.080]   it feels like you don't want to put
[01:32:28.080 --> 01:32:29.520]   all your stock in Nvidia.
[01:32:29.520 --> 01:32:32.160]   And so like Elon is doing,
[01:32:32.160 --> 01:32:35.080]   what Tesla is doing with Autopilot and Dojo
[01:32:35.080 --> 01:32:37.320]   is the Apple way.
[01:32:37.320 --> 01:32:40.320]   'Cause they're not going to share Dojo with George Hotz.
[01:32:40.320 --> 01:32:43.800]   - I know, they should sell that chip.
[01:32:43.800 --> 01:32:46.400]   Oh, they should sell, even their accelerator.
[01:32:46.400 --> 01:32:49.040]   The accelerator that's in all the cars, the 30 watt one.
[01:32:49.040 --> 01:32:50.600]   Sell it, why not?
[01:32:50.600 --> 01:32:52.680]   - So open it up.
[01:32:52.680 --> 01:32:55.800]   Like me, why does Tesla have to be a car company?
[01:32:55.800 --> 01:32:58.120]   Well, if you sell the chip, here's what you get.
[01:32:58.120 --> 01:32:59.040]   - Yeah.
[01:32:59.040 --> 01:33:00.280]   - Make some money off the chips.
[01:33:00.280 --> 01:33:02.080]   It doesn't take away from your chip.
[01:33:02.080 --> 01:33:03.880]   You're gonna make some money, free money.
[01:33:03.880 --> 01:33:06.600]   And also the world is gonna build
[01:33:06.600 --> 01:33:08.240]   an ecosystem of tooling for you.
[01:33:08.240 --> 01:33:12.860]   You're not gonna have to fix the bug in your 10H layer.
[01:33:12.860 --> 01:33:14.120]   Someone else already did.
[01:33:14.120 --> 01:33:16.760]   - Well, the question, that's an interesting question.
[01:33:16.760 --> 01:33:18.760]   I mean, that's the question Steve Jobs asked.
[01:33:18.760 --> 01:33:23.600]   That's the question Elon Musk is perhaps asking is,
[01:33:25.000 --> 01:33:27.780]   do you want Tesla stuff inside other vehicles?
[01:33:27.780 --> 01:33:32.660]   Potentially inside like iRobot Vacuum Cleaner.
[01:33:32.660 --> 01:33:33.500]   - Yeah.
[01:33:33.500 --> 01:33:37.160]   I think you should decide where your advantages are.
[01:33:37.160 --> 01:33:38.640]   I'm not saying Tesla should start selling
[01:33:38.640 --> 01:33:40.360]   battery packs to automakers.
[01:33:40.360 --> 01:33:41.720]   Because battery packs to automakers,
[01:33:41.720 --> 01:33:43.640]   they are straight up in competition with you.
[01:33:43.640 --> 01:33:46.040]   If I were Tesla, I'd keep the battery technology totally.
[01:33:46.040 --> 01:33:46.880]   - Yeah.
[01:33:46.880 --> 01:33:47.960]   - As is ours, we make batteries.
[01:33:47.960 --> 01:33:52.960]   But the thing about the Tesla TPU is anybody can build that.
[01:33:53.160 --> 01:33:54.600]   It's just a question of, you know,
[01:33:54.600 --> 01:33:57.480]   are you willing to spend the money?
[01:33:57.480 --> 01:34:00.240]   - It could be a huge source of revenue, potentially.
[01:34:00.240 --> 01:34:02.440]   - Are you willing to spend $100 million?
[01:34:02.440 --> 01:34:03.640]   Anyone can build it.
[01:34:03.640 --> 01:34:04.680]   And someone will.
[01:34:04.680 --> 01:34:06.680]   And a bunch of companies now are starting
[01:34:06.680 --> 01:34:08.040]   trying to build AI accelerators.
[01:34:08.040 --> 01:34:10.200]   Somebody's gonna get the idea right.
[01:34:10.200 --> 01:34:13.720]   And yeah, hopefully they don't get greedy.
[01:34:13.720 --> 01:34:15.780]   Because they'll just lose to the next guy who finally,
[01:34:15.780 --> 01:34:17.160]   and then eventually the Chinese are gonna make
[01:34:17.160 --> 01:34:19.560]   knockoff Nvidia chips and that's.
[01:34:19.560 --> 01:34:21.120]   - From your perspective, I don't know if you're also
[01:34:21.120 --> 01:34:24.160]   paying attention to Stan Tesla for a moment.
[01:34:24.160 --> 01:34:27.840]   Dave, Elon Musk has talked about a complete rewrite
[01:34:27.840 --> 01:34:31.680]   of the neural net that they're using.
[01:34:31.680 --> 01:34:34.800]   That seems to, again, I'm half paying attention.
[01:34:34.800 --> 01:34:39.000]   But it seems to involve basically a kind of integration
[01:34:39.000 --> 01:34:44.000]   of all the sensors to where it's a four dimensional view.
[01:34:44.000 --> 01:34:47.540]   You know, you have a 3D model of the world over time.
[01:34:47.540 --> 01:34:50.280]   And then you can, I think it's done both
[01:34:50.280 --> 01:34:54.240]   for the, actually, you know, so the neural network
[01:34:54.240 --> 01:34:56.960]   is able to, in a more holistic way,
[01:34:56.960 --> 01:34:59.360]   deal with the world and make predictions and so on.
[01:34:59.360 --> 01:35:02.660]   But also to make the annotation task more,
[01:35:02.660 --> 01:35:04.840]   you know, easier.
[01:35:04.840 --> 01:35:08.220]   Like you can annotate the world in one place
[01:35:08.220 --> 01:35:10.520]   and it kind of distributes itself across the sensors
[01:35:10.520 --> 01:35:14.080]   and across the different, like the hundreds of tasks
[01:35:14.080 --> 01:35:16.560]   that are involved in the hydranet.
[01:35:16.560 --> 01:35:19.180]   What are your thoughts about this rewrite?
[01:35:19.180 --> 01:35:22.440]   Is it just like some details that are kind of obvious
[01:35:22.440 --> 01:35:24.120]   that are steps that should be taken?
[01:35:24.120 --> 01:35:27.040]   Or is there something fundamental that could challenge
[01:35:27.040 --> 01:35:31.160]   your idea that end to end is the right solution?
[01:35:31.160 --> 01:35:33.200]   - We're in the middle of a big rewrite now as well.
[01:35:33.200 --> 01:35:34.880]   We haven't shipped a new model in a bit.
[01:35:34.880 --> 01:35:36.440]   - Of what kind?
[01:35:36.440 --> 01:35:38.280]   - We're going from 2D to 3D.
[01:35:38.280 --> 01:35:39.760]   Right now, all our stuff, like for example,
[01:35:39.760 --> 01:35:43.080]   when the car pitches back, the lane lines also pitch back
[01:35:43.080 --> 01:35:47.200]   because we're assuming the flat world hypothesis.
[01:35:47.200 --> 01:35:48.440]   The new models do not do this.
[01:35:48.440 --> 01:35:50.400]   The new models output everything in 3D.
[01:35:50.400 --> 01:35:53.620]   - So there's still no annotation.
[01:35:53.620 --> 01:35:56.540]   So the 3D is more about the output.
[01:35:56.540 --> 01:35:57.380]   Yeah.
[01:35:57.380 --> 01:35:59.540]   - We have Zs and everything.
[01:35:59.540 --> 01:36:01.380]   - Zs?
[01:36:01.380 --> 01:36:02.200]   - Yeah.
[01:36:02.200 --> 01:36:03.040]   - We added the Zs.
[01:36:03.040 --> 01:36:04.020]   - We added the Zs.
[01:36:04.020 --> 01:36:06.660]   We unified a lot of stuff as well.
[01:36:06.660 --> 01:36:08.960]   We switched from TensorFlow to PyTorch.
[01:36:08.960 --> 01:36:09.800]   - Nice.
[01:36:09.800 --> 01:36:13.780]   - My understanding of what Tesla's thing is,
[01:36:13.780 --> 01:36:15.700]   is that their annotator now annotates
[01:36:15.700 --> 01:36:17.000]   across the time dimension.
[01:36:17.780 --> 01:36:18.620]   - Mm-hmm.
[01:36:18.620 --> 01:36:22.040]   - I mean, cute.
[01:36:22.040 --> 01:36:24.440]   Why are you building an annotator?
[01:36:24.440 --> 01:36:26.760]   - I find their entire pipeline,
[01:36:26.760 --> 01:36:30.600]   I find your vision, I mean,
[01:36:30.600 --> 01:36:32.880]   the vision of end-to-end very compelling,
[01:36:32.880 --> 01:36:35.920]   but I also like the engineering of the data engine
[01:36:35.920 --> 01:36:37.440]   that they've created.
[01:36:37.440 --> 01:36:41.600]   In terms of supervised learning pipelines,
[01:36:41.600 --> 01:36:43.680]   that thing is damn impressive.
[01:36:43.680 --> 01:36:46.640]   You're basically, the idea is that you have
[01:36:46.640 --> 01:36:49.280]   hundreds of thousands of people
[01:36:49.280 --> 01:36:51.200]   that are doing data collection for you
[01:36:51.200 --> 01:36:52.400]   by doing their experience.
[01:36:52.400 --> 01:36:55.220]   So that's kind of similar to the CommAI model.
[01:36:55.220 --> 01:36:59.600]   And you're able to mine that data
[01:36:59.600 --> 01:37:01.980]   based on the kind of edge cases you need.
[01:37:01.980 --> 01:37:07.380]   I think it's harder to do in the end-to-end learning,
[01:37:07.380 --> 01:37:09.560]   the mining of the right edge cases.
[01:37:09.560 --> 01:37:11.440]   Like that's where feature engineering
[01:37:11.440 --> 01:37:14.120]   is actually really powerful,
[01:37:14.120 --> 01:37:17.280]   because us humans are able to do
[01:37:17.280 --> 01:37:19.040]   this kind of mining a little better.
[01:37:19.040 --> 01:37:21.960]   But yeah, there's obvious, as we know,
[01:37:21.960 --> 01:37:24.820]   there's obvious constraints and limitations to that idea.
[01:37:24.820 --> 01:37:28.240]   - Karpathy just tweeted, he's like,
[01:37:28.240 --> 01:37:29.600]   you get really interesting insights
[01:37:29.600 --> 01:37:32.860]   if you sort your validation set by loss,
[01:37:32.860 --> 01:37:36.400]   and look at the highest loss examples.
[01:37:36.400 --> 01:37:37.560]   - Yeah.
[01:37:37.560 --> 01:37:39.160]   - So yeah, I mean, you can do,
[01:37:39.160 --> 01:37:42.040]   we have a little data engine-like thing.
[01:37:42.040 --> 01:37:43.320]   We're training a segnet.
[01:37:43.320 --> 01:37:45.560]   And it's not fancy, it's just like,
[01:37:45.560 --> 01:37:48.280]   okay, train the new segnet,
[01:37:48.280 --> 01:37:50.040]   run it on 100,000 images,
[01:37:50.040 --> 01:37:52.160]   and now take the thousand with highest loss.
[01:37:52.160 --> 01:37:54.140]   Select 100 of those by human,
[01:37:54.140 --> 01:37:56.120]   put those, get those ones labeled,
[01:37:56.120 --> 01:37:57.840]   retrain, do it again.
[01:37:57.840 --> 01:38:01.480]   And so it's a much less well-written data engine.
[01:38:01.480 --> 01:38:03.600]   And yeah, you can take these things really far,
[01:38:03.600 --> 01:38:06.600]   and it is impressive engineering.
[01:38:06.600 --> 01:38:09.920]   And if you truly need supervised data for a problem,
[01:38:09.920 --> 01:38:12.560]   yeah, things like data engine are at the high end
[01:38:12.560 --> 01:38:14.960]   of what is attention.
[01:38:14.960 --> 01:38:15.960]   Is a human paying attention?
[01:38:15.960 --> 01:38:17.960]   I mean, we're going to probably build something
[01:38:17.960 --> 01:38:18.920]   that looks like data engine
[01:38:18.920 --> 01:38:21.120]   to push our driver monitoring further.
[01:38:21.120 --> 01:38:22.920]   But for driving itself,
[01:38:22.920 --> 01:38:24.640]   you have it all annotated beautifully
[01:38:24.640 --> 01:38:26.400]   by what the human does, so.
[01:38:26.400 --> 01:38:27.240]   - Yeah, that's interesting.
[01:38:27.240 --> 01:38:30.040]   I mean, that applies to driver attention as well.
[01:38:30.040 --> 01:38:31.180]   Do you want to detect the eyes?
[01:38:31.180 --> 01:38:33.500]   Do you want to detect blinking and pupil movement?
[01:38:33.500 --> 01:38:36.700]   Do you want to detect all the face alignments,
[01:38:36.700 --> 01:38:38.740]   the landmark detection, and so on,
[01:38:38.740 --> 01:38:41.540]   and then doing kind of reasoning based on that?
[01:38:41.540 --> 01:38:43.860]   Or do you want to take the entirety of the face over time
[01:38:43.860 --> 01:38:45.340]   and do end to end?
[01:38:45.340 --> 01:38:48.260]   I mean, it's obvious that eventually you have to do
[01:38:48.260 --> 01:38:51.340]   end to end with some calibration, some fixes, and so on.
[01:38:51.340 --> 01:38:55.740]   But it's like, I don't know when that's the right move.
[01:38:55.740 --> 01:38:57.580]   - Even if it's end to end,
[01:38:57.580 --> 01:38:59.660]   there actually is, there is no kind of,
[01:38:59.660 --> 01:39:03.380]   you have to supervise that with humans.
[01:39:03.380 --> 01:39:05.500]   Whether a human is paying attention or not
[01:39:05.500 --> 01:39:07.300]   is a completely subjective judgment.
[01:39:07.300 --> 01:39:11.580]   Like, you can try to automatically do it with some stuff,
[01:39:11.580 --> 01:39:15.100]   but you don't have, if I record a video of a human,
[01:39:15.100 --> 01:39:18.400]   I don't have true annotations anywhere in that video.
[01:39:18.400 --> 01:39:21.140]   The only way to get them is with,
[01:39:21.140 --> 01:39:22.840]   you know, other humans labeling it, really.
[01:39:22.840 --> 01:39:23.900]   - Well, I don't know.
[01:39:23.900 --> 01:39:27.900]   If you think deeply about it,
[01:39:27.900 --> 01:39:30.980]   you might be able to, depending on the task,
[01:39:30.980 --> 01:39:34.300]   maybe discover self-annotating things like,
[01:39:34.300 --> 01:39:37.020]   you know, you can look at steering wheel reverses,
[01:39:37.020 --> 01:39:37.860]   something like that.
[01:39:37.860 --> 01:39:40.580]   You can discover little moments of lapse of attention.
[01:39:40.580 --> 01:39:41.420]   - Yeah.
[01:39:41.420 --> 01:39:44.540]   - I mean, that's where psychology comes in.
[01:39:44.540 --> 01:39:48.020]   Is there indicate, 'cause you have so much data to look at.
[01:39:48.020 --> 01:39:50.460]   So you might be able to find moments
[01:39:50.460 --> 01:39:53.260]   when there's just inattention.
[01:39:53.260 --> 01:39:56.700]   Even with smartphone, if you want to detect smartphone use.
[01:39:56.700 --> 01:39:57.860]   You can start to zoom in.
[01:39:57.860 --> 01:39:59.340]   I mean, that's the goldmine,
[01:39:59.340 --> 01:40:01.500]   that sort of the comma AI,
[01:40:01.500 --> 01:40:02.940]   I mean, Tesla's doing this too, right?
[01:40:02.940 --> 01:40:06.940]   Is they're doing annotation based on,
[01:40:06.940 --> 01:40:10.500]   it's like self-supervised learning too.
[01:40:10.500 --> 01:40:13.460]   It's just a small part of the entire picture.
[01:40:13.460 --> 01:40:17.780]   That's kind of the challenge of solving a problem
[01:40:17.780 --> 01:40:18.660]   in machine learning.
[01:40:18.660 --> 01:40:23.660]   If you can discover self-annotating parts of the problem.
[01:40:23.660 --> 01:40:25.020]   Right?
[01:40:25.020 --> 01:40:27.420]   - Our driver monitoring team is half a person right now.
[01:40:27.420 --> 01:40:28.260]   - Half a person.
[01:40:28.260 --> 01:40:29.300]   - So once we have--
[01:40:29.300 --> 01:40:31.300]   - Scale to a full, it's like two people.
[01:40:31.300 --> 01:40:33.300]   - Once we have two, three people on that team,
[01:40:33.300 --> 01:40:35.980]   I definitely want to look at self-annotating stuff
[01:40:35.980 --> 01:40:36.820]   for attention.
[01:40:36.820 --> 01:40:41.900]   - Let's go back for a sec to a comma.
[01:40:41.900 --> 01:40:46.220]   And for people who are curious to try it out,
[01:40:46.220 --> 01:40:51.140]   how do you install a comma in say a 2020 Toyota Corolla?
[01:40:51.140 --> 01:40:53.420]   Or like, what are the cars that are supported?
[01:40:53.420 --> 01:40:55.500]   What are the cars that you recommend?
[01:40:55.500 --> 01:40:57.860]   And what does it take?
[01:40:57.860 --> 01:40:59.980]   You have a few videos out, but maybe through words,
[01:40:59.980 --> 01:41:02.900]   can you explain what's it take to actually install a thing?
[01:41:02.900 --> 01:41:06.620]   - So we support, I think it's 91 cars, 91 makes and models.
[01:41:06.620 --> 01:41:10.180]   We'll get to 100 this year.
[01:41:10.180 --> 01:41:11.020]   - Nice.
[01:41:11.020 --> 01:41:16.020]   - The, yeah, the 2020 Corolla, great choice.
[01:41:16.020 --> 01:41:21.220]   The 2020 Sonata, it's using the stock longitudinal.
[01:41:21.220 --> 01:41:23.260]   It's using just our lateral control.
[01:41:23.260 --> 01:41:25.140]   But it's a very refined car.
[01:41:25.140 --> 01:41:28.220]   Their longitudinal control is not bad at all.
[01:41:28.220 --> 01:41:31.740]   So yeah, Corolla, Sonata,
[01:41:31.740 --> 01:41:34.260]   or if you're willing to get your hands a little dirty
[01:41:34.260 --> 01:41:35.940]   and look in the right places on the internet,
[01:41:35.940 --> 01:41:37.540]   the Honda Civic is great,
[01:41:37.540 --> 01:41:40.620]   but you're going to have to install a modified EPS firmware
[01:41:40.620 --> 01:41:42.180]   in order to get a little bit more torque.
[01:41:42.180 --> 01:41:43.380]   And I can't help you with that.
[01:41:43.380 --> 01:41:45.980]   Common does not officially endorse that,
[01:41:45.980 --> 01:41:47.580]   but we have been doing it.
[01:41:47.580 --> 01:41:49.820]   We didn't ever release it.
[01:41:49.820 --> 01:41:51.460]   We waited for someone else to discover it.
[01:41:51.460 --> 01:41:52.900]   And then, you know.
[01:41:52.900 --> 01:41:55.740]   - And you have a Discord server where people,
[01:41:55.740 --> 01:42:00.460]   there's a very active developer community, I suppose.
[01:42:00.460 --> 01:42:04.060]   So depending on the level of experimentation,
[01:42:04.060 --> 01:42:07.660]   you're willing to do, that's a community.
[01:42:07.660 --> 01:42:11.300]   - If you just want to buy it and you have a supported car,
[01:42:11.300 --> 01:42:13.980]   it's 10 minutes to install.
[01:42:13.980 --> 01:42:15.460]   There's YouTube videos.
[01:42:15.460 --> 01:42:17.100]   It's Ikea furniture level.
[01:42:17.100 --> 01:42:19.100]   If you can set up a table from Ikea,
[01:42:19.100 --> 01:42:21.300]   you can install a Common 2 in your supported car
[01:42:21.300 --> 01:42:22.660]   and it will just work.
[01:42:22.660 --> 01:42:24.900]   And now you're like, "Oh, but I want this high-end feature
[01:42:24.900 --> 01:42:26.180]   or I want to fix this bug."
[01:42:26.180 --> 01:42:28.540]   Okay, well, welcome to the developer community.
[01:42:28.540 --> 01:42:31.020]   - So what, if I wanted to,
[01:42:31.020 --> 01:42:33.540]   this is something I asked you offline,
[01:42:33.540 --> 01:42:34.700]   like a few months ago.
[01:42:34.700 --> 01:42:38.820]   If I wanted to run my own code to,
[01:42:38.820 --> 01:42:43.420]   so use Common as a platform
[01:42:43.420 --> 01:42:45.660]   and try to run something like OpenPilot,
[01:42:45.660 --> 01:42:47.300]   what does it take to do that?
[01:42:47.300 --> 01:42:51.820]   - So there's a toggle in the settings called enable SSH.
[01:42:51.820 --> 01:42:54.620]   And if you toggle that, you can SSH into your device.
[01:42:54.620 --> 01:42:55.620]   You can modify the code.
[01:42:55.620 --> 01:42:58.260]   You can upload whatever code you want to it.
[01:42:58.260 --> 01:42:59.180]   There's a whole lot of people.
[01:42:59.180 --> 01:43:03.060]   So about 60% of people are running stock,
[01:43:03.060 --> 01:43:05.500]   about 40% of people are running forks.
[01:43:05.500 --> 01:43:07.300]   And there's a community of,
[01:43:07.300 --> 01:43:10.340]   there's a bunch of people who maintain these forks
[01:43:10.340 --> 01:43:13.060]   and these forks support different cars
[01:43:13.060 --> 01:43:15.700]   or they have different toggles.
[01:43:15.700 --> 01:43:17.380]   We try to keep away from the toggles
[01:43:17.380 --> 01:43:18.940]   that are like disabled or ever monitoring.
[01:43:18.940 --> 01:43:21.740]   But there's some people might want that kind of thing
[01:43:21.740 --> 01:43:24.540]   like, yeah, you can, it's your car.
[01:43:24.540 --> 01:43:26.620]   I'm not here to tell you,
[01:43:26.620 --> 01:43:31.020]   we have some, we ban,
[01:43:31.020 --> 01:43:32.860]   if you're trying to subvert safety features,
[01:43:32.860 --> 01:43:33.700]   you're banned from our Discord.
[01:43:33.700 --> 01:43:35.220]   I don't want anything to do with you,
[01:43:35.220 --> 01:43:36.980]   but there's some forks doing that.
[01:43:36.980 --> 01:43:38.700]   - Got it.
[01:43:38.700 --> 01:43:42.860]   So you encourage responsible forking.
[01:43:42.860 --> 01:43:43.980]   - Yeah, yeah.
[01:43:43.980 --> 01:43:46.020]   Some people, yeah, some people,
[01:43:46.020 --> 01:43:48.140]   like there's forks that will do,
[01:43:48.140 --> 01:43:52.020]   some people just like having a lot of readouts on the UI,
[01:43:52.020 --> 01:43:53.420]   like a lot of like flashing numbers.
[01:43:53.420 --> 01:43:55.100]   So there's forks that do that.
[01:43:55.100 --> 01:43:57.180]   Some people don't like the fact that it disengages
[01:43:57.180 --> 01:43:58.300]   when you press the gas pedal,
[01:43:58.300 --> 01:44:00.460]   there's forks that disable that.
[01:44:00.460 --> 01:44:01.300]   - Got it.
[01:44:01.300 --> 01:44:04.900]   Now the stock experience is what like,
[01:44:04.900 --> 01:44:08.140]   so it does both lane keeping and longitudinal control
[01:44:08.140 --> 01:44:09.700]   all together, so it's not separate,
[01:44:09.700 --> 01:44:10.980]   like it is an autopilot.
[01:44:10.980 --> 01:44:12.500]   - No, so, okay.
[01:44:12.500 --> 01:44:14.980]   Some cars we use the stock longitudinal control.
[01:44:14.980 --> 01:44:17.340]   We don't do the longitudinal control in all the cars.
[01:44:17.340 --> 01:44:19.540]   Some cars, the ACC's are pretty good in the cars.
[01:44:19.540 --> 01:44:21.340]   It's the lane keep that's atrocious in anything
[01:44:21.340 --> 01:44:23.380]   except for autopilot and super cruise.
[01:44:23.380 --> 01:44:27.820]   - But you just turn it on and it works.
[01:44:27.820 --> 01:44:29.460]   What does disengagement look like?
[01:44:29.460 --> 01:44:30.900]   - Yeah, so we have, I mean,
[01:44:30.900 --> 01:44:32.900]   I'm very concerned about mode confusion.
[01:44:32.900 --> 01:44:36.940]   I've experienced it on super cruise and autopilot
[01:44:36.940 --> 01:44:39.780]   where like autopilot, like autopilot disengages.
[01:44:39.780 --> 01:44:42.340]   I don't realize that the ACC is still on.
[01:44:42.340 --> 01:44:44.660]   The lead car moves slightly over
[01:44:44.660 --> 01:44:46.100]   and then the Tesla accelerates
[01:44:46.100 --> 01:44:47.980]   to like whatever my set speed is super fast
[01:44:47.980 --> 01:44:49.820]   and I'm like, what's going on here?
[01:44:49.820 --> 01:44:53.700]   We have engaged and disengaged.
[01:44:53.700 --> 01:44:55.620]   And this is similar to my understanding.
[01:44:55.620 --> 01:44:57.580]   I'm not a pilot, but my understanding is either
[01:44:57.580 --> 01:45:02.060]   the pilot is in control or the co-pilot is in control.
[01:45:02.060 --> 01:45:05.100]   And we have the same kind of transition system.
[01:45:05.100 --> 01:45:08.580]   Either open pilot is engaged or open pilot is disengaged.
[01:45:08.580 --> 01:45:10.140]   Engage with cruise control,
[01:45:10.140 --> 01:45:13.260]   disengage with either gas, brake, or cancel.
[01:45:13.260 --> 01:45:14.500]   - Let's talk about money.
[01:45:14.500 --> 01:45:17.340]   What's the business strategy for Karma?
[01:45:17.340 --> 01:45:18.820]   - Profitable.
[01:45:18.820 --> 01:45:21.020]   - Well, so you're-- - We did it.
[01:45:21.020 --> 01:45:22.420]   - So congratulations.
[01:45:22.420 --> 01:45:27.900]   So basically selling, so we should say Karma cost
[01:45:27.900 --> 01:45:29.940]   a thousand bucks, Karma 2?
[01:45:29.940 --> 01:45:31.460]   - 200 for the interface to the car as well.
[01:45:31.460 --> 01:45:32.940]   So it's 1,200 all of a sudden.
[01:45:32.940 --> 01:45:36.340]   - Nobody's usually up front like this.
[01:45:36.340 --> 01:45:38.140]   - Yeah, you gotta add the tack on, right?
[01:45:38.140 --> 01:45:39.580]   Yeah. - I love it.
[01:45:39.580 --> 01:45:41.060]   - I'm not gonna lie to you.
[01:45:41.060 --> 01:45:43.820]   Trust me, it will add $1,200 of value to your life.
[01:45:43.820 --> 01:45:45.540]   - Yes, it's still super cheap.
[01:45:45.540 --> 01:45:47.860]   - 30 days, no questions asked, money back guarantee,
[01:45:47.860 --> 01:45:50.420]   and prices are only going up.
[01:45:50.420 --> 01:45:52.340]   If there ever is future hardware,
[01:45:52.340 --> 01:45:53.820]   it could cost a lot more than $1,200.
[01:45:53.820 --> 01:45:55.380]   - So Karma 3 is in the works.
[01:45:55.380 --> 01:45:58.100]   It could be.
[01:45:58.100 --> 01:46:00.300]   - All I will say is future hardware is going to cost
[01:46:00.300 --> 01:46:02.900]   a lot more than the current hardware.
[01:46:02.900 --> 01:46:05.260]   - Yeah, and the people that use,
[01:46:05.260 --> 01:46:07.900]   the people I've spoken with that use Karma,
[01:46:07.900 --> 01:46:12.140]   that use open pilot, first of all, they use it a lot.
[01:46:12.140 --> 01:46:14.420]   So people that use it, they fall in love with it.
[01:46:14.420 --> 01:46:16.860]   - Oh, our retention rate is insane.
[01:46:16.860 --> 01:46:18.180]   - Which is a good sign.
[01:46:18.180 --> 01:46:19.700]   It's a really good sign.
[01:46:19.700 --> 01:46:23.780]   - 70% of Karma 2 buyers are daily active users.
[01:46:23.780 --> 01:46:25.300]   - Yeah, it's amazing.
[01:46:25.300 --> 01:46:30.740]   - Oh, also, we don't plan on stopping selling the Karma 2.
[01:46:30.740 --> 01:46:35.020]   - So whatever you create that's beyond Karma 2,
[01:46:35.020 --> 01:46:40.780]   it would be potentially a phase shift.
[01:46:40.780 --> 01:46:42.860]   Like it's so much better that,
[01:46:42.860 --> 01:46:45.780]   like you could use Karma 2 and you can use Karma whatever.
[01:46:45.780 --> 01:46:46.620]   - Depends what you want.
[01:46:46.620 --> 01:46:48.340]   - 3.41, 42.
[01:46:48.340 --> 01:46:52.020]   - Yeah, you know, autopilot, hardware one versus hardware two.
[01:46:52.020 --> 01:46:53.660]   The Karma 2 is kind of like hardware one.
[01:46:53.660 --> 01:46:54.500]   - Got it, got it.
[01:46:54.500 --> 01:46:56.380]   You can still use both, got it, got it.
[01:46:56.380 --> 01:46:58.020]   I think I heard you talk about retention rate
[01:46:58.020 --> 01:47:01.780]   with VR headsets that the average is just once.
[01:47:01.780 --> 01:47:03.900]   Just fast, I mean, it's such a fascinating way
[01:47:03.900 --> 01:47:05.820]   to think about technology.
[01:47:05.820 --> 01:47:07.460]   And this is a really, really good sign.
[01:47:07.460 --> 01:47:09.020]   And the other thing that people say about Karma
[01:47:09.020 --> 01:47:12.060]   is like they can't believe they're getting this 4,000 bucks.
[01:47:12.060 --> 01:47:16.020]   Right, it seems like some kind of steal.
[01:47:16.020 --> 01:47:20.100]   But in terms of like long-term business strategies,
[01:47:20.100 --> 01:47:25.100]   basically to put, so it's currently in like 1,000 plus cars.
[01:47:25.100 --> 01:47:28.500]   1,200.
[01:47:28.500 --> 01:47:29.340]   - More, more.
[01:47:29.340 --> 01:47:33.580]   So yeah, dailies is about 2,000,
[01:47:36.100 --> 01:47:38.980]   weeklies is about 2,500, monthlies is over 3,000.
[01:47:38.980 --> 01:47:39.820]   - Wow.
[01:47:39.820 --> 01:47:42.180]   - We've grown a lot since we last talked.
[01:47:42.180 --> 01:47:44.860]   - Is the goal, like can we talk crazy for a second?
[01:47:44.860 --> 01:47:47.620]   I mean, what's the goal to overtake Tesla?
[01:47:47.620 --> 01:47:50.020]   Let's talk, okay, so.
[01:47:50.020 --> 01:47:51.500]   - I mean, Android did overtake iOS.
[01:47:51.500 --> 01:47:52.540]   - That's exactly it, right?
[01:47:52.540 --> 01:47:55.380]   So they did it.
[01:47:55.380 --> 01:47:57.740]   I actually don't know the timeline of that one.
[01:47:57.740 --> 01:48:02.220]   But let's talk, 'cause everything is in alpha now.
[01:48:02.220 --> 01:48:04.020]   The autopilot, you could argue, is in alpha
[01:48:04.020 --> 01:48:07.740]   in terms of towards the big mission of autonomous driving.
[01:48:07.740 --> 01:48:11.420]   And so what, yes, your goal to overtake,
[01:48:11.420 --> 01:48:13.980]   to get millions of cars, essentially.
[01:48:13.980 --> 01:48:15.580]   - Of course.
[01:48:15.580 --> 01:48:16.820]   Where would it stop?
[01:48:16.820 --> 01:48:18.060]   Like it's open source software.
[01:48:18.060 --> 01:48:19.300]   It might not be millions of cars
[01:48:19.300 --> 01:48:21.420]   with a piece of Kama hardware, but yeah.
[01:48:21.420 --> 01:48:26.020]   I think OpenPilot at some point will cross over Autopilot
[01:48:26.020 --> 01:48:29.260]   in users, just like Android crossed over iOS.
[01:48:29.260 --> 01:48:31.380]   - How does Google make money from Android?
[01:48:32.020 --> 01:48:32.860]   - Uh.
[01:48:32.860 --> 01:48:34.820]   - It's complicated.
[01:48:34.820 --> 01:48:36.420]   Their own devices make money.
[01:48:36.420 --> 01:48:39.460]   - Google, Google makes money
[01:48:39.460 --> 01:48:42.260]   by just kind of having you on the internet.
[01:48:42.260 --> 01:48:43.100]   - Yes.
[01:48:43.100 --> 01:48:45.620]   - Google Search is built in, Gmail is built in.
[01:48:45.620 --> 01:48:46.540]   Android is just a shill
[01:48:46.540 --> 01:48:48.180]   for the rest of Google's ecosystem kind of.
[01:48:48.180 --> 01:48:52.460]   - Yeah, but the problem is Android is a brilliant thing.
[01:48:52.460 --> 01:48:55.060]   I mean, Android arguably changed the world.
[01:48:55.060 --> 01:48:56.420]   So there you go.
[01:48:56.420 --> 01:49:00.820]   That's, you can feel good, ethically speaking.
[01:49:00.820 --> 01:49:04.340]   But as a business strategy, it's questionable.
[01:49:04.340 --> 01:49:05.780]   - Or sell hardware.
[01:49:05.780 --> 01:49:06.620]   - Sell hardware.
[01:49:06.620 --> 01:49:08.180]   - I mean, it took Google a long time to come around to it,
[01:49:08.180 --> 01:49:10.060]   but they are now making money on the Pixel.
[01:49:10.060 --> 01:49:11.980]   - You're not about money.
[01:49:11.980 --> 01:49:13.300]   You're more about winning.
[01:49:13.300 --> 01:49:14.260]   - Yeah, of course.
[01:49:14.260 --> 01:49:18.380]   No, but if only 10% of OpenPilot devices
[01:49:18.380 --> 01:49:20.020]   come from Kama AI.
[01:49:20.020 --> 01:49:20.900]   - You still make a lot.
[01:49:20.900 --> 01:49:21.740]   - That is still, yes.
[01:49:21.740 --> 01:49:22.940]   That is a ton of money for our company.
[01:49:22.940 --> 01:49:27.220]   - But can't somebody create a better Kama using OpenPilot?
[01:49:27.220 --> 01:49:28.940]   Or are you basically saying, well, I'll compete them?
[01:49:28.940 --> 01:49:29.780]   - Well, I'll compete you.
[01:49:29.780 --> 01:49:32.700]   Can you create a better Android phone than the Google Pixel?
[01:49:32.700 --> 01:49:34.660]   I mean, you can, but like, you know.
[01:49:34.660 --> 01:49:35.500]   - I love that.
[01:49:35.500 --> 01:49:37.700]   So you're confident, like, you know what the hell
[01:49:37.700 --> 01:49:38.540]   you're doing.
[01:49:38.540 --> 01:49:39.380]   - Yeah.
[01:49:39.380 --> 01:49:43.460]   - It's a competence and merit.
[01:49:43.460 --> 01:49:44.820]   - I mean, our money, yeah, our money comes from,
[01:49:44.820 --> 01:49:46.540]   we're a consumer electronics company.
[01:49:46.540 --> 01:49:47.980]   And put it this way.
[01:49:47.980 --> 01:49:50.100]   So we sold like 3,000 Kama 2s.
[01:49:50.100 --> 01:49:52.580]   2,500 right now.
[01:49:52.580 --> 01:49:59.060]   And like, okay, we're probably gonna sell
[01:49:59.940 --> 01:50:01.900]   10,000 units next year, right?
[01:50:01.900 --> 01:50:04.340]   10,000 units, even just $1,000 a unit.
[01:50:04.340 --> 01:50:08.420]   Okay, we're at 10 million in revenue.
[01:50:08.420 --> 01:50:12.100]   Get that up to 100,000, maybe double the price of the unit.
[01:50:12.100 --> 01:50:13.580]   Now we're talking like 200 million in revenue.
[01:50:13.580 --> 01:50:14.420]   We're talking like serious.
[01:50:14.420 --> 01:50:15.820]   - You're actually making money.
[01:50:15.820 --> 01:50:18.980]   One of the rare semi-autonomous or autonomous vehicle
[01:50:18.980 --> 01:50:21.060]   companies that are actually making money.
[01:50:21.060 --> 01:50:21.900]   - Yeah.
[01:50:21.900 --> 01:50:25.420]   If you look at a model,
[01:50:25.420 --> 01:50:26.580]   and we were just talking about this yesterday.
[01:50:26.580 --> 01:50:28.340]   If you look at a model and like you're testing,
[01:50:28.340 --> 01:50:29.700]   like you're A/B testing your model.
[01:50:29.700 --> 01:50:32.380]   And if you're one branch of the A/B test,
[01:50:32.380 --> 01:50:35.300]   the losses go down very fast in the first five epochs.
[01:50:35.300 --> 01:50:37.620]   That model is probably going to converge
[01:50:37.620 --> 01:50:38.980]   to something considerably better
[01:50:38.980 --> 01:50:41.340]   than the one with the losses going down slower.
[01:50:41.340 --> 01:50:42.980]   Why do people think this is gonna stop?
[01:50:42.980 --> 01:50:45.660]   Why do people think one day there's gonna be a great like,
[01:50:45.660 --> 01:50:48.300]   well Waymo's eventually going to surpass you guys?
[01:50:48.300 --> 01:50:50.460]   Well, they're not.
[01:50:50.460 --> 01:50:54.540]   - Do you see like a world where like a Tesla
[01:50:54.540 --> 01:50:57.060]   or a car like a Tesla would be able
[01:50:57.060 --> 01:50:59.100]   to basically press a button
[01:50:59.100 --> 01:51:01.860]   and you'd like switch to open pilot?
[01:51:01.860 --> 01:51:04.340]   You know, like load in.
[01:51:04.340 --> 01:51:05.180]   - I don't know.
[01:51:05.180 --> 01:51:06.460]   So I think, so first off,
[01:51:06.460 --> 01:51:10.500]   I think that we may surpass Tesla in terms of users.
[01:51:10.500 --> 01:51:12.500]   I do not think we're gonna surpass Tesla ever
[01:51:12.500 --> 01:51:13.460]   in terms of revenue.
[01:51:13.460 --> 01:51:16.380]   I think Tesla can capture a lot more revenue per user
[01:51:16.380 --> 01:51:19.740]   than we can, but this mimics the Android iOS model.
[01:51:19.740 --> 01:51:20.580]   Exactly.
[01:51:20.580 --> 01:51:22.540]   There may be more Android devices, but you know,
[01:51:22.540 --> 01:51:24.300]   there's a lot more iPhones than Google pixels.
[01:51:24.300 --> 01:51:26.380]   So I think there'll be a lot more Tesla cars sold
[01:51:26.380 --> 01:51:27.900]   than pieces of comma hardware.
[01:51:27.900 --> 01:51:35.020]   And then as far as a Tesla owner being able to switch
[01:51:35.020 --> 01:51:39.300]   to open pilot, does iOS, does iPhones run Android?
[01:51:39.300 --> 01:51:41.860]   - No, but--
[01:51:41.860 --> 01:51:43.500]   - You can if you really wanna do it,
[01:51:43.500 --> 01:51:44.420]   but it doesn't really make sense.
[01:51:44.420 --> 01:51:45.260]   Like it's not--
[01:51:45.260 --> 01:51:46.220]   - It doesn't make sense.
[01:51:46.220 --> 01:51:47.060]   - Who cares?
[01:51:47.060 --> 01:51:51.100]   - What about if a large company like automakers,
[01:51:51.100 --> 01:51:53.740]   Ford, GM, Toyota, came to George Haas
[01:51:53.740 --> 01:51:58.020]   or on the tech space, Amazon, Facebook, Google,
[01:51:58.020 --> 01:51:59.980]   came with a large pile of cash,
[01:51:59.980 --> 01:52:05.580]   would you consider being purchased?
[01:52:05.580 --> 01:52:10.500]   Do you see that as a one possible?
[01:52:10.500 --> 01:52:12.340]   - Not seriously, no.
[01:52:12.340 --> 01:52:17.340]   I would probably see how much shit they'll entertain for me.
[01:52:17.340 --> 01:52:21.380]   And if they're willing to like jump through a bunch
[01:52:21.380 --> 01:52:23.580]   of my hoops, then maybe, but like, no,
[01:52:23.580 --> 01:52:25.220]   not the way that M&A works today.
[01:52:25.220 --> 01:52:26.740]   I mean, we've been approached.
[01:52:26.740 --> 01:52:28.020]   And I laugh in these people's faces.
[01:52:28.020 --> 01:52:29.320]   I'm like, are you kidding?
[01:52:29.320 --> 01:52:31.820]   - Yeah.
[01:52:31.820 --> 01:52:33.700]   - 'Cause it's so demeaning.
[01:52:33.700 --> 01:52:36.980]   The M&A people are so demeaning to companies.
[01:52:36.980 --> 01:52:41.300]   They treat the startup world as their innovation ecosystem.
[01:52:41.300 --> 01:52:43.340]   And they think that I'm cool with going along with that
[01:52:43.340 --> 01:52:46.300]   so I can have some of their scam, fake Fed dollars.
[01:52:46.300 --> 01:52:47.700]   You know, Fed coin.
[01:52:47.700 --> 01:52:49.300]   What am I gonna do with more Fed coin?
[01:52:49.300 --> 01:52:50.140]   - Fed coin.
[01:52:50.140 --> 01:52:51.420]   - Fed coin, man.
[01:52:51.420 --> 01:52:52.260]   - I love that.
[01:52:52.260 --> 01:52:54.380]   So that's the cool thing about podcasting actually
[01:52:54.380 --> 01:52:56.260]   is people criticize.
[01:52:56.260 --> 01:52:59.540]   I don't know if you're familiar with Spotify
[01:52:59.540 --> 01:53:01.980]   giving Joe Rogan a hundred million.
[01:53:01.980 --> 01:53:03.740]   - I've heard something about that.
[01:53:03.740 --> 01:53:06.300]   - And you know, they respect,
[01:53:06.300 --> 01:53:10.220]   despite all the shit that people are talking about Spotify,
[01:53:10.220 --> 01:53:15.300]   people understand that podcasters like Joe Rogan
[01:53:15.300 --> 01:53:17.220]   know what the hell they're doing.
[01:53:17.260 --> 01:53:21.220]   So they give them money and say, just do what you do.
[01:53:21.220 --> 01:53:25.460]   And like the equivalent for you would be like,
[01:53:25.460 --> 01:53:28.500]   George, do what the hell you do 'cause you're good at it.
[01:53:28.500 --> 01:53:30.500]   Try not to murder too many people.
[01:53:30.500 --> 01:53:33.540]   Like try, like there's some kind of common sense things
[01:53:33.540 --> 01:53:37.500]   like just don't go on a weird rampage of.
[01:53:37.500 --> 01:53:40.860]   - Yeah, it comes down to what companies I could respect.
[01:53:40.860 --> 01:53:41.700]   Right?
[01:53:41.700 --> 01:53:44.540]   You know, could I respect GM?
[01:53:44.540 --> 01:53:45.380]   Never.
[01:53:46.820 --> 01:53:47.660]   Well, I couldn't.
[01:53:47.660 --> 01:53:50.820]   I mean, could I respect like a Hyundai?
[01:53:50.820 --> 01:53:52.260]   More so.
[01:53:52.260 --> 01:53:53.100]   Right?
[01:53:53.100 --> 01:53:53.940]   That's a lot closer.
[01:53:53.940 --> 01:53:54.780]   - Toyota?
[01:53:54.780 --> 01:53:55.820]   What's your?
[01:53:55.820 --> 01:53:59.380]   - Nah, nah, Korean is the way.
[01:53:59.380 --> 01:54:02.100]   I think that, you know, the Japanese, the Germans, the US,
[01:54:02.100 --> 01:54:05.700]   they're all too, you know, they all think they're too great.
[01:54:05.700 --> 01:54:06.540]   To be honest.
[01:54:06.540 --> 01:54:07.580]   - What about the tech companies?
[01:54:07.580 --> 01:54:08.500]   Apple?
[01:54:08.500 --> 01:54:11.060]   - Apple is, of the tech companies that I could respect,
[01:54:11.060 --> 01:54:12.260]   Apple's the closest.
[01:54:12.260 --> 01:54:13.100]   Yeah.
[01:54:13.100 --> 01:54:13.940]   I mean, I could never.
[01:54:13.940 --> 01:54:14.780]   - It would be ironic.
[01:54:14.780 --> 01:54:18.580]   It would be ironic if, if, if Comma AI is,
[01:54:18.580 --> 01:54:19.940]   is acquired by Apple.
[01:54:19.940 --> 01:54:21.980]   - I mean, Facebook, look, I quit Facebook 10 years ago
[01:54:21.980 --> 01:54:24.460]   because I didn't respect their business model.
[01:54:24.460 --> 01:54:28.580]   Google has declined so fast in the last five years.
[01:54:28.580 --> 01:54:31.300]   - What are your thoughts about Waymo
[01:54:31.300 --> 01:54:33.140]   and its present and its future?
[01:54:33.140 --> 01:54:37.460]   Let me, let me, let me start by saying something nice,
[01:54:37.460 --> 01:54:40.980]   which is I've visited them a few times
[01:54:40.980 --> 01:54:45.460]   and have, have ridden in their cars
[01:54:45.460 --> 01:54:49.940]   and the engineering that they're doing,
[01:54:49.940 --> 01:54:51.860]   both the research and the actual development
[01:54:51.860 --> 01:54:53.580]   and the engineering they're doing
[01:54:53.580 --> 01:54:55.300]   and the scale they're actually achieving
[01:54:55.300 --> 01:54:58.620]   by doing it all themselves is really impressive.
[01:54:58.620 --> 01:55:01.660]   And the, the balance of safety and innovation
[01:55:01.660 --> 01:55:05.980]   and like the cars work really well
[01:55:05.980 --> 01:55:07.420]   for the routes they drive.
[01:55:07.420 --> 01:55:10.860]   Like they drive fast, which was very surprising to me.
[01:55:10.860 --> 01:55:13.100]   Like it drives like the speed limit
[01:55:13.100 --> 01:55:16.300]   or faster than the speed limit it goes.
[01:55:16.300 --> 01:55:17.900]   And it works really damn well.
[01:55:17.900 --> 01:55:19.300]   And the interface is nice.
[01:55:19.300 --> 01:55:20.460]   - In Chandler, Arizona, yeah.
[01:55:20.460 --> 01:55:22.500]   - Yeah, in Chandler, Arizona, very specific environment.
[01:55:22.500 --> 01:55:27.500]   So it, I, you know, it gives me enough material in my mind
[01:55:27.500 --> 01:55:30.460]   to push back against the madmen of the world,
[01:55:30.460 --> 01:55:32.700]   like George Hotz, to be like,
[01:55:32.700 --> 01:55:37.260]   like, cause you kind of imply there's zero probability
[01:55:37.260 --> 01:55:38.220]   they're going to win.
[01:55:38.220 --> 01:55:39.060]   - Yeah.
[01:55:39.060 --> 01:55:43.220]   - And after I've used, after I've written in it,
[01:55:43.220 --> 01:55:44.420]   to me, it's not zero.
[01:55:44.420 --> 01:55:46.740]   - Oh, it's not for technology reasons.
[01:55:46.740 --> 01:55:47.980]   - Bureaucracy?
[01:55:47.980 --> 01:55:49.460]   - No, it's worse than that.
[01:55:49.460 --> 01:55:52.060]   It's actually for product reasons, I think.
[01:55:52.060 --> 01:55:53.380]   - Oh, you think they're just not capable
[01:55:53.380 --> 01:55:55.620]   of creating an amazing product?
[01:55:55.620 --> 01:55:56.900]   - No, I think that the product
[01:55:56.900 --> 01:55:59.380]   that they're building doesn't make sense.
[01:55:59.380 --> 01:56:02.420]   So a few things.
[01:56:02.420 --> 01:56:04.740]   You say the Waymo's are fast.
[01:56:04.740 --> 01:56:08.620]   Benchmark a Waymo against a competent Uber driver.
[01:56:09.180 --> 01:56:10.020]   - Right.
[01:56:10.020 --> 01:56:11.140]   - Right, the Uber driver's faster.
[01:56:11.140 --> 01:56:12.260]   - It's not even about speed.
[01:56:12.260 --> 01:56:14.660]   It's the thing you said, it's about the experience
[01:56:14.660 --> 01:56:16.340]   of being stuck at a stop sign,
[01:56:16.340 --> 01:56:18.940]   because pedestrians are crossing nonstop.
[01:56:18.940 --> 01:56:22.220]   - I like when my Uber driver doesn't come to a full stop
[01:56:22.220 --> 01:56:24.460]   at the stop sign, you know?
[01:56:24.460 --> 01:56:29.460]   And so let's say the Waymo's are 20% slower
[01:56:29.460 --> 01:56:31.820]   than an Uber, right?
[01:56:31.820 --> 01:56:35.060]   You can argue that they're going to be cheaper.
[01:56:35.060 --> 01:56:37.700]   And I argue that users already have the choice
[01:56:37.700 --> 01:56:39.380]   to trade off money for speed.
[01:56:39.380 --> 01:56:40.500]   It's called Uber Pool.
[01:56:40.500 --> 01:56:45.940]   I think it's like 15% of rides are Uber Pools, right?
[01:56:45.940 --> 01:56:49.380]   Users are not willing to trade off money for speed.
[01:56:49.380 --> 01:56:52.300]   So the whole product that they're building
[01:56:52.300 --> 01:56:54.700]   is not going to be competitive
[01:56:54.700 --> 01:56:56.860]   with traditional ride-sharing networks.
[01:56:56.860 --> 01:56:57.700]   - Right.
[01:56:57.700 --> 01:57:04.340]   - And also, whether there's profit to be made
[01:57:04.340 --> 01:57:07.340]   depends entirely on one company having a monopoly.
[01:57:07.340 --> 01:57:12.060]   I think that the level for autonomous ride-sharing vehicles
[01:57:12.060 --> 01:57:14.780]   market is going to look a lot like the scooter market,
[01:57:14.780 --> 01:57:18.660]   if even the technology does come to exist, which I question.
[01:57:18.660 --> 01:57:20.420]   Who's doing well in that market?
[01:57:20.420 --> 01:57:22.340]   It's a race to the bottom, you know?
[01:57:22.340 --> 01:57:25.660]   - Well, it could be closer to like an Uber and a Lyft,
[01:57:25.660 --> 01:57:27.980]   where it's just a one or two players.
[01:57:27.980 --> 01:57:31.180]   - Well, the scooter people have given up
[01:57:31.180 --> 01:57:32.980]   trying to market scooters
[01:57:32.980 --> 01:57:35.500]   as a practical means of transportation.
[01:57:35.500 --> 01:57:37.820]   And they're just like, they're super fun to ride.
[01:57:37.820 --> 01:57:39.060]   Look at wheels, I love those things.
[01:57:39.060 --> 01:57:41.060]   And they're great on that front.
[01:57:41.060 --> 01:57:44.660]   But from an actual transportation product perspective,
[01:57:44.660 --> 01:57:46.260]   I do not think scooters are viable,
[01:57:46.260 --> 01:57:49.140]   and I do not think level four autonomous cars are viable.
[01:57:49.140 --> 01:57:51.540]   - If you, let's play a fun experiment.
[01:57:51.540 --> 01:57:56.540]   If you ran, let's do a Tesla and let's do Waymo.
[01:57:56.540 --> 01:58:01.380]   If Elon Musk took a vacation for a year,
[01:58:01.380 --> 01:58:03.820]   he just said, screw it, I'm going to go live on an island,
[01:58:03.820 --> 01:58:06.420]   no electronics, and the board decides
[01:58:06.420 --> 01:58:09.140]   that we need to find somebody to run the company,
[01:58:09.140 --> 01:58:12.220]   and they decide that you should run the company for a year,
[01:58:12.220 --> 01:58:14.060]   how do you run Tesla differently?
[01:58:14.060 --> 01:58:16.540]   - I wouldn't change much.
[01:58:16.540 --> 01:58:17.900]   - Do you think they're on the right track?
[01:58:17.900 --> 01:58:18.740]   - I wouldn't change.
[01:58:18.740 --> 01:58:21.660]   I mean, I'd have some minor changes,
[01:58:21.660 --> 01:58:26.660]   but even my debate with Tesla about end-to-end versus SegNets,
[01:58:26.660 --> 01:58:30.540]   like, that's just software, who cares, right?
[01:58:30.540 --> 01:58:32.020]   Like, it's not going to,
[01:58:32.020 --> 01:58:34.420]   it's not like you're doing something terrible with SegNets.
[01:58:34.420 --> 01:58:35.460]   You're probably building something
[01:58:35.460 --> 01:58:36.780]   that's at least going to help you debug
[01:58:36.780 --> 01:58:39.420]   the end-to-end system a lot, right?
[01:58:39.420 --> 01:58:42.220]   It's very easy to transition from what they have
[01:58:42.220 --> 01:58:44.460]   to like an end-to-end kind of thing.
[01:58:44.460 --> 01:58:50.540]   - And then I presume you would, in the Model Y,
[01:58:50.540 --> 01:58:51.580]   or maybe in the Model 3,
[01:58:51.580 --> 01:58:53.620]   start adding driver sensing with infrared.
[01:58:53.620 --> 01:58:58.620]   - Yes, I would add infrared lights right away to those cars.
[01:58:59.620 --> 01:59:01.220]   - And start collecting that data
[01:59:01.220 --> 01:59:03.020]   and do all that kind of stuff, yeah.
[01:59:03.020 --> 01:59:04.780]   - Very much, I think they're already kind of doing it.
[01:59:04.780 --> 01:59:06.700]   It's an incredibly minor change.
[01:59:06.700 --> 01:59:08.060]   If I actually were CEO of Tesla,
[01:59:08.060 --> 01:59:09.060]   first off, I'd be horrified
[01:59:09.060 --> 01:59:11.340]   that I wouldn't be able to do a better job as Elon,
[01:59:11.340 --> 01:59:13.780]   and then I would try to understand
[01:59:13.780 --> 01:59:14.940]   the way he's done things before.
[01:59:14.940 --> 01:59:17.740]   - You would also have to take over his Twitter, so.
[01:59:17.740 --> 01:59:19.540]   - God, I don't tweet.
[01:59:19.540 --> 01:59:20.900]   - Yeah, what's your Twitter situation?
[01:59:20.900 --> 01:59:23.100]   Why are you so quiet on Twitter?
[01:59:23.100 --> 01:59:27.020]   It says, "Dukama," is like, what's your social media?
[01:59:27.020 --> 01:59:30.380]   It's like, what's your social network presence like?
[01:59:30.380 --> 01:59:34.420]   'Cause on Instagram, you do live streams.
[01:59:34.420 --> 01:59:39.340]   You understand the music of the internet,
[01:59:39.340 --> 01:59:41.580]   but you don't always fully engage into it.
[01:59:41.580 --> 01:59:42.900]   You're part-time.
[01:59:42.900 --> 01:59:44.300]   - I used to have a Twitter.
[01:59:44.300 --> 01:59:47.740]   Yeah, I mean, Instagram is a pretty place.
[01:59:47.740 --> 01:59:49.140]   Instagram is a beautiful place.
[01:59:49.140 --> 01:59:49.980]   It glorifies beauty.
[01:59:49.980 --> 01:59:53.460]   I like Instagram's values as a network.
[01:59:53.460 --> 01:59:55.300]   Twitter glorifies conflict,
[01:59:55.300 --> 02:00:00.300]   and it glorifies shots, taking shots at people,
[02:00:00.300 --> 02:00:02.700]   and it's like, you know,
[02:00:02.700 --> 02:00:06.260]   Twitter and Donald Trump are perfectly,
[02:00:06.260 --> 02:00:08.500]   they're perfect for each other.
[02:00:08.500 --> 02:00:11.980]   - So Tesla's on the right track, in your view.
[02:00:11.980 --> 02:00:12.820]   - Yeah.
[02:00:12.820 --> 02:00:16.620]   - Okay, so let's try, let's really try this experiment.
[02:00:16.620 --> 02:00:19.740]   If you ran Waymo, let's say they're,
[02:00:19.740 --> 02:00:20.580]   I don't know if you agree,
[02:00:20.580 --> 02:00:22.620]   but they seem to be at the head of the pack
[02:00:22.620 --> 02:00:27.100]   of the kind of, what would you call that approach?
[02:00:27.100 --> 02:00:29.100]   Like, it's not necessarily LIDAR-based,
[02:00:29.100 --> 02:00:29.940]   'cause it's not about LIDAR.
[02:00:29.940 --> 02:00:31.540]   - Level four Robotaxi.
[02:00:31.540 --> 02:00:35.780]   - Level four Robotaxi, all in before making any revenue.
[02:00:35.780 --> 02:00:38.580]   So they're probably at the head of the pack.
[02:00:38.580 --> 02:00:42.580]   If you were, said, "Hey, George,
[02:00:42.580 --> 02:00:44.580]   "can you please run this company for a year?"
[02:00:44.580 --> 02:00:46.140]   How would you change it?
[02:00:46.140 --> 02:00:49.900]   - I would go, I would get Anthony Lewandowski out of jail,
[02:00:49.900 --> 02:00:51.860]   and I would put him in charge of the company.
[02:00:52.700 --> 02:00:54.940]   (laughing)
[02:00:54.940 --> 02:00:58.260]   - Let's try to break that apart.
[02:00:58.260 --> 02:00:59.820]   Why do you, do you want to make,
[02:00:59.820 --> 02:01:01.620]   do you want to destroy the company by doing that,
[02:01:01.620 --> 02:01:03.860]   or do you mean, or do you mean,
[02:01:03.860 --> 02:01:09.540]   you like renegade-style thinking that pushes,
[02:01:09.540 --> 02:01:11.420]   that like throws away bureaucracy
[02:01:11.420 --> 02:01:12.700]   and goes to first principle thinking?
[02:01:12.700 --> 02:01:14.180]   What do you mean by that?
[02:01:14.180 --> 02:01:16.220]   - I think Anthony Lewandowski is a genius,
[02:01:16.220 --> 02:01:19.380]   and I think he would come up with a much better idea
[02:01:19.380 --> 02:01:21.060]   of what to do with Waymo than me.
[02:01:22.060 --> 02:01:24.940]   - So you mean that unironically, he is a genius?
[02:01:24.940 --> 02:01:27.780]   - Oh yes, oh absolutely, without a doubt.
[02:01:27.780 --> 02:01:31.100]   I mean, I'm not saying there's no shortcomings,
[02:01:31.100 --> 02:01:34.220]   but in the interactions I've had with him, yeah.
[02:01:34.220 --> 02:01:37.180]   He's also willing to take,
[02:01:37.180 --> 02:01:39.500]   like who knows what he would do with Waymo?
[02:01:39.500 --> 02:01:40.620]   I mean, he's also out there,
[02:01:40.620 --> 02:01:41.860]   like far more out there than I am.
[02:01:41.860 --> 02:01:43.340]   - Yeah, there's big risks.
[02:01:43.340 --> 02:01:44.420]   - Yeah. - What do you make of him?
[02:01:44.420 --> 02:01:47.140]   I was going to talk to him in this podcast,
[02:01:47.140 --> 02:01:49.060]   and I was going back and forth.
[02:01:49.300 --> 02:01:51.780]   I'm such a gullible, naive human.
[02:01:51.780 --> 02:01:53.980]   Like I see the best in people,
[02:01:53.980 --> 02:01:56.220]   and I slowly started to realize
[02:01:56.220 --> 02:01:58.420]   that there might be some people out there
[02:01:58.420 --> 02:02:04.820]   that like have multiple faces to the world.
[02:02:04.820 --> 02:02:08.180]   They're like deceiving and dishonest.
[02:02:08.180 --> 02:02:13.100]   I still refuse to, like I just, I trust people,
[02:02:13.100 --> 02:02:14.620]   and I don't care if I get hurt by it,
[02:02:14.620 --> 02:02:17.060]   but like, you know, sometimes you have to be
[02:02:17.060 --> 02:02:19.220]   a little bit careful, especially platform-wise
[02:02:19.220 --> 02:02:21.060]   and podcast-wise.
[02:02:21.060 --> 02:02:23.180]   What am I supposed to think?
[02:02:23.180 --> 02:02:25.340]   So you think he's a good person?
[02:02:25.340 --> 02:02:27.820]   - Oh, I don't know.
[02:02:27.820 --> 02:02:29.940]   I don't really make moral judgments.
[02:02:29.940 --> 02:02:30.780]   - I mean, it's difficult to-
[02:02:30.780 --> 02:02:32.500]   - Oh, and I mean this about the Waymo.
[02:02:32.500 --> 02:02:34.980]   Actually, I mean that whole idea very non-ironically
[02:02:34.980 --> 02:02:36.220]   about what I would do.
[02:02:36.220 --> 02:02:38.140]   The problem with putting me in charge of Waymo
[02:02:38.140 --> 02:02:41.700]   is Waymo is already $10 billion in the hole, right?
[02:02:41.700 --> 02:02:43.740]   Whatever idea Waymo does, look,
[02:02:43.740 --> 02:02:46.700]   comm is profitable, comm has raised $8.1 million.
[02:02:46.700 --> 02:02:48.180]   That's small, you know, that's small money.
[02:02:48.180 --> 02:02:50.700]   Like I can build a reasonable consumer electronics company
[02:02:50.700 --> 02:02:53.700]   and succeed wildly at that and still never be able
[02:02:53.700 --> 02:02:55.780]   to pay back Waymo's $10 billion.
[02:02:55.780 --> 02:02:58.540]   - So I think the basic idea with Waymo,
[02:02:58.540 --> 02:03:00.860]   well, forget the $10 billion because they have some backing,
[02:03:00.860 --> 02:03:03.300]   but your basic thing is like,
[02:03:03.300 --> 02:03:05.620]   what can we do to start making some money?
[02:03:05.620 --> 02:03:07.940]   - Well, no, I mean, my bigger idea is like,
[02:03:07.940 --> 02:03:10.300]   whatever the idea is that's gonna save Waymo,
[02:03:10.300 --> 02:03:11.460]   I don't have it.
[02:03:11.460 --> 02:03:13.460]   It's gonna have to be a big risk idea,
[02:03:13.460 --> 02:03:15.260]   and I cannot think of a better person
[02:03:15.260 --> 02:03:17.820]   than Anthony Lewandowski to do it.
[02:03:17.820 --> 02:03:20.220]   So that is completely what I would do as CEO of Waymo.
[02:03:20.220 --> 02:03:22.620]   I would call myself a transitionary CEO,
[02:03:22.620 --> 02:03:24.900]   do everything I can to fix that situation up.
[02:03:24.900 --> 02:03:25.740]   - Transitionary CEO.
[02:03:25.740 --> 02:03:26.580]   - Yeah.
[02:03:26.580 --> 02:03:28.700]   - Yeah.
[02:03:28.700 --> 02:03:29.700]   - 'Cause I can't do it, right?
[02:03:29.700 --> 02:03:32.180]   Like I can't, I can't, I mean,
[02:03:32.180 --> 02:03:34.340]   I can talk about how what I really wanna do
[02:03:34.340 --> 02:03:37.380]   is just apologize for all those corny, you know,
[02:03:37.380 --> 02:03:38.340]   ad campaigns and be like,
[02:03:38.340 --> 02:03:40.140]   here's the real state of the technology.
[02:03:40.140 --> 02:03:42.220]   - Yeah, that's, like I have several criticism.
[02:03:42.220 --> 02:03:44.260]   I'm a little bit more bullish on Waymo
[02:03:44.260 --> 02:03:46.020]   than you seem to be.
[02:03:46.020 --> 02:03:49.780]   But one criticism I have is it went into corny mode
[02:03:49.780 --> 02:03:50.860]   too early.
[02:03:50.860 --> 02:03:53.620]   Like it's still a startup, it hasn't delivered on anything.
[02:03:53.620 --> 02:03:56.300]   So it should be like more renegade
[02:03:56.300 --> 02:03:59.260]   and show off the engineering that they're doing,
[02:03:59.260 --> 02:04:00.540]   which just can be impressive,
[02:04:00.540 --> 02:04:02.300]   as opposed to doing these weird commercials
[02:04:02.300 --> 02:04:07.020]   of like your friendly, your friendly car company.
[02:04:07.020 --> 02:04:07.940]   - I mean, that's my biggest,
[02:04:07.940 --> 02:04:10.060]   my biggest snipe at Waymo was always,
[02:04:10.060 --> 02:04:11.420]   that guy's a paid actor.
[02:04:11.420 --> 02:04:13.620]   That guy's not a Waymo user, he's a paid actor.
[02:04:13.620 --> 02:04:15.380]   Look here, I found his call sheet.
[02:04:15.380 --> 02:04:17.380]   - Do kind of like what SpaceX is doing
[02:04:17.380 --> 02:04:18.940]   with the rocket launches.
[02:04:18.940 --> 02:04:20.940]   Just get, put the nerds up front,
[02:04:20.940 --> 02:04:25.140]   put the engineers up front and just like show failures too.
[02:04:25.140 --> 02:04:25.980]   Just--
[02:04:25.980 --> 02:04:27.860]   - I love, I love SpaceX's, yeah.
[02:04:27.860 --> 02:04:29.820]   - Yeah, the thing that they're doing is right.
[02:04:29.820 --> 02:04:32.380]   And it just feels like the right, but--
[02:04:32.380 --> 02:04:34.380]   - We're all so excited to see them succeed.
[02:04:34.380 --> 02:04:35.220]   - Yeah.
[02:04:35.220 --> 02:04:37.300]   - I can't wait to see Waymo fail, you know?
[02:04:37.300 --> 02:04:39.380]   Like you lie to me, I want you to fail.
[02:04:39.380 --> 02:04:41.140]   You tell me the truth, you be honest with me,
[02:04:41.140 --> 02:04:42.140]   I want you to succeed.
[02:04:42.140 --> 02:04:42.980]   - Yeah.
[02:04:43.980 --> 02:04:48.980]   - Yeah, and that requires the renegade CEO, right?
[02:04:48.980 --> 02:04:51.500]   I'm with you, I'm with you.
[02:04:51.500 --> 02:04:53.020]   I still have a little bit of faith in Waymo
[02:04:53.020 --> 02:04:56.620]   for the renegade CEO to step forward, but--
[02:04:56.620 --> 02:05:00.620]   - It's not, it's not John Krafcik.
[02:05:00.620 --> 02:05:02.980]   - Yeah, it's, you can't--
[02:05:02.980 --> 02:05:04.980]   - It's not Chris Armstead.
[02:05:04.980 --> 02:05:07.660]   And those people may be very good at certain things.
[02:05:07.660 --> 02:05:08.500]   - Yeah.
[02:05:08.500 --> 02:05:10.340]   - But they're not renegades.
[02:05:10.340 --> 02:05:12.020]   - Yeah, because these companies are fundamentally,
[02:05:12.020 --> 02:05:14.260]   even though we're talking about billion dollars,
[02:05:14.260 --> 02:05:15.660]   all these crazy numbers,
[02:05:15.660 --> 02:05:19.300]   they're still like early stage startups.
[02:05:19.300 --> 02:05:21.860]   - I mean, and I just, if you are pre-revenue
[02:05:21.860 --> 02:05:24.260]   and you've raised $10 billion, I have no idea.
[02:05:24.260 --> 02:05:26.500]   Like, this just doesn't work.
[02:05:26.500 --> 02:05:28.020]   You know, it's against everything Silicon Valley.
[02:05:28.020 --> 02:05:30.340]   Where's your minimum viable product?
[02:05:30.340 --> 02:05:31.580]   Where's your users?
[02:05:31.580 --> 02:05:33.340]   Where's your growth numbers?
[02:05:33.340 --> 02:05:36.260]   This is traditional Silicon Valley.
[02:05:36.260 --> 02:05:38.020]   Why do you not apply it to what you think
[02:05:38.020 --> 02:05:39.580]   you're too big to fail already?
[02:05:41.740 --> 02:05:45.740]   - How do you think autonomous driving will change society?
[02:05:45.740 --> 02:05:50.740]   So the mission is, for Kama, to solve self-driving.
[02:05:50.740 --> 02:05:54.220]   Do you have like a vision of the world
[02:05:54.220 --> 02:05:55.780]   of how it'll be different?
[02:05:55.780 --> 02:06:00.060]   Is it as simple as A to B transportation?
[02:06:00.060 --> 02:06:02.500]   Or is there like, 'cause these are robots.
[02:06:02.500 --> 02:06:05.860]   - It's not about autonomous driving in and of itself.
[02:06:05.860 --> 02:06:07.500]   It's what the technology enables.
[02:06:09.980 --> 02:06:12.220]   I think it's the coolest applied AI problem.
[02:06:12.220 --> 02:06:16.380]   I like it because it has a clear path to monetary value.
[02:06:16.380 --> 02:06:21.460]   But as far as that being the thing that changes the world,
[02:06:21.460 --> 02:06:25.500]   I mean, no, like there's cute things we're doing in common.
[02:06:25.500 --> 02:06:26.980]   Like who'd have thought you could stick a phone
[02:06:26.980 --> 02:06:29.100]   on the windshield and it'll drive?
[02:06:29.100 --> 02:06:31.180]   But like, really the product that you're building
[02:06:31.180 --> 02:06:33.940]   is not something that people were not capable
[02:06:33.940 --> 02:06:35.780]   of imagining 50 years ago.
[02:06:35.780 --> 02:06:37.820]   So no, it doesn't change the world on that front.
[02:06:37.820 --> 02:06:39.500]   Could people have imagined the internet 50 years ago?
[02:06:39.500 --> 02:06:42.620]   Only true genius visionaries.
[02:06:42.620 --> 02:06:45.140]   Everyone could have imagined autonomous cars 50 years ago.
[02:06:45.140 --> 02:06:47.060]   It's like a car, but I don't drive it.
[02:06:47.060 --> 02:06:49.340]   - See, I have this sense, and I told you,
[02:06:49.340 --> 02:06:54.260]   like my long-term dream is robots
[02:06:54.260 --> 02:06:57.580]   with whom you have deep connections.
[02:06:57.580 --> 02:07:02.660]   And there's different trajectories towards that.
[02:07:02.660 --> 02:07:04.500]   And I've been thinking,
[02:07:04.500 --> 02:07:07.140]   so I've been thinking of launching a startup.
[02:07:08.340 --> 02:07:09.820]   I see autonomous vehicles
[02:07:09.820 --> 02:07:11.540]   as a potential trajectory to that.
[02:07:11.540 --> 02:07:16.580]   That's not where the direction I would like to go,
[02:07:16.580 --> 02:07:19.140]   but I also see Tesla or even Kamiya
[02:07:19.140 --> 02:07:24.140]   like pivoting into robotics broadly defined at some stage
[02:07:24.140 --> 02:07:27.140]   in the way, like you're mentioning,
[02:07:27.140 --> 02:07:28.540]   the internet didn't expect.
[02:07:28.540 --> 02:07:32.580]   - Let's solve, you know, when I say a comma about this,
[02:07:32.580 --> 02:07:33.420]   we could talk about this,
[02:07:33.420 --> 02:07:35.740]   but let's solve self-driving cars first.
[02:07:35.740 --> 02:07:37.100]   Gotta stay focused on the mission.
[02:07:37.100 --> 02:07:39.220]   Don't, you're not too big to fail.
[02:07:39.220 --> 02:07:41.340]   For however much I think calm is winning,
[02:07:41.340 --> 02:07:42.740]   like, no, no, no, no, no.
[02:07:42.740 --> 02:07:44.980]   You're winning when you solve level five self-driving cars.
[02:07:44.980 --> 02:07:46.780]   And until then you haven't won.
[02:07:46.780 --> 02:07:48.180]   And, you know, again,
[02:07:48.180 --> 02:07:50.020]   you want to be arrogant in the face of other people?
[02:07:50.020 --> 02:07:50.860]   Great.
[02:07:50.860 --> 02:07:51.900]   You want to be arrogant in the face of nature?
[02:07:51.900 --> 02:07:52.740]   You're an idiot.
[02:07:52.740 --> 02:07:56.380]   - Stay mission focused, brilliantly put.
[02:07:56.380 --> 02:07:58.620]   Like I mentioned, thinking of launching a startup,
[02:07:58.620 --> 02:08:01.180]   I've been considering, actually before COVID,
[02:08:01.180 --> 02:08:03.340]   I've been thinking of moving to San Francisco.
[02:08:03.340 --> 02:08:05.140]   - Oh, I wouldn't go there.
[02:08:05.980 --> 02:08:08.100]   - So why is, well,
[02:08:08.100 --> 02:08:10.580]   and now I'm thinking about potentially Austin
[02:08:10.580 --> 02:08:13.620]   and we're in San Diego now.
[02:08:13.620 --> 02:08:14.900]   - San Diego, come here.
[02:08:14.900 --> 02:08:16.500]   - So why, what,
[02:08:16.500 --> 02:08:20.540]   I mean, you're such an interesting human.
[02:08:20.540 --> 02:08:23.020]   You've launched so many successful things.
[02:08:23.020 --> 02:08:26.180]   What, why San Diego?
[02:08:26.180 --> 02:08:27.020]   What do you recommend?
[02:08:27.020 --> 02:08:29.260]   Why not San Francisco?
[02:08:29.260 --> 02:08:31.780]   Have you thought, so in your case,
[02:08:31.780 --> 02:08:33.740]   San Diego with Qualcomm and Snapdragon,
[02:08:33.740 --> 02:08:37.340]   I mean, that's an amazing combination, but.
[02:08:37.340 --> 02:08:38.540]   - That wasn't really why.
[02:08:38.540 --> 02:08:39.500]   - That wasn't the why?
[02:08:39.500 --> 02:08:41.300]   - No, I mean, Qualcomm was an afterthought.
[02:08:41.300 --> 02:08:42.860]   Qualcomm was, it was a nice thing to think about.
[02:08:42.860 --> 02:08:45.140]   It's like, you can have a tech company here.
[02:08:45.140 --> 02:08:45.980]   And a good one.
[02:08:45.980 --> 02:08:48.900]   I mean, you know, I like Qualcomm, but no.
[02:08:48.900 --> 02:08:50.500]   - Well, so why San Diego better than San Francisco?
[02:08:50.500 --> 02:08:51.860]   Why does San Francisco suck?
[02:08:51.860 --> 02:08:52.700]   - Well, so, okay.
[02:08:52.700 --> 02:08:54.060]   So first off, we all kind of said, like,
[02:08:54.060 --> 02:08:55.260]   we want to stay in California.
[02:08:55.260 --> 02:08:59.980]   People like the ocean, you know, California for its flaws.
[02:08:59.980 --> 02:09:02.380]   It's like a lot of the flaws of California
[02:09:02.380 --> 02:09:03.900]   are not necessarily California as a whole,
[02:09:03.900 --> 02:09:05.780]   and they're much more San Francisco specific.
[02:09:05.780 --> 02:09:06.700]   - Yeah.
[02:09:06.700 --> 02:09:09.880]   - San Francisco, so I think first-tier cities in general
[02:09:09.880 --> 02:09:11.540]   have stopped wanting growth.
[02:09:11.540 --> 02:09:15.500]   Well, you have, like, in San Francisco, you know,
[02:09:15.500 --> 02:09:18.780]   the voting class always votes to not build more houses
[02:09:18.780 --> 02:09:20.020]   because they own all the houses.
[02:09:20.020 --> 02:09:21.860]   And they're like, well, you know,
[02:09:21.860 --> 02:09:23.860]   once people have figured out how to vote themselves
[02:09:23.860 --> 02:09:25.220]   more money, they're gonna do it.
[02:09:25.220 --> 02:09:27.900]   It is so insanely corrupt.
[02:09:27.900 --> 02:09:31.620]   It is not balanced at all, like, political party-wise.
[02:09:31.620 --> 02:09:34.660]   You know, it's a one-party city, and--
[02:09:34.660 --> 02:09:36.700]   - For all the discussion of diversity,
[02:09:36.700 --> 02:09:42.180]   it stops lacking real diversity of thought,
[02:09:42.180 --> 02:09:47.180]   of background, of approaches, of strategies, of ideas.
[02:09:47.180 --> 02:09:51.180]   It's kind of a strange place.
[02:09:51.180 --> 02:09:54.400]   That it's the loudest people about diversity
[02:09:54.400 --> 02:09:56.460]   and the biggest lack of diversity.
[02:09:56.460 --> 02:09:58.660]   - Well, I mean, that's what they say, right?
[02:09:58.660 --> 02:10:00.380]   It's the projection.
[02:10:00.380 --> 02:10:01.220]   - Projection, yeah.
[02:10:01.220 --> 02:10:02.180]   (both laughing)
[02:10:02.180 --> 02:10:03.020]   Yeah, it's interesting.
[02:10:03.020 --> 02:10:04.540]   And even people in Silicon Valley tell me
[02:10:04.540 --> 02:10:07.980]   that it's, like, high up people,
[02:10:07.980 --> 02:10:10.140]   everybody is like, this is a terrible place.
[02:10:10.140 --> 02:10:10.980]   It doesn't make sense.
[02:10:10.980 --> 02:10:13.260]   - I mean, and coronavirus is really what killed it.
[02:10:13.260 --> 02:10:18.260]   San Francisco was the number one exodus during coronavirus.
[02:10:18.260 --> 02:10:21.740]   - We still think San Diego is a good place to be.
[02:10:21.740 --> 02:10:22.560]   - Yeah.
[02:10:22.560 --> 02:10:24.780]   Yeah, I mean, we'll see.
[02:10:24.780 --> 02:10:29.780]   We'll see what happens with California a bit longer term.
[02:10:29.780 --> 02:10:32.100]   I mean, Austin's an interesting choice.
[02:10:32.100 --> 02:10:35.740]   I don't have really anything bad to say about Austin either,
[02:10:35.740 --> 02:10:38.540]   except for the extreme heat in the summer.
[02:10:38.540 --> 02:10:40.220]   But that's very on the surface, right?
[02:10:40.220 --> 02:10:43.740]   I think as far as an ecosystem goes, it's cool.
[02:10:43.740 --> 02:10:45.620]   I personally love Colorado.
[02:10:45.620 --> 02:10:47.140]   - Colorado's great.
[02:10:47.140 --> 02:10:48.260]   - Yeah, I mean, you have these states
[02:10:48.260 --> 02:10:51.460]   that are just way better run.
[02:10:51.460 --> 02:10:55.380]   California is, especially San Francisco,
[02:10:55.380 --> 02:10:56.580]   it's on its high horse.
[02:10:58.460 --> 02:11:02.660]   - Can I ask you for advice to me and to others
[02:11:02.660 --> 02:11:07.580]   about what's it take to build a successful startup?
[02:11:07.580 --> 02:11:09.220]   - Oh, I don't know, I haven't done that.
[02:11:09.220 --> 02:11:10.780]   Talk to someone who did that.
[02:11:10.780 --> 02:11:12.140]   - Well, if you know.
[02:11:12.140 --> 02:11:16.460]   This is like another book of yours
[02:11:16.460 --> 02:11:18.900]   that I'll buy for $67, I suppose.
[02:11:18.900 --> 02:11:20.660]   So there's--
[02:11:20.660 --> 02:11:22.860]   (laughing)
[02:11:22.860 --> 02:11:24.180]   - One of these days I'll sell out.
[02:11:24.180 --> 02:11:25.020]   Yeah, that's right.
[02:11:25.020 --> 02:11:26.060]   Jail breaks are gonna be a dollar
[02:11:26.060 --> 02:11:27.900]   and books are gonna be 67.
[02:11:27.900 --> 02:11:32.100]   - How I Jailbroke the iPhone by George Hotz.
[02:11:32.100 --> 02:11:32.940]   - That's right.
[02:11:32.940 --> 02:11:35.740]   How I Jailbroke the iPhone and you can too.
[02:11:35.740 --> 02:11:36.580]   - And you can too.
[02:11:36.580 --> 02:11:37.900]   - By George Hotz, 67 dollars.
[02:11:37.900 --> 02:11:39.060]   - In 21 days.
[02:11:39.060 --> 02:11:40.420]   - That's right, that's right.
[02:11:40.420 --> 02:11:42.340]   - Oh God, okay, I can't wait.
[02:11:42.340 --> 02:11:44.980]   But so you have an introspective,
[02:11:44.980 --> 02:11:49.500]   you have built a very unique company.
[02:11:49.500 --> 02:11:52.380]   I mean, not you, but you and others.
[02:11:52.380 --> 02:11:54.740]   But I don't know.
[02:11:54.740 --> 02:11:56.980]   There's no, there's nothing.
[02:11:56.980 --> 02:11:57.900]   You have an introspective,
[02:11:57.900 --> 02:12:00.940]   you haven't really sat down and thought about like,
[02:12:00.940 --> 02:12:04.220]   well, like if you and I, we're having a bunch of,
[02:12:04.220 --> 02:12:08.180]   we're having some beers and you're seeing that I'm depressed
[02:12:08.180 --> 02:12:09.940]   and whatever, I'm struggling.
[02:12:09.940 --> 02:12:11.660]   There's no advice you can give?
[02:12:11.660 --> 02:12:13.180]   - Oh, I mean.
[02:12:13.180 --> 02:12:14.580]   - More beer?
[02:12:14.580 --> 02:12:15.420]   - More beer.
[02:12:15.420 --> 02:12:17.660]   (laughing)
[02:12:17.660 --> 02:12:22.700]   Yeah, I think it's all very like situation dependent.
[02:12:22.700 --> 02:12:25.940]   Okay, if I can give a generic piece of advice,
[02:12:25.940 --> 02:12:28.220]   it's the technology always wins.
[02:12:28.220 --> 02:12:33.220]   The better technology always wins and lying always loses.
[02:12:33.220 --> 02:12:37.300]   Build technology and don't lie.
[02:12:37.300 --> 02:12:40.540]   - I'm with you, I agree very much.
[02:12:40.540 --> 02:12:41.660]   - Long run, long run, sure.
[02:12:41.660 --> 02:12:42.500]   - It's the long run, yeah.
[02:12:42.500 --> 02:12:43.340]   - And you know what?
[02:12:43.340 --> 02:12:44.900]   The market can remain irrational longer
[02:12:44.900 --> 02:12:46.580]   than you can remain solid.
[02:12:46.580 --> 02:12:47.420]   True fact.
[02:12:47.420 --> 02:12:49.300]   - Well, this is an interesting point
[02:12:49.300 --> 02:12:52.660]   'cause I ethically and just as a human believe that
[02:12:54.420 --> 02:12:59.420]   like hype and smoke and mirrors is not at any stage
[02:12:59.420 --> 02:13:02.780]   of the company is a good strategy.
[02:13:02.780 --> 02:13:05.820]   I mean, there's some like, you know, PR magic
[02:13:05.820 --> 02:13:07.100]   kind of like, you know.
[02:13:07.100 --> 02:13:08.580]   - Oh, hype around a new product, right?
[02:13:08.580 --> 02:13:09.820]   If there's a call to action,
[02:13:09.820 --> 02:13:12.660]   if there's like a call to action, like buy my new GPU,
[02:13:12.660 --> 02:13:14.980]   look at it, it takes up three slots and it's this big,
[02:13:14.980 --> 02:13:16.300]   it's huge, buy my GPU.
[02:13:16.300 --> 02:13:17.140]   Yeah, that's great.
[02:13:17.140 --> 02:13:18.300]   - But like if you look at, you know,
[02:13:18.300 --> 02:13:20.820]   especially in the AI space broadly,
[02:13:20.820 --> 02:13:23.540]   but autonomous vehicles, like you can raise
[02:13:23.540 --> 02:13:26.620]   a huge amount of money on nothing.
[02:13:26.620 --> 02:13:29.260]   And the question to me is like, I'm against that.
[02:13:29.260 --> 02:13:32.740]   I'll never be part of that, I don't think.
[02:13:32.740 --> 02:13:35.880]   I hope not, willingly not.
[02:13:35.880 --> 02:13:40.060]   But like, is there something to be said
[02:13:40.060 --> 02:13:45.060]   to essentially lying to raise money,
[02:13:45.060 --> 02:13:47.940]   like fake it till you make it kind of thing?
[02:13:47.940 --> 02:13:50.220]   - I mean, just as Billy McFarland in the Fyre Festival,
[02:13:50.220 --> 02:13:54.260]   like we all experienced, you know, what happens with that?
[02:13:54.260 --> 02:13:57.500]   No, no, don't fake it till you make it.
[02:13:57.500 --> 02:14:00.580]   Be honest and hope you make it the whole way.
[02:14:00.580 --> 02:14:01.980]   - The technology wins.
[02:14:01.980 --> 02:14:03.020]   - Right, the technology wins.
[02:14:03.020 --> 02:14:05.780]   And like, there is, I'm not, you know,
[02:14:05.780 --> 02:14:07.060]   you just like the anti-hype, you know,
[02:14:07.060 --> 02:14:08.940]   that's a Slava KPSS reference,
[02:14:08.940 --> 02:14:13.180]   but hype isn't necessarily bad.
[02:14:13.180 --> 02:14:17.620]   I loved camping out for the iPhones, you know,
[02:14:17.620 --> 02:14:21.100]   and as long as the hype is backed by like substance,
[02:14:21.100 --> 02:14:23.860]   as long as it's backed by something I can actually buy,
[02:14:23.860 --> 02:14:26.820]   and like it's real, then hype is great
[02:14:26.820 --> 02:14:28.780]   and it's a great feeling.
[02:14:28.780 --> 02:14:30.940]   It's when the hype is backed by lies
[02:14:30.940 --> 02:14:32.140]   that it's a bad feeling.
[02:14:32.140 --> 02:14:34.620]   - I mean, a lot of people call Elon Musk a fraud.
[02:14:34.620 --> 02:14:35.720]   - How could he be a fraud?
[02:14:35.720 --> 02:14:37.820]   - I've noticed this, this kind of interesting effect,
[02:14:37.820 --> 02:14:42.820]   which is he does tend to over-promise and deliver.
[02:14:42.820 --> 02:14:45.740]   What's the better way to phrase it?
[02:14:45.740 --> 02:14:49.300]   Promise a timeline that he doesn't deliver on,
[02:14:49.300 --> 02:14:51.780]   he delivers much later on.
[02:14:51.780 --> 02:14:52.900]   What do you think about that?
[02:14:52.900 --> 02:14:56.260]   'Cause I do that, I think that's a programmer thing too.
[02:14:56.260 --> 02:14:57.580]   I do that as well.
[02:14:57.580 --> 02:15:01.340]   You think that's a really bad thing to do or is that okay?
[02:15:01.340 --> 02:15:03.980]   - I think that's, again, as long as like,
[02:15:03.980 --> 02:15:06.940]   you're working toward it and you're gonna deliver on it
[02:15:06.940 --> 02:15:10.340]   and it's not too far off, right?
[02:15:10.340 --> 02:15:14.980]   Right, like, you know, the whole autonomous vehicle thing,
[02:15:14.980 --> 02:15:19.780]   it's like, I mean, I still think Tesla's on track to beat us.
[02:15:19.780 --> 02:15:21.920]   I still think even with their missteps,
[02:15:21.920 --> 02:15:24.460]   they have advantages we don't have.
[02:15:24.460 --> 02:15:28.140]   You know, Elon is better than me
[02:15:28.140 --> 02:15:33.100]   at like marshaling massive amounts of resources.
[02:15:33.100 --> 02:15:36.300]   So, you know, I still think given the fact
[02:15:36.300 --> 02:15:38.300]   they're maybe making some wrong decisions,
[02:15:38.300 --> 02:15:39.380]   they'll end up winning.
[02:15:39.380 --> 02:15:42.700]   And like, it's fine to hype it
[02:15:42.700 --> 02:15:45.260]   if you're actually gonna win, right?
[02:15:45.260 --> 02:15:47.340]   If Elon says, look, we're gonna be landing rockets
[02:15:47.340 --> 02:15:49.500]   back on earth in a year and it takes four,
[02:15:49.500 --> 02:15:53.660]   like, he landed a rocket back on earth
[02:15:53.660 --> 02:15:55.540]   and he was working toward it the whole time.
[02:15:55.540 --> 02:15:57.180]   I think there's some amount of like,
[02:15:57.180 --> 02:15:59.380]   I think what becomes wrong is if you know
[02:15:59.380 --> 02:16:01.680]   you're not gonna meet that deadline, if you're lying.
[02:16:01.680 --> 02:16:03.380]   - Yeah, that's brilliantly put.
[02:16:03.380 --> 02:16:06.940]   Like, this is what people don't understand, I think.
[02:16:06.940 --> 02:16:09.540]   Like, Elon believes everything he says.
[02:16:09.540 --> 02:16:12.220]   - He does, as far as I can tell, he does.
[02:16:12.220 --> 02:16:14.820]   And I detected that in myself too.
[02:16:14.820 --> 02:16:17.660]   Like, if I, it's only bullshit
[02:16:17.660 --> 02:16:21.500]   if you're like conscious of yourself lying.
[02:16:21.500 --> 02:16:22.580]   - Yeah, I think so.
[02:16:22.580 --> 02:16:23.500]   - Yeah.
[02:16:23.500 --> 02:16:25.740]   - Now you can't take that to such an extreme, right?
[02:16:25.740 --> 02:16:27.660]   Like in a way, I think maybe Billy McFarlane
[02:16:27.660 --> 02:16:30.100]   believed everything he said too.
[02:16:30.100 --> 02:16:31.540]   - Right, that's how you start a cult
[02:16:31.540 --> 02:16:34.340]   and everybody kills themselves, yeah.
[02:16:34.340 --> 02:16:36.100]   - Yeah, like it's, you need, you need,
[02:16:36.100 --> 02:16:39.380]   if there's like some factor on it, it's fine.
[02:16:39.380 --> 02:16:41.860]   And you need some people to like, you know,
[02:16:41.860 --> 02:16:42.980]   keep you in check.
[02:16:42.980 --> 02:16:46.620]   But like, if you deliver on most of the things you say
[02:16:46.620 --> 02:16:48.820]   and just the timelines are off, yeah.
[02:16:48.820 --> 02:16:50.340]   - It does piss people off though.
[02:16:50.340 --> 02:16:53.340]   I wonder, but who cares?
[02:16:53.340 --> 02:16:55.300]   In a long arc of history, the people,
[02:16:55.300 --> 02:16:58.140]   everybody gets pissed off at the people who succeed.
[02:16:58.140 --> 02:17:00.340]   Which is one of the things that frustrates me
[02:17:00.340 --> 02:17:05.260]   about this world is they don't celebrate
[02:17:05.260 --> 02:17:07.700]   the success of others.
[02:17:07.700 --> 02:17:12.700]   Like there's so many people that want Elon to fail.
[02:17:12.700 --> 02:17:14.940]   It's so fascinating to me.
[02:17:14.940 --> 02:17:17.340]   Like what is wrong with you?
[02:17:17.340 --> 02:17:21.700]   So Elon Musk talks about like people short,
[02:17:21.700 --> 02:17:23.620]   like they talk about financial,
[02:17:23.620 --> 02:17:25.420]   but I think it's much bigger than the financials.
[02:17:25.420 --> 02:17:27.940]   I've seen like the human factors community.
[02:17:27.940 --> 02:17:31.620]   They want other people to fail.
[02:17:31.620 --> 02:17:32.980]   Why, why, why?
[02:17:32.980 --> 02:17:35.940]   Like even people, the harshest thing is like,
[02:17:36.820 --> 02:17:38.180]   you know, even people that like seem
[02:17:38.180 --> 02:17:42.020]   to really hate Donald Trump, they want him to fail.
[02:17:42.020 --> 02:17:43.060]   Or like the other president,
[02:17:43.060 --> 02:17:45.940]   or they want Barack Obama to fail.
[02:17:45.940 --> 02:17:47.140]   It's like.
[02:17:47.140 --> 02:17:49.780]   - We're all on the same boat, man.
[02:17:49.780 --> 02:17:51.700]   - It's weird, but I want that,
[02:17:51.700 --> 02:17:54.140]   I would love to inspire that part of the world to change
[02:17:54.140 --> 02:17:58.620]   because damn it, if the human species is gonna survive,
[02:17:58.620 --> 02:18:00.500]   we should celebrate success.
[02:18:00.500 --> 02:18:02.860]   Like it seems like the efficient thing to do
[02:18:02.860 --> 02:18:06.340]   in this objective function that like we're all striving for
[02:18:06.340 --> 02:18:09.100]   is to celebrate the ones that like figure out
[02:18:09.100 --> 02:18:11.860]   how to like do better at that objective function
[02:18:11.860 --> 02:18:16.620]   as opposed to dragging them down back into the mud.
[02:18:16.620 --> 02:18:19.100]   - I think there is, this is the speech I always give
[02:18:19.100 --> 02:18:21.740]   about the commenters on hacker news.
[02:18:21.740 --> 02:18:23.300]   So first off, something to remember
[02:18:23.300 --> 02:18:24.780]   about the internet in general
[02:18:24.780 --> 02:18:29.380]   is commenters are not representative of the population.
[02:18:29.380 --> 02:18:30.780]   I don't comment on anything.
[02:18:30.780 --> 02:18:34.220]   Commenters are representative
[02:18:34.220 --> 02:18:36.700]   of a certain sliver of the population.
[02:18:36.700 --> 02:18:39.580]   And on hacker news, a common thing I'll see
[02:18:39.580 --> 02:18:42.100]   is when you'll see something that's like,
[02:18:42.100 --> 02:18:47.100]   you know, promises to be wild out there and innovative.
[02:18:47.100 --> 02:18:49.700]   There is some amount of, you know,
[02:18:49.700 --> 02:18:50.980]   checking them back to earth,
[02:18:50.980 --> 02:18:53.900]   but there's also some amount of if this thing succeeds,
[02:18:53.900 --> 02:18:59.420]   well, I'm 36 and I've worked at large tech companies
[02:18:59.420 --> 02:19:00.380]   my whole life.
[02:19:00.380 --> 02:19:03.500]   They can't succeed.
[02:19:03.500 --> 02:19:05.540]   Because if they succeed, that would mean
[02:19:05.540 --> 02:19:08.060]   that I could have done something different with my life.
[02:19:08.060 --> 02:19:10.140]   But we know that I could have, we know that I could have,
[02:19:10.140 --> 02:19:11.940]   and that's why they're going to fail.
[02:19:11.940 --> 02:19:13.100]   And they have to root for them to fail,
[02:19:13.100 --> 02:19:15.940]   to kind of maintain their world image.
[02:19:15.940 --> 02:19:17.900]   So tune it out.
[02:19:17.900 --> 02:19:20.500]   - And they comment, well, it's hard.
[02:19:20.500 --> 02:19:25.220]   So one of the things I'm considering startup wise
[02:19:25.220 --> 02:19:27.620]   is to change that.
[02:19:27.620 --> 02:19:31.700]   'Cause I think it's also a technology problem.
[02:19:31.700 --> 02:19:33.060]   It's a platform problem.
[02:19:33.060 --> 02:19:33.900]   - I agree.
[02:19:33.900 --> 02:19:36.060]   - It's like, because the thing you said,
[02:19:36.060 --> 02:19:37.420]   most people don't comment.
[02:19:37.420 --> 02:19:41.740]   I think most people want to comment.
[02:19:41.740 --> 02:19:45.260]   They just don't because it's all the assholes
[02:19:45.260 --> 02:19:46.100]   who are commenting.
[02:19:46.100 --> 02:19:48.020]   - Exactly, I don't want to be grouped in with them.
[02:19:48.020 --> 02:19:49.460]   - You don't want to be at a party
[02:19:49.460 --> 02:19:50.860]   where everyone is an asshole.
[02:19:50.860 --> 02:19:54.540]   But that's a platform problem.
[02:19:54.540 --> 02:19:56.140]   - I can't believe what Reddit's become.
[02:19:56.140 --> 02:19:59.500]   I can't believe the group think in Reddit comments.
[02:19:59.500 --> 02:20:02.740]   - There's a, and Reddit's an interesting one
[02:20:02.740 --> 02:20:05.140]   because they're subreddits.
[02:20:05.140 --> 02:20:09.300]   And so you can still see, especially small subreddits
[02:20:09.300 --> 02:20:14.300]   that are little havens of joy and positivity and deep,
[02:20:14.300 --> 02:20:18.900]   even disagreement, but nuanced discussion.
[02:20:18.900 --> 02:20:21.740]   But it's only small little pockets.
[02:20:21.740 --> 02:20:23.500]   But that's emergent.
[02:20:23.500 --> 02:20:26.820]   The platform's not helping that or hurting that.
[02:20:26.820 --> 02:20:31.140]   So I guess naturally, something about the internet,
[02:20:31.140 --> 02:20:35.100]   if you don't put in a lot of effort to encourage nuance
[02:20:35.100 --> 02:20:37.220]   and positive, good vibes,
[02:20:37.220 --> 02:20:41.180]   it's naturally going to decline into chaos.
[02:20:41.180 --> 02:20:42.900]   - I would love to see someone do this as well.
[02:20:42.900 --> 02:20:43.820]   - Yeah.
[02:20:43.820 --> 02:20:46.100]   - I think it's, yeah, very doable.
[02:20:46.100 --> 02:20:49.380]   - I think actually, so I feel like
[02:20:49.380 --> 02:20:52.060]   Twitter could be overthrown.
[02:20:52.060 --> 02:20:54.300]   - Yashua Bach talked about how like,
[02:20:54.300 --> 02:20:58.020]   if you have like and retweet,
[02:20:58.020 --> 02:21:02.140]   like that's only positive wiring, right?
[02:21:02.140 --> 02:21:05.620]   The only way to do anything like negative there
[02:21:05.620 --> 02:21:08.300]   is with a comment.
[02:21:08.300 --> 02:21:12.420]   And that's like that asymmetry is what gives,
[02:21:12.420 --> 02:21:15.620]   you know, Twitter its particular toxicness.
[02:21:15.620 --> 02:21:18.060]   Whereas I find YouTube comments to be much better
[02:21:18.060 --> 02:21:21.540]   because YouTube comments have an up and a down
[02:21:21.540 --> 02:21:23.500]   and they don't show the downvotes.
[02:21:23.500 --> 02:21:26.820]   - Without getting into depth of this particular discussion,
[02:21:26.820 --> 02:21:29.580]   the point is to explore possibilities
[02:21:29.580 --> 02:21:30.860]   and get a lot of data on it.
[02:21:30.860 --> 02:21:34.820]   Because I mean, I could disagree with what you just said.
[02:21:34.820 --> 02:21:36.700]   The point is it's unclear.
[02:21:36.700 --> 02:21:39.060]   It hasn't been explored in a really rich way.
[02:21:39.060 --> 02:21:44.740]   Like these questions of how to create platforms
[02:21:44.740 --> 02:21:46.100]   that encourage positivity.
[02:21:46.100 --> 02:21:49.620]   Yeah, I think it's a technology problem.
[02:21:49.620 --> 02:21:52.020]   And I think we'll look back at Twitter as it is now.
[02:21:52.020 --> 02:21:53.900]   Maybe it'll happen within Twitter,
[02:21:53.900 --> 02:21:56.500]   but most likely somebody overthrows them
[02:21:56.500 --> 02:22:00.180]   is we'll look back at Twitter and say,
[02:22:00.180 --> 02:22:03.220]   we can't believe we put up with this level of toxicity.
[02:22:03.220 --> 02:22:05.100]   - You need a different business model too.
[02:22:05.100 --> 02:22:07.700]   Any social network that fundamentally has advertising
[02:22:07.700 --> 02:22:10.460]   as a business model, this was in the Social Dilemma,
[02:22:10.460 --> 02:22:11.740]   which I didn't watch, but I liked it.
[02:22:11.740 --> 02:22:12.620]   It's like, you know, there's always the,
[02:22:12.620 --> 02:22:15.380]   you know, you're the product, you're not the,
[02:22:15.380 --> 02:22:17.980]   but they had a nuanced take on it that I really liked.
[02:22:17.980 --> 02:22:22.980]   And it said, the product being sold is influence over you.
[02:22:24.660 --> 02:22:28.780]   The product being sold is literally your influence on you.
[02:22:28.780 --> 02:22:34.580]   That can't be, if that's your idea, okay, well,
[02:22:34.580 --> 02:22:35.460]   you know, guess what?
[02:22:35.460 --> 02:22:37.100]   It cannot be toxic.
[02:22:37.100 --> 02:22:39.460]   - Yeah, maybe there's ways to spin it,
[02:22:39.460 --> 02:22:42.580]   like with giving a lot more control to the user
[02:22:42.580 --> 02:22:44.700]   and transparency to see what is happening to them
[02:22:44.700 --> 02:22:47.340]   as opposed to in the shadows, it's possible,
[02:22:47.340 --> 02:22:49.420]   but that can't be the primary source of--
[02:22:49.420 --> 02:22:52.020]   - But the users aren't, no one's going to use that.
[02:22:52.020 --> 02:22:54.020]   - It depends, it depends, it depends.
[02:22:54.020 --> 02:22:57.140]   I think that the, you're not going to,
[02:22:57.140 --> 02:23:00.260]   you can't depend on self-awareness of the users.
[02:23:00.260 --> 02:23:04.460]   - It's a longer discussion because you can't depend on it,
[02:23:04.460 --> 02:23:09.460]   but you can reward self-awareness.
[02:23:09.460 --> 02:23:12.340]   Like if for the ones who are willing to put in the work
[02:23:12.340 --> 02:23:16.100]   of self-awareness, you can reward them and incentivize
[02:23:16.100 --> 02:23:18.860]   and perhaps be pleasantly surprised how many people
[02:23:18.860 --> 02:23:23.300]   are willing to be self-aware on the internet.
[02:23:23.300 --> 02:23:24.380]   Like we are in real life.
[02:23:24.380 --> 02:23:26.940]   Like I'm putting in a lot of effort with you right now,
[02:23:26.940 --> 02:23:30.340]   being self-aware about if I say something stupid or mean,
[02:23:30.340 --> 02:23:32.740]   I'll like look at your body language.
[02:23:32.740 --> 02:23:34.380]   Like I'm putting in that effort, it's costly,
[02:23:34.380 --> 02:23:38.740]   for an introvert, very costly, but on the internet, fuck it.
[02:23:38.740 --> 02:23:42.940]   Like most people are like, I don't care if this hurts
[02:23:42.940 --> 02:23:46.260]   somebody, I don't care if this is not interesting
[02:23:46.260 --> 02:23:48.660]   or if this is, yeah, it's mean or whatever.
[02:23:48.660 --> 02:23:50.980]   - I think so much of the engagement today on the internet
[02:23:50.980 --> 02:23:53.060]   is so disingenuine too.
[02:23:53.060 --> 02:23:54.620]   You're not doing this out of a genuine,
[02:23:54.620 --> 02:23:55.500]   this is what you think.
[02:23:55.500 --> 02:23:57.700]   You're doing this just straight up to manipulate others.
[02:23:57.700 --> 02:23:59.540]   Whether you're, you just became an ad.
[02:23:59.540 --> 02:24:04.460]   - Okay, let's talk about a fun topic, which is programming.
[02:24:04.460 --> 02:24:07.020]   Here's another book idea for you, let me pitch.
[02:24:07.020 --> 02:24:09.700]   What's your perfect programming setup?
[02:24:09.700 --> 02:24:12.660]   So like, this by George Hotz.
[02:24:12.660 --> 02:24:17.540]   So like, what, listen, you're--
[02:24:17.540 --> 02:24:20.380]   - Give me a MacBook Air, sit me in a corner of a hotel room
[02:24:20.380 --> 02:24:21.220]   and you know, I'll still have food.
[02:24:21.220 --> 02:24:22.060]   - So you really don't care.
[02:24:22.060 --> 02:24:27.020]   You don't fetishize like multiple monitors, keyboard.
[02:24:27.020 --> 02:24:30.220]   - Those things are nice and I'm not going to say no to them,
[02:24:30.220 --> 02:24:33.260]   but did they automatically unlock tons of productivity?
[02:24:33.260 --> 02:24:34.100]   No, not at all.
[02:24:34.100 --> 02:24:36.260]   I have definitely been more productive on a MacBook Air
[02:24:36.260 --> 02:24:38.260]   in a corner of a hotel room.
[02:24:38.260 --> 02:24:41.580]   - What about IDE?
[02:24:41.580 --> 02:24:45.940]   So which operating system do you love?
[02:24:45.940 --> 02:24:48.980]   What text editor do you use, IDE?
[02:24:48.980 --> 02:24:52.980]   What, is there something that is like the perfect,
[02:24:52.980 --> 02:24:56.980]   if you could just say the perfect productivity setup
[02:24:56.980 --> 02:24:57.820]   for George Hotz.
[02:24:57.820 --> 02:24:58.660]   - Doesn't matter.
[02:24:58.660 --> 02:24:59.500]   - Doesn't matter.
[02:24:59.500 --> 02:25:00.380]   - It really doesn't matter.
[02:25:00.380 --> 02:25:03.100]   You know, I guess I code most of the time in Vim.
[02:25:03.100 --> 02:25:04.980]   Like literally I'm using an editor from the 70s.
[02:25:04.980 --> 02:25:07.300]   You know, you didn't make anything better.
[02:25:07.300 --> 02:25:09.020]   Okay, VS Code is nice for reading code.
[02:25:09.020 --> 02:25:10.780]   There's a few things that are nice about it.
[02:25:10.780 --> 02:25:13.420]   I think that there, you can build much better tools.
[02:25:13.420 --> 02:25:17.260]   How like, Ida's X refs work way better than VS Codes, why?
[02:25:18.620 --> 02:25:20.180]   - Yeah, actually that's a good question.
[02:25:20.180 --> 02:25:21.020]   Like why?
[02:25:21.020 --> 02:25:24.300]   I still use, sorry, Emacs for most.
[02:25:24.300 --> 02:25:28.860]   I've actually never, I have to confess something dark.
[02:25:28.860 --> 02:25:31.060]   So I've never used Vim.
[02:25:31.060 --> 02:25:34.780]   I think maybe I'm just afraid
[02:25:34.780 --> 02:25:39.340]   that my life has been like a waste.
[02:25:39.340 --> 02:25:40.660]   (both laughing)
[02:25:40.660 --> 02:25:43.500]   I'm so, I'm not evangelical about Emacs.
[02:25:43.500 --> 02:25:44.620]   I think that.
[02:25:44.620 --> 02:25:47.140]   - This is how I feel about Tenderflow versus PyTorch.
[02:25:47.140 --> 02:25:47.980]   - Yeah.
[02:25:47.980 --> 02:25:49.540]   - I think just like we've switched everything
[02:25:49.540 --> 02:25:51.860]   to PyTorch now, but months into the switch,
[02:25:51.860 --> 02:25:54.540]   I have felt like I've wasted years on Tenderflow.
[02:25:54.540 --> 02:25:56.220]   I can't believe it.
[02:25:56.220 --> 02:25:58.380]   I can't believe how much better PyTorch is.
[02:25:58.380 --> 02:26:01.460]   I've used Emacs and Vim, doesn't matter.
[02:26:01.460 --> 02:26:03.700]   - Yeah, still just my heart somehow,
[02:26:03.700 --> 02:26:04.660]   I fell in love with Lisp.
[02:26:04.660 --> 02:26:05.500]   I don't know why.
[02:26:05.500 --> 02:26:08.100]   You can't, the heart wants what the heart wants.
[02:26:08.100 --> 02:26:10.460]   I don't understand it, but it just connected with me.
[02:26:10.460 --> 02:26:11.660]   Maybe it's the functional language
[02:26:11.660 --> 02:26:13.220]   that at first I connected with.
[02:26:13.220 --> 02:26:15.820]   Maybe it's because so many of the AI courses
[02:26:15.820 --> 02:26:17.860]   before the deep learning revolution were taught
[02:26:17.860 --> 02:26:19.460]   with Lisp in mind.
[02:26:19.460 --> 02:26:20.300]   I don't know.
[02:26:20.300 --> 02:26:22.300]   I don't know what it is, but I'm stuck with it.
[02:26:22.300 --> 02:26:23.500]   But at the same time, like,
[02:26:23.500 --> 02:26:25.100]   why am I not using a modern ID
[02:26:25.100 --> 02:26:26.300]   for some of these programming?
[02:26:26.300 --> 02:26:27.220]   I don't know.
[02:26:27.220 --> 02:26:28.460]   - They're not that much better.
[02:26:28.460 --> 02:26:30.140]   I've used modern IDs too.
[02:26:30.140 --> 02:26:32.020]   - But at the same time, so to just,
[02:26:32.020 --> 02:26:33.020]   well, not to disagree with you,
[02:26:33.020 --> 02:26:35.860]   but like, I like multiple monitors.
[02:26:35.860 --> 02:26:38.340]   Like I have to do work on a laptop
[02:26:38.340 --> 02:26:41.340]   and it's a pain in the ass.
[02:26:41.340 --> 02:26:45.380]   And also I'm addicted to the Kinesis weird keyboard.
[02:26:45.380 --> 02:26:46.660]   You could see there.
[02:26:46.660 --> 02:26:48.580]   - Yeah, yeah, yeah.
[02:26:48.580 --> 02:26:50.140]   - Yeah, so you don't have any of that.
[02:26:50.140 --> 02:26:51.820]   You can just be on a MacBook.
[02:26:51.820 --> 02:26:53.020]   - I mean, look at work.
[02:26:53.020 --> 02:26:55.260]   I have three 24 inch monitors.
[02:26:55.260 --> 02:26:56.580]   I have a happy hacking keyboard.
[02:26:56.580 --> 02:26:58.940]   I have a Razer Deathadder mouse.
[02:26:58.940 --> 02:27:01.300]   - But it's not essential for you.
[02:27:01.300 --> 02:27:02.140]   - No.
[02:27:02.140 --> 02:27:04.700]   - Let's go to a day in the life of George Hotz.
[02:27:04.700 --> 02:27:08.780]   What is the perfect day productivity-wise?
[02:27:08.780 --> 02:27:12.420]   So we're not talking about like Hunter S. Thompson drugs.
[02:27:12.420 --> 02:27:13.260]   - Yeah, yeah, yeah.
[02:27:13.260 --> 02:27:16.420]   - And let's look at productivity.
[02:27:16.420 --> 02:27:19.820]   Like what's the day look like, like hour by hour?
[02:27:19.820 --> 02:27:22.140]   Is there any regularities
[02:27:22.140 --> 02:27:25.940]   that create a magical George Hotz experience?
[02:27:25.940 --> 02:27:28.300]   - I can remember three days in my life.
[02:27:28.300 --> 02:27:30.300]   And I remember these days vividly
[02:27:30.300 --> 02:27:35.300]   when I've gone through kind of radical transformations
[02:27:35.300 --> 02:27:37.900]   to the way I think.
[02:27:37.900 --> 02:27:40.140]   And what I would give, I would pay $100,000
[02:27:40.140 --> 02:27:42.980]   if I could have one of these days tomorrow.
[02:27:42.980 --> 02:27:44.780]   The days have been so impactful.
[02:27:44.780 --> 02:27:47.820]   And one was first discovering Eliezer Yudkowsky
[02:27:47.820 --> 02:27:50.780]   on the singularity and reading that stuff.
[02:27:50.780 --> 02:27:52.780]   And like, you know, my mind was blown.
[02:27:52.780 --> 02:27:57.860]   The next was discovering the Hutter Prize
[02:27:57.860 --> 02:27:59.940]   and that AI is just compression.
[02:27:59.940 --> 02:28:03.860]   Like finally understanding AIXI and what all of that was.
[02:28:03.860 --> 02:28:05.340]   You know, I like read about it when I was 18, 19,
[02:28:05.340 --> 02:28:06.500]   I didn't understand it.
[02:28:06.500 --> 02:28:08.420]   And then the fact that like lossless compression
[02:28:08.420 --> 02:28:12.260]   implies intelligence, the day that I was shown that.
[02:28:12.260 --> 02:28:14.180]   And then the third one is controversial,
[02:28:14.180 --> 02:28:17.380]   the day I found a blog called Unqualified Reservations
[02:28:17.380 --> 02:28:20.660]   and read that and I was like.
[02:28:20.660 --> 02:28:21.500]   - Wait, which one is that?
[02:28:21.500 --> 02:28:23.020]   That's, what's the guy's name?
[02:28:23.020 --> 02:28:24.060]   - Curtis Yarvin.
[02:28:24.060 --> 02:28:27.700]   - Yeah, so many people tell me I'm supposed to talk to him.
[02:28:27.700 --> 02:28:28.520]   - Yeah, the day--
[02:28:28.520 --> 02:28:29.860]   - He sounds insane.
[02:28:29.860 --> 02:28:30.700]   - Oh, definitely.
[02:28:30.700 --> 02:28:33.140]   - Or brilliant, but insane or both, I don't know.
[02:28:33.140 --> 02:28:35.180]   - The day I found that blog was another like,
[02:28:35.180 --> 02:28:37.180]   this was during like Gamergate
[02:28:37.180 --> 02:28:39.060]   and kind of the run up to the 2016 election.
[02:28:39.060 --> 02:28:42.900]   And I'm like, wow, okay, the world makes sense now.
[02:28:42.900 --> 02:28:45.660]   This, like I had a framework now to interpret this,
[02:28:45.660 --> 02:28:47.260]   just like I got the framework for AI
[02:28:47.260 --> 02:28:49.460]   and a framework to interpret technological progress.
[02:28:49.460 --> 02:28:52.860]   Like those days when I discovered these new frameworks were.
[02:28:52.860 --> 02:28:53.700]   - Oh, interesting.
[02:28:53.700 --> 02:28:57.180]   So it's not about, but what was special about those days?
[02:28:57.180 --> 02:28:58.820]   How did those days come to be?
[02:28:58.820 --> 02:29:00.020]   Is it just you got lucky?
[02:29:00.020 --> 02:29:01.460]   Like-- - Sure.
[02:29:01.460 --> 02:29:04.940]   - I like, you just encountered a Hutter Prize
[02:29:04.940 --> 02:29:07.260]   on Hacker News or something like that?
[02:29:07.260 --> 02:29:09.780]   Like what?
[02:29:09.780 --> 02:29:11.860]   - But you see, I don't think it's just,
[02:29:11.860 --> 02:29:13.300]   see, I don't think it's just that like,
[02:29:13.300 --> 02:29:14.860]   I could have gotten lucky at any point.
[02:29:14.860 --> 02:29:16.300]   I think that in a way--
[02:29:16.300 --> 02:29:17.900]   - You were ready at that moment.
[02:29:17.900 --> 02:29:18.720]   - Yeah, exactly.
[02:29:18.720 --> 02:29:20.060]   - To receive the information.
[02:29:20.060 --> 02:29:24.380]   But is there some magic to the day today
[02:29:24.380 --> 02:29:29.140]   of like eating breakfast and it's the mundane things?
[02:29:29.140 --> 02:29:30.340]   - Nah. - Nothing.
[02:29:30.340 --> 02:29:32.980]   - Nah, I drift through life.
[02:29:32.980 --> 02:29:34.260]   - Without structure.
[02:29:34.260 --> 02:29:36.260]   - I drift through life hoping and praying
[02:29:36.260 --> 02:29:38.620]   that I will get another day like those days.
[02:29:38.620 --> 02:29:40.380]   - And there's nothing in particular you do
[02:29:40.380 --> 02:29:44.840]   to be a receptacle for another, for day number four?
[02:29:44.840 --> 02:29:48.140]   - No, I didn't do anything to get the other ones.
[02:29:48.140 --> 02:29:51.180]   So I don't think I have to really do anything now.
[02:29:51.180 --> 02:29:53.500]   I took a month long trip to New York
[02:29:53.500 --> 02:29:56.340]   and the Ethereum thing was the highlight of it,
[02:29:56.340 --> 02:29:58.060]   but the rest of it was pretty terrible.
[02:29:58.060 --> 02:30:00.940]   I did a two week road trip and I got,
[02:30:00.940 --> 02:30:03.380]   I had to turn around, I had to turn around
[02:30:03.380 --> 02:30:06.700]   driving in Gunnison, Colorado.
[02:30:06.700 --> 02:30:10.700]   Passed through Gunnison and the snow starts coming down.
[02:30:10.700 --> 02:30:12.180]   There's a pass up there called Monarch Pass
[02:30:12.180 --> 02:30:13.060]   in order to get through to Denver,
[02:30:13.060 --> 02:30:14.780]   you gotta get over the Rockies.
[02:30:14.780 --> 02:30:16.840]   And I had to turn my car around.
[02:30:16.840 --> 02:30:20.300]   I couldn't, I watched a F-150 go off the road.
[02:30:20.300 --> 02:30:21.660]   I'm like, I gotta go back.
[02:30:21.660 --> 02:30:26.060]   And like that day was meaningful 'cause like it was real.
[02:30:26.060 --> 02:30:28.940]   Like I actually had to turn my car around.
[02:30:28.940 --> 02:30:31.280]   It's rare that anything even real happens in my life.
[02:30:31.280 --> 02:30:35.180]   Even as mundane as the fact that yeah, there was snow,
[02:30:35.180 --> 02:30:36.980]   I had to turn around, stay in Gunnison,
[02:30:36.980 --> 02:30:37.820]   leave the next day.
[02:30:37.820 --> 02:30:40.260]   - Something about that moment felt real.
[02:30:40.260 --> 02:30:43.180]   Okay, so actually it's interesting to break apart
[02:30:43.180 --> 02:30:45.380]   the three moments you mentioned, if it's okay.
[02:30:45.380 --> 02:30:48.540]   So I always have trouble pronouncing his name,
[02:30:48.540 --> 02:30:50.600]   but Alowser Yerkowski.
[02:30:50.600 --> 02:30:57.820]   So what, how did your worldview change
[02:30:57.820 --> 02:31:02.820]   in starting to consider the exponential growth of AI and AGI
[02:31:03.500 --> 02:31:06.460]   that he thinks about and the threats
[02:31:06.460 --> 02:31:09.380]   of artificial intelligence and all that kind of ideas?
[02:31:09.380 --> 02:31:12.500]   Can you, is it just like, can you maybe break apart
[02:31:12.500 --> 02:31:15.620]   like what exactly was so magical to you
[02:31:15.620 --> 02:31:17.420]   as a transformational experience?
[02:31:17.420 --> 02:31:20.380]   - Today everyone knows him for threats and AI safety.
[02:31:20.380 --> 02:31:22.300]   This was pre that stuff.
[02:31:22.300 --> 02:31:24.740]   There was, I don't think a mention of AI safety on the page.
[02:31:24.740 --> 02:31:26.020]   - Oh.
[02:31:26.020 --> 02:31:27.900]   - This is old Yerkowski stuff.
[02:31:27.900 --> 02:31:29.100]   He'd probably denounce it all now.
[02:31:29.100 --> 02:31:29.940]   He'd probably be like,
[02:31:29.940 --> 02:31:32.220]   that's exactly what I didn't want to happen.
[02:31:32.220 --> 02:31:35.380]   Sorry, man. (laughs)
[02:31:35.380 --> 02:31:37.780]   - Is there something specific you can take from his work
[02:31:37.780 --> 02:31:38.740]   that you can remember?
[02:31:38.740 --> 02:31:43.740]   - Yeah, it was this realization that computers double
[02:31:43.740 --> 02:31:47.740]   in power every 18 months and humans do not.
[02:31:47.740 --> 02:31:50.180]   And they haven't crossed yet,
[02:31:50.180 --> 02:31:52.740]   but if you have one thing that's doubling every 18 months
[02:31:52.740 --> 02:31:55.140]   and one thing that's staying like this,
[02:31:55.140 --> 02:31:58.020]   here's your log graph, here's your line,
[02:31:58.020 --> 02:31:59.920]   calculate that.
[02:32:01.980 --> 02:32:03.540]   - And that, did that open the door
[02:32:03.540 --> 02:32:05.140]   to the exponential thinking?
[02:32:05.140 --> 02:32:06.900]   Like thinking that like, you know what,
[02:32:06.900 --> 02:32:11.580]   with technology we can actually transform the world.
[02:32:11.580 --> 02:32:13.740]   - It opened the door to human obsolescence.
[02:32:13.740 --> 02:32:16.980]   It opened the door to realize that in my lifetime,
[02:32:16.980 --> 02:32:19.740]   humans are going to be replaced.
[02:32:19.740 --> 02:32:22.540]   - And then the matching idea to that
[02:32:22.540 --> 02:32:24.980]   of artificial intelligence with the Hutter Prize.
[02:32:24.980 --> 02:32:27.940]   You know, I'm torn.
[02:32:27.940 --> 02:32:30.220]   I go back and forth on what I think about it.
[02:32:30.220 --> 02:32:31.380]   - Yeah.
[02:32:31.380 --> 02:32:36.180]   - But the basic thesis is it's a nice compelling notion
[02:32:36.180 --> 02:32:38.340]   that we can reduce the task of creating
[02:32:38.340 --> 02:32:41.380]   an intelligent system, a generally intelligent system,
[02:32:41.380 --> 02:32:42.980]   into the task of compression.
[02:32:42.980 --> 02:32:46.300]   So you can think of all of intelligence in the universe,
[02:32:46.300 --> 02:32:48.480]   in fact, as a kind of compression.
[02:32:48.480 --> 02:32:52.420]   Do you find that, was that just at the time
[02:32:52.420 --> 02:32:53.900]   you found that as a compelling idea
[02:32:53.900 --> 02:32:56.980]   or do you still find that a compelling idea?
[02:32:56.980 --> 02:32:59.180]   - I still find that a compelling idea.
[02:32:59.180 --> 02:33:02.180]   I think that it's not that useful day to day,
[02:33:02.180 --> 02:33:06.260]   but actually one of maybe my quests before that
[02:33:06.260 --> 02:33:09.340]   was a search for the definition of the word intelligence.
[02:33:09.340 --> 02:33:10.860]   And I never had one.
[02:33:10.860 --> 02:33:14.700]   And I definitely have a definition of the word compression.
[02:33:14.700 --> 02:33:18.440]   It's a very simple, straightforward one.
[02:33:18.440 --> 02:33:19.740]   And you know what compression is.
[02:33:19.740 --> 02:33:21.100]   You know what lossless, is lossless compression,
[02:33:21.100 --> 02:33:22.940]   not lossy, lossless compression.
[02:33:22.940 --> 02:33:25.620]   And that that is equivalent to intelligence,
[02:33:25.620 --> 02:33:27.540]   which I believe, I'm not sure how useful
[02:33:27.540 --> 02:33:28.980]   that definition is day to day,
[02:33:28.980 --> 02:33:32.340]   but like I now have a framework to understand what it is.
[02:33:32.340 --> 02:33:36.300]   - And he just 10xed the prize for that competition
[02:33:36.300 --> 02:33:37.740]   like recently a few months ago.
[02:33:37.740 --> 02:33:39.580]   You ever thought of taking a crack at that?
[02:33:39.580 --> 02:33:41.140]   - Oh, I did.
[02:33:41.140 --> 02:33:41.980]   Oh, I did.
[02:33:41.980 --> 02:33:44.660]   I spent the next, after I found the prize,
[02:33:44.660 --> 02:33:47.820]   I spent the next six months of my life trying it.
[02:33:47.820 --> 02:33:50.180]   And well, that's when I started learning everything
[02:33:50.180 --> 02:33:53.380]   about AI and then I worked at Vicarious for a bit
[02:33:53.380 --> 02:33:55.340]   and then I read all the deep learning stuff
[02:33:55.340 --> 02:33:58.500]   and I'm like, okay, now I'm caught up to modern AI.
[02:33:58.500 --> 02:33:59.340]   - Wow.
[02:33:59.340 --> 02:34:01.420]   - And I had a really good framework to put it all in
[02:34:01.420 --> 02:34:02.780]   from the compression stuff.
[02:34:02.780 --> 02:34:06.820]   Right, like some of the first deep learning models
[02:34:06.820 --> 02:34:07.940]   I played with were,
[02:34:07.940 --> 02:34:12.060]   GTT, GPT basically, but before transformers,
[02:34:12.060 --> 02:34:17.060]   before it was still RNNs to do character prediction.
[02:34:17.060 --> 02:34:19.980]   - But by the way, on the compression side,
[02:34:19.980 --> 02:34:22.980]   I mean, especially neural networks,
[02:34:22.980 --> 02:34:25.540]   what do you make of the lossless requirement
[02:34:25.540 --> 02:34:26.620]   with the Hutter Prize?
[02:34:26.620 --> 02:34:31.540]   So, you know, human intelligence and neural networks
[02:34:31.540 --> 02:34:33.380]   can probably compress stuff pretty well,
[02:34:33.380 --> 02:34:35.300]   but it'll be lossy.
[02:34:35.300 --> 02:34:36.740]   It's imperfect.
[02:34:36.740 --> 02:34:37.940]   - You can turn a lossy compression
[02:34:37.940 --> 02:34:39.660]   into a lossless compressor pretty easily
[02:34:39.660 --> 02:34:41.140]   using an arithmetic encoder, right?
[02:34:41.140 --> 02:34:42.620]   You can take an arithmetic encoder
[02:34:42.620 --> 02:34:44.660]   and you can just encode the noise
[02:34:44.660 --> 02:34:46.260]   with maximum efficiency, right?
[02:34:46.260 --> 02:34:48.980]   So even if you can't predict exactly
[02:34:48.980 --> 02:34:50.540]   what the next character is,
[02:34:50.540 --> 02:34:52.580]   the better a probability distribution
[02:34:52.580 --> 02:34:54.300]   you can put over the next character,
[02:34:54.300 --> 02:34:56.180]   you can then use an arithmetic encoder
[02:34:56.180 --> 02:34:58.060]   to, right, you don't have to know
[02:34:58.060 --> 02:34:59.300]   whether it's an E or an I,
[02:34:59.300 --> 02:35:01.060]   you just have to put good probabilities on them
[02:35:01.060 --> 02:35:02.980]   and then, you know, code those.
[02:35:02.980 --> 02:35:06.460]   And if you have, it's a bits of entropy thing, right?
[02:35:06.460 --> 02:35:07.700]   - So let me, on that topic,
[02:35:07.700 --> 02:35:10.220]   it'd be interesting as a little side tour,
[02:35:10.220 --> 02:35:13.580]   what are your thoughts in this year about GPT-3
[02:35:13.580 --> 02:35:16.140]   and these language models and these transformers?
[02:35:16.140 --> 02:35:20.660]   Is there something interesting to you as an AI researcher,
[02:35:20.660 --> 02:35:22.540]   or is there something interesting to you
[02:35:22.540 --> 02:35:24.740]   as an autonomous vehicle developer?
[02:35:24.740 --> 02:35:27.500]   - Nah, I think it's overhyped.
[02:35:27.500 --> 02:35:29.060]   I mean, it's not, like, it's cool.
[02:35:29.060 --> 02:35:30.860]   It's cool for what it is, but no,
[02:35:30.860 --> 02:35:33.540]   we're not just gonna be able to scale up to GPT-12
[02:35:33.540 --> 02:35:35.380]   and get general purpose intelligence.
[02:35:35.380 --> 02:35:38.740]   Like, your loss function is literally just,
[02:35:38.740 --> 02:35:41.500]   you know, cross-entropy loss on the character, right?
[02:35:41.500 --> 02:35:44.900]   Like, that's not the loss function of general intelligence.
[02:35:44.900 --> 02:35:45.980]   - Is that obvious to you?
[02:35:45.980 --> 02:35:47.420]   - Yes.
[02:35:47.420 --> 02:35:50.060]   - Can you imagine that,
[02:35:50.060 --> 02:35:53.340]   like, to play devil's advocate on yourself,
[02:35:53.340 --> 02:35:55.900]   is it possible that you can,
[02:35:55.900 --> 02:35:58.620]   that GPT-12 will achieve general intelligence
[02:35:58.620 --> 02:36:02.020]   with something as dumb as this kind of loss function?
[02:36:02.020 --> 02:36:05.100]   - I guess it depends what you mean by general intelligence.
[02:36:05.100 --> 02:36:07.900]   So there's another problem with the GPTs,
[02:36:07.900 --> 02:36:11.100]   and that's that they don't have a,
[02:36:11.100 --> 02:36:13.060]   they don't have long-term memory.
[02:36:13.060 --> 02:36:13.900]   - Right.
[02:36:13.900 --> 02:36:18.660]   - Right, so, like, just GPT-12,
[02:36:18.660 --> 02:36:21.180]   a scaled-up version of GPT-2 or 3,
[02:36:22.460 --> 02:36:26.100]   I find it hard to believe.
[02:36:26.100 --> 02:36:27.500]   - Well, you can scale it in,
[02:36:27.500 --> 02:36:32.100]   so it's a hard-coded length,
[02:36:32.100 --> 02:36:34.300]   but you can make it wider and wider and wider.
[02:36:34.300 --> 02:36:35.140]   - Yeah.
[02:36:35.140 --> 02:36:40.860]   You're gonna get cool things from those systems,
[02:36:40.860 --> 02:36:44.860]   but I don't think you're ever gonna get something
[02:36:44.860 --> 02:36:47.620]   that can, like, you know, build me a rocket ship.
[02:36:47.620 --> 02:36:49.460]   - What about soft driving?
[02:36:49.460 --> 02:36:53.180]   So, you know, you can use transformer with video,
[02:36:53.180 --> 02:36:54.780]   for example.
[02:36:54.780 --> 02:36:57.260]   You think, is there something in there?
[02:36:57.260 --> 02:37:01.340]   - No, because, I mean, look, we use a GRU.
[02:37:01.340 --> 02:37:04.300]   We use a GRU, we could change that GRU out to a transformer.
[02:37:04.300 --> 02:37:09.540]   I think driving is much more Markovian than language.
[02:37:09.540 --> 02:37:11.500]   - So Markovian, you mean, like, the memory,
[02:37:11.500 --> 02:37:12.820]   which aspect of Markov chains?
[02:37:12.820 --> 02:37:16.380]   - Markovian, I mean that, like, most of the information
[02:37:16.380 --> 02:37:18.540]   in the state at T minus one is also in the,
[02:37:18.540 --> 02:37:20.180]   is in state T. - I see, yeah.
[02:37:20.180 --> 02:37:22.860]   - Right, and it kind of, like, drops off nicely like this,
[02:37:22.860 --> 02:37:23.940]   whereas sometimes with language,
[02:37:23.940 --> 02:37:25.940]   you have to refer back to the third paragraph
[02:37:25.940 --> 02:37:27.300]   on the second page.
[02:37:27.300 --> 02:37:28.140]   - I feel like--
[02:37:28.140 --> 02:37:30.100]   - There's not many, like, you can say, like,
[02:37:30.100 --> 02:37:32.460]   speed limit signs, but there's really not many things
[02:37:32.460 --> 02:37:33.900]   in autonomous driving that look like that.
[02:37:33.900 --> 02:37:37.180]   - But if you look at, to play devil's advocate,
[02:37:37.180 --> 02:37:39.780]   is the risk estimation thing that you've talked about
[02:37:39.780 --> 02:37:43.180]   is kind of interesting, is it feels like there might be
[02:37:43.180 --> 02:37:47.860]   some longer-term aggregation of context necessary
[02:37:47.860 --> 02:37:51.340]   to be able to figure out, like, the context?
[02:37:51.340 --> 02:37:55.860]   I'm not even sure I'm believing my own devil's advocate.
[02:37:55.860 --> 02:37:58.300]   - We have a nice, like, vision model,
[02:37:58.300 --> 02:38:00.900]   which outputs, like, a one or two four-dimensional
[02:38:00.900 --> 02:38:02.120]   perception space.
[02:38:02.120 --> 02:38:04.780]   Can I try transformers on it?
[02:38:04.780 --> 02:38:06.620]   Sure, I probably will.
[02:38:06.620 --> 02:38:08.260]   At some point, we'll try transformers,
[02:38:08.260 --> 02:38:09.700]   and then we'll just see, do they do better?
[02:38:09.700 --> 02:38:10.540]   Sure, I'm--
[02:38:10.540 --> 02:38:12.220]   - But it might not be a game changer, you're saying.
[02:38:12.220 --> 02:38:14.880]   - No, well, I'm not, like, might transformers work better
[02:38:14.880 --> 02:38:16.120]   than GRUs for autonomous driving?
[02:38:16.120 --> 02:38:17.580]   Sure, might we switch?
[02:38:17.580 --> 02:38:19.260]   Sure, is this some radical change?
[02:38:19.260 --> 02:38:21.740]   No, okay, we use a slightly different, you know,
[02:38:21.740 --> 02:38:23.860]   we switch from RNNs to GRUs, like, okay,
[02:38:23.860 --> 02:38:26.900]   maybe it's GRUs to transformers, but no, it's not, yeah.
[02:38:26.900 --> 02:38:30.540]   - Well, on the topic of general intelligence,
[02:38:30.540 --> 02:38:32.180]   I don't know how much I've talked to you about it.
[02:38:32.180 --> 02:38:37.180]   Like, what, do you think we'll actually build an AGI?
[02:38:37.180 --> 02:38:40.740]   Like, if you look at Ray Kurzweil with singularity,
[02:38:40.740 --> 02:38:43.400]   do you have, like, an intuition about,
[02:38:43.400 --> 02:38:45.420]   you're kind of saying driving is easy.
[02:38:45.420 --> 02:38:46.260]   - Yeah.
[02:38:46.260 --> 02:38:52.500]   - I tend to personally believe that solving driving
[02:38:52.500 --> 02:38:56.580]   will have really deep, important impacts
[02:38:56.580 --> 02:38:59.300]   on our ability to solve general intelligence.
[02:38:59.300 --> 02:39:03.400]   Like, I think driving doesn't require general intelligence,
[02:39:03.400 --> 02:39:05.240]   but I think they're going to be neighbors
[02:39:05.240 --> 02:39:08.380]   in a way that it's, like, deeply tied.
[02:39:08.380 --> 02:39:11.540]   'Cause it's so, like, driving is so deeply connected
[02:39:11.540 --> 02:39:15.020]   to the human experience that I think solving one
[02:39:15.020 --> 02:39:17.420]   will help solve the other.
[02:39:17.420 --> 02:39:20.980]   But, so I don't see driving as, like, easy
[02:39:20.980 --> 02:39:23.540]   and almost, like, separate than general intelligence.
[02:39:23.540 --> 02:39:26.700]   But, like, what's your vision of a future with a singularity?
[02:39:26.700 --> 02:39:28.180]   Do you see there'll be a single moment,
[02:39:28.180 --> 02:39:30.620]   like a singularity, where it'll be a phase shift?
[02:39:30.620 --> 02:39:32.380]   Are we in the singularity now?
[02:39:32.380 --> 02:39:33.860]   Like, what, do you have crazy ideas
[02:39:33.860 --> 02:39:35.780]   about the future in terms of AGI?
[02:39:35.780 --> 02:39:38.020]   - We're definitely in the singularity now.
[02:39:38.020 --> 02:39:38.860]   - We are?
[02:39:38.860 --> 02:39:40.180]   - Of course, of course.
[02:39:40.180 --> 02:39:41.500]   Look at the bandwidth between people.
[02:39:41.500 --> 02:39:43.940]   The bandwidth between people goes up, all right?
[02:39:44.820 --> 02:39:47.300]   The singularity is just, you know, when the bandwidth, but--
[02:39:47.300 --> 02:39:48.660]   - What do you mean by the bandwidth of people?
[02:39:48.660 --> 02:39:51.380]   - Communications, tools, the whole world is networked.
[02:39:51.380 --> 02:39:52.300]   The whole world is networked,
[02:39:52.300 --> 02:39:54.780]   and we raise the speed of that network, right?
[02:39:54.780 --> 02:39:57.420]   - Oh, so you think the communication of information
[02:39:57.420 --> 02:40:01.020]   in a distributed way is an empowering thing
[02:40:01.020 --> 02:40:02.340]   for collective intelligence?
[02:40:02.340 --> 02:40:03.700]   - Oh, I didn't say it's necessarily a good thing,
[02:40:03.700 --> 02:40:05.540]   but I think that's, like, when I think of the definition
[02:40:05.540 --> 02:40:08.220]   of the singularity, yeah, it seems kind of right.
[02:40:08.220 --> 02:40:12.100]   - I see, like, it's a change in the world
[02:40:12.100 --> 02:40:14.940]   beyond which, like, the world would be transformed
[02:40:14.940 --> 02:40:16.740]   in ways that we can't possibly imagine.
[02:40:16.740 --> 02:40:18.380]   - No, I mean, I think we're in the singularity now
[02:40:18.380 --> 02:40:19.700]   in the sense that there's, like, you know,
[02:40:19.700 --> 02:40:22.340]   one world and a monoculture, and it's also linked.
[02:40:22.340 --> 02:40:24.780]   - Yeah, I mean, I kind of share the intuition
[02:40:24.780 --> 02:40:27.700]   that the singularity will originate
[02:40:27.700 --> 02:40:31.220]   from the collective intelligence of us ants
[02:40:31.220 --> 02:40:35.340]   versus the, like, some single system AGI type thing.
[02:40:35.340 --> 02:40:37.020]   - Oh, I totally agree with that.
[02:40:37.020 --> 02:40:39.100]   Yeah, I don't really believe in, like,
[02:40:39.100 --> 02:40:41.200]   a hard takeoff AGI kind of thing.
[02:40:42.200 --> 02:40:47.200]   Yeah, I don't think, I don't even think AI
[02:40:47.200 --> 02:40:49.520]   is all that different in kind
[02:40:49.520 --> 02:40:52.080]   from what we've already been building.
[02:40:52.080 --> 02:40:55.440]   With respect to driving, I think driving is a subset
[02:40:55.440 --> 02:40:56.640]   of general intelligence, and I think
[02:40:56.640 --> 02:40:58.080]   it's a pretty complete subset.
[02:40:58.080 --> 02:41:00.320]   I think the tools we develop at Kama
[02:41:00.320 --> 02:41:02.320]   will also be extremely helpful
[02:41:02.320 --> 02:41:05.240]   to solving general intelligence, and that's, I think,
[02:41:05.240 --> 02:41:06.640]   the real reason why I'm doing it.
[02:41:06.640 --> 02:41:08.440]   I don't care about self-driving cars.
[02:41:08.440 --> 02:41:10.960]   It's a cool problem to beat people at.
[02:41:10.960 --> 02:41:14.560]   - But, I mean, yeah, you're kind of, you're of two minds.
[02:41:14.560 --> 02:41:16.400]   So one, you do have to have a mission,
[02:41:16.400 --> 02:41:19.080]   and you want to focus and make sure you get there.
[02:41:19.080 --> 02:41:22.320]   You can't forget that, but at the same time,
[02:41:22.320 --> 02:41:26.080]   there is a thread that's much bigger
[02:41:26.080 --> 02:41:28.520]   than connects the entirety of your effort,
[02:41:28.520 --> 02:41:31.220]   that's much bigger than just driving.
[02:41:31.220 --> 02:41:33.400]   - With AI and with general intelligence,
[02:41:33.400 --> 02:41:35.240]   it is so easy to delude yourself
[02:41:35.240 --> 02:41:37.360]   into thinking you've figured something out when you haven't.
[02:41:37.360 --> 02:41:39.840]   If we build a level five self-driving car,
[02:41:39.840 --> 02:41:43.360]   we have indisputably built something.
[02:41:43.360 --> 02:41:44.800]   Is it general intelligence?
[02:41:44.800 --> 02:41:45.920]   I'm not gonna debate that.
[02:41:45.920 --> 02:41:47.520]   I will say we've built something
[02:41:47.520 --> 02:41:49.720]   that provides huge financial value.
[02:41:49.720 --> 02:41:50.560]   - Yeah, beautifully put.
[02:41:50.560 --> 02:41:53.680]   That's the engineering credo, like just build the thing.
[02:41:53.680 --> 02:41:58.680]   It's like, that's why I'm with Elon on Go to Mars.
[02:41:58.680 --> 02:41:59.760]   - Yeah, it's a great one.
[02:41:59.760 --> 02:42:03.680]   - You can argue like, who the hell cares about going to Mars?
[02:42:03.680 --> 02:42:07.480]   But the reality is, set that as a mission, get it done,
[02:42:07.480 --> 02:42:09.520]   and then you're going to crack some problem
[02:42:09.520 --> 02:42:11.600]   that you've never even expected
[02:42:11.600 --> 02:42:13.920]   in the process of doing that, yeah.
[02:42:13.920 --> 02:42:16.200]   - Yeah, I mean, no, I think if I had a choice
[02:42:16.200 --> 02:42:18.520]   between humanity going to Mars and solving self-driving cars,
[02:42:18.520 --> 02:42:21.240]   I think going to Mars is better,
[02:42:21.240 --> 02:42:23.560]   but I don't know, I'm more suited for self-driving cars.
[02:42:23.560 --> 02:42:25.040]   I'm an information guy, I'm not a modernist,
[02:42:25.040 --> 02:42:26.560]   I'm a postmodernist.
[02:42:26.560 --> 02:42:27.840]   - Postmodernist.
[02:42:27.840 --> 02:42:29.600]   All right, beautifully put.
[02:42:29.600 --> 02:42:32.200]   Let me drag you back to programming for a sec.
[02:42:32.200 --> 02:42:35.120]   What three, maybe three to five programming languages
[02:42:35.120 --> 02:42:36.600]   should people learn, do you think?
[02:42:36.600 --> 02:42:38.200]   Like if you look at yourself,
[02:42:38.200 --> 02:42:41.800]   what did you get the most out of, from learning?
[02:42:41.800 --> 02:42:45.920]   - Well, so everybody should learn C and assembly.
[02:42:45.920 --> 02:42:47.120]   We'll start with those two.
[02:42:47.120 --> 02:42:48.080]   - Assembly?
[02:42:48.080 --> 02:42:49.920]   - Yeah, if you can't code in assembly,
[02:42:49.920 --> 02:42:51.680]   you don't know what the computer's doing.
[02:42:51.680 --> 02:42:53.280]   You don't understand, like,
[02:42:53.280 --> 02:42:54.800]   you don't have to be great in assembly,
[02:42:54.800 --> 02:42:56.600]   but you have to code in it.
[02:42:56.600 --> 02:42:58.920]   And then like, you have to appreciate assembly
[02:42:58.920 --> 02:43:02.000]   in order to appreciate all the great things C gets you.
[02:43:02.000 --> 02:43:03.360]   And then you have to code in C
[02:43:03.360 --> 02:43:06.320]   in order to appreciate all the great things Python gets you.
[02:43:06.320 --> 02:43:07.840]   So I'll just say assembly C in Python,
[02:43:07.840 --> 02:43:09.720]   and we'll start with those three.
[02:43:09.720 --> 02:43:11.760]   - The memory allocation of C,
[02:43:11.760 --> 02:43:16.320]   and the fact that, so assembly gives you a sense
[02:43:16.320 --> 02:43:18.600]   of just how many levels of abstraction
[02:43:18.600 --> 02:43:20.640]   you get to work on in modern day programming.
[02:43:20.640 --> 02:43:22.880]   - Yeah, yeah, yeah, graph coloring for assignment,
[02:43:22.880 --> 02:43:24.960]   register assignment in compilers.
[02:43:24.960 --> 02:43:26.600]   Like, you know, you gotta do, you know, the compiler,
[02:43:26.600 --> 02:43:28.320]   your computer only has a certain number of registers,
[02:43:28.320 --> 02:43:30.840]   yet you can have all the variables you want in a C function.
[02:43:30.840 --> 02:43:31.680]   You know, appreciate it.
[02:43:31.680 --> 02:43:34.480]   - So you get to start to build intuition about compilation,
[02:43:34.480 --> 02:43:36.000]   like what a compiler gets you.
[02:43:37.360 --> 02:43:38.480]   What else?
[02:43:38.480 --> 02:43:41.240]   - Well, then there's kind of a,
[02:43:41.240 --> 02:43:44.200]   so those are all very imperative programming languages.
[02:43:44.200 --> 02:43:47.680]   Then there's two other paradigms for programming
[02:43:47.680 --> 02:43:49.240]   that everybody should be familiar with.
[02:43:49.240 --> 02:43:51.240]   And one of them is functional.
[02:43:51.240 --> 02:43:54.120]   You should learn Haskell and take that all the way through,
[02:43:54.120 --> 02:43:57.280]   learn a language with dependent types like Coq,
[02:43:57.280 --> 02:43:58.960]   learn that whole space,
[02:43:58.960 --> 02:44:02.720]   like the very PL theory heavy languages.
[02:44:02.720 --> 02:44:04.840]   - And Haskell is your favorite functional?
[02:44:04.840 --> 02:44:06.520]   Is that the go-to, you'd say?
[02:44:06.520 --> 02:44:08.560]   - Yeah, I'm not a great Haskell programmer.
[02:44:08.560 --> 02:44:10.560]   I wrote a compiler in Haskell once.
[02:44:10.560 --> 02:44:11.440]   There's another paradigm,
[02:44:11.440 --> 02:44:12.560]   and actually there's one more paradigm
[02:44:12.560 --> 02:44:14.160]   that I'll even talk about after that
[02:44:14.160 --> 02:44:15.120]   that I never used to talk about
[02:44:15.120 --> 02:44:15.960]   when I would think about this,
[02:44:15.960 --> 02:44:18.440]   but the next paradigm is learn Verilog or VHDL.
[02:44:18.440 --> 02:44:22.280]   Understand this idea of all of the instructions
[02:44:22.280 --> 02:44:23.160]   execute at once.
[02:44:23.160 --> 02:44:28.680]   If I have a block in Verilog and I write stuff in it,
[02:44:28.680 --> 02:44:29.880]   it's not sequential.
[02:44:29.880 --> 02:44:31.280]   They all execute at once.
[02:44:31.280 --> 02:44:34.720]   And then like, think like that.
[02:44:34.720 --> 02:44:36.000]   That's how hardware works.
[02:44:36.600 --> 02:44:39.960]   So I guess assembly doesn't quite get you that.
[02:44:39.960 --> 02:44:42.200]   Assembly is more about compilation,
[02:44:42.200 --> 02:44:44.200]   and Verilog is more about the hardware,
[02:44:44.200 --> 02:44:48.520]   like giving a sense of what actually the hardware is doing.
[02:44:48.520 --> 02:44:50.440]   - Assembly C, Python are straight,
[02:44:50.440 --> 02:44:52.360]   like they sit right on top of each other.
[02:44:52.360 --> 02:44:55.600]   In fact, C is, well, C is kind of coded in C,
[02:44:55.600 --> 02:44:57.840]   but you could imagine the first C was coded in assembly,
[02:44:57.840 --> 02:44:59.960]   and Python is actually coded in C.
[02:44:59.960 --> 02:45:02.640]   So you can straight up go on that.
[02:45:02.640 --> 02:45:04.400]   - Got it.
[02:45:04.400 --> 02:45:06.960]   - And then Verilog gives you, that's brilliant.
[02:45:06.960 --> 02:45:07.800]   Okay.
[02:45:07.800 --> 02:45:09.840]   - And then I think there's another one now.
[02:45:09.840 --> 02:45:12.680]   Everyone should, Karpathy calls it programming 2.0,
[02:45:12.680 --> 02:45:16.520]   which is learn a, I'm not even kidding.
[02:45:16.520 --> 02:45:18.640]   Don't learn TensorFlow, learn PyTorch.
[02:45:18.640 --> 02:45:20.200]   - So machine learning.
[02:45:20.200 --> 02:45:21.560]   We've got to come up with a better term
[02:45:21.560 --> 02:45:26.120]   than programming 2.0, or, but yeah.
[02:45:26.120 --> 02:45:28.120]   - It's a programming language, learning.
[02:45:28.120 --> 02:45:32.360]   - I wonder if it can be formalized a little bit better.
[02:45:32.360 --> 02:45:34.880]   Which feels like we're in the early days
[02:45:34.880 --> 02:45:37.080]   of what that actually entails.
[02:45:37.080 --> 02:45:39.200]   - Data-driven programming?
[02:45:39.200 --> 02:45:41.560]   - Data-driven programming, yeah.
[02:45:41.560 --> 02:45:43.040]   But it's so fundamentally different
[02:45:43.040 --> 02:45:44.840]   as a paradigm than the others.
[02:45:44.840 --> 02:45:48.280]   Like it almost requires a different skill set.
[02:45:48.280 --> 02:45:52.000]   But you think it's still, yeah.
[02:45:52.000 --> 02:45:56.360]   And PyTorch versus TensorFlow, PyTorch wins?
[02:45:56.360 --> 02:45:57.440]   - It's the fourth paradigm.
[02:45:57.440 --> 02:45:59.360]   It's the fourth paradigm that I've kind of seen.
[02:45:59.360 --> 02:46:04.360]   There's like this imperative functional hardware.
[02:46:04.360 --> 02:46:06.320]   I don't know a better word for it.
[02:46:06.320 --> 02:46:08.160]   And then ML.
[02:46:08.160 --> 02:46:13.160]   - Do you have advice for people that wanna get into programming,
[02:46:13.160 --> 02:46:16.160]   wanna learn programming?
[02:46:16.160 --> 02:46:18.160]   You have a video,
[02:46:18.160 --> 02:46:22.840]   what is programming noob lessons, exclamation point.
[02:46:22.840 --> 02:46:24.600]   And I think the top comment is like,
[02:46:24.600 --> 02:46:26.480]   warning, this is not for noobs.
[02:46:27.840 --> 02:46:32.560]   Do you have a noob, like a TLDW for that video,
[02:46:32.560 --> 02:46:37.560]   but also a noob-friendly advice
[02:46:37.560 --> 02:46:39.520]   on how to get into programming?
[02:46:39.520 --> 02:46:41.400]   - You are never going to learn programming
[02:46:41.400 --> 02:46:44.640]   by watching a video called Learn Programming.
[02:46:44.640 --> 02:46:46.680]   The only way to learn programming, I think,
[02:46:46.680 --> 02:46:48.040]   and the only one is the only way,
[02:46:48.040 --> 02:46:50.160]   everyone I've ever met who can program well,
[02:46:50.160 --> 02:46:51.960]   learned it all in the same way.
[02:46:51.960 --> 02:46:53.800]   They had something they wanted to do,
[02:46:53.800 --> 02:46:56.560]   and then they tried to do it.
[02:46:56.560 --> 02:46:58.200]   And then they were like,
[02:46:58.200 --> 02:47:01.040]   oh, well, okay, this is kind of,
[02:47:01.040 --> 02:47:02.840]   it'd be nice if the computer could kind of do this thing.
[02:47:02.840 --> 02:47:04.400]   And then that's how you learn.
[02:47:04.400 --> 02:47:06.480]   You just keep pushing on a project.
[02:47:06.480 --> 02:47:10.840]   So the only advice I have for learning programming
[02:47:10.840 --> 02:47:12.080]   is go program.
[02:47:12.080 --> 02:47:14.600]   - Somebody wrote to me a question like,
[02:47:14.600 --> 02:47:15.840]   we don't really,
[02:47:15.840 --> 02:47:19.000]   they're looking to learn about recurring neural networks.
[02:47:19.000 --> 02:47:19.840]   And he's saying like,
[02:47:19.840 --> 02:47:22.520]   my company's thinking of using recurring neural networks
[02:47:22.520 --> 02:47:24.040]   for time series data,
[02:47:24.120 --> 02:47:27.360]   but we don't really have an idea of where to use it yet.
[02:47:27.360 --> 02:47:28.200]   We just want to,
[02:47:28.200 --> 02:47:30.360]   like, do you have any advice on how to learn about,
[02:47:30.360 --> 02:47:33.320]   these are these kind of general machine learning questions.
[02:47:33.320 --> 02:47:35.160]   And I think the answer is,
[02:47:35.160 --> 02:47:39.280]   like, actually have a problem that you're trying to solve.
[02:47:39.280 --> 02:47:41.160]   - I see that stuff.
[02:47:41.160 --> 02:47:43.400]   Oh my God, when people talk like that, they're like,
[02:47:43.400 --> 02:47:45.440]   I heard machine learning is important.
[02:47:45.440 --> 02:47:47.480]   Could you help us integrate machine learning
[02:47:47.480 --> 02:47:49.560]   with macaroni and cheese production?
[02:47:49.560 --> 02:47:53.960]   You just, I don't even, you can't help these people.
[02:47:53.960 --> 02:47:55.920]   Like who lets you run anything?
[02:47:55.920 --> 02:47:58.360]   Who lets that kind of person run anything?
[02:47:58.360 --> 02:48:00.840]   - I think we're all,
[02:48:00.840 --> 02:48:03.200]   we're all beginners at some point, so.
[02:48:03.200 --> 02:48:04.800]   - It's not like they're a beginner.
[02:48:04.800 --> 02:48:06.120]   It's like,
[02:48:06.120 --> 02:48:08.600]   my problem is not that they don't know about machine learning.
[02:48:08.600 --> 02:48:10.800]   My problem is that they think that machine learning
[02:48:10.800 --> 02:48:13.700]   has something to say about macaroni and cheese production.
[02:48:13.700 --> 02:48:17.000]   Or like, I heard about this new technology.
[02:48:17.000 --> 02:48:18.960]   How can I use it for why?
[02:48:18.960 --> 02:48:21.120]   Like, I don't know what it is,
[02:48:21.120 --> 02:48:23.480]   but how can I use it for why?
[02:48:23.480 --> 02:48:24.320]   - That's true.
[02:48:24.320 --> 02:48:26.080]   And you have to build up an intuition of how,
[02:48:26.080 --> 02:48:27.560]   'cause you might be able to figure out a way,
[02:48:27.560 --> 02:48:29.240]   but like the prerequisites,
[02:48:29.240 --> 02:48:32.200]   you should have a macaroni and cheese problem to solve first.
[02:48:32.200 --> 02:48:33.400]   - Exactly.
[02:48:33.400 --> 02:48:36.880]   - And then two, you should have more traditional,
[02:48:36.880 --> 02:48:39.280]   like the learning process should involve
[02:48:39.280 --> 02:48:41.880]   more traditionally applicable problems
[02:48:41.880 --> 02:48:44.480]   in the space of whatever that is, of machine learning,
[02:48:44.480 --> 02:48:47.080]   and then see if it can be applied to macaroni and cheese.
[02:48:47.080 --> 02:48:49.120]   - At least start with, tell me about a problem.
[02:48:49.120 --> 02:48:50.680]   Like if you have a problem, you're like,
[02:48:50.680 --> 02:48:51.920]   you know, some of my boxes
[02:48:51.920 --> 02:48:54.440]   aren't getting enough macaroni in them.
[02:48:54.440 --> 02:48:56.760]   Can we use machine learning to solve this problem?
[02:48:56.760 --> 02:48:58.360]   That's much, much better than,
[02:48:58.360 --> 02:49:01.520]   how do I apply machine learning to macaroni and cheese?
[02:49:01.520 --> 02:49:04.760]   - One big thing, maybe this is me
[02:49:04.760 --> 02:49:06.360]   talking to the audience a little bit,
[02:49:06.360 --> 02:49:09.840]   'cause I get these days so many messages,
[02:49:09.840 --> 02:49:13.840]   advice on how to like learn stuff, okay?
[02:49:13.840 --> 02:49:18.120]   My, this is not me being mean.
[02:49:18.120 --> 02:49:20.520]   I think this is quite profound actually,
[02:49:20.520 --> 02:49:22.760]   is you should Google it.
[02:49:22.760 --> 02:49:23.800]   - Oh yeah.
[02:49:23.800 --> 02:49:28.200]   - Like one of the like skills
[02:49:28.200 --> 02:49:31.160]   that you should really acquire as an engineer,
[02:49:31.160 --> 02:49:33.040]   as a researcher, as a thinker,
[02:49:33.040 --> 02:49:36.600]   like one, there's two complimentary skills.
[02:49:36.600 --> 02:49:39.080]   Like one is with a blank sheet of paper
[02:49:39.080 --> 02:49:41.600]   with no internet to think deeply.
[02:49:41.600 --> 02:49:44.720]   And then the other is to Google the crap
[02:49:44.720 --> 02:49:45.920]   out of the questions you have.
[02:49:45.920 --> 02:49:47.240]   Like that's actually a skill.
[02:49:47.240 --> 02:49:49.120]   I don't know if people often talk about,
[02:49:49.120 --> 02:49:51.960]   but like doing research, like pulling at the thread,
[02:49:51.960 --> 02:49:53.760]   like looking up different words,
[02:49:53.760 --> 02:49:57.960]   going into like GitHub repositories with two stars
[02:49:57.960 --> 02:49:59.600]   and like looking how they did stuff,
[02:49:59.600 --> 02:50:03.360]   like looking at the code or going on Twitter,
[02:50:03.360 --> 02:50:05.680]   seeing like there's little pockets of brilliant people
[02:50:05.680 --> 02:50:07.520]   that are like having discussions.
[02:50:07.520 --> 02:50:09.760]   Like if you're a neuroscientist,
[02:50:09.760 --> 02:50:11.520]   go into signal processing community.
[02:50:11.520 --> 02:50:12.720]   If you're an AI person,
[02:50:12.720 --> 02:50:15.760]   go into the psychology community,
[02:50:15.760 --> 02:50:18.720]   like switch communities, like keep searching,
[02:50:18.720 --> 02:50:19.880]   searching, searching,
[02:50:19.880 --> 02:50:23.760]   'cause it's so much better to invest
[02:50:23.760 --> 02:50:25.760]   in like finding somebody else
[02:50:25.760 --> 02:50:27.200]   who already solved your problem
[02:50:27.200 --> 02:50:30.800]   than it is to try to solve the problem.
[02:50:30.800 --> 02:50:34.280]   And 'cause they've often invested years of their life,
[02:50:34.280 --> 02:50:37.320]   like entire communities are probably already out there
[02:50:37.320 --> 02:50:39.120]   who have tried to solve your problem.
[02:50:39.120 --> 02:50:40.880]   - I think they're the same thing.
[02:50:40.880 --> 02:50:44.080]   I think you go try to solve the problem
[02:50:44.080 --> 02:50:46.080]   and then in trying to solve the problem,
[02:50:46.080 --> 02:50:47.640]   if you're good at solving problems,
[02:50:47.640 --> 02:50:49.720]   you'll stumble upon the person who solved it already.
[02:50:49.720 --> 02:50:52.160]   - Yeah, but the stumbling is really important.
[02:50:52.160 --> 02:50:54.080]   I think that's a skill that people should really put,
[02:50:54.080 --> 02:50:57.520]   especially in undergrad, like search.
[02:50:57.520 --> 02:50:58.520]   If you ask me a question,
[02:50:58.520 --> 02:51:02.000]   how should I get started in deep learning, like especially,
[02:51:02.000 --> 02:51:07.080]   like that is just so Googleable.
[02:51:07.080 --> 02:51:09.880]   Like the whole point is you Google that
[02:51:09.880 --> 02:51:13.800]   and you get a million pages and just start looking at them.
[02:51:13.800 --> 02:51:14.640]   - Yeah.
[02:51:14.640 --> 02:51:16.240]   - Start pulling at the thread, start exploring,
[02:51:16.240 --> 02:51:19.160]   start taking notes, start getting advice
[02:51:19.160 --> 02:51:22.720]   from a million people that already like spent their life
[02:51:22.720 --> 02:51:24.960]   answering that question actually.
[02:51:24.960 --> 02:51:26.240]   - Oh, well, yeah, I mean, that's definitely also,
[02:51:26.240 --> 02:51:27.760]   yeah, when people like ask me things like that,
[02:51:27.760 --> 02:51:29.360]   I'm like, trust me, the top answer on Google
[02:51:29.360 --> 02:51:31.920]   is much, much better than anything I'm going to tell you.
[02:51:31.920 --> 02:51:32.760]   Right?
[02:51:32.760 --> 02:51:34.360]   - Yeah.
[02:51:34.360 --> 02:51:38.000]   People ask, it's an interesting question.
[02:51:38.000 --> 02:51:39.840]   Let me know if you have any recommendations.
[02:51:39.840 --> 02:51:43.600]   What three books, technical or fiction or philosophical,
[02:51:43.600 --> 02:51:47.760]   had an impact on your life or you would recommend perhaps?
[02:51:47.760 --> 02:51:51.120]   - Maybe we'll start with the least controversial,
[02:51:51.120 --> 02:51:52.120]   Infinite Jest.
[02:51:52.120 --> 02:51:57.520]   Infinite Jest is a--
[02:51:57.520 --> 02:51:58.880]   - David Foster Wallace.
[02:51:58.880 --> 02:52:01.180]   - Yeah, it's a book about wireheading, really.
[02:52:01.180 --> 02:52:07.520]   Very enjoyable to read, very well-written.
[02:52:07.520 --> 02:52:11.160]   You will grow as a person reading this book.
[02:52:11.160 --> 02:52:14.720]   It's effort, and I'll set that up for the second book,
[02:52:14.720 --> 02:52:16.120]   which is pornography.
[02:52:16.120 --> 02:52:17.520]   That's called Atlas Shrugged.
[02:52:17.520 --> 02:52:21.160]   Which--
[02:52:21.160 --> 02:52:22.600]   - Atlas Shrugged is pornography?
[02:52:22.600 --> 02:52:23.760]   - Yeah, I mean, it is.
[02:52:23.760 --> 02:52:26.880]   I will not defend the, I will not say Atlas Shrugged
[02:52:26.880 --> 02:52:28.520]   is a well-written book.
[02:52:28.520 --> 02:52:30.360]   It is entertaining to read, certainly,
[02:52:30.360 --> 02:52:31.560]   just like pornography.
[02:52:31.560 --> 02:52:33.720]   The production value isn't great.
[02:52:33.720 --> 02:52:36.400]   You know, there's a 60-page monologue in there
[02:52:36.400 --> 02:52:38.880]   that Anne Rand's editor really wanted to take out,
[02:52:38.880 --> 02:52:42.720]   and she paid out of her pocket
[02:52:42.720 --> 02:52:45.180]   to keep that 60-page monologue in the book.
[02:52:45.180 --> 02:52:51.200]   But it is a great book for a kind of framework
[02:52:51.200 --> 02:52:55.960]   of human relations, and I know a lot of people are like,
[02:52:55.960 --> 02:52:58.080]   yeah, but it's a terrible framework.
[02:52:58.080 --> 02:53:00.500]   Yeah, but it's a framework.
[02:53:00.500 --> 02:53:02.360]   - Just for context, in a couple days,
[02:53:02.360 --> 02:53:06.240]   I'm speaking with, for probably four-plus hours,
[02:53:06.240 --> 02:53:10.200]   with Yaron Brook, who's the main living,
[02:53:10.200 --> 02:53:12.240]   remaining objectivist.
[02:53:12.240 --> 02:53:13.320]   Objectivist.
[02:53:13.320 --> 02:53:14.560]   - Interesting.
[02:53:14.560 --> 02:53:19.360]   - So I've always found this philosophy quite interesting
[02:53:19.360 --> 02:53:20.240]   on many levels.
[02:53:20.240 --> 02:53:23.920]   One of how repulsive some percent,
[02:53:23.920 --> 02:53:26.240]   large percent of the population find it,
[02:53:26.240 --> 02:53:30.720]   which is always funny to me when people are unable
[02:53:30.720 --> 02:53:35.320]   to even read a philosophy because of some,
[02:53:35.320 --> 02:53:37.560]   I think that says more about their
[02:53:37.560 --> 02:53:40.520]   psychological perspective on it.
[02:53:40.520 --> 02:53:41.360]   - Yeah.
[02:53:41.360 --> 02:53:45.280]   - But there is something about objectivism
[02:53:45.280 --> 02:53:48.760]   and Anne Rand's philosophy that's deeply connected
[02:53:48.760 --> 02:53:50.720]   to this idea of capitalism,
[02:53:50.720 --> 02:53:54.660]   of the ethical life is the productive life,
[02:53:54.660 --> 02:54:00.700]   that was always compelling to me.
[02:54:00.700 --> 02:54:03.120]   It didn't seem as, like I didn't seem to interpret it
[02:54:03.120 --> 02:54:05.600]   in the negative sense that some people do.
[02:54:05.600 --> 02:54:07.960]   - To be fair, I read that book when I was 19.
[02:54:07.960 --> 02:54:09.560]   - So you had an impact at that point, yeah.
[02:54:09.560 --> 02:54:13.600]   - Yeah, and the bad guys in the book have this slogan,
[02:54:13.600 --> 02:54:15.240]   "From each according to their ability,
[02:54:15.240 --> 02:54:17.240]   "to each according to their need."
[02:54:17.240 --> 02:54:19.720]   And I'm looking at this and I'm like,
[02:54:19.720 --> 02:54:21.560]   these are the most, this is Team Rocket level
[02:54:21.560 --> 02:54:22.900]   cartoonishness, right?
[02:54:22.900 --> 02:54:23.800]   No bad guy.
[02:54:23.800 --> 02:54:25.760]   And then when I realized that was actually the slogan
[02:54:25.760 --> 02:54:29.840]   of the Communist Party, I'm like, wait a second.
[02:54:29.840 --> 02:54:31.600]   Wait, no, no, no, no, no.
[02:54:31.600 --> 02:54:34.080]   You're telling me this really happened?
[02:54:34.080 --> 02:54:34.920]   - Yeah, it's interesting.
[02:54:34.920 --> 02:54:36.680]   I mean, one of the criticisms of her work
[02:54:36.680 --> 02:54:39.320]   is she has a cartoonish view of good and evil.
[02:54:39.320 --> 02:54:44.360]   The reality, as Jordan Peterson says,
[02:54:44.360 --> 02:54:48.000]   is that each of us have the capacity for good and evil in us
[02:54:48.000 --> 02:54:49.960]   as opposed to like, there's some characters
[02:54:49.960 --> 02:54:52.240]   who are purely evil and some characters that are purely good.
[02:54:52.240 --> 02:54:55.280]   - And that's in a way why it's pornographic.
[02:54:55.280 --> 02:54:57.040]   - The production value, I love it.
[02:54:57.040 --> 02:54:59.560]   - Well, evil is punished and that's very clearly,
[02:55:01.080 --> 02:55:06.080]   there's no, just like porn doesn't have character growth.
[02:55:06.080 --> 02:55:09.600]   Well, neither does "Alice in Wonderland."
[02:55:09.600 --> 02:55:10.920]   - Brilliant, well put.
[02:55:10.920 --> 02:55:14.280]   But as a 19-year-old George Hotz, it was good enough.
[02:55:14.280 --> 02:55:15.960]   - Yeah, yeah, yeah, yeah.
[02:55:15.960 --> 02:55:17.000]   - What's the third?
[02:55:17.000 --> 02:55:17.960]   You have something?
[02:55:17.960 --> 02:55:21.560]   - I could give, these two I'll just throw out.
[02:55:21.560 --> 02:55:22.400]   They're sci-fi.
[02:55:22.400 --> 02:55:25.120]   "Permutation City," great thing to start thinking
[02:55:25.120 --> 02:55:26.680]   about copies of yourself.
[02:55:26.680 --> 02:55:27.520]   And then--
[02:55:27.520 --> 02:55:28.360]   - Who's that by?
[02:55:28.360 --> 02:55:29.180]   Sorry, I didn't talk.
[02:55:29.180 --> 02:55:30.900]   - He's Greg Egan.
[02:55:30.900 --> 02:55:34.460]   That might not be his real name, some Australian guy.
[02:55:34.460 --> 02:55:36.740]   Might not be Australian, I don't know.
[02:55:36.740 --> 02:55:38.780]   And then this one's online.
[02:55:38.780 --> 02:55:41.380]   It's called "The Metamorphosis of Prime Intellect."
[02:55:41.380 --> 02:55:45.460]   It's a story set in a post-singularity world.
[02:55:45.460 --> 02:55:46.740]   It's interesting.
[02:55:46.740 --> 02:55:49.180]   - Is there, can you, in either of the worlds,
[02:55:49.180 --> 02:55:51.580]   do you find something philosophically interesting in them
[02:55:51.580 --> 02:55:52.780]   that you can comment on?
[02:55:52.780 --> 02:55:55.780]   - I mean, it is clear to me that
[02:55:57.820 --> 02:56:00.620]   "Metamorphosis of Prime Intellect" is written by
[02:56:00.620 --> 02:56:03.780]   an engineer, which is,
[02:56:03.780 --> 02:56:11.220]   it's very almost a pragmatic take on a utopia, in a way.
[02:56:11.220 --> 02:56:13.820]   - Positive or negative?
[02:56:13.820 --> 02:56:17.900]   - That's up to you to decide reading the book.
[02:56:17.900 --> 02:56:21.580]   And the ending of it is very interesting as well,
[02:56:21.580 --> 02:56:23.660]   and I didn't realize what it was.
[02:56:23.660 --> 02:56:25.260]   I first read that when I was 15.
[02:56:25.260 --> 02:56:27.540]   I've reread that book several times in my life.
[02:56:27.540 --> 02:56:29.220]   And it's short, it's 50 pages.
[02:56:29.220 --> 02:56:30.740]   Everyone should go read it.
[02:56:30.740 --> 02:56:33.100]   - What's, sorry, this is a little tangent.
[02:56:33.100 --> 02:56:34.660]   I've been working through the foundation.
[02:56:34.660 --> 02:56:37.060]   I've been, I haven't read much sci-fi my whole life,
[02:56:37.060 --> 02:56:40.180]   and I'm trying to fix that in the last few months.
[02:56:40.180 --> 02:56:42.140]   That's been a little side project.
[02:56:42.140 --> 02:56:46.180]   What's, to you, is the greatest sci-fi novel
[02:56:46.180 --> 02:56:47.700]   that people should read?
[02:56:47.700 --> 02:56:49.180]   Or is there, or--
[02:56:49.180 --> 02:56:51.100]   - I mean, I would, yeah, I would say like, yeah,
[02:56:51.100 --> 02:56:53.820]   "Permutation City," "Metamorphosis of Prime Intellect."
[02:56:53.820 --> 02:56:56.220]   I don't know, I didn't like "Foundation."
[02:56:56.220 --> 02:56:58.780]   I thought it was way too modernist.
[02:56:58.780 --> 02:57:00.780]   - Do you like "Dune" and like all of those?
[02:57:00.780 --> 02:57:01.620]   - I've never read "Dune."
[02:57:01.620 --> 02:57:02.820]   I've never read "Dune."
[02:57:02.820 --> 02:57:04.460]   I have to read it.
[02:57:04.460 --> 02:57:07.300]   "Fire Upon the Deep" is interesting.
[02:57:07.300 --> 02:57:10.500]   Okay, I mean, look, everyone should read,
[02:57:10.500 --> 02:57:11.340]   everyone should read "Neuromancer."
[02:57:11.340 --> 02:57:12.820]   Everyone should read "Snow Crash."
[02:57:12.820 --> 02:57:15.460]   If you haven't read those, like start there.
[02:57:15.460 --> 02:57:16.500]   - Yeah, I haven't read "Snow Crash."
[02:57:16.500 --> 02:57:17.340]   - You haven't read "Snow Crash?"
[02:57:17.340 --> 02:57:19.980]   - No. - Oh, it's very entertaining.
[02:57:19.980 --> 02:57:22.140]   Go to Lesher Bach, and if you want the controversial one,
[02:57:22.140 --> 02:57:23.260]   "Bronze Age Mindset."
[02:57:25.300 --> 02:57:27.700]   - All right, I'll look into that one.
[02:57:27.700 --> 02:57:30.340]   - Those aren't sci-fi, but just to round out books.
[02:57:30.340 --> 02:57:34.740]   - So a bunch of people asked me on Twitter and Reddit
[02:57:34.740 --> 02:57:36.900]   and so on for advice.
[02:57:36.900 --> 02:57:39.460]   So what advice would you give a young person today
[02:57:39.460 --> 02:57:40.580]   about life?
[02:57:40.580 --> 02:57:42.380]   Another one.
[02:57:42.380 --> 02:57:47.460]   What, yeah, I mean, looking back,
[02:57:47.460 --> 02:57:49.420]   especially when you're younger,
[02:57:49.420 --> 02:57:51.580]   you did, and you continued it,
[02:57:51.580 --> 02:57:54.860]   you've accomplished a lot of interesting things.
[02:57:54.860 --> 02:57:57.860]   Is there some advice from those,
[02:57:57.860 --> 02:58:01.860]   from that life of yours that you can pass on?
[02:58:01.860 --> 02:58:03.700]   - If college ever opens again,
[02:58:03.700 --> 02:58:07.620]   I would love to give a graduation speech.
[02:58:07.620 --> 02:58:11.300]   At that point, I will put a lot of somewhat satirical effort
[02:58:11.300 --> 02:58:12.300]   into this question.
[02:58:12.300 --> 02:58:15.820]   - Yeah, at this, you haven't written anything at this point.
[02:58:15.820 --> 02:58:16.700]   - Oh, you know what?
[02:58:16.700 --> 02:58:19.900]   Always wear sunscreen, this is water, like.
[02:58:19.900 --> 02:58:21.100]   - I think you're plagiarizing.
[02:58:21.100 --> 02:58:23.900]   - I mean, you know, but that's the,
[02:58:23.900 --> 02:58:26.220]   that's the like, well, clean your room,
[02:58:26.220 --> 02:58:28.580]   you know, yeah, you can plagiarize from all of this stuff.
[02:58:28.580 --> 02:58:29.420]   And it's,
[02:58:29.420 --> 02:58:34.580]   there is no,
[02:58:34.580 --> 02:58:37.660]   self-help books aren't designed to help you,
[02:58:37.660 --> 02:58:40.060]   they're designed to make you feel good.
[02:58:40.060 --> 02:58:42.740]   Like whatever advice I could give,
[02:58:42.740 --> 02:58:45.820]   you already know, everyone already knows.
[02:58:45.820 --> 02:58:47.500]   Sorry, it doesn't feel good.
[02:58:47.500 --> 02:58:53.060]   Right, like, you know, you know,
[02:58:53.060 --> 02:58:56.900]   if I tell you that you should, you know,
[02:58:56.900 --> 02:58:59.660]   eat well and read more,
[02:58:59.660 --> 02:59:01.980]   and it's not gonna do anything.
[02:59:01.980 --> 02:59:05.100]   I think the whole like genre of those kind of questions
[02:59:05.100 --> 02:59:07.500]   is meaningless.
[02:59:07.500 --> 02:59:08.340]   - I don't know.
[02:59:08.340 --> 02:59:10.580]   - If anything, it's don't worry so much about that stuff.
[02:59:10.580 --> 02:59:12.460]   Don't be so caught up in your head.
[02:59:12.460 --> 02:59:14.300]   - Right, I mean, you're, yeah,
[02:59:14.300 --> 02:59:16.420]   in the sense that your whole life,
[02:59:16.420 --> 02:59:19.060]   if your whole existence is like moving version
[02:59:19.060 --> 02:59:20.700]   of that advice.
[02:59:20.700 --> 02:59:21.540]   I don't know.
[02:59:21.820 --> 02:59:22.660]   - Yeah.
[02:59:22.660 --> 02:59:25.540]   There's something, I mean,
[02:59:25.540 --> 02:59:27.940]   there's something in you that resists that kind of thinking
[02:59:27.940 --> 02:59:29.860]   and that in itself is,
[02:59:29.860 --> 02:59:34.540]   it's just illustrative of who you are.
[02:59:34.540 --> 02:59:36.820]   And there's something to learn from that.
[02:59:36.820 --> 02:59:39.820]   I think you're clearly not overthinking stuff.
[02:59:39.820 --> 02:59:42.180]   - Yeah, and you know what?
[02:59:42.180 --> 02:59:43.380]   - There's a gut thing.
[02:59:43.380 --> 02:59:45.060]   - Even when I talk about my advice,
[02:59:45.060 --> 02:59:47.460]   I'm like, my advice is only relevant to me.
[02:59:47.460 --> 02:59:48.780]   It's not relevant to anybody else.
[02:59:48.780 --> 02:59:50.020]   I'm not saying you should go out
[02:59:50.020 --> 02:59:51.660]   if you're the kind of person who overthinks things
[02:59:51.660 --> 02:59:52.740]   to stop overthinking things.
[02:59:52.740 --> 02:59:54.100]   It's not bad.
[02:59:54.100 --> 02:59:54.980]   It doesn't work for me.
[02:59:54.980 --> 02:59:55.820]   Maybe it works for you.
[02:59:55.820 --> 02:59:56.660]   I don't know.
[02:59:56.660 --> 02:59:59.420]   - Let me ask you about love.
[02:59:59.420 --> 03:00:00.260]   - Yeah.
[03:00:00.260 --> 03:00:05.140]   - I think last time we talked about the meaning of life
[03:00:05.140 --> 03:00:08.620]   and it was kind of about winning.
[03:00:08.620 --> 03:00:09.460]   - Of course.
[03:00:09.460 --> 03:00:13.140]   - I don't think I've talked to you about love much,
[03:00:13.140 --> 03:00:15.020]   whether romantic or just love
[03:00:15.020 --> 03:00:18.100]   for the common humanity amongst us all.
[03:00:18.100 --> 03:00:21.420]   What role has love played in your life?
[03:00:21.420 --> 03:00:26.380]   In this quest for winning, where does love fit in?
[03:00:26.380 --> 03:00:29.900]   - Well, the word love I think means several different things.
[03:00:29.900 --> 03:00:32.100]   There's love in the sense of,
[03:00:32.100 --> 03:00:33.580]   maybe I could just say there's like love
[03:00:33.580 --> 03:00:37.900]   in the sense of opiates and love in the sense of oxytocin
[03:00:37.900 --> 03:00:42.900]   and then love in the sense of maybe like a love for math.
[03:00:42.900 --> 03:00:47.540]   I don't think fits into either of those first two paradigms.
[03:00:48.540 --> 03:00:53.540]   - So each of those, have they given something to you
[03:00:53.540 --> 03:00:56.940]   in your life?
[03:00:56.940 --> 03:00:59.180]   - I'm not that big of a fan of the first two.
[03:00:59.180 --> 03:01:01.660]   - Why?
[03:01:01.660 --> 03:01:06.380]   - The same reason I'm not a fan of,
[03:01:06.380 --> 03:01:09.900]   the same reason I don't do opiates and don't take ecstasy.
[03:01:09.900 --> 03:01:13.700]   And there were times, look, I've tried both.
[03:01:15.660 --> 03:01:18.420]   I like opiates way more than I like ecstasy.
[03:01:18.420 --> 03:01:23.420]   But they're not, the ethical life is the productive life.
[03:01:23.420 --> 03:01:27.100]   So maybe that's my problem with those.
[03:01:27.100 --> 03:01:29.460]   And then like, yeah, a sense of, I don't know,
[03:01:29.460 --> 03:01:32.220]   like abstract love for humanity.
[03:01:32.220 --> 03:01:34.540]   I mean, the abstract love for humanity,
[03:01:34.540 --> 03:01:36.220]   I'm like, yeah, I've always felt that.
[03:01:36.220 --> 03:01:40.500]   And I guess it's hard for me to imagine not feeling it.
[03:01:40.500 --> 03:01:43.580]   And maybe there's people who don't and I don't know.
[03:01:43.580 --> 03:01:46.540]   - Yeah, there's just like a background thing that's there.
[03:01:46.540 --> 03:01:49.620]   I mean, since we brought up drugs, let me ask you.
[03:01:49.620 --> 03:01:54.020]   This is becoming more and more a part of my life
[03:01:54.020 --> 03:01:55.580]   'cause I'm talking to a few researchers
[03:01:55.580 --> 03:01:57.740]   that are working on psychedelics.
[03:01:57.740 --> 03:02:00.500]   I've eaten shrooms a couple of times
[03:02:00.500 --> 03:02:04.020]   and it was fascinating to me that like the mind can go,
[03:02:04.020 --> 03:02:08.260]   just fascinating the mind can go to places
[03:02:08.260 --> 03:02:09.540]   I didn't imagine it could go.
[03:02:09.540 --> 03:02:12.820]   And it was very friendly and positive and exciting
[03:02:12.820 --> 03:02:16.180]   and everything was kind of hilarious in the place.
[03:02:16.180 --> 03:02:18.260]   Wherever my mind went, that's where I went.
[03:02:18.260 --> 03:02:20.980]   Is, what do you think about psychedelics?
[03:02:20.980 --> 03:02:22.940]   Do you think they have,
[03:02:22.940 --> 03:02:24.740]   where do you think the mind goes?
[03:02:24.740 --> 03:02:25.940]   Have you done psychedelics?
[03:02:25.940 --> 03:02:28.700]   Where do you think the mind goes?
[03:02:28.700 --> 03:02:32.300]   Is there something useful to learn about the places it goes?
[03:02:32.300 --> 03:02:33.380]   Once you come back?
[03:02:33.380 --> 03:02:38.100]   - You know, I find it interesting that this idea
[03:02:38.100 --> 03:02:40.460]   that psychedelics have something to teach
[03:02:40.460 --> 03:02:43.860]   is almost unique to psychedelics, right?
[03:02:43.860 --> 03:02:46.620]   People don't argue this about amphetamines.
[03:02:46.620 --> 03:02:50.300]   And I'm not really sure why.
[03:02:50.300 --> 03:02:53.860]   I think all of the drugs have lessons to teach.
[03:02:53.860 --> 03:02:55.220]   I think there's things to learn from opiates.
[03:02:55.220 --> 03:02:56.660]   I think there's things to learn from amphetamines.
[03:02:56.660 --> 03:02:58.220]   I think there's things to learn from psychedelics,
[03:02:58.220 --> 03:02:59.780]   things to learn from marijuana.
[03:02:59.780 --> 03:03:04.300]   But also at the same time,
[03:03:04.300 --> 03:03:06.820]   recognize that I don't think you're learning things
[03:03:06.820 --> 03:03:07.660]   about the world.
[03:03:07.660 --> 03:03:09.220]   I think you're learning things about yourself.
[03:03:09.220 --> 03:03:10.060]   - Yes.
[03:03:10.700 --> 03:03:12.660]   - And you know, what's the,
[03:03:12.660 --> 03:03:14.500]   even, it might've even been,
[03:03:14.500 --> 03:03:17.260]   might've even been a Timothy Leary quote.
[03:03:17.260 --> 03:03:18.180]   I don't wanna misquote him,
[03:03:18.180 --> 03:03:20.260]   but the idea is basically like, you know,
[03:03:20.260 --> 03:03:21.620]   everybody should look behind the door,
[03:03:21.620 --> 03:03:22.780]   but then once you've seen behind the door,
[03:03:22.780 --> 03:03:24.460]   you don't need to keep going back.
[03:03:24.460 --> 03:03:29.940]   So, I mean, and that's my thoughts on all real drug use too.
[03:03:29.940 --> 03:03:31.100]   So maybe for caffeine.
[03:03:31.100 --> 03:03:37.180]   - It's a little experience that it's good to have, but.
[03:03:37.180 --> 03:03:39.500]   - Oh yeah, no, I mean, yeah, I guess, yes.
[03:03:39.500 --> 03:03:40.940]   Psychedelics are definitely.
[03:03:40.940 --> 03:03:43.980]   - So you're a fan of new experiences, I suppose.
[03:03:43.980 --> 03:03:44.820]   - Yes.
[03:03:44.820 --> 03:03:45.900]   - 'Cause they all contain a little,
[03:03:45.900 --> 03:03:47.060]   especially the first few times,
[03:03:47.060 --> 03:03:49.780]   it contains some lessons that can be picked up.
[03:03:49.780 --> 03:03:54.000]   - Yeah, and I'll revisit psychedelics maybe once a year.
[03:03:54.000 --> 03:03:57.740]   Usually smaller doses.
[03:03:57.740 --> 03:04:01.500]   Maybe they turn up the learning rate of your brain.
[03:04:01.500 --> 03:04:03.220]   I've heard that, I like that.
[03:04:03.220 --> 03:04:04.300]   - Yeah, that's cool.
[03:04:04.300 --> 03:04:06.300]   - Big learning rates have pros and cons.
[03:04:06.300 --> 03:04:09.460]   - Last question, and this is a little weird one,
[03:04:09.460 --> 03:04:11.820]   but you've called yourself crazy in the past.
[03:04:11.820 --> 03:04:16.180]   First of all, on a scale of one to 10,
[03:04:16.180 --> 03:04:18.060]   how crazy would you say are you?
[03:04:18.060 --> 03:04:19.260]   - Oh, I mean, it depends how you,
[03:04:19.260 --> 03:04:20.860]   you know, when you compare me to Elon Musk
[03:04:20.860 --> 03:04:23.580]   and Anthony Lewandowski, not so crazy.
[03:04:23.580 --> 03:04:24.940]   - So like a seven?
[03:04:24.940 --> 03:04:27.060]   - Let's go with six.
[03:04:27.060 --> 03:04:29.380]   - Six, six, six.
[03:04:29.380 --> 03:04:30.220]   What?
[03:04:30.220 --> 03:04:33.020]   - I like seven, seven's a good number.
[03:04:33.020 --> 03:04:34.700]   - Seven, all right, well,
[03:04:34.700 --> 03:04:37.260]   I'm sure day by day it changes, right?
[03:04:37.260 --> 03:04:39.740]   But you're in that area.
[03:04:39.740 --> 03:04:43.540]   In thinking about that,
[03:04:43.540 --> 03:04:45.860]   what do you think is the role of madness?
[03:04:45.860 --> 03:04:50.720]   Is that a feature or a bug if you were to dissect your brain?
[03:04:50.720 --> 03:04:56.780]   - So, okay, from like a mental health lens on crazy,
[03:04:56.780 --> 03:04:59.140]   I'm not sure I really believe in that.
[03:04:59.140 --> 03:05:02.940]   I'm not sure I really believe in a lot of that stuff,
[03:05:02.940 --> 03:05:05.220]   right, this concept of, okay, you know,
[03:05:05.220 --> 03:05:09.620]   you get over to like hardcore bipolar and schizophrenia,
[03:05:09.620 --> 03:05:13.220]   these things are clearly real, somewhat biological.
[03:05:13.220 --> 03:05:14.580]   And then over here on the spectrum,
[03:05:14.580 --> 03:05:18.420]   you have like ADD and oppositional defiance disorder
[03:05:18.420 --> 03:05:20.820]   and these things that are like,
[03:05:20.820 --> 03:05:22.880]   wait, this is normal spectrum human behavior.
[03:05:22.880 --> 03:05:27.880]   Like this isn't, you know, where's the line here
[03:05:27.880 --> 03:05:31.380]   and why is this like a problem?
[03:05:31.380 --> 03:05:33.340]   So there's this whole, you know,
[03:05:33.340 --> 03:05:35.900]   the neurodiversity of humanity is huge.
[03:05:35.900 --> 03:05:37.700]   Like people think I'm always on drugs.
[03:05:37.700 --> 03:05:39.020]   People are always saying this to me on my streams.
[03:05:39.020 --> 03:05:41.420]   I'm like, guys, you know, like I'm real open with my drug use.
[03:05:41.420 --> 03:05:43.060]   I'd tell you if I was on drugs.
[03:05:43.060 --> 03:05:45.700]   And I mean, I had like a cup of coffee this morning,
[03:05:45.700 --> 03:05:47.340]   but other than that, this is just me.
[03:05:47.340 --> 03:05:49.920]   - You're witnessing my brain in action.
[03:05:49.920 --> 03:05:55.660]   So the word madness doesn't even make sense
[03:05:55.660 --> 03:05:59.760]   in the rich neurodiversity of humans.
[03:06:01.040 --> 03:06:02.460]   - I think it makes sense,
[03:06:02.460 --> 03:06:07.060]   but only for like some insane extremes.
[03:06:07.060 --> 03:06:11.780]   Like if you are actually like visibly hallucinating,
[03:06:11.780 --> 03:06:15.020]   you know, that's okay.
[03:06:15.020 --> 03:06:17.480]   - But there is the kind of spectrum on which you stand out.
[03:06:17.480 --> 03:06:22.060]   Like that's like if I were to look, you know,
[03:06:22.060 --> 03:06:25.100]   at decorations on a Christmas tree or something like that,
[03:06:25.100 --> 03:06:28.820]   like if you were a decoration, that would catch my eye.
[03:06:28.820 --> 03:06:30.880]   Like that thing is sparkly.
[03:06:30.880 --> 03:06:31.720]   (laughing)
[03:06:31.720 --> 03:06:34.540]   Whatever the hell that thing is.
[03:06:34.540 --> 03:06:37.400]   There's something to that.
[03:06:37.400 --> 03:06:42.140]   Just like refusing to be boring,
[03:06:42.140 --> 03:06:44.620]   or maybe boring is the wrong word, but to,
[03:06:44.620 --> 03:06:51.840]   yeah, I mean, be willing to sparkle, you know?
[03:06:51.840 --> 03:06:54.240]   - It's like somewhat constructed.
[03:06:54.240 --> 03:06:57.060]   I mean, I am who I choose to be.
[03:06:58.680 --> 03:07:01.040]   I want to say things as true as I can see them.
[03:07:01.040 --> 03:07:04.520]   I'm not gonna lie.
[03:07:04.520 --> 03:07:06.600]   - But that's a really important feature in itself.
[03:07:06.600 --> 03:07:09.080]   So like whatever the neurodiversity of your,
[03:07:09.080 --> 03:07:13.800]   whatever your brain is, not putting constraints on it
[03:07:13.800 --> 03:07:17.720]   that force it to fit into the mold of what society
[03:07:17.720 --> 03:07:20.660]   is like defines what you're supposed to be.
[03:07:20.660 --> 03:07:22.360]   So you're one of the specimens
[03:07:22.360 --> 03:07:26.500]   that doesn't mind being yourself.
[03:07:27.820 --> 03:07:31.720]   Being right is super important,
[03:07:31.720 --> 03:07:33.780]   except at the expense of being wrong.
[03:07:33.780 --> 03:07:38.480]   - Without breaking that apart,
[03:07:38.480 --> 03:07:40.720]   I think it's a beautiful way to end it, George.
[03:07:40.720 --> 03:07:43.080]   You're one of the most special humans I know.
[03:07:43.080 --> 03:07:44.600]   It's truly an honor to talk to you.
[03:07:44.600 --> 03:07:45.800]   Thanks so much for doing it.
[03:07:45.800 --> 03:07:47.640]   - Thank you for having me.
[03:07:47.640 --> 03:07:49.240]   - Thanks for listening to this conversation
[03:07:49.240 --> 03:07:52.320]   with George Hatz, and thank you to our sponsors.
[03:07:52.320 --> 03:07:54.680]   Four Sigmatic, which is the maker
[03:07:54.680 --> 03:07:58.600]   of delicious mushroom coffee, Decoding Digital,
[03:07:58.600 --> 03:08:02.140]   which is a tech podcast that I listen to and enjoy,
[03:08:02.140 --> 03:08:07.080]   and ExpressVPN, which is the VPN I've used for many years.
[03:08:07.080 --> 03:08:09.260]   Please check out these sponsors in the description
[03:08:09.260 --> 03:08:13.100]   to get a discount and to support this podcast.
[03:08:13.100 --> 03:08:15.540]   If you enjoy this thing, subscribe on YouTube,
[03:08:15.540 --> 03:08:17.940]   review it with Five Stars and Apple Podcast,
[03:08:17.940 --> 03:08:20.680]   follow on Spotify, support on Patreon,
[03:08:20.680 --> 03:08:24.500]   or connect with me on Twitter @LexFriedman.
[03:08:24.500 --> 03:08:27.060]   And now, let me leave you with some words
[03:08:27.060 --> 03:08:30.740]   from the great and powerful Linus Torvald.
[03:08:30.740 --> 03:08:32.060]   Talk is cheap.
[03:08:32.060 --> 03:08:33.120]   Show me the code.
[03:08:33.120 --> 03:08:37.180]   Thank you for listening, and hope to see you next time.
[03:08:37.180 --> 03:08:39.760]   (upbeat music)
[03:08:39.760 --> 03:08:42.340]   (upbeat music)
[03:08:42.340 --> 03:08:52.340]   [BLANK_AUDIO]

