
[00:00:00.000 --> 00:00:04.640]   We need to talk about convolutional neural networks.
[00:00:04.640 --> 00:00:06.560]   So far, everything that we've done
[00:00:06.560 --> 00:00:08.560]   applies to any type of input.
[00:00:08.560 --> 00:00:11.480]   Multilayer perceptrons is a generic algorithm
[00:00:11.480 --> 00:00:14.360]   that you can use on literally any kind of input
[00:00:14.360 --> 00:00:17.160]   if you can get it into an array of numbers.
[00:00:17.160 --> 00:00:18.760]   But convolutional neural networks
[00:00:18.760 --> 00:00:23.120]   only apply to certain data sets, and images are one of them.
[00:00:23.120 --> 00:00:26.360]   The reason is when we apply a multilayer perceptron,
[00:00:26.360 --> 00:00:28.460]   it throws out a lot of data.
[00:00:28.460 --> 00:00:33.280]   When my little friend here looks at something and gets an image,
[00:00:33.280 --> 00:00:35.160]   the image actually has some structure to it.
[00:00:35.160 --> 00:00:36.280]   It's a 2D array.
[00:00:36.280 --> 00:00:38.600]   And the x, y coordinates, we have an intuition
[00:00:38.600 --> 00:00:40.160]   that they do matter.
[00:00:40.160 --> 00:00:42.440]   And if I took an image and I shifted every pixel
[00:00:42.440 --> 00:00:45.280]   to the left, all the numbers change,
[00:00:45.280 --> 00:00:49.040]   but what's actually in that image probably stays the same.
[00:00:49.040 --> 00:00:51.080]   So we need some new mathematical operators
[00:00:51.080 --> 00:00:53.920]   that take advantage of that, and the operator that's generally
[00:00:53.920 --> 00:00:57.600]   used is called a convolution.
[00:00:57.600 --> 00:01:01.160]   Convolutions have been around for a really long time.
[00:01:01.160 --> 00:01:03.400]   Even convolutional neural networks
[00:01:03.400 --> 00:01:04.800]   have been around for a long time,
[00:01:04.800 --> 00:01:09.240]   but around 2011 or 2012, applying convolutions
[00:01:09.240 --> 00:01:13.040]   to object recognition inside images with neural nets
[00:01:13.040 --> 00:01:16.320]   went mainstream with huge success.
[00:01:16.320 --> 00:01:19.720]   The task of recognizing what's in an arbitrary image
[00:01:19.720 --> 00:01:22.740]   went from being basically impossible for any machine
[00:01:22.740 --> 00:01:26.560]   learning algorithm to having better than human level
[00:01:26.560 --> 00:01:31.240]   accuracy in many cases using CNNs.
[00:01:31.240 --> 00:01:34.000]   This kind of improvement, from your 30% error rate
[00:01:34.000 --> 00:01:37.360]   to better than human accuracy over just a few years,
[00:01:37.360 --> 00:01:39.960]   is probably one of the biggest breakthroughs in AI,
[00:01:39.960 --> 00:01:42.000]   and it's probably a big part of the reason
[00:01:42.000 --> 00:01:43.700]   you're actually watching this video.
[00:01:43.700 --> 00:01:47.480]   Let's apply convolutions to our image recognition problem.
[00:01:47.480 --> 00:01:49.720]   What is a convolution?
[00:01:49.720 --> 00:01:52.340]   Now, generally speaking, there's 1D convolutions
[00:01:52.340 --> 00:01:54.360]   and 2D convolutions and 3D convolutions,
[00:01:54.360 --> 00:01:56.720]   and you might have seen a convolution in signal
[00:01:56.720 --> 00:02:00.440]   processing or in your math class involving somehow Fourier
[00:02:00.440 --> 00:02:01.640]   transforms.
[00:02:01.640 --> 00:02:04.160]   But in this case, what we mean by convolution
[00:02:04.160 --> 00:02:07.440]   is actually a 2D discrete convolution.
[00:02:07.440 --> 00:02:10.160]   And so you can think of that as sliding a window of weights
[00:02:10.160 --> 00:02:13.000]   across an image, or more practically,
[00:02:13.000 --> 00:02:17.400]   you can think of it as a very low budget Photoshop filter.
[00:02:17.400 --> 00:02:19.640]   So assuming you're in the right directory,
[00:02:19.640 --> 00:02:23.840]   you can find a little tool I wrote called ConvDemo.
[00:02:23.840 --> 00:02:25.560]   This is mostly for demonstration purposes.
[00:02:25.560 --> 00:02:28.720]   I start with a kernel set to a 3 by 3 array,
[00:02:28.720 --> 00:02:32.040]   where each value is equal to 0.1.
[00:02:32.040 --> 00:02:34.200]   And now, what this convolution does
[00:02:34.200 --> 00:02:37.520]   is it scans across the image, and it takes each pixel value
[00:02:37.520 --> 00:02:41.440]   in a 3 by 3 chunk, and it multiplies it by 0.1,
[00:02:41.440 --> 00:02:43.140]   and then sums the number, and then
[00:02:43.140 --> 00:02:46.760]   puts the sum in the first value of the output image.
[00:02:46.760 --> 00:02:48.720]   Now, we step one pixel to the right,
[00:02:48.720 --> 00:02:50.680]   and we do the same thing with the same weights,
[00:02:50.680 --> 00:02:52.720]   and we do it over and over and over
[00:02:52.720 --> 00:02:54.400]   for every pixel in the image.
[00:02:54.400 --> 00:02:57.480]   This convolution basically averages over a 3 by 3 block,
[00:02:57.480 --> 00:03:00.560]   so essentially, you can think of it as a blur.
[00:03:00.560 --> 00:03:02.600]   OK, so what do you think this does
[00:03:02.600 --> 00:03:04.680]   if I make the middle of the kernel 1,
[00:03:04.680 --> 00:03:05.960]   and all the other numbers 0?
[00:03:05.960 --> 00:03:15.920]   I actually get the original image back.
[00:03:15.920 --> 00:03:17.460]   And now, what do you think this does
[00:03:17.460 --> 00:03:19.800]   if I make the middle of the kernel 2,
[00:03:19.800 --> 00:03:21.040]   and all the other numbers 0?
[00:03:22.040 --> 00:03:23.720]   It actually makes everything brighter,
[00:03:23.720 --> 00:03:25.720]   which kind of makes sense, because it multiplies
[00:03:25.720 --> 00:03:27.720]   each of the pixels by 2, exactly.
[00:03:27.720 --> 00:03:29.040]   And now, I won't go through this.
[00:03:29.040 --> 00:03:30.040]   I'll leave this to you.
[00:03:30.040 --> 00:03:31.800]   But you should think about, what does this
[00:03:31.800 --> 00:03:33.640]   do if I make the middle of the kernel 1,
[00:03:33.640 --> 00:03:35.480]   and the number above it negative 1?
[00:03:35.480 --> 00:03:36.960]   Or what do you think this does if I
[00:03:36.960 --> 00:03:39.480]   make the middle of the kernel 1, and all the other numbers
[00:03:39.480 --> 00:03:41.560]   around it negative 0.1?
[00:03:41.560 --> 00:03:44.280]   And if you play with this convodemo for a while,
[00:03:44.280 --> 00:03:47.880]   you'll get a great intuition for what convolutions can do.
[00:03:47.880 --> 00:03:49.520]   There's actually one more thing that I
[00:03:49.520 --> 00:03:51.120]   want to talk about, and it's actually
[00:03:51.120 --> 00:03:53.080]   one more piece of this that's important,
[00:03:53.080 --> 00:03:55.840]   which is how we do convolution on multiple images.
[00:03:55.840 --> 00:03:57.840]   And a really important case is where
[00:03:57.840 --> 00:03:59.640]   this happens on color images.
[00:03:59.640 --> 00:04:03.000]   So actually, my little dog was originally a color image.
[00:04:03.000 --> 00:04:05.600]   So the way we passed it into the neural net
[00:04:05.600 --> 00:04:09.680]   is we broke it up into red, green, and blue components.
[00:04:09.680 --> 00:04:13.460]   So actually, we do a 3 by 3 convolution
[00:04:13.460 --> 00:04:16.800]   on the red data, the green data, the blue data,
[00:04:16.800 --> 00:04:19.400]   and then we add up all the results.
[00:04:19.400 --> 00:04:22.560]   So sometimes, convolutions take a step of more than one.
[00:04:22.560 --> 00:04:24.920]   If each iteration moved by a step of 2,
[00:04:24.920 --> 00:04:28.320]   we would say that the convolution had a stride of 2.
[00:04:28.320 --> 00:04:30.440]   How we handle the edges also matters.
[00:04:30.440 --> 00:04:32.560]   If we do a 3 by 3 convolution on an image
[00:04:32.560 --> 00:04:34.880]   and we don't go over the edges, the output image
[00:04:34.880 --> 00:04:37.360]   is a little smaller than the input image.
[00:04:37.360 --> 00:04:39.440]   This is what Keras does by default.
[00:04:39.440 --> 00:04:41.640]   So if you want to preserve the image size,
[00:04:41.640 --> 00:04:44.760]   you have to add 0's around the input image.
[00:04:44.760 --> 00:04:46.560]   And that's called 0 padding.
[00:04:46.560 --> 00:04:49.800]   Now, there's another simpler transformation, also
[00:04:49.800 --> 00:04:52.600]   very common in neural networks, called pooling.
[00:04:52.600 --> 00:04:55.200]   If convolutions are a bad Photoshop filter,
[00:04:55.200 --> 00:04:58.260]   pooling is like a bad resizing algorithm.
[00:04:58.260 --> 00:05:01.600]   Typically, pooling takes a 2 by 2 region of an image
[00:05:01.600 --> 00:05:04.160]   and chooses the max value in each region.
[00:05:04.160 --> 00:05:06.240]   This is max pooling.
[00:05:06.240 --> 00:05:09.900]   Average pooling takes the average in each region.
[00:05:09.900 --> 00:05:13.660]   Either way, this shrinks the image by a factor of 2.
[00:05:13.660 --> 00:05:16.960]   You can actually achieve similar results with strides,
[00:05:16.960 --> 00:05:18.640]   but we want to shrink down the image
[00:05:18.640 --> 00:05:22.040]   so we can do convolutions at different scales.
[00:05:22.040 --> 00:05:24.720]   Now, let's go to the code and put it all together.
[00:05:24.720 --> 00:05:27.000]   So open up CNN.py.
[00:05:27.000 --> 00:05:29.760]   And you'll notice that this is very, very similar to what
[00:05:29.760 --> 00:05:33.120]   we left off in the last video of a multilayer perceptron,
[00:05:33.120 --> 00:05:37.160]   but there's a nasty little wrinkle on lines 19 and 20.
[00:05:37.160 --> 00:05:40.160]   And that's because Keras' 2D convolutions
[00:05:40.160 --> 00:05:43.560]   want three-dimensional input, because most images are
[00:05:43.560 --> 00:05:45.500]   actually two-dimensional, but then have
[00:05:45.500 --> 00:05:48.140]   a third dimension for color.
[00:05:48.140 --> 00:05:51.360]   Our images happen to be two-dimensional only
[00:05:51.360 --> 00:05:52.860]   because they're grayscale, and they
[00:05:52.860 --> 00:05:54.540]   don't have a color dimension.
[00:05:54.540 --> 00:05:57.420]   So we need to use the handy reshape function that
[00:05:57.420 --> 00:06:00.980]   takes our image and adds an extra dimension of length 1
[00:06:00.980 --> 00:06:05.420]   to fit Keras' convolution API.
[00:06:05.420 --> 00:06:09.020]   Now, line 29 is the other line that changes.
[00:06:09.020 --> 00:06:11.060]   Until now, we've actually flattened out
[00:06:11.060 --> 00:06:13.860]   all of our models for perceptrons.
[00:06:13.860 --> 00:06:16.500]   But in this case, we're going to do a convolution instead
[00:06:16.500 --> 00:06:18.540]   of the flattening.
[00:06:18.540 --> 00:06:20.980]   Notice that we still have to explicitly tell our model
[00:06:20.980 --> 00:06:26.140]   that the input shape is 28 by 28 by 1.
[00:06:26.140 --> 00:06:29.980]   We're actually doing 32 3 by 3 convolutions in parallel,
[00:06:29.980 --> 00:06:32.260]   and we're going to need to learn the weights of each
[00:06:32.260 --> 00:06:35.140]   of those convolution parameters.
[00:06:35.140 --> 00:06:37.000]   Next, we add a max pooling layer,
[00:06:37.000 --> 00:06:38.980]   which shrinks down the network.
[00:06:38.980 --> 00:06:42.760]   Then we add a flattened layer because the next dense layer
[00:06:42.760 --> 00:06:45.740]   is a perceptron and expects a one-dimensional input.
[00:06:45.740 --> 00:06:49.540]   Let's try running our model.
[00:06:49.540 --> 00:06:53.100]   But before we do it, let's be sure to call model.summary
[00:06:53.100 --> 00:06:55.100]   and check out the output.
[00:06:55.100 --> 00:06:56.860]   OK, because we're in a new directory,
[00:06:56.860 --> 00:07:01.540]   we need to call wnbinit one more time to set this up.
[00:07:01.540 --> 00:07:04.620]   We can use the same username, and we can optionally
[00:07:04.620 --> 00:07:07.100]   select a new project or use our previous project
[00:07:07.100 --> 00:07:11.100]   if you've been following along with previous videos.
[00:07:11.100 --> 00:07:15.500]   Now, Python cnn.py actually trains the model.
[00:07:15.500 --> 00:07:17.260]   And here's where a GPU would start
[00:07:17.260 --> 00:07:19.460]   to really speed things up.
[00:07:19.460 --> 00:07:22.540]   There's only 320 parameters in our convolution layer,
[00:07:22.540 --> 00:07:26.020]   but they dramatically affect the output.
[00:07:26.020 --> 00:07:27.940]   You can see because we don't have pooling,
[00:07:27.940 --> 00:07:31.340]   it actually shrinks our image input from 28 by 28
[00:07:31.340 --> 00:07:33.620]   to 26 by 26 in the convolution.
[00:07:33.620 --> 00:07:36.660]   But it actually makes 32 different output images.
[00:07:36.660 --> 00:07:38.380]   So the output size of our convolution layer
[00:07:38.380 --> 00:07:40.860]   is really large.
[00:07:40.860 --> 00:07:43.420]   The max pooling layer has no free parameters
[00:07:43.420 --> 00:07:45.180]   because it always does the same thing,
[00:07:45.180 --> 00:07:48.260]   but it cuts the size of our images in half.
[00:07:48.260 --> 00:07:52.180]   The flattened layer, of course, also has no free parameters.
[00:07:52.180 --> 00:07:54.740]   The hidden dense layer has a ton of free parameters
[00:07:54.740 --> 00:08:01.020]   because it has 128 outputs and 5,408 inputs.
[00:08:01.020 --> 00:08:04.340]   Our total model has nearly 1 million free parameters
[00:08:04.340 --> 00:08:06.540]   and only 60,000 training data points.
[00:08:06.540 --> 00:08:09.700]   So what should we be worried about?
[00:08:09.700 --> 00:08:12.300]   I hope you guessed overfitting, because if you did,
[00:08:12.300 --> 00:08:14.180]   that means you've been paying attention.
[00:08:14.180 --> 00:08:17.140]   And I hope that you're thinking, if I have overfitting,
[00:08:17.140 --> 00:08:21.380]   I should do one thing, and that is to add Dropout.
[00:08:21.380 --> 00:08:23.780]   Now, if you were my student and we were in person,
[00:08:23.780 --> 00:08:26.980]   I would make you do that on your own to check understanding.
[00:08:26.980 --> 00:08:30.260]   I can't do that on the video, but I think you should try.
[00:08:30.260 --> 00:08:32.260]   I'll show you how to add Dropout to the network.
[00:08:32.260 --> 00:08:33.900]   And there's more than one way to do it.
[00:08:33.900 --> 00:08:35.260]   Here's how I would do it.
[00:08:35.260 --> 00:08:37.020]   Typically, you want to put Dropout
[00:08:37.020 --> 00:08:41.300]   between layers that actually have a free parameter.
[00:08:41.300 --> 00:08:44.220]   So in this case, it'll be after the max pooling layer
[00:08:44.220 --> 00:08:46.420]   and then after the final dense layer.
[00:08:46.420 --> 00:08:50.260]   You could also put Dropout before the very first layer.
[00:08:50.260 --> 00:08:53.860]   Typical inputs to the Dropout is about 0.4,
[00:08:53.860 --> 00:08:57.820]   which means that 40% of the things coming into the Dropout
[00:08:57.820 --> 00:08:59.260]   gets set to 0.
[00:08:59.260 --> 00:09:02.060]   You could set this anywhere between 20% and 50%.
[00:09:02.060 --> 00:09:03.740]   It tends to not matter that much.
[00:09:03.740 --> 00:09:10.980]   This is going to take a while to train.
[00:09:10.980 --> 00:09:14.300]   So while we wait, I'll tell you about one more improvement
[00:09:14.300 --> 00:09:14.840]   that we have.
[00:09:14.840 --> 00:09:17.420]   We're actually at 98% accuracy or so.
[00:09:17.420 --> 00:09:20.200]   But to get to 99% accuracy, we need
[00:09:20.200 --> 00:09:22.980]   to have multiple convolutional layers.
[00:09:22.980 --> 00:09:25.660]   And the intuition for why is that a convolution only
[00:09:25.660 --> 00:09:28.020]   acts at one scale.
[00:09:28.020 --> 00:09:31.140]   But if we shrink the image down and we do another convolution,
[00:09:31.140 --> 00:09:34.100]   then we can detect patterns at multiple scales.
[00:09:34.100 --> 00:09:37.980]   So a typical architecture for most object recognition tasks
[00:09:37.980 --> 00:09:40.380]   will actually have multiple convolutions
[00:09:40.380 --> 00:09:42.640]   and some kind of shrinking operation
[00:09:42.640 --> 00:09:44.160]   in between the convolutions.
[00:09:44.160 --> 00:09:45.860]   Max pooling is probably the most common,
[00:09:45.860 --> 00:09:49.500]   but you'll see other strategies for shrinking the image down.
[00:09:49.500 --> 00:09:51.660]   So this diagram actually lays out
[00:09:51.660 --> 00:09:55.420]   a very typical convolutional neural network.
[00:09:55.420 --> 00:09:57.740]   The bottom here is the input 8.
[00:09:57.740 --> 00:09:59.780]   And that's actually the digit 8 from the same data
[00:09:59.780 --> 00:10:00.840]   set we're looking at.
[00:10:00.840 --> 00:10:02.460]   And then the next layer from the bottom
[00:10:02.460 --> 00:10:04.700]   is the first convolutional layer.
[00:10:04.700 --> 00:10:06.380]   Then there's another max pooling layer,
[00:10:06.380 --> 00:10:08.260]   followed by another convolutional layer,
[00:10:08.260 --> 00:10:09.900]   followed by another max pooling layer,
[00:10:09.900 --> 00:10:11.620]   followed by some hidden layers.
[00:10:11.620 --> 00:10:14.580]   And if you look closely, you can see the output
[00:10:14.580 --> 00:10:17.420]   that the eighth neuron is lit up on the top.
[00:10:17.420 --> 00:10:19.260]   I love this visualization because you
[00:10:19.260 --> 00:10:22.180]   can see all the crazy things the convolutions do.
[00:10:22.180 --> 00:10:23.580]   Now, here's a challenge.
[00:10:23.580 --> 00:10:27.940]   Can you actually create this network by modifying cnn.py?
[00:10:27.940 --> 00:10:30.540]   So stop this video and give it a try if you like.
[00:10:30.540 --> 00:10:33.800]   I do it by adding two layers in the middle of this network.
[00:10:33.800 --> 00:10:36.300]   You can actually tune the number of outputs of this layer
[00:10:36.300 --> 00:10:38.220]   and the number of nodes in the hidden layer.
[00:10:38.220 --> 00:10:39.120]   Experiment.
[00:10:39.120 --> 00:10:41.620]   See if you can build interesting network architectures that
[00:10:41.620 --> 00:10:43.720]   get higher accuracy.
[00:10:43.720 --> 00:10:45.740]   We're actually getting almost perfect accuracy
[00:10:45.740 --> 00:10:47.300]   on this data set at this point, so it
[00:10:47.300 --> 00:10:50.280]   might be time to try applying this to something else.
[00:10:50.280 --> 00:10:53.660]   And for your educational entertainment,
[00:10:53.660 --> 00:10:56.260]   I have a completely new but very similar data
[00:10:56.260 --> 00:10:58.100]   set called Fashion MNIST.
[00:10:58.100 --> 00:11:02.980]   It's 60,000 images, but instead of being handwritten numbers,
[00:11:02.980 --> 00:11:05.020]   they're images of clothes.
[00:11:05.020 --> 00:11:08.740]   And the categories are t-shirt, trouser, pullover, dress, coat,
[00:11:08.740 --> 00:11:12.900]   sandal, shirt, sneaker, bag, and ankle boot.
[00:11:12.900 --> 00:11:18.940]   In fact, if you open up fashion.py,
[00:11:18.940 --> 00:11:21.380]   I have some skeleton code that starts off
[00:11:21.380 --> 00:11:23.980]   where this whole set of lessons began,
[00:11:23.980 --> 00:11:26.500]   with a very, very basic perceptron.
[00:11:26.500 --> 00:11:28.020]   Can you apply what you've learned
[00:11:28.020 --> 00:11:32.220]   to build a fashion classifier on a similar data set?
[00:11:32.220 --> 00:11:35.260]   Let me know how it goes and comment below.
[00:11:35.260 --> 00:11:40.180]   OK, so so far, we've covered the basic introductory class
[00:11:40.180 --> 00:11:41.380]   into neural networks.
[00:11:41.380 --> 00:11:43.780]   We've done perceptrons, multilayer perceptrons,
[00:11:43.780 --> 00:11:45.660]   and convolutional neural networks.
[00:11:45.660 --> 00:11:47.220]   This should be a basic understanding
[00:11:47.220 --> 00:11:49.740]   that lets you actually use real deep learning
[00:11:49.740 --> 00:11:50.860]   code in the wild.
[00:11:50.860 --> 00:11:52.940]   But from here on out, it gets super fun.
[00:11:52.940 --> 00:11:55.140]   We can apply this to lots of different applications.
[00:11:55.140 --> 00:11:57.340]   And I want to show you about autoencoders, which
[00:11:57.340 --> 00:11:58.820]   are a really cool architecture.
[00:11:58.820 --> 00:12:00.620]   I want to show you about transfer learning.
[00:12:00.620 --> 00:12:03.180]   I want to show you about text and lots of other things.
[00:12:03.180 --> 00:12:04.940]   But you should actually enjoy the knowledge
[00:12:04.940 --> 00:12:07.300]   you have right now on convolutional neural networks.
[00:12:07.300 --> 00:12:09.020]   Go out and build an object classifier
[00:12:09.020 --> 00:12:10.260]   on a new set of data.
[00:12:10.260 --> 00:12:11.700]   I think you'll really have fun.
[00:12:11.700 --> 00:12:12.200]   OK, yeah.
[00:12:12.200 --> 00:12:16.900]   I just came from a board meeting.
[00:12:16.900 --> 00:12:21.300]   And they told me if I don't get a million subscribers next week,
[00:12:21.300 --> 00:12:23.100]   they're going to fire me.
[00:12:23.100 --> 00:12:27.180]   So it would really mean a lot if you guys could subscribe
[00:12:27.180 --> 00:12:31.220]   and like and click on my Patreon.
[00:12:31.220 --> 00:12:32.940]   What else?
[00:12:32.940 --> 00:12:35.180]   (laughing)

