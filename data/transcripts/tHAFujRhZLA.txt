
[00:00:00.000 --> 00:00:06.060]   Hello, today I'm going to be talking to you about how Weights and Biases can help teams
[00:00:06.060 --> 00:00:10.480]   better collaborate in the machine learning development lifecycle.
[00:00:10.480 --> 00:00:14.600]   Machine learning development is as much a coordination problem between different personas
[00:00:14.600 --> 00:00:17.400]   as it is a technical challenge.
[00:00:17.400 --> 00:00:23.760]   So first let's talk about the model development lifecycle and all the personas involved.
[00:00:23.760 --> 00:00:29.440]   First we have model training, where an ML engineer will take a model off the shelf and
[00:00:29.440 --> 00:00:32.320]   train it according to their needs.
[00:00:32.320 --> 00:00:36.600]   This requires that they track their metrics and results, version their model checkpoints
[00:00:36.600 --> 00:00:41.780]   and datasets, as they continue to iterate as quickly as possible on different modeling
[00:00:41.780 --> 00:00:45.720]   approaches to see what works the best.
[00:00:45.720 --> 00:00:51.040]   After training a model, the ML engineer then needs to evaluate it to ensure that it's
[00:00:51.040 --> 00:00:54.720]   going to perform well in production.
[00:00:54.720 --> 00:00:59.240]   Weights and Biases provide several tools to help the ML practitioner keep track of their
[00:00:59.240 --> 00:01:07.840]   work, such as experiment tracking for visualizing their metrics and charts, artifacts for versioning
[00:01:07.840 --> 00:01:13.960]   their model checkpoints and datasets, tables for recording predictions from the model,
[00:01:13.960 --> 00:01:19.480]   and sweeps for hyperparameter optimization.
[00:01:19.480 --> 00:01:24.400]   Behind the scenes there is another persona, the ML Ops Engineer, who is helping the ML
[00:01:24.400 --> 00:01:28.080]   engineer work quicker and efficiently.
[00:01:28.080 --> 00:01:32.320]   They help scale the infrastructure that the ML engineer uses to train and evaluate the
[00:01:32.320 --> 00:01:40.120]   models, along with provisioning environments for them to operate in, building tooling,
[00:01:40.120 --> 00:01:45.000]   model management, and taking the models that ML engineers produce and deploying them into
[00:01:45.000 --> 00:01:47.280]   production.
[00:01:47.280 --> 00:01:53.400]   Weights and Biases can serve as the key handoff point between these two teams, where when
[00:01:53.400 --> 00:01:58.840]   an ML engineer produces a model, they can store it in the Weights and Biases model registry,
[00:01:58.840 --> 00:02:03.160]   and then the ML Ops Engineer can take it and deploy it to production.
[00:02:03.160 --> 00:02:10.680]   In addition, Weights and Biases Launch enables ML Ops Engineers to configure complex infrastructure
[00:02:10.680 --> 00:02:17.360]   like Kubernetes clusters and make them much more accessible to the ML engineer to launch
[00:02:17.360 --> 00:02:21.360]   experiments into.
[00:02:21.360 --> 00:02:27.840]   After evaluating a model, a team then needs to document this whole process, so that in
[00:02:27.840 --> 00:02:35.560]   the future it can be clearly understood what exact version of a dataset a model was trained
[00:02:35.560 --> 00:02:41.800]   on and all the metrics associated with that model.
[00:02:41.800 --> 00:02:46.400]   Managers and project stakeholders will be interested in this documentation.
[00:02:46.400 --> 00:02:52.340]   They will be concerned with model approvals, user management, and stakeholder alignment
[00:02:52.340 --> 00:02:56.680]   of the end product, which incorporates this ML model.
[00:02:56.680 --> 00:03:02.320]   Weights and Biases provides reports, which allow all these personas to capture this whole
[00:03:02.320 --> 00:03:09.240]   development lifecycle into a living document that can be handed off to different stakeholders.
[00:03:09.240 --> 00:03:13.500]   And finally, we take this model and deploy it into production, which will mainly be the
[00:03:13.500 --> 00:03:15.880]   job of the ML Ops Engineer.
[00:03:15.880 --> 00:03:21.580]   All told, this process can take anywhere from 8 to 12 weeks just for a 10 person team.
[00:03:21.580 --> 00:03:27.700]   So anything that we can do to reduce this cycle can save a lot of time and money.
[00:03:27.700 --> 00:03:31.600]   So let's jump into Weights and Biases.
[00:03:31.600 --> 00:03:36.280]   Before I do that, I want to show exactly how this works.
[00:03:36.280 --> 00:03:41.020]   Weights and Biases is a very low effort integration into your existing pipelines.
[00:03:41.020 --> 00:03:45.740]   All you have to do is add a couple lines of code to your existing training scripts or
[00:03:45.740 --> 00:03:51.520]   evaluation scripts in order to log metrics and also to log artifacts like your model
[00:03:51.520 --> 00:03:53.740]   checkpoints.
[00:03:53.740 --> 00:03:57.480]   Once you do that, all of that information will appear on the dashboard where the different
[00:03:57.480 --> 00:04:04.700]   personas mentioned before can understand results, debug model performance, build reports, and
[00:04:04.700 --> 00:04:09.420]   facilitate model CI/CD.
[00:04:09.420 --> 00:04:16.860]   Weights and Biases has hundreds of integrations with various popular libraries in the ML ecosystem
[00:04:16.860 --> 00:04:22.780]   like PyTorch Lightning, Hugging Face, and many others.
[00:04:22.780 --> 00:04:29.300]   So let's jump into a demo of how this ML lifecycle happens in Weights and Biases.
[00:04:29.300 --> 00:04:34.220]   Let's say you are a new machine learning engineer on a team and your manager has tasked you
[00:04:34.220 --> 00:04:40.500]   with training the core computer vision model on the most recent dataset, building a report,
[00:04:40.500 --> 00:04:44.220]   and staging the model for production.
[00:04:44.220 --> 00:04:48.020]   As a new team member, you'll want to understand the context of this project.
[00:04:48.020 --> 00:04:52.340]   Here in this project workspace, you can see all past experiments that have been recorded
[00:04:52.340 --> 00:04:59.020]   over time, along with their training metrics, evaluation metrics, along with performance
[00:04:59.020 --> 00:05:05.260]   charts, and even individual model predictions on rich media so you can understand the model's
[00:05:05.260 --> 00:05:14.420]   predictions at a more granular level.
[00:05:14.420 --> 00:05:19.140]   To inspect an individual experiment, we can click on it and see the charts and metrics
[00:05:19.140 --> 00:05:22.160]   associated with it.
[00:05:22.160 --> 00:05:27.680]   On top of these metrics, Weights and Biases records information about the entire execution
[00:05:27.680 --> 00:05:35.260]   context of this experiment, such as who created it, when it was created, the operating system,
[00:05:35.260 --> 00:05:37.820]   Python version, hardware information.
[00:05:37.820 --> 00:05:47.020]   We'll even track system utilization like CPU and GPU memory usage, the Python library
[00:05:47.020 --> 00:05:52.500]   requirements that you had installed at the time an experiment was executed, and we'll
[00:05:52.500 --> 00:05:58.060]   track code state like the git commit hash, the training script, or notebook that was
[00:05:58.060 --> 00:06:02.100]   used to run the experiment.
[00:06:02.100 --> 00:06:05.620]   Weights and Biases keeps track of everything you would need to replicate this experiment
[00:06:05.620 --> 00:06:07.340]   exactly.
[00:06:07.340 --> 00:06:14.140]   So how can we reuse this and retrain our model on the most recent dataset?
[00:06:14.140 --> 00:06:20.420]   Weights and Biases launch provides a clean way for turning past experiments into re-executable
[00:06:20.420 --> 00:06:25.460]   jobs that can be ported to various different environments.
[00:06:25.460 --> 00:06:30.740]   I click on the Jobs tab, we can see all jobs we have at our disposal.
[00:06:30.740 --> 00:06:33.660]   The Nature Model training job is the one I'm looking for.
[00:06:33.660 --> 00:06:38.780]   This is the retraining job the team uses for the Core Computer Vision model.
[00:06:38.780 --> 00:06:43.080]   Here I can see all past instantiations of this job as people have retrained the model
[00:06:43.080 --> 00:06:45.900]   over time.
[00:06:45.900 --> 00:06:50.980]   Going to Job Details we can learn a little bit more about how to use it.
[00:06:50.980 --> 00:06:55.380]   We have this RunConfig which we can tweak to change the behavior of the job, like the
[00:06:55.380 --> 00:06:59.420]   dataset it was trained on or other hyperparameters.
[00:06:59.420 --> 00:07:03.040]   The MLOps engineering team has provisioned a Kubernetes cluster where we can go ahead
[00:07:03.040 --> 00:07:05.660]   and launch this job into.
[00:07:05.660 --> 00:07:11.140]   Furthermore, the job is going to produce some model artifact checkpoints.
[00:07:11.140 --> 00:07:15.620]   This will become important later on where we will want to take one of these checkpoints
[00:07:15.620 --> 00:07:19.780]   and stage it for production in the model registry.
[00:07:19.780 --> 00:07:24.300]   Let's go ahead and start this process.
[00:07:24.300 --> 00:07:31.740]   Clicking Launch I can configure the job with the dataset and hyperparameters I see fit.
[00:07:31.740 --> 00:07:36.900]   I can then choose the Kubernetes cluster I want to launch it into and then go ahead and
[00:07:36.900 --> 00:07:38.780]   launch it.
[00:07:38.780 --> 00:07:44.540]   This is going to launch this containerized job into the Kubernetes cluster and here shortly
[00:07:44.540 --> 00:07:50.500]   we'll start to see metrics populate in the dashboard.
[00:07:50.500 --> 00:07:54.660]   While we wait for that we can check out the Artifacts tab.
[00:07:54.660 --> 00:08:00.580]   This is where the model artifacts checkpoints will land once the job completes.
[00:08:00.580 --> 00:08:07.820]   Artifacts is your one-stop shop for managing any serialized data as part of your pipelines,
[00:08:07.820 --> 00:08:13.140]   whether those are model checkpoints or datasets or evaluation results.
[00:08:13.140 --> 00:08:16.780]   The job that we launch is going to generate six checkpoints.
[00:08:16.780 --> 00:08:21.420]   Here we see the best one has already been labeled with an alias.
[00:08:21.420 --> 00:08:25.160]   An alias uniquely identifies a model checkpoint.
[00:08:25.160 --> 00:08:30.420]   Here we see and start to see the checkpoint start to come in as that model starts to train
[00:08:30.420 --> 00:08:36.020]   on my Kubernetes cluster.
[00:08:36.020 --> 00:08:39.700]   For a given checkpoint you can inspect everything about it from the experiment which produced
[00:08:39.700 --> 00:08:47.900]   it to metadata to the specific files within that checkpoint.
[00:08:47.900 --> 00:08:52.540]   Weights and biases, artifacts is agnostic to the structure or format of files you decide
[00:08:52.540 --> 00:08:56.580]   to log as an artifact.
[00:08:56.580 --> 00:09:01.980]   When you generate a checkpoint you can then decide whether you want to register that checkpoint
[00:09:01.980 --> 00:09:05.220]   in the model registry.
[00:09:05.220 --> 00:09:09.140]   Usually in the course of training you generate several different checkpoints but the best
[00:09:09.140 --> 00:09:11.740]   one is the one you care about moving to production.
[00:09:11.740 --> 00:09:16.100]   So let's go ahead and register that model into the model registry.
[00:09:16.100 --> 00:09:21.840]   Here I can choose which registered model I want to link it under and then I can give
[00:09:21.840 --> 00:09:26.380]   it an alias like staging to indicate that while this model performs well it needs to
[00:09:26.380 --> 00:09:31.460]   go through further testing before it gets deployed.
[00:09:31.460 --> 00:09:35.980]   Linking a model to the registry is akin to bookmarking it, surfacing it to our teammates
[00:09:35.980 --> 00:09:42.060]   who may be concerned with consuming it like the MLOps engineering team.
[00:09:42.060 --> 00:09:47.460]   Flipping to the model registry we can see all of the model tasks that my team is currently
[00:09:47.460 --> 00:09:48.980]   working on.
[00:09:48.980 --> 00:09:54.140]   I'm mainly concerned with the nature classification task but the model registry is that central
[00:09:54.140 --> 00:10:00.900]   landing page for all registered models across your organization.
[00:10:00.900 --> 00:10:04.400]   For this core computer vision model I can see all past versions that have ever been
[00:10:04.400 --> 00:10:10.980]   linked and clicking on a specific one I can easily understand the experiment which generated
[00:10:10.980 --> 00:10:20.220]   it, its metadata, along with lineage as well.
[00:10:20.220 --> 00:10:29.260]   So all upstream and downstream runs which concern this specific model.
[00:10:29.260 --> 00:10:33.860]   We see here for instance that a downstream run is consuming this model and running unit
[00:10:33.860 --> 00:10:40.620]   tests on it.
[00:10:40.620 --> 00:10:44.380]   I can also click to view more details about the registered model like viewing the model
[00:10:44.380 --> 00:10:50.740]   card which describes the model at a high level along with how to consume it and how it was
[00:10:50.740 --> 00:10:55.020]   created.
[00:10:55.020 --> 00:11:00.620]   The model registry is also a point where you can configure model CI/CD.
[00:11:00.620 --> 00:11:05.660]   Here we have two automations attached to this registered model which allow Weights and Biases
[00:11:05.660 --> 00:11:15.780]   to talk to other systems such as GitHub Actions which facilitate model testing and deployment.
[00:11:15.780 --> 00:11:21.820]   Going to these two automations I can see I have two configured.
[00:11:21.820 --> 00:11:27.220]   One when a new model is added to this registered model this kicks off a GitHub Action that
[00:11:27.220 --> 00:11:31.580]   runs unit tests on the model and builds an evaluation report.
[00:11:31.580 --> 00:11:36.580]   The second one occurs when a production alias is added to the registered model that will
[00:11:36.580 --> 00:11:43.180]   trigger a GitHub Action that containerizes the model artifact and deploys it to GCP.
[00:11:43.180 --> 00:11:50.340]   Both of these automations are simple webhooks which ping GitHub Actions to run the relevant
[00:11:50.340 --> 00:11:53.340]   workload.
[00:11:53.340 --> 00:11:58.820]   If I go to GitHub Actions here we can see the testing and reporting action workflow
[00:11:58.820 --> 00:12:03.500]   is already occurring because I linked that new model checkpoint.
[00:12:03.500 --> 00:12:10.660]   Clicking into it we can see that it's logging into Weights and Biases, pulling down the
[00:12:10.660 --> 00:12:16.980]   artifacts and running unit tests and generating a comparison report.
[00:12:16.980 --> 00:12:23.300]   Weights and Biases reports are a powerful way to surface all the results around model
[00:12:23.300 --> 00:12:26.620]   training and evaluation.
[00:12:26.620 --> 00:12:30.220]   Reports can be programmatically generated as they were in that GitHub Action workflow
[00:12:30.220 --> 00:12:39.120]   I just described and templatized so you can facilitate standardized model CI/CD.
[00:12:39.120 --> 00:12:43.580]   Here we have a report auto-generated which compares the staging and production versions
[00:12:43.580 --> 00:12:50.220]   of the Core Computer Vision model that we just retrained.
[00:12:50.220 --> 00:12:54.420]   In this report we have embedded the model registry so we can click into specific versions
[00:12:54.420 --> 00:13:00.580]   of the model that we might care about, along with specific training and evaluation metrics
[00:13:00.580 --> 00:13:07.540]   for the staging and production models in particular, so I can understand and inspect individual
[00:13:07.540 --> 00:13:12.460]   predictions from these two models.
[00:13:12.460 --> 00:13:18.020]   From here I can also kick off deployment of the model by simply adding the production
[00:13:18.020 --> 00:13:20.660]   alias to the staging model.
[00:13:20.660 --> 00:13:27.540]   This will again trigger GitHub Actions to containerize the model and deploy it to GCP
[00:13:27.540 --> 00:13:32.900]   through a web repository dispatch.
[00:13:32.900 --> 00:13:34.660]   So let's go ahead and do that.
[00:13:34.660 --> 00:13:41.500]   I can go to the staging model and here add the alias production.
[00:13:41.500 --> 00:13:46.980]   Note that this production alias is purple meaning that it's protected so only model
[00:13:46.980 --> 00:13:49.860]   registry admins can make this change.
[00:13:49.860 --> 00:13:56.180]   This prevents random editing of the model registry and affecting important production
[00:13:56.180 --> 00:14:01.020]   models.
[00:14:01.020 --> 00:14:09.420]   Coming back to GitHub Actions we can see that the build and deploy GitHub
[00:14:09.420 --> 00:14:12.220]   Action has been kicked off.
[00:14:12.220 --> 00:14:18.020]   It's retrieving the artifacts from Weights and Biases, building a container, and deploying
[00:14:18.020 --> 00:14:27.220]   it to Kubernetes.
[00:14:27.220 --> 00:14:31.700]   So as an overview we started as a new team member, trained a Core Computer Vision model
[00:14:31.700 --> 00:14:37.820]   in the latest dataset, staged the model for production, which automatically unit tested
[00:14:37.820 --> 00:14:43.180]   the model, generated a report, allowed us to inspect results, pass it off to our manager
[00:14:43.180 --> 00:14:48.220]   who then could approve and deploy the model to production.
[00:14:48.820 --> 00:14:52.820]   [Music]
[00:14:52.820 --> 00:14:54.880]   you

