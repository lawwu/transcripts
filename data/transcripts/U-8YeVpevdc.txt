
[00:00:00.000 --> 00:00:06.240]   So I'm not gonna take much time and try to keep it very short and crisp. So I'm Sayak,
[00:00:06.240 --> 00:00:10.800]   I'm a deep learning associate at PyImageSearch. I work at the intersection of deep learning and
[00:00:10.800 --> 00:00:16.720]   computer vision. And I also work with Lavanya quite extensively to work out reports and stuff.
[00:00:16.720 --> 00:00:23.200]   So in this talk, I'm going to be presenting about TensorFlow Lite and all the cool features
[00:00:23.760 --> 00:00:31.040]   that you could use from it to optimize your models, which could be really helpful, specifically for
[00:00:31.040 --> 00:00:39.200]   production purposes. So yeah, this is a little acknowledgement to the ones that really helped me
[00:00:39.200 --> 00:00:47.200]   in putting together the slides. So the ideal audience for this short session would be machine
[00:00:47.200 --> 00:00:54.080]   learning developers that have worked on image-based models mainly because the support for model
[00:00:54.080 --> 00:01:00.720]   optimization is kind of limited at this stage. But yeah, image-based models would typically work
[00:01:00.720 --> 00:01:06.640]   off the shelf. They would work right off the bat in TensorFlow Lite. That's why I specifically
[00:01:06.640 --> 00:01:12.320]   mentioned about it. And also this talk would be, I think, very beneficial for the mobile developers
[00:01:12.320 --> 00:01:19.920]   looking for ways to plug machine learning models in their applications. Okay, so this is going to
[00:01:19.920 --> 00:01:27.120]   be the rough agenda. I'll first be discussing the motivation behind doing on-device machine learning.
[00:01:27.120 --> 00:01:34.960]   I'm sure you must be familiar with this, but just to keep it complete. So I'll be also
[00:01:34.960 --> 00:01:40.720]   introducing you to TensorFlow Lite in case someone hasn't heard about it, and also different
[00:01:40.720 --> 00:01:45.360]   TensorFlow Lite usage scenarios and some of the best practices and if time permits, we'll do
[00:01:45.360 --> 00:01:53.440]   Q&A as well. Okay, so the motivation behind doing on-device machine learning is quite evident,
[00:01:53.440 --> 00:02:00.880]   and it has strong reasons. The number one reason being there would be some mission-critical
[00:02:00.880 --> 00:02:06.960]   applications where batch prediction is not really, you know, desirable. You would need to have
[00:02:06.960 --> 00:02:13.840]   instantaneous prediction, and that means that your application must be delivering extremely low
[00:02:13.840 --> 00:02:19.600]   latencies and close-knit interactions between all the different components that might be involved
[00:02:19.600 --> 00:02:27.760]   in your application. And the second point would be not all the regions in the world have good
[00:02:27.760 --> 00:02:34.080]   internet connectivity, specifically the developing countries. So how to make machine learning more
[00:02:34.080 --> 00:02:39.360]   and more accessible, machine learning products more and more accessible in those areas where
[00:02:39.360 --> 00:02:44.000]   network connectivity is still a thing. In those cases, you cannot really host your heavyweight
[00:02:44.000 --> 00:02:49.440]   machine learning models on cloud and have them consumed in your application. So in those cases
[00:02:49.440 --> 00:02:55.280]   as well, on-device machine learning does make a lot of sense. And thirdly, privacy preserving.
[00:02:55.280 --> 00:03:04.800]   This was probably one of the most talked about topics in today's salon. So privacy preserving,
[00:03:04.800 --> 00:03:14.560]   what if you wouldn't want the data to not leave the device where your application is being accessed
[00:03:14.560 --> 00:03:22.000]   from? So that's another issue. So that was kind of the three main motivations behind doing
[00:03:22.000 --> 00:03:29.360]   on-device machine learning. And yeah, let's just quickly get to the crux or the big elephant of
[00:03:29.360 --> 00:03:36.160]   the session. What's TensorFlow Lite? So I'm just going to read it out from the official statement.
[00:03:36.160 --> 00:03:40.720]   TensorFlow Lite is a production-ready cross-platform framework for deploying machine
[00:03:40.720 --> 00:03:47.360]   learning on mobile devices and embedded systems. So it has the right amount of tooling, be it for
[00:03:47.360 --> 00:03:54.080]   Swift-based applications, be it for Flutter-based applications, be it for native Android-based
[00:03:54.080 --> 00:04:00.400]   applications. TensorFlow Lite provides you the right tooling for incorporating machine learning
[00:04:00.400 --> 00:04:06.240]   models across different frameworks and platforms. And it also provides you the right amount of
[00:04:06.240 --> 00:04:10.880]   tooling for a number of different embedded devices, as you will see later in the talk.
[00:04:12.400 --> 00:04:17.840]   So what are all the things that TensorFlow Lite can do for us? First and foremost,
[00:04:17.840 --> 00:04:27.040]   it can optimize our models. You would not want to end up your 15 MB mobile application where
[00:04:27.040 --> 00:04:34.720]   your machine learning model is of 13 MB size, probably. You would want to reduce the size as
[00:04:34.720 --> 00:04:41.120]   much as possible. And part of model optimization also means that reducing your model size and also
[00:04:41.120 --> 00:04:49.680]   at the same time not letting it affect the performance. And also, most of the modern-day
[00:04:49.680 --> 00:04:56.480]   mobile devices come with GPUs and sophisticated hardware accelerators to improve graphics
[00:04:56.480 --> 00:05:01.760]   rendering and stuff like that. So TensorFlow Lite can also take advantage of those hardware
[00:05:01.760 --> 00:05:08.960]   accelerators. It also supports HDPU-based hardware accelerations and stuff with the use of a concept
[00:05:08.960 --> 00:05:14.880]   called delegation, which we'll talk about later in the talk. Different tools, of course. As I
[00:05:14.880 --> 00:05:21.040]   mentioned, there are different easy and right tooling methods for integration of machine learning
[00:05:21.040 --> 00:05:27.040]   models in both mobile devices and a number of embedded systems, including microcontrollers
[00:05:27.040 --> 00:05:35.600]   as well. And yeah, you can find out more using this link below. I'll provide all the necessary
[00:05:35.600 --> 00:05:39.920]   links, including the link to this presentation so that you can refer to when it's needed.
[00:05:39.920 --> 00:05:47.520]   So let's discuss some of the different scenarios where TensorFlow Lite can be used.
[00:05:47.520 --> 00:05:55.200]   First, I would start with model optimization. So why is it required? I'm sure you might have
[00:05:55.200 --> 00:06:00.560]   a motivation behind this, but just to make it consistent and complete. Size reduction. You
[00:06:00.560 --> 00:06:08.160]   wouldn't want a 1GB model in a 1.2GB mobile application. You definitely want to reduce the
[00:06:08.160 --> 00:06:14.880]   size of that model to be able to, you know, more production ready and to be able to more and more
[00:06:14.880 --> 00:06:23.440]   accessible for a number of different devices. Not all the devices that support running 1.9GB
[00:06:23.440 --> 00:06:30.400]   mobile applications. So I hope you got the idea. And when you try to reduce the size of any
[00:06:30.400 --> 00:06:36.480]   particular model, it also, you know, results in latency deductions and stuff like that, meaning
[00:06:36.480 --> 00:06:44.000]   that it would take, it would likely take a lesser time to run a forward pass with input data along
[00:06:44.000 --> 00:06:49.840]   with your model. So that would definitely result in latency deductions and stuff. And it also
[00:06:49.840 --> 00:06:56.080]   depends on the size of the model as well. The number of parameters your forward pass is
[00:06:56.080 --> 00:07:03.520]   catering to and also accelerated compatibility. As I mentioned, model optimization can take
[00:07:03.520 --> 00:07:09.760]   huge amounts of benefits when the model graphs are being executed on specialized hardware
[00:07:09.760 --> 00:07:16.160]   accelerators and stuff. So that's another reason. And there are different optimization options
[00:07:16.160 --> 00:07:22.320]   available in TensorFlow. Not the latest ones like rewinding or linear mode connectivity and stuff,
[00:07:22.320 --> 00:07:28.400]   but it has all the options that are heavily well studied and well researched upon,
[00:07:28.400 --> 00:07:35.520]   like quantization and pruning. We'll mainly be talking about quantization because that's kind
[00:07:35.520 --> 00:07:42.640]   of widely supported natively by TensorFlow Lite. Pruning will leave it for another day.
[00:07:42.640 --> 00:07:50.080]   So yeah, let's talk about what is quantization. So keep noting down your questions. If you may
[00:07:50.080 --> 00:07:56.080]   have, I'll try to get to them by the end of this talk. From a layman's perspective,
[00:07:56.080 --> 00:08:00.880]   quantization is nothing but reducing the precision of the numbers used to represent
[00:08:00.880 --> 00:08:09.360]   a model's parameters, which is most often done using float32. Although there are float64 or
[00:08:09.360 --> 00:08:15.600]   float128 based models, but float32 is the most common one. And by quantization, we essentially
[00:08:15.600 --> 00:08:23.040]   mean reducing that numerical precision to a lower one so that we can represent model's parameter
[00:08:23.040 --> 00:08:30.640]   using a lower precision-based scheme. And it ultimately reduces the size of the overall model.
[00:08:30.640 --> 00:08:37.760]   And also there's something called VC complexity, which is used to kind of measure the computational
[00:08:37.760 --> 00:08:41.360]   or theoretical complexity of any machine learning model, but we'll not get into that today.
[00:08:42.640 --> 00:08:47.840]   As I mentioned, this also results in smaller model sizes and of course, faster computation,
[00:08:47.840 --> 00:08:53.600]   the lesser number of parameters in your model, the faster the computation will be naturally.
[00:08:53.600 --> 00:09:01.440]   And there are two types of quantization, mainly. So the first one is post-training quantization,
[00:09:01.440 --> 00:09:06.240]   which is natively supported by TensorFlow Lite. There's other one as well, quantization-aware
[00:09:06.240 --> 00:09:10.400]   training, which is supported by the model optimization toolkit in TensorFlow,
[00:09:11.200 --> 00:09:17.600]   yeah, by TensorFlow. And it's kind of plays out really well with TensorFlow Lite. I'll share a
[00:09:17.600 --> 00:09:23.920]   notebook that would show all of this baking recipes. No worries. And let's talk about
[00:09:23.920 --> 00:09:28.560]   post-training quantization for a bit. So post-training quantization happens after a
[00:09:28.560 --> 00:09:35.840]   model is trained, not during the model training process. And let's take a very simple example to
[00:09:35.840 --> 00:09:42.400]   make things more clear. So just to be sure this is in, this is all in TensorFlow 2.x and the folks
[00:09:42.400 --> 00:09:48.640]   that are familiar with Keras, this would be tremendously helpful for you. So this is a very
[00:09:48.640 --> 00:09:54.960]   simple data. So the relationship that we are trying to learn here is y equals to 2x minus one.
[00:09:54.960 --> 00:10:01.280]   Here's how you define your model. Very humble model, nothing very fancy here. Here's how you
[00:10:01.280 --> 00:10:08.000]   train it. All very simple. And here's all the line of lines of code that's needed to optimize it.
[00:10:08.000 --> 00:10:18.800]   So three lines of code. And I'll talk about these things later in the talk, but the main thing that
[00:10:18.800 --> 00:10:26.560]   you need to care about is this optimization option, tf.lite.optimize.optimize for size.
[00:10:27.120 --> 00:10:33.040]   And that's about it. And in the first line of this block, you are basically asking the
[00:10:33.040 --> 00:10:41.440]   TensorFlow Lite converter to load a model that is of Keras model instance type. And in the second
[00:10:41.440 --> 00:10:46.000]   line, you are just mentioning what kind of optimization you are looking for. And then
[00:10:46.000 --> 00:10:51.040]   you are in the final step, you are just converting the model from a Keras model to a tf.lite
[00:10:51.040 --> 00:10:57.120]   compatible model. And you get a number of different options in the optimizations
[00:10:57.120 --> 00:11:02.880]   configuration, you can either, you know, optimize for latency, you can either optimize for size,
[00:11:02.880 --> 00:11:09.920]   or you can just let TensorFlow Lite decide what kind of optimization that would be best for the
[00:11:09.920 --> 00:11:15.840]   given model. And here's how you serialize the converted model, which is pretty straightforward.
[00:11:17.440 --> 00:11:21.280]   Sorry, I missed the opening and closing parentheses. And sorry about that.
[00:11:21.280 --> 00:11:25.120]   If dot close, there should be an opening and closing.
[00:11:25.120 --> 00:11:32.800]   So there are different types of post training quantization recipes available in TensorFlow Lite.
[00:11:32.800 --> 00:11:40.560]   So based on your needs of the application, you would want to, you know, select a particular
[00:11:40.560 --> 00:11:46.800]   strategy. For example, you can see with dynamic rich quantization, you get benefits like size
[00:11:46.800 --> 00:11:53.680]   reduction up to 4x to or to typically two to three x speed up in models inference and
[00:11:53.680 --> 00:12:01.200]   accuracies kind of retained, but with full integer quantization accuracy might be dropped
[00:12:01.200 --> 00:12:07.440]   off a little but you would get more speed up benefits and more, you know, size reduction and
[00:12:07.440 --> 00:12:13.200]   stuff. And on the far right, you can see the kind of hardware compatibility you would get with
[00:12:13.200 --> 00:12:18.320]   different post training quantization techniques. So I've linked all of this in the slide itself
[00:12:18.320 --> 00:12:26.160]   so that it's easily referable. So as opposed to post training quantization, in quantization,
[00:12:26.160 --> 00:12:31.520]   I've had training, we trying to, you know, compensate for the information loss that would
[00:12:31.520 --> 00:12:38.400]   get in introduced by quantization. So typically, when you reduce the precision scheme,
[00:12:40.240 --> 00:12:44.720]   using which the models parameters would get represented, you would, it would, you know,
[00:12:44.720 --> 00:12:53.040]   result in information loss and stuff, because you are not precisely denoting all the parameters that
[00:12:53.040 --> 00:12:58.400]   your model is learning. So it's bound to, you know, incur some information loss and stuff.
[00:12:58.400 --> 00:13:04.720]   So in quantization, our training, we try to replicate the kind of loss information loss
[00:13:04.720 --> 00:13:10.240]   that would get introduced with real quantization and stuff. So So the idea is pretty crazy, and
[00:13:10.240 --> 00:13:17.600]   it's hugely effective. So quantization training aware is absolutely possible with model optimization
[00:13:17.600 --> 00:13:23.120]   toolkit. And you can definitely check out this blog to know more about it. And here I can show
[00:13:23.120 --> 00:13:31.680]   you the kind of benefits that you can get using quantization aware training. Let me go to the
[00:13:31.680 --> 00:13:43.280]   notebook quick. Okay, it's opened already. So yeah, so in this notebook, I'm using the flowers
[00:13:43.280 --> 00:13:54.240]   data set. I'm also using fine fine tuning recipes and stuff. So here you can see the, the normal
[00:13:54.240 --> 00:14:00.560]   model trained using normal fine tuning takes about three 40 seconds to train and the validation
[00:14:00.560 --> 00:14:08.240]   accuracy we are getting that's about 88.55. And here we are, you know, incorporating quantization
[00:14:08.240 --> 00:14:13.600]   aware training to make the model aware of the kind of loss that would get introduced if the
[00:14:13.600 --> 00:14:19.840]   model would get quantized. And of course, the model takes longer to train because the model is still
[00:14:19.840 --> 00:14:25.760]   figuring out what's the best way to move this weight in order to best compensate for the
[00:14:25.760 --> 00:14:32.000]   information loss that would get induced for quantization. And it takes a double time in fact,
[00:14:32.000 --> 00:14:37.760]   but the validation accuracy is still higher. And I have kind of executed this across five
[00:14:37.760 --> 00:14:44.880]   different trends and I have I have gotten similar results. So yeah. So here I'm showing you the
[00:14:44.880 --> 00:14:50.960]   sizes and stuff. Both the models using the normal fine tuning approach and using the normal fine
[00:14:50.960 --> 00:15:04.640]   tuning and the quantization training approach both ways 35 MBs and 35 MBs. So yeah. As so here I'm
[00:15:04.640 --> 00:15:10.480]   using post training quantization to quantize the model trained using quantization aware training.
[00:15:10.480 --> 00:15:23.200]   So it weighs 3.3 MB from 35 MB to 3.3 MB. And here I am quantizing the model trained using the
[00:15:23.200 --> 00:15:32.000]   normal fine tuning approach. So it's it's result is far better here. I'm getting 2.9 MB from 35
[00:15:32.000 --> 00:15:39.040]   MB. But let's compare them with respect to accuracy. So I'm running this TensorFlow Lite model
[00:15:39.600 --> 00:15:48.560]   through the validation data set of the flower of the flowers data. And here I'm using the
[00:15:48.560 --> 00:15:55.120]   quantized model of the model that got trained using quantization aware training. So I'm getting
[00:15:55.120 --> 00:16:06.160]   99.98.72, which is kind of retained if you take a look at this metric. So we got about 92.91
[00:16:06.160 --> 00:16:10.880]   using the quantization aware training. And when that same model trained using quantization
[00:16:10.880 --> 00:16:18.480]   aware training got converted to, you know, a real TensorFlow Lite model, we are still able to retain
[00:16:18.480 --> 00:16:24.160]   that performance. But when it comes to the more the quantized version of the model trained using
[00:16:24.160 --> 00:16:34.320]   the normal fine tuning approach, we see that we get 75.09% accuracy. So which is substantially
[00:16:34.320 --> 00:16:42.000]   less. So here you can see we can still reduce model size to a great extent without having to
[00:16:42.000 --> 00:16:48.640]   compromise on the accuracy side. So I hope that would motivate you to get on boarded with
[00:16:48.640 --> 00:16:54.800]   TensorFlow Lite and try out the different recipes that I showed in the notebook. And I think Lavanya
[00:16:54.800 --> 00:17:00.720]   will also put up the links to all of this notebook so that you can refer to when it's needed.
[00:17:02.720 --> 00:17:09.600]   So a couple of more things. So we saw in the notebook, the kind of advantages that we would
[00:17:09.600 --> 00:17:15.120]   get using a combination of quantization aware training and post training quantization.
[00:17:15.120 --> 00:17:21.600]   And here are more speed versus accuracy trade offs, which you could refer to when you are
[00:17:21.600 --> 00:17:27.040]   designing your applications as well. And the supported hardware just got increased with time.
[00:17:28.480 --> 00:17:34.080]   A closer look at latency, maybe this would be far more interesting for your use cases and stuff.
[00:17:34.080 --> 00:17:40.400]   So here's a floating point, mobile and V2 model without any quantization or optimization.
[00:17:40.400 --> 00:17:49.120]   It's, it's being inferred on a single traded CPU and the CPU takes 37 milliseconds to run
[00:17:49.120 --> 00:17:56.400]   a forward pass on a single input data data point. That means a single image. When the model is
[00:17:56.400 --> 00:18:05.440]   quantized to fixed point on on the same CPU, it takes 13 milliseconds using a mobile based GPU
[00:18:05.440 --> 00:18:10.640]   hardware accelerator using all the open CL delegacies that you would get from TensorFlow
[00:18:10.640 --> 00:18:17.680]   Lite's GPU delegates. And when the model is converted to float 16 position, you'd get the
[00:18:17.680 --> 00:18:24.880]   inferences five milliseconds and stuff and using the HTP USB accelerator. And when the model is
[00:18:24.880 --> 00:18:31.200]   quantized to the full integer position scheme, the inferences two milliseconds. So from 37
[00:18:31.200 --> 00:18:39.280]   milliseconds to two milliseconds, the journey is quite amazing. I would say. So another TF like
[00:18:39.280 --> 00:18:46.240]   you just use a scenario is model maker. So this might be very useful for the folks that do not
[00:18:46.240 --> 00:18:53.200]   want to invest in machine learning expertise, but they would want to, you know, plug in their own
[00:18:53.200 --> 00:19:01.120]   data sets and train custom models without much of machine learning expertise. So here's all the
[00:19:01.120 --> 00:19:08.960]   code that one would need in order to, you know, train in order to use transfer learning on top of
[00:19:08.960 --> 00:19:14.720]   a pre-trained network and use and train an image classifier on the flowers data set. So you would
[00:19:14.720 --> 00:19:20.960]   load the data you create and retrain the model here. We are using an efficient net based model.
[00:19:20.960 --> 00:19:27.600]   So that's pretty cool. And here's how you would predict and evaluate the results. And here's how
[00:19:27.600 --> 00:19:34.400]   you, how you convert it to TensorFlow Lite model. Basically you, here's how you optimize the model
[00:19:34.400 --> 00:19:43.040]   and serialize it. And let me introduce TensorFlow Lite model maker. As you can see, it's a transfer
[00:19:43.040 --> 00:19:49.360]   learning library when using TensorFlow Lite. And as I already mentioned, it will be very useful for
[00:19:49.360 --> 00:19:55.680]   the folks that do not want to invest in machine learning expertise. So here's all the code that's
[00:19:55.680 --> 00:20:04.240]   needed to make use of all of this and stuff. It's one, two, three, four, four lines of code without
[00:20:04.240 --> 00:20:12.480]   the input. So I have linked a notebook, a full, fully worked out notebook that you might want to
[00:20:12.480 --> 00:20:18.720]   check out for, and feel free to, you know, plug your own data sets and let me know if you run into
[00:20:19.360 --> 00:20:24.000]   any issues and stuff. And typically the default model is efficient netlines,
[00:20:24.000 --> 00:20:29.520]   like zero, the B01, and you can define your own models configuration. It supports ResNet 50 and
[00:20:29.520 --> 00:20:36.480]   other models as well. So that should be easy to follow. And model maker typically works with
[00:20:36.480 --> 00:20:42.640]   a limited number of models at this point in time, but more support is coming really soon.
[00:20:43.680 --> 00:20:48.160]   As you can see, it already supports BERT, which is pretty cool. I think it would be
[00:20:48.160 --> 00:20:56.960]   interesting to see Jack's text attacks that he presented and figure out how to incorporate text
[00:20:56.960 --> 00:21:03.680]   attack to train more adversarially robust BERT based text classifiers and still be able to
[00:21:03.680 --> 00:21:11.920]   take advantage of model maker at the same time. So here's where you can find all the pre-trained
[00:21:11.920 --> 00:21:18.480]   TensorFlow Lite models, much like the traditional models that you get with TensorFlow or PyTorch.
[00:21:18.480 --> 00:21:24.640]   There's a model zoo for TensorFlow Lite models as well. So here's where you can find all of them.
[00:21:24.640 --> 00:21:31.760]   And for the folks that are developing mobile applications or that are hacking around
[00:21:31.760 --> 00:21:38.320]   embedded and microcontroller based devices, the next, the next set of slides are for you.
[00:21:40.560 --> 00:21:48.240]   Yeah, I love Captain America. So, yeah. So for the folks that are working in Android,
[00:21:48.240 --> 00:21:55.200]   so here's, here's a rough mobile application structure. If you are familiar with Android
[00:21:55.200 --> 00:22:02.880]   studio, this might seem very familiar. So you put your my model dot TF Lite, which is basically a
[00:22:02.880 --> 00:22:08.960]   TF Lite model in the assets folder. And you would also supply all the metadata that would be
[00:22:08.960 --> 00:22:17.120]   required in order to consume it using any wrapper class within your application. But with TensorFlow
[00:22:17.120 --> 00:22:23.040]   code gen tool, you can, you know, supply a TensorFlow Lite model to the code gen tool.
[00:22:23.040 --> 00:22:31.120]   And the code gen tool will automatically, you know, yield an Android library, which,
[00:22:31.120 --> 00:22:38.080]   which is directly consumable using any Android application. So that's cool. So here's how to
[00:22:38.080 --> 00:22:43.040]   use the TensorFlow Lite code gen tool here. You are supplying the TensorFlow Lite model
[00:22:43.040 --> 00:22:48.320]   and you are, and in the next couple of the command line arguments, you are basically
[00:22:48.320 --> 00:22:53.280]   configuring what you would like to call this package. What would be the main class that would
[00:22:53.280 --> 00:23:01.120]   be invoked from the application and where you would like to, you know, generate this package.
[00:23:01.120 --> 00:23:08.720]   So this is how easy it is. But before TensorFlow Lite code gen, you, you had to, you know, put your
[00:23:08.720 --> 00:23:14.560]   TensorFlow Lite model in the assets folder, just as I mentioned, and you had to, you know, use
[00:23:14.560 --> 00:23:19.920]   any wrapper class, any utility class in order to consume that. But with TensorFlow Lite code gen
[00:23:19.920 --> 00:23:28.400]   tool, you are just getting a library, which you can directly use. So, yeah, here I have linked a
[00:23:28.400 --> 00:23:33.440]   couple of examples, which you could refer to and jumpstart your machine learning based applications.
[00:23:33.440 --> 00:23:38.320]   And I'll be looking forward to see what you are building with TensorFlow Lite cavalry.
[00:23:38.320 --> 00:23:43.440]   And as you can see, it supports question answering as well and style transfer. So if you were
[00:23:43.440 --> 00:23:49.520]   thinking it's only limited to image classification, which is kind of boring, maybe, but yeah, you will
[00:23:49.520 --> 00:23:55.360]   have to agree image classification is still one of the predominant use cases widely in the industry
[00:23:55.360 --> 00:23:59.920]   that they use. So yeah, it supports question and answering and style transfer based models and,
[00:23:59.920 --> 00:24:05.840]   and a number of different other things. So I encourage you to check out the examples
[00:24:05.840 --> 00:24:13.280]   that are linked here. So the for the folks that are working with embedded stuff and microcontroller
[00:24:13.280 --> 00:24:19.360]   related stuff, here's the kind of here's a list of manufacturers supported by TensorFlow Lite
[00:24:19.360 --> 00:24:26.720]   already arm is there already cadence, synopsis and others as well. So TensorFlow Lite is also
[00:24:26.720 --> 00:24:33.920]   available for arm 64 baseboards and TensorFlow Lite has a very dedicated build for Raspberry Pi.
[00:24:33.920 --> 00:24:40.240]   So if you were thinking how to best optimize your model for Raspberry Pi and other arm 64
[00:24:40.240 --> 00:24:48.400]   baseboards, we have got you covered. So yeah. And as I mentioned a while back, TensorFlow Lite
[00:24:48.400 --> 00:24:56.640]   is furiously fast. If the TensorFlow Lite compatible models are run on HTPUSB accelerators
[00:24:56.640 --> 00:25:03.840]   or HTPUSB compatible boards and stuff recently by mid search released its book called Raspberry Pi
[00:25:03.840 --> 00:25:10.080]   for computer vision. There we typically, you know, advise all our readers to go for a combination of
[00:25:10.080 --> 00:25:18.000]   HTPUSB accelerator and Raspberry Pi for all your embedded systems still did stuff because
[00:25:18.000 --> 00:25:24.800]   in our experience we found we found that combination to work the best. We compared
[00:25:24.800 --> 00:25:32.800]   the performance against Jetson Nano and other coprocessor based devices like neural compute
[00:25:32.800 --> 00:25:39.840]   stick, but we found the combination of HTPUSB accelerator and Raspberry Pi typically works
[00:25:39.840 --> 00:25:47.840]   right off the bat. So Arduino, so TensorFlow Lite has a dedicated module for Arduino. It in fact
[00:25:47.840 --> 00:25:53.440]   has a separate library, but the amount of tooling and the kind of tooling still remains the same.
[00:25:53.440 --> 00:26:01.200]   You can use it from the Arduino ID directly. And there are some sample applications which you might
[00:26:01.200 --> 00:26:07.280]   want to refer to if you are interested in using the TensorFlow Lite Arduino library for your
[00:26:07.280 --> 00:26:15.040]   Arduino use cases. So there are some best practices. These are the general rules to,
[00:26:15.040 --> 00:26:23.200]   you know, remember while using TensorFlow Lite based stuff. So if you are willing to use TensorFlow
[00:26:23.200 --> 00:26:30.080]   Lite model, first see if there's a pre trained model, pre trained TensorFlow Lite model already
[00:26:30.080 --> 00:26:37.120]   available that can do the job because in that way you will save a lot of resources first and foremost.
[00:26:37.120 --> 00:26:43.200]   So there are different state of the art models available for different domains and tasks as
[00:26:43.200 --> 00:26:49.520]   you might have already guessed. So here are all the domains that are which TensorFlow Lite
[00:26:49.520 --> 00:26:55.200]   models currently caters to. So as you can see, there are a number of different, you know, models
[00:26:55.200 --> 00:27:00.720]   across a number of different domains that are already available using all the TensorFlow Lite
[00:27:00.720 --> 00:27:07.440]   delicacies that I just demonstrated. So yeah, state of the art NLP for mobile developers,
[00:27:07.440 --> 00:27:14.000]   Albert or Albert Lite already available and a quantized version of mobile bird is coming soon.
[00:27:14.000 --> 00:27:21.440]   State of the art vision, of course, efficient net light. The TensorFlow Lite variant is already
[00:27:21.440 --> 00:27:26.800]   available. And on the left, you can see the performance is still kind of retained. And the
[00:27:26.800 --> 00:27:35.600]   latency is much, much reduced. So that's a good thing. So know your trade offs and optimize
[00:27:35.600 --> 00:27:42.240]   accordingly. You might not want to develop an application where it's very hard to, you know,
[00:27:42.240 --> 00:27:47.280]   afford the drop in accuracy. As I mentioned with quantization, there might be some drop in the
[00:27:47.280 --> 00:27:53.600]   accuracy. But if you are plugging in quantization aware training with the TensorFlow Lite recipes,
[00:27:53.600 --> 00:27:58.960]   that might not be the case also. So if accuracy is super duper important for your application, so
[00:27:59.760 --> 00:28:03.760]   then you might want to, you know, go for separate optimization based options.
[00:28:03.760 --> 00:28:11.200]   And yeah, you might want to ask, can accuracy be compensated with speed, in some cases,
[00:28:11.200 --> 00:28:17.680]   you wouldn't want to do that. So you might want to also draw a balance between speed and accuracy.
[00:28:17.680 --> 00:28:22.000]   So always refer to this chart that I mentioned and take the decision accordingly.
[00:28:22.000 --> 00:28:29.280]   And use TensorFlow Lite delegates wherever possible. And I think you already are familiar
[00:28:29.280 --> 00:28:35.360]   with the concept of delegation. But I'll just read out for the sake of completeness. A TensorFlow
[00:28:35.360 --> 00:28:39.840]   Lite delegate is a way to delegate parts or all of your graph execution to another executor. So
[00:28:39.840 --> 00:28:45.680]   CPU, your commodity CPU might not be the best one to execute all of the computations that's needed
[00:28:45.680 --> 00:28:51.680]   for a forward pass. So in those cases, if you use a special specialized hardware accelerator,
[00:28:51.680 --> 00:28:56.560]   in order to run those computations, you would get tremendous speed ups. So here are all the
[00:28:56.560 --> 00:29:02.880]   different delegates that are supported by TensorFlow Lite. And I have also mentioned the
[00:29:02.880 --> 00:29:10.080]   kind of precision formats that are supported. So GPU, as all the modern phones come up, come with
[00:29:10.080 --> 00:29:15.200]   GPU based hardware accelerators these days. So GPU based delegation is absolutely doable.
[00:29:15.200 --> 00:29:23.680]   In TensorFlow Lite, by GPU, I mean the HTPU and stuff in an API is available. I mean, this must
[00:29:23.680 --> 00:29:29.840]   be very favorite with the Android folks. Hexagon delegates are also available and also 4ML based
[00:29:29.840 --> 00:29:37.760]   delegates for newer iPhones and iPads. There's a comprehensive guide on using TensorFlow Lite
[00:29:37.760 --> 00:29:45.040]   delegates and stuff and I encourage you to check them all out. So know more about the kind of
[00:29:45.040 --> 00:29:52.720]   support that's provided for your target device because not all the support are unified. It
[00:29:52.720 --> 00:29:58.000]   changes with respect to the target runtime. So it's very important to study the kind of
[00:29:58.000 --> 00:30:02.400]   optimized kernels that are supported for your target runtime by the TensorFlow Lite environment.
[00:30:02.400 --> 00:30:10.880]   So which precision are they supported because your target runtime might not support float 16
[00:30:10.880 --> 00:30:18.400]   based optimization. For example, HTPU is all about in date based optimization. So you might want to
[00:30:18.400 --> 00:30:23.920]   study the kind of optimized layers that are supported for your target runtime by TensorFlow
[00:30:23.920 --> 00:30:30.400]   Lite and then decide accordingly. So no more here, no more about the performance based practices,
[00:30:30.400 --> 00:30:36.000]   which discusses how to open the threads for consuming the TensorFlow Lite models within
[00:30:36.000 --> 00:30:41.120]   your application and stuff. It discusses some other relevant information regarding profiling
[00:30:41.120 --> 00:30:46.720]   your TensorFlow Lite based model from Android studios and stuff. So be sure to check it out
[00:30:46.720 --> 00:30:52.480]   if you're interested. And if you're looking for a comprehensive set of learning resources for
[00:30:52.480 --> 00:30:57.520]   TensorFlow Lite, which I think you should be because you are getting model based optimizations
[00:30:57.520 --> 00:31:04.080]   for free and without having to compensate much on its performance. So I have listed all the
[00:31:04.080 --> 00:31:10.240]   relevant links here, which you would want to check out maybe. And slides are available here. I'm sure
[00:31:10.240 --> 00:31:15.840]   Lavanya must have put all the links in the respective channels. And if you have any
[00:31:15.840 --> 00:31:23.840]   questions, feel free to reach out to me. My Twitter is @risingsayak. And if you are interested,
[00:31:23.840 --> 00:31:37.040]   I wrote a blog post with weights and biases in the previous week, I guess. So yeah, it discusses a
[00:31:37.040 --> 00:31:43.120]   number of different quantization recipes using TensorFlow Lite and the model optimization toolkit.
[00:31:43.120 --> 00:31:49.840]   So it's quite comprehensive, and it will help you to get on boarded with these tools and techniques
[00:31:49.840 --> 00:31:56.400]   easily. So I think we are done and I'm happy to take questions.
[00:31:56.400 --> 00:32:03.760]   Thanks. I love that you can use TF Lite to go from 37 to 2 milliseconds. That's insane to me.
[00:32:03.760 --> 00:32:13.920]   That's crazy. We do have a lot of questions. Let's see. So someone, Vijay asks, the performance
[00:32:13.920 --> 00:32:20.400]   number 2 milliseconds you showed on TPU, what is the image size for that? And is that only
[00:32:20.400 --> 00:32:28.800]   inference time, not the transfer of image and results? Yeah, that's only the inference time.
[00:32:28.800 --> 00:32:35.680]   And the image size was 2 to 4, 2 to 4, 3. That's the standard one for image-based models.
[00:32:35.680 --> 00:32:43.120]   Gotcha. And then Kyle asks, have you worked with object detection models in TF Lite? Have you seen
[00:32:43.120 --> 00:32:50.080]   any of these models implemented using MobileNet? Yeah. So the book that I mentioned, we released
[00:32:50.080 --> 00:32:56.080]   from PyImageSearch, the Raspberry Pi for computer vision one. So we used TensorFlow Lite version one
[00:32:56.080 --> 00:33:01.600]   there. So yeah, object detection-based models are typically supported using the version one,
[00:33:01.600 --> 00:33:08.080]   but not by the version two. The support is coming, as I mentioned in this slide as well.
[00:33:08.080 --> 00:33:13.280]   So if you are willing to optimize your object detection-based models that are supported,
[00:33:13.280 --> 00:33:20.000]   that has compatible optimized layers, you would want to go for version one. And yeah, we have
[00:33:20.000 --> 00:33:27.440]   reduced models from 235 MB to 10 MBs using TensorFlow Lite object detection-based models.
[00:33:27.440 --> 00:33:35.040]   And typically the models that are supported are the ones like SSD MobileNet or SSD InceptionNet.
[00:33:35.040 --> 00:33:41.280]   So that's the kind of object detection models that are supported. Thanks. Boris said really
[00:33:41.280 --> 00:33:46.240]   impressive results. What's the typical maximum model size you can use on mobiles?
[00:33:48.960 --> 00:33:55.360]   I think that depends on the kind of users you are wanting to cater to and also the kind of
[00:33:55.360 --> 00:34:00.720]   accessibility you are wanting to cater to with your applications and stuff. You might not want
[00:34:00.720 --> 00:34:08.000]   to, you know, deploy your applications on the lower versions of Android mobile phones or iPhones.
[00:34:08.000 --> 00:34:16.160]   So it really depends where your application would get used the most. And if you are willing to,
[00:34:16.160 --> 00:34:22.400]   you know, cater to a broad set of mobile phones, then I would typically recommend a size between
[00:34:22.400 --> 00:34:26.800]   3 MB to 5 MB. But I have seen, you know, folks going for more cutting-edge stuff.
[00:34:26.800 --> 00:34:32.560]   They do not care about the accessibility part and they are, you know, willing to go about
[00:34:32.560 --> 00:34:39.360]   up to 15 MBs of models and size. But 15 MB is something that's very typical. And that's
[00:34:39.360 --> 00:34:42.240]   also the highest gap that I have seen in the recent days.
[00:34:43.120 --> 00:34:50.480]   Thanks. And then Vijay asks, can you please elaborate on the quantization-aware training,
[00:34:50.480 --> 00:34:56.480]   the Keras API, what's the best strategy? And then also you talked about pruning. What does
[00:34:56.480 --> 00:35:03.760]   pruning actually do to the train model? Yeah. So in quantization-aware training,
[00:35:03.760 --> 00:35:10.560]   it compensates for the information loss that would get introduced by quantization. So it does so by,
[00:35:10.560 --> 00:35:18.640]   you know, introducing fake quantization ops in the original model graph and trains the model in a way
[00:35:18.640 --> 00:35:25.600]   that is aware of the information loss that would get induced when you actually quantize the model.
[00:35:25.600 --> 00:35:30.000]   So here in this report, in this Watson-Biasis report, I have shown all the different
[00:35:30.000 --> 00:35:36.000]   kind of quantization strategy that's available. And if you are, you know, if you are training
[00:35:36.000 --> 00:35:40.320]   image classification-based models using transfer learning, so here's how you would, you know,
[00:35:40.320 --> 00:35:44.960]   define your transfer learning training loop or define your model in the first place.
[00:35:44.960 --> 00:35:53.840]   And then it's just two lines of code to, you know, instruct your model, hey, train using
[00:35:53.840 --> 00:35:57.680]   quantization-aware training. So that's all the line of code that's needed.
[00:36:01.520 --> 00:36:06.800]   So the next question, I'm not sure if I'm going to get this across right, but if I don't,
[00:36:06.800 --> 00:36:10.000]   please let me know if I'm wrong. Is it my question?
[00:36:10.000 --> 00:36:20.560]   Sorry, not yours. I'm going to ask yours next. So, Jack's like, did I mess up? What type of
[00:36:20.560 --> 00:36:27.040]   quantization schema are you using if you're optimizing for size or when you're optimizing
[00:36:27.040 --> 00:36:32.640]   for latency, like int8, pow2, which is the best one for TF Lite?
[00:36:32.640 --> 00:36:41.600]   Again, yeah, again, so if I'm understanding this correctly, you are basically asking when,
[00:36:41.600 --> 00:36:48.880]   what kind of quantization techniques. So it kind of depends, you know, like you might want to refer
[00:36:48.880 --> 00:36:55.520]   to this chart to decide what kind of quantization you would like to go for. Maybe speed is,
[00:36:56.480 --> 00:37:02.800]   the speed is super important for your application, but in some cases for mission critical
[00:37:02.800 --> 00:37:08.640]   applications, accuracy is super important. So it's okay to, you know, compensate on the speed part
[00:37:08.640 --> 00:37:14.560]   and be able to retain the accuracy part of things. So in integer, integer 8-based quantization
[00:37:14.560 --> 00:37:21.360]   techniques, as we can see, there is some accuracy loss, but in my experience, when you are training
[00:37:21.360 --> 00:37:26.400]   the model using quantization, I've had training in the first place and then using post-training
[00:37:26.400 --> 00:37:30.880]   quantization, then you are still able to, you know, retain the accuracy part as I showed
[00:37:30.880 --> 00:37:39.680]   in this notebook, where using a 2.9, using a 3.3 MB model, it's still giving you a 99,
[00:37:39.680 --> 00:37:45.200]   92.9% accuracy on the validation set. So it depends on the kind of application and the kind
[00:37:45.200 --> 00:37:49.520]   of trade-offs that's super important for your application. And based on that, you might want
[00:37:49.520 --> 00:37:53.920]   to decide the kind of optimizations or quantization strategies you would want to go.
[00:37:53.920 --> 00:38:00.720]   Nice. And then the last question is coming from Jack. He asks, are there specific tasks for which
[00:38:00.720 --> 00:38:09.200]   models are more compressible? Like for example, is QA question answering better than object
[00:38:09.200 --> 00:38:14.880]   detection for model compression? That's a very good question. In fact, so I haven't done the
[00:38:14.880 --> 00:38:20.720]   ablation study. I haven't also seen any paper that does the ablation study as to, you know,
[00:38:20.720 --> 00:38:27.600]   where model compression is really affecting if it's task specific and stuff. But I saw in one
[00:38:27.600 --> 00:38:34.240]   of the talks, Jonathan Frankel, the lottery ticket hypothesis guy, he actually commented on how
[00:38:34.240 --> 00:38:42.000]   compression ratios actually affects your model performance with respect to a given task. So we
[00:38:42.000 --> 00:38:47.520]   can, you know, take this offline on Slack and I'll be sure to link you to the talk that I just referred
[00:38:47.520 --> 00:38:49.920]   to. Thank you. Awesome.

