
[00:00:00.000 --> 00:00:10.220]   When people overbuzz AI, I ask them, "Okay, what did AI change in your life?
[00:00:10.220 --> 00:00:13.080]   What did AI change, really, truly?"
[00:00:13.080 --> 00:00:17.400]   Don't tell me you set a timer on Alexa or Google Home.
[00:00:17.400 --> 00:00:18.400]   That's not life-changing.
[00:00:18.400 --> 00:00:19.400]   What was life-changing?
[00:00:19.400 --> 00:00:21.760]   It came from AI, right?
[00:00:21.760 --> 00:00:26.140]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:26.140 --> 00:00:28.360]   and I'm your host, Lukas Biewald.
[00:00:28.360 --> 00:00:35.460]   Jerome Pizzenti was VP of AI at Meta, which is one of the most exciting places where AI
[00:00:35.460 --> 00:00:37.420]   research is happening.
[00:00:37.420 --> 00:00:43.160]   Before that, he was CEO of Benevolent AI, and before that, he was VP of Machine Learning
[00:00:43.160 --> 00:00:44.640]   at IBM Watson.
[00:00:44.640 --> 00:00:49.060]   So he's had a long career and seen a ton of different applications and lots of change
[00:00:49.060 --> 00:00:51.500]   in the state of the art in machine learning.
[00:00:51.500 --> 00:00:55.820]   This is a super fun conversation, and I hope you enjoy it.
[00:00:55.820 --> 00:01:01.920]   The first question that's top of mind is just with all the advances in large language models
[00:01:01.920 --> 00:01:03.540]   that we keep seeing.
[00:01:03.540 --> 00:01:12.620]   I know Meta had Blender, but I was wondering if you have a point of view, or Meta had a
[00:01:12.620 --> 00:01:20.140]   point of view on building a large language model differently than a DeepMind or an OpenAI,
[00:01:20.140 --> 00:01:22.220]   and how you think about that.
[00:01:22.220 --> 00:01:28.020]   Oh, wow, you go right deep into the channels there.
[00:01:28.020 --> 00:01:34.180]   I mean, I would say the large transformer models, I think at this point, it's not just
[00:01:34.180 --> 00:01:35.260]   language model, right?
[00:01:35.260 --> 00:01:41.580]   So the transformer large models are starting to really like, and being able to be used
[00:01:41.580 --> 00:01:42.700]   in multiple tasks.
[00:01:42.700 --> 00:01:52.460]   So I think this is a trend that everybody is following, size, multimodality, more data,
[00:01:52.460 --> 00:02:00.540]   more self-supervision, actually, and less classical supervision, and then trying to
[00:02:00.540 --> 00:02:02.940]   do multiple tasks at the same time.
[00:02:02.940 --> 00:02:04.740]   I think this is working really well.
[00:02:04.740 --> 00:02:07.100]   That's why people call them foundational model.
[00:02:07.100 --> 00:02:09.440]   I'm not sure I agree with that term.
[00:02:09.440 --> 00:02:16.660]   So I do think everybody's going in that direction, and that's paying out handsomely.
[00:02:16.660 --> 00:02:24.180]   Where I would say I'm a little bit more cautious is I think these models have lots of problems.
[00:02:24.180 --> 00:02:29.580]   And solving these problems is not trivialized, not easy.
[00:02:29.580 --> 00:02:33.900]   I would say there's two abuse class of problems I've seen, and so the people who will be able
[00:02:33.900 --> 00:02:37.580]   to solve that really will be onto something very interesting.
[00:02:37.580 --> 00:02:40.040]   One is control.
[00:02:40.040 --> 00:02:44.660]   So when you have this language model, I don't know how much you've played with Xtable Diffusion
[00:02:44.660 --> 00:02:47.300]   or GPT-3.
[00:02:47.300 --> 00:02:52.580]   It's really, really surprising in the things it gives you, but sometimes it really doesn't
[00:02:52.580 --> 00:02:53.900]   give you what you want at all.
[00:02:53.900 --> 00:02:56.580]   It's not necessarily what you ask.
[00:02:56.580 --> 00:03:02.900]   Sometimes it has big artifacts that show that it's not humanly generated.
[00:03:02.900 --> 00:03:05.860]   And it's not quite clear how you get rid of all this.
[00:03:05.860 --> 00:03:09.060]   And there's this whole thing around prompt crafting.
[00:03:09.060 --> 00:03:14.780]   I think it's interesting, but I don't think you can...
[00:03:14.780 --> 00:03:17.900]   It's kind of scary to say you're going to do like, "This is going to be a new type of
[00:03:17.900 --> 00:03:18.900]   software engineering."
[00:03:18.900 --> 00:03:22.340]   Because it's so unreliable.
[00:03:22.340 --> 00:03:27.700]   And so that's the first piece, which is how do you make all these models more controllable,
[00:03:27.700 --> 00:03:31.900]   which is you have a higher guarantee of what the outcome is going to be.
[00:03:31.900 --> 00:03:33.340]   And the second is bias.
[00:03:33.340 --> 00:03:37.700]   Obviously, intelligence is about bias, but if you type something...
[00:03:37.700 --> 00:03:43.420]   I mean, the easiest way to do it is on this new image generation models.
[00:03:43.420 --> 00:03:47.220]   If you type CEO, guess what you get.
[00:03:47.220 --> 00:03:49.180]   If you type assistant, guess what you get.
[00:03:49.180 --> 00:03:54.900]   If you type fast food worker, or if you type banker, it's striking.
[00:03:54.900 --> 00:04:00.300]   I mean, it works 100% of the time, and you get extreme bias.
[00:04:00.300 --> 00:04:02.780]   And it means you can't really just use this in production.
[00:04:02.780 --> 00:04:05.060]   I think it would be terrible.
[00:04:05.060 --> 00:04:06.300]   So very exciting.
[00:04:06.300 --> 00:04:08.300]   I think everybody's seeing the trend there.
[00:04:08.300 --> 00:04:16.540]   It's working, scale, multimodality, multitask, self-supervision, but they are not very controllable
[00:04:16.540 --> 00:04:20.420]   and they have huge bias issues.
[00:04:20.420 --> 00:04:27.100]   Do you feel like there are still intrinsic cognitive limitations, like a Gary Marcus
[00:04:27.100 --> 00:04:29.100]   might say on Twitter?
[00:04:29.100 --> 00:04:35.780]   Where do you stand on the promise of this technique with transformers?
[00:04:35.780 --> 00:04:42.820]   I'm definitely like, you have the spectrum of Gary Marcus on the left, and you have,
[00:04:42.820 --> 00:04:47.580]   I think, people who are extremely enthusiastic talking about AGI on the right.
[00:04:47.580 --> 00:04:49.060]   I'm squarely in the middle.
[00:04:49.060 --> 00:04:51.260]   Oh no, this is going to be boring.
[00:04:51.260 --> 00:04:52.260]   Yes, yes.
[00:04:52.260 --> 00:04:58.060]   I mean, I can tell you some things that are very controversial.
[00:04:58.060 --> 00:05:07.060]   So no, I think Gary really overdoes it because the progress is undeniable.
[00:05:07.060 --> 00:05:09.620]   I mean, everybody seeing the systems is surprised.
[00:05:09.620 --> 00:05:13.620]   I mean, I've been in this space for more than 20 years and I look at the stuff and I'm blown
[00:05:13.620 --> 00:05:14.620]   away.
[00:05:14.620 --> 00:05:15.620]   Yeah, totally.
[00:05:15.620 --> 00:05:18.380]   And you had asked me a year ago, will we have made this progress?
[00:05:18.380 --> 00:05:19.380]   And I would have guessed it.
[00:05:19.380 --> 00:05:21.940]   I thought that this task were higher.
[00:05:21.940 --> 00:05:27.460]   But the thing what happened is that the more you get closer to human level intelligence,
[00:05:27.460 --> 00:05:30.380]   the more you realize that the task is much harder.
[00:05:30.380 --> 00:05:35.540]   And so some people are like, oh my God, we're going to lose our job as developers, as creators.
[00:05:35.540 --> 00:05:36.540]   No way that's going to happen.
[00:05:36.540 --> 00:05:41.940]   I mean, like we're still millions away because as soon as you make some progress, you realize
[00:05:41.940 --> 00:05:47.580]   that, and it's something people have said, but it is that the goalpost actually looks
[00:05:47.580 --> 00:05:51.620]   further because you realize actually intelligence is a much wider space.
[00:05:51.620 --> 00:05:52.620]   It's much more complicated.
[00:05:52.620 --> 00:05:57.380]   So you realize that the system still makes very, very silly mistakes that humans wouldn't
[00:05:57.380 --> 00:06:00.940]   make, but it does things that you didn't think would be possible.
[00:06:00.940 --> 00:06:04.980]   So I am squarely in the middle, which is, I don't think we are anywhere close to human
[00:06:04.980 --> 00:06:05.980]   intelligence.
[00:06:05.980 --> 00:06:07.900]   I also think that AGI is a volition term.
[00:06:07.900 --> 00:06:13.220]   It doesn't mean anything because intelligence is by definition, never general.
[00:06:13.220 --> 00:06:17.980]   And then I don't buy Gary because look, you can't deny the progress.
[00:06:17.980 --> 00:06:22.100]   It's like, you look a little bit like a fool if you deny that.
[00:06:22.100 --> 00:06:25.180]   But it's such a much bigger problem than people imagine.
[00:06:25.180 --> 00:06:28.420]   So as we'd say at MetaFacebook, we're 1% down.
[00:06:28.420 --> 00:06:29.420]   I don't really believe it.
[00:06:29.420 --> 00:06:35.140]   We're 1% down, but we did go 1% out of the way and that's a huge accomplishment.
[00:06:35.140 --> 00:06:36.860]   1% what?
[00:06:36.860 --> 00:06:38.580]   1% to human intelligence.
[00:06:38.580 --> 00:06:39.580]   So we made progress.
[00:06:39.580 --> 00:06:40.580]   Oh, I see.
[00:06:40.580 --> 00:06:41.580]   Right, right, right.
[00:06:41.580 --> 00:06:43.340]   We made real progress, right?
[00:06:43.340 --> 00:06:50.740]   But it's such a, intelligence is so amazing that you still have a long way to go.
[00:06:50.740 --> 00:06:57.700]   And don't you feel like the stuff that we're building is starting to help building the
[00:06:57.700 --> 00:06:59.060]   next generation of that stuff?
[00:06:59.060 --> 00:07:03.340]   Like I kind of can't believe how well the code generation works.
[00:07:03.340 --> 00:07:04.340]   Like I've been using it.
[00:07:04.340 --> 00:07:06.700]   That one is also super overstated.
[00:07:06.700 --> 00:07:09.340]   I mean, absolutely.
[00:07:09.340 --> 00:07:12.180]   Because I mean, you are in software, right?
[00:07:12.180 --> 00:07:15.660]   I mean, like I give you a piece of code, okay.
[00:07:15.660 --> 00:07:19.060]   And I tell you it's 99% accurate.
[00:07:19.060 --> 00:07:20.060]   How good does it give you?
[00:07:20.060 --> 00:07:22.620]   I think the problem is that generating code that's not accurate.
[00:07:22.620 --> 00:07:27.260]   I mean, sometimes finding a bug is way harder than writing the code from scratch.
[00:07:27.260 --> 00:07:32.700]   So I think the way to think of codecs and this stuff like that, it's like an auto-complete.
[00:07:32.700 --> 00:07:36.860]   It's a very smart auto-complete the same way when you write your email right now, Gmail
[00:07:36.860 --> 00:07:41.580]   does auto-complete and it can complete sentences and it's quite smart and it's quite impressive.
[00:07:41.580 --> 00:07:44.380]   And if you cherry pick the results, it looks amazing.
[00:07:44.380 --> 00:07:46.980]   And it's very surprising what it can do.
[00:07:46.980 --> 00:07:51.660]   But it writes something and then you have to say like, "Well, is it actually accurate?"
[00:07:51.660 --> 00:07:52.660]   You don't have guarantees.
[00:07:52.660 --> 00:07:56.940]   And not having guarantees in code is a huge, huge problem.
[00:07:56.940 --> 00:08:01.740]   I mean, like really bug-free code is worth a million times code.
[00:08:01.740 --> 00:08:03.860]   I mean, it's not the size of the code that matters.
[00:08:03.860 --> 00:08:05.740]   So I'm really cautious on this one.
[00:08:05.740 --> 00:08:09.100]   I do think it's a useful developer tool.
[00:08:09.100 --> 00:08:10.220]   People will use it.
[00:08:10.220 --> 00:08:13.980]   Like they use auto-complete to write email, but it's not going to put developers out of
[00:08:13.980 --> 00:08:14.980]   a job.
[00:08:14.980 --> 00:08:15.980]   No way.
[00:08:15.980 --> 00:08:22.980]   And especially it's tricky when you write code because you need to have guarantees.
[00:08:22.980 --> 00:08:27.580]   >>David: Well, I certainly feel like it helps me write code faster.
[00:08:27.580 --> 00:08:29.980]   So I imagine better versions of it could...
[00:08:29.980 --> 00:08:34.980]   I mean, it seems very far from putting someone out of a job, but it seems like it could make
[00:08:34.980 --> 00:08:35.980]   people more productive.
[00:08:35.980 --> 00:08:38.980]   >>Lucien: It may write you faster, but is it better or is it worse?
[00:08:38.980 --> 00:08:39.980]   >>David: Yeah.
[00:08:39.980 --> 00:08:42.300]   >>Lucien: It's definitely, you can write worse code faster.
[00:08:42.300 --> 00:08:44.300]   I'll give you that.
[00:08:44.300 --> 00:08:46.380]   That's for sure.
[00:08:46.380 --> 00:08:48.260]   Is it really allowing you to write...
[00:08:48.260 --> 00:08:53.140]   I think it will, and I also believe it will make people faster.
[00:08:53.140 --> 00:08:56.140]   But how much will depend on the validity of the code?
[00:08:56.140 --> 00:09:01.460]   If you had a system that could guarantee you that the code is accurate, that would be like
[00:09:01.460 --> 00:09:02.540]   a complete revolution.
[00:09:02.540 --> 00:09:03.620]   This is not what it is, right?
[00:09:03.620 --> 00:09:08.820]   And so I think it's all, again, having guarantees and having control over the outputs is something
[00:09:08.820 --> 00:09:12.420]   that's really one of the big challenge of these models.
[00:09:12.420 --> 00:09:14.140]   Making sure that what it says is accurate.
[00:09:14.140 --> 00:09:15.140]   That's another thing.
[00:09:15.140 --> 00:09:17.700]   These language model, they hallucinate.
[00:09:17.700 --> 00:09:19.700]   Avoiding that is really, really, really tricky.
[00:09:19.700 --> 00:09:20.700]   >>David: Yeah.
[00:09:20.700 --> 00:09:27.860]   I guess going back to my earlier question, now we're seeing a whole bunch of different
[00:09:27.860 --> 00:09:34.140]   big models coming out that all seem functionally like transformers, trained in a huge corpus
[00:09:34.140 --> 00:09:40.060]   at basically all text that anyone can find as far as I can tell at high volume.
[00:09:40.060 --> 00:09:47.220]   Do you feel like the research is converging on this one technique or do you feel like
[00:09:47.220 --> 00:09:52.380]   DeepMind and Meta have different strategies and points of view there?
[00:09:52.380 --> 00:10:01.340]   >>Lucien: Well, actually, you should have seen Yan's tweet a few days back and he's
[00:10:01.340 --> 00:10:02.420]   like, "Hey, it's weird.
[00:10:02.420 --> 00:10:06.380]   Nobody talks about reinforcement learning anymore."
[00:10:06.380 --> 00:10:13.500]   And so, which is, Yan had said, I don't remember, he said, "Oh, that means we don't really need
[00:10:13.500 --> 00:10:14.500]   the chair anymore."
[00:10:14.500 --> 00:10:16.500]   I don't know if you remember this metaphor of the cake, right?
[00:10:16.500 --> 00:10:20.860]   To say, "Okay, the chair is the reinforcement learning and supervised learning is the icing
[00:10:20.860 --> 00:10:22.300]   and the body of the cake."
[00:10:22.300 --> 00:10:27.020]   The Zuri was unsupervised and is self-supervised.
[00:10:27.020 --> 00:10:31.540]   And so, he really, I think, predicted that it would happen.
[00:10:31.540 --> 00:10:36.220]   It has happened, which is like, and it's from an information theory perspective makes sense.
[00:10:36.220 --> 00:10:39.660]   When you do reinforcement learning, you get very little information whether you're right
[00:10:39.660 --> 00:10:40.660]   or wrong.
[00:10:40.660 --> 00:10:44.020]   It's kind of like a binary yes/no, you are going in the right direction.
[00:10:44.020 --> 00:10:47.420]   With supervision, you just use just a label.
[00:10:47.420 --> 00:10:50.140]   And with self-supervision, it's where you use the whole data.
[00:10:50.140 --> 00:10:55.060]   And so, maximizing the information you get out of the data is definitely the trend.
[00:10:55.060 --> 00:10:57.420]   And I think that's where we're going.
[00:10:57.420 --> 00:11:00.460]   You see self-supervision happening in every area of the field.
[00:11:00.460 --> 00:11:07.140]   The flip side also is, transformers are just working amazingly well and scale is working
[00:11:07.140 --> 00:11:08.140]   amazingly well.
[00:11:08.140 --> 00:11:10.780]   And the combination of all this, right now it's a trend.
[00:11:10.780 --> 00:11:17.020]   I don't think we have a secret sauce that would be, or we had as you know, a long ago
[00:11:17.020 --> 00:11:18.020]   there.
[00:11:18.020 --> 00:11:19.020]   Right, right, right.
[00:11:19.020 --> 00:11:20.020]   Interesting.
[00:11:20.020 --> 00:11:25.940]   Do you feel this concern that very few people will be able to do this training at large
[00:11:25.940 --> 00:11:26.940]   scale?
[00:11:26.940 --> 00:11:31.300]   What do you think academic institutions do in a world where the most exciting results
[00:11:31.300 --> 00:11:34.500]   are coming from very, very high volume training?
[00:11:34.500 --> 00:11:38.340]   Yeah, it is concerning.
[00:11:38.340 --> 00:11:43.060]   And I can tell you that the costs of this system and these models, I mean, just before
[00:11:43.060 --> 00:11:48.720]   I left, we put online one of the biggest super cluster out there.
[00:11:48.720 --> 00:11:50.780]   And it's just extremely expensive.
[00:11:50.780 --> 00:11:58.100]   I can't tell you what the cost, but it's staggeringly expensive.
[00:11:58.100 --> 00:12:01.340]   So yes, it is worrisome and it does work.
[00:12:01.340 --> 00:12:06.340]   But I do believe that we are kind of wasteful in the way we do things today.
[00:12:06.340 --> 00:12:07.580]   We're not really optimizing.
[00:12:07.580 --> 00:12:12.540]   It was very interesting to see stable diffusion come out really quickly after DALY.
[00:12:12.540 --> 00:12:16.820]   So I'm a huge proponent of like, you know, open sourcing of open models.
[00:12:16.820 --> 00:12:22.100]   I'm actually, you know, Meta had done it with OPT-175, you know, but it was cool to see
[00:12:22.100 --> 00:12:26.580]   stable diffusion come out after DALY, but not only like releasing open source, but also
[00:12:26.580 --> 00:12:28.220]   shrink wrapping.
[00:12:28.220 --> 00:12:32.620]   So now that I'm by myself, I actually have been running it on my own computer or on a
[00:12:32.620 --> 00:12:35.740]   Colab, you know, it's pretty cheap and that's kind of cool.
[00:12:35.740 --> 00:12:41.020]   I'm like, well, okay, I haven't been able to train my own version yet, but at least
[00:12:41.020 --> 00:12:42.020]   it's a bit more manageable.
[00:12:42.020 --> 00:12:48.500]   But overall, I am a little worried and I'm not seeing, you know, how we can avoid this,
[00:12:48.500 --> 00:12:53.340]   even how well it works, but we also have efficiency gains we can make, you know.
[00:12:53.340 --> 00:12:56.900]   You know, we always talk about sort of like the practical applications here and how they're
[00:12:56.900 --> 00:12:59.720]   different than research.
[00:12:59.720 --> 00:13:04.720]   Can you talk a little bit about at Meta, like what were the applications that like really
[00:13:04.720 --> 00:13:08.860]   mattered to Meta that they were using and how that kind of differed from the research
[00:13:08.860 --> 00:13:09.860]   interests?
[00:13:09.860 --> 00:13:15.700]   Let me ask you a question because that's something I feel like when people overbuzz AI, you know,
[00:13:15.700 --> 00:13:18.660]   I ask them, it's like, okay, what did AI change in your life?
[00:13:18.660 --> 00:13:21.540]   You know, I mean, can you answer that question?
[00:13:21.540 --> 00:13:22.540]   Yes, in your life.
[00:13:22.540 --> 00:13:26.700]   What did AI change really, truly, you know, something, don't tell me like you set a timer
[00:13:26.700 --> 00:13:29.380]   on Alexa or Google Home.
[00:13:29.380 --> 00:13:30.380]   Okay.
[00:13:30.380 --> 00:13:31.380]   That's not a life changing.
[00:13:31.380 --> 00:13:33.500]   So what was life changing that came from AI, right?
[00:13:33.500 --> 00:13:34.500]   That's interesting.
[00:13:34.500 --> 00:13:39.660]   I mean, I feel like my life is not that different than, you know, someone in the, in the eighties,
[00:13:39.660 --> 00:13:45.420]   but by that sense, I actually love, I love listening to music with a, like an agent where
[00:13:45.420 --> 00:13:49.900]   I can just request it by, by saying it, it's like delightful, but I wouldn't say it's like
[00:13:49.900 --> 00:13:52.940]   life changing.
[00:13:52.940 --> 00:13:58.500]   I mean, I assume that like all the recommendation systems that I interact with probably guide
[00:13:58.500 --> 00:13:59.500]   me.
[00:13:59.500 --> 00:14:00.780]   I mean, I feel mostly happy about that.
[00:14:00.780 --> 00:14:04.380]   Like I do feel like, I remember when Amazon kind of first came out with the recommendation
[00:14:04.380 --> 00:14:06.500]   system, it just felt so great.
[00:14:06.500 --> 00:14:11.500]   It was like, there's a whole world of like books that I want to read that I didn't know
[00:14:11.500 --> 00:14:12.500]   about.
[00:14:12.500 --> 00:14:15.820]   So that might be the most, I don't know.
[00:14:15.820 --> 00:14:16.820]   What do you, what do you think?
[00:14:16.820 --> 00:14:17.820]   I mean, you've probably thought about this more than me.
[00:14:17.820 --> 00:14:18.820]   It's a good point.
[00:14:18.820 --> 00:14:20.060]   So I mean, actually it's interesting what you say, right?
[00:14:20.060 --> 00:14:23.660]   So I will challenge that the first one, I mean, I don't think many people knew that
[00:14:23.660 --> 00:14:26.260]   life changing is that I can ask something for a musician to play it.
[00:14:26.260 --> 00:14:27.260]   Yeah, life changing is way too strong.
[00:14:27.260 --> 00:14:28.260]   Yeah, for sure.
[00:14:28.260 --> 00:14:29.260]   But it is true.
[00:14:29.260 --> 00:14:33.100]   So to answer your question, you guessed right, which is, you know, a place like Meta, recommender
[00:14:33.100 --> 00:14:35.980]   systems are just hugely impactful.
[00:14:35.980 --> 00:14:41.660]   And in two areas, you know, one is advertisements and the other is organic recommendation.
[00:14:41.660 --> 00:14:47.860]   And just that, you know, by the time I left, my team was a few thousand people and justify
[00:14:47.860 --> 00:14:53.220]   the entirety of the budget, you know, by far, you know, multiple, the ROI of investing in
[00:14:53.220 --> 00:14:57.580]   this system, you know, with larger scale, especially you can imagine an advertisement
[00:14:57.580 --> 00:14:59.220]   is really staggering.
[00:14:59.220 --> 00:15:04.020]   So if you ask me, that's actually, it's kind of disappointing if you think about it, but
[00:15:04.020 --> 00:15:09.140]   the most, you know, the most successful application of AI so far has been advertisements.
[00:15:09.140 --> 00:15:13.780]   And I would say maybe the second most successful has been recommender system in like, you know,
[00:15:13.780 --> 00:15:16.420]   apps like TikTok, for example.
[00:15:16.420 --> 00:15:17.420]   But it's kind of behind the scenes.
[00:15:17.420 --> 00:15:18.660]   Wait, wait, wait, but actually you're a search guy.
[00:15:18.660 --> 00:15:20.700]   Don't you think maybe, I should have said search.
[00:15:20.700 --> 00:15:22.540]   I feel like web search is incredible.
[00:15:22.540 --> 00:15:25.860]   No, because web search came out without AI, right?
[00:15:25.860 --> 00:15:26.860]   I mean, I mean, that's true.
[00:15:26.860 --> 00:15:32.820]   The whole history of AI at Google, I was, I would like to be a fly on the wall there.
[00:15:32.820 --> 00:15:37.620]   Actually there was a, I assume that I got interviewed by a carousel show just recently
[00:15:37.620 --> 00:15:44.940]   and he was talking about how, how much reluctance there was at Google to use AI in search, you
[00:15:44.940 --> 00:15:49.060]   know, like actually it's a, it's a fairly recent story actually.
[00:15:49.060 --> 00:15:54.860]   And today, even some people, I mean, I do think actually AI is very useful in search,
[00:15:54.860 --> 00:15:57.420]   and I wouldn't put that in the category of kind of behind the scene.
[00:15:57.420 --> 00:16:00.620]   You don't really understand what it's doing, you know.
[00:16:00.620 --> 00:16:01.820]   But it's also a late story.
[00:16:01.820 --> 00:16:07.020]   Whereas in recommender system and ads, I think it came much earlier as a fundamental block.
[00:16:07.020 --> 00:16:10.820]   Whereas I think Google worked pretty well early on with traditional information retrieval
[00:16:10.820 --> 00:16:13.380]   techniques, you know.
[00:16:13.380 --> 00:16:14.380]   But so you're right.
[00:16:14.380 --> 00:16:17.580]   I mean, if you ask me directly a question, recommenders are the big thing.
[00:16:17.580 --> 00:16:21.660]   The second big thing, which is especially when I, you know, I was there, it was moderation.
[00:16:21.660 --> 00:16:24.340]   So moderation at scale can only be done with AI.
[00:16:24.340 --> 00:16:29.460]   You know, so moderation at scale is done, and I think you can look at the stats as a
[00:16:29.460 --> 00:16:34.620]   report, it's done every three months, but now we are up to like high nineties, you know,
[00:16:34.620 --> 00:16:35.620]   in most of the things.
[00:16:35.620 --> 00:16:42.700]   Even though there were 30,000 people doing manual moderation that pair with the AI, the
[00:16:42.700 --> 00:16:45.580]   amount of data to process is so great, right?
[00:16:45.580 --> 00:16:50.100]   That majority of the first action is done by AI, you know, in the 95% plus.
[00:16:50.100 --> 00:16:55.580]   For things like, you know, hate speech or bullying or a lot of complex problems.
[00:16:55.580 --> 00:17:00.100]   Doesn't mean it works perfectly, but it creates enough friction that I think it doesn't make
[00:17:00.100 --> 00:17:02.900]   the system overall much better.
[00:17:02.900 --> 00:17:10.100]   And what do you scale up to that, you know, that massive like volume and sort of the massive
[00:17:10.100 --> 00:17:11.100]   volume of inference?
[00:17:11.100 --> 00:17:16.020]   Like what sort of changes about how you approach a problem like that?
[00:17:16.020 --> 00:17:20.700]   Like say, you know, moderation at scale, trying to moderate everything that's coming into
[00:17:20.700 --> 00:17:21.700]   Facebook.
[00:17:21.700 --> 00:17:29.220]   I mean, I don't know if you're asking in terms of like the actual application or the support
[00:17:29.220 --> 00:17:30.620]   of that application, you know?
[00:17:30.620 --> 00:17:35.060]   So that is, so support application is very, very hard.
[00:17:35.060 --> 00:17:39.660]   I mean, like the whole MNOps aspect is just, you know, and we could discuss that.
[00:17:39.660 --> 00:17:40.660]   It's really, really hard.
[00:17:40.660 --> 00:17:43.980]   I don't think in my tenure at Facebook Meta, we solved it.
[00:17:43.980 --> 00:17:45.820]   We solved some part of it, especially with PyTorch.
[00:17:45.820 --> 00:17:48.580]   I think it was great success, but after it's hard.
[00:17:48.580 --> 00:17:55.060]   So all the system that evolved quickly at scale, very, very hard.
[00:17:55.060 --> 00:18:01.380]   On the other side, from a user perspective, you know, scale is tricky because you can
[00:18:01.380 --> 00:18:02.860]   have the impression it works well.
[00:18:02.860 --> 00:18:06.260]   So all our stats show, hey, we made a lot of progress.
[00:18:06.260 --> 00:18:09.700]   You should look at since we introduced AI and hate speech, you know, the amount of hate
[00:18:09.700 --> 00:18:11.820]   speech in the platform went down three X.
[00:18:11.820 --> 00:18:17.340]   Unfortunately, that doesn't mean that's the experience of people and it doesn't mean it's
[00:18:17.340 --> 00:18:19.940]   true for anybody, anywhere in the world.
[00:18:19.940 --> 00:18:21.260]   Very, very interesting problem.
[00:18:21.260 --> 00:18:23.340]   The experience, for example, is very interesting.
[00:18:23.340 --> 00:18:29.140]   It doesn't matter if you like match your policies and you remove hate speech, what matters actually
[00:18:29.140 --> 00:18:30.860]   how people experience your product.
[00:18:30.860 --> 00:18:32.940]   And that's a very different story.
[00:18:32.940 --> 00:18:36.980]   And then the experience of people depends a lot on from where they are in the world.
[00:18:36.980 --> 00:18:40.140]   And you know, the language aspect, the cultural aspects are very, very important there.
[00:18:40.140 --> 00:18:44.740]   You know, it's interesting that you say, actually, I was kind of curious about both sort of the
[00:18:44.740 --> 00:18:50.220]   technical and non-technical challenges, but you know, since you bring up PyTorch, I would
[00:18:50.220 --> 00:18:55.580]   not have thought that PyTorch was something that you think of as sort of like helping
[00:18:55.580 --> 00:18:56.580]   with the operations.
[00:18:56.580 --> 00:19:00.900]   I feel like when it came out, it seemed oriented more towards research, but I guess maybe I'm
[00:19:00.900 --> 00:19:01.900]   wrong there.
[00:19:01.900 --> 00:19:03.700]   Oh yeah, that's a long story.
[00:19:03.700 --> 00:19:05.300]   So I can tell you a little bit of the story how it happened.
[00:19:05.300 --> 00:19:06.300]   Tell me the story, please.
[00:19:06.300 --> 00:19:07.300]   Yeah.
[00:19:07.300 --> 00:19:11.500]   So I joined Facebook at the time, right in 2018.
[00:19:11.500 --> 00:19:18.260]   The company had decided to go on a dual path, you know, with PyTorch, Café2 and Onyx in
[00:19:18.260 --> 00:19:20.140]   the middle.
[00:19:20.140 --> 00:19:23.900]   And I thought, man, that's just such a hack.
[00:19:23.900 --> 00:19:24.900]   That's a non-decision.
[00:19:24.900 --> 00:19:28.460]   I think the decision was made two months before I arrived.
[00:19:28.460 --> 00:19:31.940]   And it's the one thing, usually when you go and join a company like this, you do not want
[00:19:31.940 --> 00:19:32.940]   to make decisions early.
[00:19:32.940 --> 00:19:36.180]   But this is one decision where I told the team, actually, I didn't say, hey, we should
[00:19:36.180 --> 00:19:37.180]   do PyTorch.
[00:19:37.180 --> 00:19:39.260]   I'm like, no way we're going to do this.
[00:19:39.260 --> 00:19:44.220]   And we need, from experience, I knew that we needed to be on a platform that had community
[00:19:44.220 --> 00:19:45.700]   support.
[00:19:45.700 --> 00:19:50.380]   And so I told the team, okay, you're going to have to pick, you know, one framework that
[00:19:50.380 --> 00:19:54.580]   we know will have traction in the community.
[00:19:54.580 --> 00:19:58.100]   And they were honest and they knew that that could not be Café2 at the time.
[00:19:58.100 --> 00:19:59.900]   The community support there really dropped.
[00:19:59.900 --> 00:20:03.420]   PyTorch was a rising star, but not production ready.
[00:20:03.420 --> 00:20:07.220]   And really the only one that had all these aspects was TensorFlow at the time.
[00:20:07.220 --> 00:20:14.580]   But the team was convinced that the model of PyTorch was better and allowing to do more
[00:20:14.580 --> 00:20:17.660]   dynamic graphs.
[00:20:17.660 --> 00:20:21.100]   And so they came back and said, hey, we think we can make it happen.
[00:20:21.100 --> 00:20:26.420]   And we can make PyTorch a contender, both on the research front and the production front.
[00:20:26.420 --> 00:20:28.720]   And that's where the company bet.
[00:20:28.720 --> 00:20:35.500]   And for the past four years after the decision, we've been moving almost everything at Meta
[00:20:35.500 --> 00:20:37.100]   from Café2 to PyTorch.
[00:20:37.100 --> 00:20:38.100]   And people love PyTorch.
[00:20:38.100 --> 00:20:40.580]   So it's not actually a hard thing to convince people.
[00:20:40.580 --> 00:20:42.220]   It's just amazing.
[00:20:42.220 --> 00:20:47.940]   It's a better tool to do exploration, but it didn't mean we had all the MLFs around
[00:20:47.940 --> 00:20:48.940]   it.
[00:20:48.940 --> 00:20:51.820]   And to this day, we still are trying to really figure it out.
[00:20:51.820 --> 00:20:54.180]   So it's not easy, but it was the right choice.
[00:20:54.180 --> 00:20:59.580]   PyTorch definitely, as you share a scene, it's a product that people love and you want
[00:20:59.580 --> 00:21:01.780]   to start from that.
[00:21:01.780 --> 00:21:05.820]   And so that gave us a lot of traction, that's the right direction, but they still lack a
[00:21:05.820 --> 00:21:07.780]   lot of the infrastructure around it.
[00:21:07.780 --> 00:21:10.340]   And there are a lot of reasons for that, and we could discuss it.
[00:21:10.340 --> 00:21:14.380]   Well, do you have a theory of why it's so loved?
[00:21:14.380 --> 00:21:19.460]   Because we watched this firsthand, when we started, Weights & Biases, TensorFlow had
[00:21:19.460 --> 00:21:24.500]   a clear lead and we watched PyTorch overtake it just on our own logs.
[00:21:24.500 --> 00:21:27.900]   It was a really dramatic shift.
[00:21:27.900 --> 00:21:35.140]   And it's funny because from my perspective, and I've dabbled with both, they seem pretty
[00:21:35.140 --> 00:21:36.460]   feature comparable to me.
[00:21:36.460 --> 00:21:41.380]   I mean, there was sort of in the early days, there was obviously PyTorch had the sort of
[00:21:41.380 --> 00:21:44.020]   just-in-time generation of the graph.
[00:21:44.020 --> 00:21:50.180]   But do you have a theory about why PyTorch seems like it was so much better loved?
[00:21:50.180 --> 00:21:53.380]   Yeah, I mean, I'll give you another little anecdote.
[00:21:53.380 --> 00:21:57.780]   The reason actually I felt stronger with this when I joined Meta is before I joined, in
[00:21:57.780 --> 00:22:03.540]   my team, I remember we had also this problem.
[00:22:03.540 --> 00:22:09.140]   At the time you had Theano, you had like other systems, and we were a small team and I was
[00:22:09.140 --> 00:22:11.300]   in a startup, and we were a small team.
[00:22:11.300 --> 00:22:12.300]   We already had a few frameworks.
[00:22:12.300 --> 00:22:13.300]   I said, "We can't do this.
[00:22:13.300 --> 00:22:14.300]   We've got to agree on one."
[00:22:14.300 --> 00:22:16.660]   And so I think we agreed on one.
[00:22:16.660 --> 00:22:18.820]   I think it was like TensorFlow.
[00:22:18.820 --> 00:22:21.020]   And six months later, they're like, "No, no, no.
[00:22:21.020 --> 00:22:22.020]   We got to use PyTorch.
[00:22:22.020 --> 00:22:23.020]   No way we can."
[00:22:23.020 --> 00:22:24.540]   And I'm like, "We made the decision."
[00:22:24.540 --> 00:22:25.540]   And we went to PyTorch.
[00:22:25.540 --> 00:22:26.540]   I'm like, "Oh, okay.
[00:22:26.540 --> 00:22:27.980]   There is something there."
[00:22:27.980 --> 00:22:33.140]   I actually think that the reason is simple, is that the people who developed PyTorch,
[00:22:33.140 --> 00:22:36.140]   Thumis in particular, had a design mindset.
[00:22:36.140 --> 00:22:41.580]   If I were in the mantra, it was actually a user-centric design.
[00:22:41.580 --> 00:22:43.900]   And it's funny because I think the people who did it didn't necessarily know it.
[00:22:43.900 --> 00:22:48.060]   They only showed it in the new technology, but it really definitely had the researcher
[00:22:48.060 --> 00:22:50.140]   in mind and what they wanted to do.
[00:22:50.140 --> 00:22:51.140]   And you can feel it.
[00:22:51.140 --> 00:22:53.300]   The problem with TensorFlow is that it was retrofitted.
[00:22:53.300 --> 00:22:58.660]   So even if now, because of influence, it's there, it has been plugged on top, it still
[00:22:58.660 --> 00:23:01.500]   feels like it's coupled up together.
[00:23:01.500 --> 00:23:03.700]   And it's hard to acquire the love.
[00:23:03.700 --> 00:23:05.060]   You can lose it.
[00:23:05.060 --> 00:23:06.060]   It's hard to gain.
[00:23:06.060 --> 00:23:11.940]   And so it's really about user-friendliness, researcher-friendliness, actually.
[00:23:11.940 --> 00:23:16.900]   And I think also the fact that research is driving the narrative in AI today.
[00:23:16.900 --> 00:23:18.540]   It's not a stable shield.
[00:23:18.540 --> 00:23:24.180]   And so that really put PyTorch at the center, I think, of that universe.
[00:23:24.180 --> 00:23:28.980]   And so what were the important pieces that you had to put around it to make it really
[00:23:28.980 --> 00:23:31.780]   work for you in a production environment?
[00:23:31.780 --> 00:23:35.420]   I mean, the challenge with PyTorch, I tell you, the really...
[00:23:35.420 --> 00:23:40.620]   complex stuff is that it's almost like an anti-pattern.
[00:23:40.620 --> 00:23:43.660]   Let me try to explain that.
[00:23:43.660 --> 00:23:51.140]   I think there's this saying that early optimization is the root of all evil.
[00:23:51.140 --> 00:23:54.980]   But the challenge with something like PyTorch is that you need to do early optimization.
[00:23:54.980 --> 00:23:56.580]   You don't have a way around it.
[00:23:56.580 --> 00:23:57.580]   Why?
[00:23:57.580 --> 00:24:02.400]   Because you need to create a system that gives a lot of flexibility to users to do a lot
[00:24:02.400 --> 00:24:09.140]   of things, yet is optimized because scale matters, efficiency and speed matter.
[00:24:09.140 --> 00:24:13.900]   So you have this constant challenge, especially in the interest of the operator internally,
[00:24:13.900 --> 00:24:15.580]   to have things that really follow.
[00:24:15.580 --> 00:24:19.780]   Like, if you couldn't do Transformers today, PyTorch would be awesome and everything else.
[00:24:19.780 --> 00:24:20.780]   Forget it, right?
[00:24:20.780 --> 00:24:21.780]   Nobody would use it, right?
[00:24:21.780 --> 00:24:25.980]   So very quickly, when you see where the trend is going, you have to go and put a very good
[00:24:25.980 --> 00:24:28.140]   operator and you need to optimize it.
[00:24:28.140 --> 00:24:32.020]   So it is constant progress there doing this.
[00:24:32.020 --> 00:24:33.020]   That's one challenge.
[00:24:33.020 --> 00:24:38.860]   The other challenge is we had to give that team, and I'm really a big believer in focus.
[00:24:38.860 --> 00:24:40.860]   And in this case, it was a constant battle.
[00:24:40.860 --> 00:24:47.060]   I say, "Hey, look, you have to focus and I cannot make it simpler for you and you cannot
[00:24:47.060 --> 00:24:48.540]   screw it up."
[00:24:48.540 --> 00:24:51.220]   One is you cannot screw the external community.
[00:24:51.220 --> 00:24:55.620]   You have to create something that people will continue loving and you cannot make it bloated.
[00:24:55.620 --> 00:24:59.820]   The problem when you start creating enterprise software, production software, it becomes
[00:24:59.820 --> 00:25:01.740]   bloated, becomes difficult to use.
[00:25:01.740 --> 00:25:02.740]   You can't do this.
[00:25:02.740 --> 00:25:07.260]   At the same time, you have to make it work for us internally.
[00:25:07.260 --> 00:25:09.500]   And it has to have all the production aspects.
[00:25:09.500 --> 00:25:13.380]   It has to be deployable, has to be production ready, which most people in the research community
[00:25:13.380 --> 00:25:15.620]   don't see, don't understand.
[00:25:15.620 --> 00:25:17.780]   So we had to have these two objectives.
[00:25:17.780 --> 00:25:19.620]   And that's hard.
[00:25:19.620 --> 00:25:23.300]   The team suffered through, but I think they did actually quite an amazing job at keeping
[00:25:23.300 --> 00:25:26.380]   it because ultimately, Meta is going there.
[00:25:26.380 --> 00:25:32.540]   It will be at 100% PyTorch in a very soon future.
[00:25:32.540 --> 00:25:36.140]   And I think the community still loves and adopts it.
[00:25:36.140 --> 00:25:41.180]   Was there some experience that you were talking about that made you understand the value of
[00:25:41.180 --> 00:25:42.180]   community support?
[00:25:42.180 --> 00:25:45.740]   Like were you using something at a different company where they didn't have the community
[00:25:45.740 --> 00:25:46.740]   support?
[00:25:46.740 --> 00:25:49.820]   You just mentioned that a couple of times that it's like so essential to use technology
[00:25:49.820 --> 00:25:52.100]   that the community believes in.
[00:25:52.100 --> 00:25:59.900]   Yeah, because I've seen companies being stuck in a dead end.
[00:25:59.900 --> 00:26:02.660]   I mean, actually, you could almost argue, I don't know, maybe they're going to help
[00:26:02.660 --> 00:26:09.580]   me for this, but PHP and hack at Facebook is a really tricky one.
[00:26:09.580 --> 00:26:10.580]   They can own it.
[00:26:10.580 --> 00:26:13.260]   I mean, Facebook is so big, then I guess, Meta is something they can own it.
[00:26:13.260 --> 00:26:16.820]   But I really think this is not good.
[00:26:16.820 --> 00:26:21.380]   I think you see it dying on the vine and you are adopting a technology that just doesn't
[00:26:21.380 --> 00:26:22.380]   progress anymore.
[00:26:22.380 --> 00:26:28.980]   And so I've seen it for many systems, I would say, but only I would say like the big data
[00:26:28.980 --> 00:26:33.340]   systems, the containerization system.
[00:26:33.340 --> 00:26:35.700]   You can see there's always kind of like one winner.
[00:26:35.700 --> 00:26:39.340]   And if you make the wrong choice, you're kind of stuck at some point moving off from it.
[00:26:39.340 --> 00:26:40.340]   Right, right.
[00:26:40.340 --> 00:26:43.740]   I thought you were going to maybe mention IBM Watson.
[00:26:43.740 --> 00:26:46.740]   I'm kind of curious what that experience is like.
[00:26:46.740 --> 00:26:51.700]   That is a very different story.
[00:26:51.700 --> 00:26:54.020]   I can tell you more about this.
[00:26:54.020 --> 00:26:57.020]   I mean, I think Watson was, I mean, a good thing for me is that I went there through
[00:26:57.020 --> 00:26:58.020]   an acquisition.
[00:26:58.020 --> 00:27:02.180]   So I had created an AI company and IBM acquired it.
[00:27:02.180 --> 00:27:03.180]   It was great for everybody.
[00:27:03.180 --> 00:27:06.300]   So I was very happy.
[00:27:06.300 --> 00:27:09.740]   But actually, I think when IBM created the Watson units, that was a bold move.
[00:27:09.740 --> 00:27:17.180]   It was really about saying, "Hey, we believe there is a commercial potential in AI."
[00:27:17.180 --> 00:27:20.180]   That was 2013.
[00:27:20.180 --> 00:27:23.340]   At the time, actually, not many people were talking.
[00:27:23.340 --> 00:27:24.620]   We're talking about AI.
[00:27:24.620 --> 00:27:28.460]   The deep learning revolution came around like 2011, 2012.
[00:27:28.460 --> 00:27:30.220]   People were seeing it's coming.
[00:27:30.220 --> 00:27:34.820]   Actually, Jopardy, the challenge when they did it with Watson, did not use deep learning,
[00:27:34.820 --> 00:27:35.820]   which is kind of interesting.
[00:27:35.820 --> 00:27:36.820]   It's a bit of a dirty secret.
[00:27:36.820 --> 00:27:38.220]   It used very little machine learning.
[00:27:38.220 --> 00:27:41.940]   It used traditional NLP and managed to get something pretty good.
[00:27:41.940 --> 00:27:44.060]   And so they made this big bet on it.
[00:27:44.060 --> 00:27:47.740]   And I think it was really obviously the right bet.
[00:27:47.740 --> 00:27:50.100]   And it was early and it was good.
[00:27:50.100 --> 00:27:51.100]   But there were challenges, right?
[00:27:51.100 --> 00:27:53.700]   The challenge is that you had to be patient.
[00:27:53.700 --> 00:27:59.780]   And so I tend to say like, you need to be impatient for profit and patient for revenue.
[00:27:59.780 --> 00:28:00.980]   And IBM did the opposite.
[00:28:00.980 --> 00:28:04.380]   They were impatient for revenue and patient for profit.
[00:28:04.380 --> 00:28:11.340]   And they did a lot of this very large engagement promising the moon that you may spend $10
[00:28:11.340 --> 00:28:12.540]   billion to make a billion.
[00:28:12.540 --> 00:28:14.980]   That's not a very good business.
[00:28:14.980 --> 00:28:20.780]   And so what I was focused on when I was there was really try to shrink RAPAE and put it
[00:28:20.780 --> 00:28:21.780]   as cloud services.
[00:28:21.780 --> 00:28:27.500]   At the time, we came up with this idea of putting AI in the cloud as services to do
[00:28:27.500 --> 00:28:28.500]   speech, to do conversation.
[00:28:28.500 --> 00:28:32.900]   And to this day, I think that's still the majority of what Watson is doing.
[00:28:32.900 --> 00:28:37.020]   I think it was very ahead of the game.
[00:28:37.020 --> 00:28:40.340]   But the other problem is IBM didn't have much of a cloud.
[00:28:40.340 --> 00:28:44.220]   So I felt a little bit stuck when I was there because I think it's the right strategy.
[00:28:44.220 --> 00:28:45.980]   I think we're getting traction.
[00:28:45.980 --> 00:28:53.300]   But I'm building on an infrastructure that's not as robust as issue on Amazon or Microsoft.
[00:28:53.300 --> 00:28:56.420]   And then you went into drug discovery, didn't you?
[00:28:56.420 --> 00:28:59.540]   Which is super hot now, I feel like.
[00:28:59.540 --> 00:29:00.540]   Is that right?
[00:29:00.540 --> 00:29:01.540]   Yeah.
[00:29:01.540 --> 00:29:04.980]   I'm very lucky to be the co-CEO of a company called Benevolent AI.
[00:29:04.980 --> 00:29:06.260]   I think it's a fascinating field.
[00:29:06.260 --> 00:29:09.820]   I'm a huge believer that it will happen.
[00:29:09.820 --> 00:29:16.540]   And so if you can see that there's a lot of promising things happening in AI.
[00:29:16.540 --> 00:29:22.100]   Even at MEDA, in the research team fair, we were doing things around understanding the
[00:29:22.100 --> 00:29:30.940]   function of proteins, looking at making prediction around free energy on small molecules and
[00:29:30.940 --> 00:29:31.940]   catalysis.
[00:29:31.940 --> 00:29:35.300]   Very interesting stuff you can do with AI today.
[00:29:35.300 --> 00:29:41.180]   Now that said, it hasn't really completely changed the field.
[00:29:41.180 --> 00:29:47.580]   I actually think that drug discovery needs a bit of what I would call a Tesla revolution,
[00:29:47.580 --> 00:29:51.300]   which is you need a tech company to take it head on.
[00:29:51.300 --> 00:29:57.180]   But it has such a huge amount of domain knowledge that it's a very hard problem.
[00:29:57.180 --> 00:30:00.100]   It's similar in some way to what Elon did with Tesla.
[00:30:00.100 --> 00:30:03.820]   It takes 15 years to understand why you have to build a car.
[00:30:03.820 --> 00:30:05.940]   And I think drug discovery is even bigger than that.
[00:30:05.940 --> 00:30:07.980]   It's even more complicated.
[00:30:07.980 --> 00:30:13.820]   But the decision process of this company is that when they approach technology, the saying
[00:30:13.820 --> 00:30:20.100]   there is there's no good model out there, but some model are more useful than others.
[00:30:20.100 --> 00:30:21.100]   Okay.
[00:30:21.100 --> 00:30:22.100]   That's what they say out there.
[00:30:22.100 --> 00:30:25.420]   And the reason is the models are more useful is because they just use them to justify the
[00:30:25.420 --> 00:30:27.900]   decision they had made before.
[00:30:27.900 --> 00:30:31.260]   So that's the way drugs are made these days.
[00:30:31.260 --> 00:30:34.300]   A lot of decision made, not a lot of data to support it.
[00:30:34.300 --> 00:30:35.300]   A lot of influence.
[00:30:35.300 --> 00:30:38.180]   You have a concept called a key opinion leader.
[00:30:38.180 --> 00:30:40.740]   That's how decisions are made there.
[00:30:40.740 --> 00:30:41.740]   I'm not a big fan.
[00:30:41.740 --> 00:30:45.540]   Influence authority, that's not, I think, how a business should be run.
[00:30:45.540 --> 00:30:47.100]   But that's how it is right now.
[00:30:47.100 --> 00:30:52.660]   So I'm really looking forward to a big disruption and maybe I'll get involved in this again.
[00:30:52.660 --> 00:30:54.780]   That would be cool.
[00:30:54.780 --> 00:31:00.380]   When we started Weights & Biases, we didn't think that we'd have many pharma customers
[00:31:00.380 --> 00:31:02.620]   and now we work with most of them.
[00:31:02.620 --> 00:31:07.500]   So it does seem like, at least the pharma companies believe pretty strongly that there's
[00:31:07.500 --> 00:31:15.220]   something there for deep learning, I think, to help with drug discovery.
[00:31:15.220 --> 00:31:20.180]   Do you have a sense for what the breakthroughs have been that have made things like AlphaFold
[00:31:20.180 --> 00:31:21.180]   work well?
[00:31:21.180 --> 00:31:26.180]   Well, yeah, I mean, there are different channels.
[00:31:26.180 --> 00:31:34.220]   But what I find remarkable is that, and I still don't quite understand it, which is
[00:31:34.220 --> 00:31:40.780]   it does seem that deep learning and especially even the transformer architecture, for example,
[00:31:40.780 --> 00:31:50.660]   are kind of able to understand grammar of things, of images, of text, but also of proteins,
[00:31:50.660 --> 00:31:52.100]   for example.
[00:31:52.100 --> 00:31:57.620]   So in a particular, we had a project where you just feed hundreds of millions of proteins
[00:31:57.620 --> 00:32:03.940]   to a language model and the system from there is able to predict function pretty well without
[00:32:03.940 --> 00:32:07.020]   having seen anything, like this very little supervised data.
[00:32:07.020 --> 00:32:11.580]   So it's something that I'm just, I'm not sure I understand because it's not like our brain
[00:32:11.580 --> 00:32:14.260]   understand molecules, right?
[00:32:14.260 --> 00:32:19.740]   But that means there's this kind of generic computation that works well in so many areas.
[00:32:19.740 --> 00:32:21.700]   And it's just still blows my mind.
[00:32:21.700 --> 00:32:26.180]   I understand that it can do it for language and for images because, I mean, humans can
[00:32:26.180 --> 00:32:31.020]   do that, but humans can't understand, can't fold molecules, understand their functions.
[00:32:31.020 --> 00:32:37.740]   And so why is it working and why can you predict, can you do quantum calculation better with,
[00:32:37.740 --> 00:32:40.300]   I don't know, it's really, really interesting.
[00:32:40.300 --> 00:32:45.300]   So it seems to me like this generic, even more than human intelligence.
[00:32:45.300 --> 00:32:50.140]   Yeah, it does seem like an opportunity to do something that humans really can't do.
[00:32:50.140 --> 00:32:51.140]   Right.
[00:32:51.140 --> 00:32:53.300]   That's the case, yes.
[00:32:53.300 --> 00:32:54.300]   Yeah.
[00:32:54.300 --> 00:32:58.980]   But there are lots, I mean, so back to your question, there are actually lots, you know,
[00:32:58.980 --> 00:33:05.940]   you have the chemistry, you have the biology, you have the clinical trials, you have patient
[00:33:05.940 --> 00:33:06.940]   data.
[00:33:06.940 --> 00:33:08.780]   There are actually many, many stages.
[00:33:08.780 --> 00:33:10.740]   There's the target identification.
[00:33:10.740 --> 00:33:15.220]   So for a bit about AI, one of the big thing we're doing is try to mine the literature
[00:33:15.220 --> 00:33:19.940]   to come up with a new graph, find new relationships, new targets.
[00:33:19.940 --> 00:33:22.340]   So it's very, very early in the game.
[00:33:22.340 --> 00:33:26.620]   Then you have companies that try to figure out, okay, given a target, what are the right
[00:33:26.620 --> 00:33:28.780]   molecules that can affect that target?
[00:33:28.780 --> 00:33:33.220]   You know, can we do some AI assisted chemistry there?
[00:33:33.220 --> 00:33:37.380]   And then there are people who try to understand better, like the biological aspects, you know,
[00:33:37.380 --> 00:33:40.300]   like how docking actually works.
[00:33:40.300 --> 00:33:44.500]   And so, and then you have like the patient data and you have like the imagery of the
[00:33:44.500 --> 00:33:46.500]   patient data and how can you understand it?
[00:33:46.500 --> 00:33:48.100]   You know, can you deduct from there?
[00:33:48.100 --> 00:33:50.340]   And then you can, you combine that with genetic information.
[00:33:50.340 --> 00:33:55.740]   So actually there are really literally like, I don't know, like dozens of places where
[00:33:55.740 --> 00:33:57.740]   it can affect.
[00:33:57.740 --> 00:34:02.500]   I was talking to a friend of mine who just started a company to think of like how to
[00:34:02.500 --> 00:34:05.220]   design what I think he called it promoters.
[00:34:05.220 --> 00:34:12.300]   So not the piece that's active, but the thing that like, for example, in an RNA based medication,
[00:34:12.300 --> 00:34:17.660]   but the thing that's going to say how much is going to be, you know, how potent it's
[00:34:17.660 --> 00:34:18.660]   going to be.
[00:34:18.660 --> 00:34:22.980]   Like the little code that you don't pay attention to DNA that usually tells you how much is
[00:34:22.980 --> 00:34:26.460]   used and how much the cell is going to be affected.
[00:34:26.460 --> 00:34:29.940]   And I had no idea this thing existed, but you need a code for that.
[00:34:29.940 --> 00:34:33.380]   It's a few hundred amino acids there.
[00:34:33.380 --> 00:34:37.060]   And so actually using AI for that might be very good.
[00:34:37.060 --> 00:34:39.980]   And the advice I gave him is like, Hey, go use transformer.
[00:34:39.980 --> 00:34:46.300]   I bet you they're going to train them on DNA and you'll figure out, but I don't know.
[00:34:46.300 --> 00:34:51.500]   But anyway, there are a lot of aspects, you know, of the process where it can, I would
[00:34:51.500 --> 00:34:55.020]   say, you know, dozens.
[00:34:55.020 --> 00:34:58.540]   And it sounds like it's something that you're excited about right now and looking into.
[00:34:58.540 --> 00:35:00.060]   Yes, it is.
[00:35:00.060 --> 00:35:01.060]   Yeah.
[00:35:01.140 --> 00:35:06.500]   I really, what excites me is, you know, how do you get, I'm convinced that you're going
[00:35:06.500 --> 00:35:12.460]   to see a lot of like what we call like business processes being improved throughout the industry.
[00:35:12.460 --> 00:35:15.820]   I think you're going to see, it's slow by the way, you know, like you're going to see
[00:35:15.820 --> 00:35:20.860]   companies adopting for part of the processes, like insurance companies and banking and healthcare.
[00:35:20.860 --> 00:35:22.260]   Like they're going to take little blocks.
[00:35:22.260 --> 00:35:27.140]   They're going to work with these B2B companies and they're going to adopt it.
[00:35:27.140 --> 00:35:31.420]   What I'm more excited with is like, you know, how do you change entirely your field, you
[00:35:31.420 --> 00:35:33.460]   know, that it's, and I think you have transportation.
[00:35:33.460 --> 00:35:38.740]   I mean, a lot of people are trying that, you know, with self-driving car or other kind of
[00:35:38.740 --> 00:35:40.780]   self-driving, you know, maybe that's going to come first.
[00:35:40.780 --> 00:35:45.020]   You have healthcare, you know, and you have drug discovery, like paired, you know, I think
[00:35:45.020 --> 00:35:49.020]   you have education as well that could be completely transformed, but I'd love to do something
[00:35:49.020 --> 00:35:53.860]   that not just like, you know, take the current companies and just like, you know, incrementally
[00:35:53.860 --> 00:35:58.220]   improve them, which I think is what is going to happen naturally, but change the game.
[00:35:58.220 --> 00:36:00.380]   I think in drug discovery, you can change the game.
[00:36:00.380 --> 00:36:01.980]   You can change the decision process.
[00:36:01.980 --> 00:36:06.060]   You can change, you know, so the attrition that you have right now that makes a drug
[00:36:06.060 --> 00:36:10.340]   cost a billion dollars would be diminished by 10X, you know.
[00:36:10.340 --> 00:36:15.980]   Well, so I totally agree with you on drug discovery and, you know, autonomous vehicles.
[00:36:15.980 --> 00:36:21.380]   I mean, you know, I think, you know, you'd be blind not to sort of see the opportunity
[00:36:21.380 --> 00:36:25.820]   there and the success that folks are having, but I don't actually know that I've seen a
[00:36:25.820 --> 00:36:27.500]   ton of success in education.
[00:36:27.500 --> 00:36:31.580]   This seems like a surprising, it seems like education actually has like the least amount
[00:36:31.580 --> 00:36:33.940]   of technology inserted into it.
[00:36:33.940 --> 00:36:35.780]   Yeah, I agree with you.
[00:36:35.780 --> 00:36:37.260]   So it's a field I'm very interested in.
[00:36:37.260 --> 00:36:44.260]   I've been looking into it and I mean, the way I put it, I actually just, like I wrote
[00:36:44.260 --> 00:36:47.740]   a little like position document for this for me recently.
[00:36:47.740 --> 00:36:53.620]   And I'm like, the way I put it is that I think education is completely in the war for attention.
[00:36:53.620 --> 00:36:56.940]   You know, education is completely outgunned today, you know.
[00:36:56.940 --> 00:37:02.700]   So if you're a teenager, right, do you want to go to a boring lecture or you want to go
[00:37:02.700 --> 00:37:08.900]   on TikTok and seeing stuff by millions of creators, you know, that really is adapted
[00:37:08.900 --> 00:37:13.100]   to your interests and understand what you like, what makes you, you know, what gets,
[00:37:13.100 --> 00:37:18.420]   you know, a system that gets you versus a system that's static and, you know, the same
[00:37:18.420 --> 00:37:21.540]   way of educating that was, you know, 500 years ago.
[00:37:21.540 --> 00:37:24.420]   So it doesn't mean there's no opportunity there.
[00:37:24.420 --> 00:37:29.980]   I think they are, but culturally it's also a difficult field, right.
[00:37:29.980 --> 00:37:33.460]   But think of it, you know, the way I put it is like, you know, like, look what's happening
[00:37:33.460 --> 00:37:34.460]   on TikTok.
[00:37:34.460 --> 00:37:38.780]   Actually kids go on TikTok, like my daughters, they send me stuff like, oh, look at this
[00:37:38.780 --> 00:37:40.380]   guy, he teaches me math on TikTok.
[00:37:40.380 --> 00:37:41.700]   I'm like, come on.
[00:37:41.700 --> 00:37:43.300]   That's entertaining.
[00:37:43.300 --> 00:37:49.540]   It's sure that's the way to do it, but it shows you the potential, you know, like to
[00:37:49.540 --> 00:37:51.060]   make it a lot more engaging.
[00:37:51.060 --> 00:37:55.620]   You know, you have to engage the user, you have to make it compelling to them.
[00:37:55.620 --> 00:37:57.580]   And I think there are technique and there is AI to do that.
[00:37:57.580 --> 00:37:59.460]   I think we understand that pretty well, actually.
[00:37:59.460 --> 00:38:03.620]   And so that I think is an opportunity.
[00:38:03.620 --> 00:38:04.620]   Interesting.
[00:38:04.620 --> 00:38:05.620]   All right.
[00:38:05.620 --> 00:38:11.300]   Excited to learn more about this.
[00:38:11.300 --> 00:38:17.580]   As someone who likes to learn, you know, I actually think, I think YouTube has become
[00:38:17.580 --> 00:38:23.620]   such an incredible educational resource, even on deep technical topics.
[00:38:23.620 --> 00:38:27.340]   And I think the voting is surprisingly effective too.
[00:38:27.340 --> 00:38:31.540]   I would have thought that it would be hard for really good educators to sort of like
[00:38:31.540 --> 00:38:38.460]   bubble up to the surface on very advanced topics, but it seems like it's a pretty good,
[00:38:38.460 --> 00:38:39.460]   I don't know.
[00:38:39.460 --> 00:38:42.420]   I guess YouTube is working well for me.
[00:38:42.420 --> 00:38:43.420]   I've been learning more math.
[00:38:43.420 --> 00:38:44.420]   I agree.
[00:38:44.420 --> 00:38:48.620]   And you look at, you know, I think that's the thing that's, I'm not sure it works for
[00:38:48.620 --> 00:38:56.540]   younger students, but I think for adult education, I think for, you know, high school education,
[00:38:56.540 --> 00:39:02.540]   you know, a lot of them start bypassing, you know, the traditional way and going to, but
[00:39:02.540 --> 00:39:04.660]   YouTube is also not an educational platform, right?
[00:39:04.660 --> 00:39:08.660]   So the other way to learn, like personally, I love learning through practice, right?
[00:39:08.660 --> 00:39:09.660]   And through exercise.
[00:39:09.660 --> 00:39:10.660]   Totally.
[00:39:10.660 --> 00:39:12.460]   So actually, I mean, I think people have different styles.
[00:39:12.460 --> 00:39:15.460]   So I have a hard time staying in front of a lecture.
[00:39:15.460 --> 00:39:19.980]   I love practice and I love something that would, the frustration I have with all the
[00:39:19.980 --> 00:39:24.140]   education system today is that they don't start by kind of constantly evaluating you.
[00:39:24.140 --> 00:39:25.140]   Like what are my gaps?
[00:39:25.140 --> 00:39:27.220]   You know, what do I need to practice next?
[00:39:27.220 --> 00:39:29.740]   What's the optimal thing that I can do next?
[00:39:29.740 --> 00:39:30.740]   Right.
[00:39:30.740 --> 00:39:34.020]   What's, you know, you know, a lot of systems today really tell you like, what is the best
[00:39:34.020 --> 00:39:35.620]   next thing I can show you?
[00:39:35.620 --> 00:39:36.620]   That's how TikTok works.
[00:39:36.620 --> 00:39:40.380]   So like, what is the thing that's going to make you really, really want to come back
[00:39:40.380 --> 00:39:42.140]   on TikTok?
[00:39:42.140 --> 00:39:43.980]   I don't think education works like this today.
[00:39:43.980 --> 00:39:48.340]   You know, what is the thing that's going to make me more informed and want to stay and
[00:39:48.340 --> 00:39:49.740]   continue that course?
[00:39:49.740 --> 00:39:51.780]   Well, I hope you work on this.
[00:39:51.780 --> 00:39:52.780]   I'm, I'm.
[00:39:52.780 --> 00:39:56.300]   Well, see, you think drug discovery is complicated.
[00:39:56.300 --> 00:40:01.780]   Oh my God, like education is also like complicated, but it's like, that's the problem, you know,
[00:40:01.780 --> 00:40:05.780]   like healthcare, education, you know, like all is drug discovery.
[00:40:05.780 --> 00:40:09.140]   And it's complex fields that are hard to disrupt, you know.
[00:40:09.140 --> 00:40:10.260]   Right, right.
[00:40:10.260 --> 00:40:17.020]   I was wondering, you know, like Meta has made this huge bet on, you know, augmented reality
[00:40:17.020 --> 00:40:18.740]   as far as I understand.
[00:40:18.740 --> 00:40:22.700]   Do you think that like machine learning has a role to play there?
[00:40:22.700 --> 00:40:26.140]   Has caused some of the interest in like AR or VR?
[00:40:26.140 --> 00:40:28.120]   It's not a space that I understand super well.
[00:40:28.120 --> 00:40:31.580]   So let me give you a framing for it, you know.
[00:40:31.580 --> 00:40:33.940]   So the challenge with this new kind of interface.
[00:40:33.940 --> 00:40:39.420]   So let's assume, which is not a guarantee, that it's going to be a set of glasses that
[00:40:39.420 --> 00:40:43.460]   you put on your head and let's say it's going to be the next platform, right?
[00:40:43.460 --> 00:40:47.060]   Because let's be honest, I think, you know, like phones are amazing invention, but they're
[00:40:47.060 --> 00:40:48.420]   kind of a frustrating invention, right?
[00:40:48.420 --> 00:40:50.420]   You have a little, little screen like this.
[00:40:50.420 --> 00:40:53.820]   I mean, like I see ourselves like always on that little screen.
[00:40:53.820 --> 00:40:57.800]   I think my prediction to you is like in 30 years, people will look back and say, my God,
[00:40:57.800 --> 00:41:01.460]   this is like the Stone Age, you know, of interfaces.
[00:41:01.460 --> 00:41:03.300]   So you know, something is going to change it.
[00:41:03.300 --> 00:41:08.100]   The thing, the challenge with glasses, right, is that it's not a imperative interface.
[00:41:08.100 --> 00:41:10.220]   Like I'm not typing.
[00:41:10.220 --> 00:41:14.580]   I'm not like in some of the phone is a little bit less imperative than a computer or a keyboard.
[00:41:14.580 --> 00:41:18.180]   That means like you're clearly telling the computer what you want.
[00:41:18.180 --> 00:41:20.300]   You know, when you type, you type the key.
[00:41:20.300 --> 00:41:21.900]   There's no ambiguity there.
[00:41:21.900 --> 00:41:26.820]   I think the touchscreen was a little bit more of an implicit interface, which is like, you
[00:41:26.820 --> 00:41:30.660]   know, oh, you know, it's not exactly sure what you're saying.
[00:41:30.660 --> 00:41:33.700]   I'm going to try to make it's actually using a little bit of machine learning underneath
[00:41:33.700 --> 00:41:35.380]   there to figure out what you're talking about.
[00:41:35.380 --> 00:41:40.060]   It's not like groundbreaking machine learning, you know, like to figure out like what exact
[00:41:40.060 --> 00:41:43.780]   word and it's using actually some of these language model when you type on your keyword.
[00:41:43.780 --> 00:41:45.780]   But imagine now you have glasses, right?
[00:41:45.780 --> 00:41:48.060]   There's no more inputs, you know?
[00:41:48.060 --> 00:41:49.060]   So what is it?
[00:41:49.060 --> 00:41:53.340]   And, you know, one of the obvious one is voice, but it's very likely that it's not going to
[00:41:53.340 --> 00:41:56.140]   be a lot just voice for sure.
[00:41:56.140 --> 00:41:57.260]   It's not going to be just voice.
[00:41:57.260 --> 00:41:59.540]   It's going to be kind of gesture.
[00:41:59.540 --> 00:42:00.820]   It's going to be motion.
[00:42:00.820 --> 00:42:04.780]   You know, one thing that Meta is working on is a little bracelet there.
[00:42:04.780 --> 00:42:08.820]   You know, they acquired a company that did this, I think is very, very interesting.
[00:42:08.820 --> 00:42:14.660]   So you can maybe type in the air or move your finger silently.
[00:42:14.660 --> 00:42:15.860]   It's going to be like motion.
[00:42:15.860 --> 00:42:18.460]   It's going to be trying to understand your intent.
[00:42:18.460 --> 00:42:20.380]   So the problem with glasses is you don't have a keyboard.
[00:42:20.380 --> 00:42:24.140]   You know, you can't enter an information and you can't tell a glass what you want, but
[00:42:24.140 --> 00:42:27.340]   you'll need to have a rich interface that understands you.
[00:42:27.340 --> 00:42:30.680]   And so AI has to play a role there.
[00:42:30.680 --> 00:42:31.980]   And it's a very challenging role.
[00:42:31.980 --> 00:42:36.860]   It's like creating a contextual interface, you know, that understand all the context
[00:42:36.860 --> 00:42:42.740]   around and lets you really direct the system you have on your face.
[00:42:42.740 --> 00:42:45.900]   This is probably a speech interface, I'm guessing?
[00:42:45.900 --> 00:42:51.620]   Well, speech, the problem is that so speech is part of it, but well, our guess is, you
[00:42:51.620 --> 00:42:57.180]   know, our guess wise is that speech may not play as big a role as you think it will.
[00:42:57.180 --> 00:42:58.660]   Because, you know, it's like, I don't know.
[00:42:58.660 --> 00:43:04.580]   I mean, you know, when can you really speak to a computer, like your phone, right?
[00:43:04.580 --> 00:43:05.580]   As Siri, right?
[00:43:05.580 --> 00:43:08.140]   How often do you use it?
[00:43:08.140 --> 00:43:09.140]   I never use it.
[00:43:09.140 --> 00:43:10.140]   So I don't use it.
[00:43:10.140 --> 00:43:12.660]   Yeah, I never use it either because it's awkward, right?
[00:43:12.660 --> 00:43:13.660]   I'm in the middle of here.
[00:43:13.660 --> 00:43:16.020]   I'm going to talk to my phone like this.
[00:43:16.020 --> 00:43:19.860]   So actually talking to the glasses while it's possible.
[00:43:19.860 --> 00:43:23.060]   And then, you know, if you saw like when I came up with the Ray-Ban and my team actually
[00:43:23.060 --> 00:43:24.060]   did the speech for it.
[00:43:24.060 --> 00:43:25.060]   It's nice.
[00:43:25.060 --> 00:43:26.060]   It works well.
[00:43:26.060 --> 00:43:27.660]   But there's a lot, many places where you want to do this.
[00:43:27.660 --> 00:43:31.860]   So maybe you want to do more motion, your gestures, other things, a combination of all
[00:43:31.860 --> 00:43:32.860]   these things.
[00:43:32.860 --> 00:43:34.980]   Tap, you know, like so.
[00:43:34.980 --> 00:43:39.620]   So the interface will be a lot more complex, multi-modal than we assume.
[00:43:39.620 --> 00:43:41.020]   It's not going to be just speech.
[00:43:41.020 --> 00:43:42.020]   Interesting.
[00:43:42.020 --> 00:43:46.780]   Okay, another totally different question that I had that I was wondering if you had a thought
[00:43:46.780 --> 00:43:52.620]   on is, you know, one thing that's been really striking is NVIDIA's like total stranglehold
[00:43:52.620 --> 00:43:53.620]   on the training market.
[00:43:53.620 --> 00:43:57.860]   I mean, there's like some stuff coming out of Google, but it doesn't seem like it has
[00:43:57.860 --> 00:44:00.740]   tons of traction, at least in training.
[00:44:00.740 --> 00:44:02.820]   Do you have a sense for why that might be?
[00:44:02.820 --> 00:44:06.740]   It's lasted a lot longer than I would have thought.
[00:44:06.740 --> 00:44:11.220]   And there's lots of startups that compete and, you know, people working on chips, but
[00:44:11.220 --> 00:44:14.060]   somehow it just doesn't seem to move.
[00:44:14.060 --> 00:44:15.060]   Oh, I know.
[00:44:15.060 --> 00:44:17.900]   I would say I know all about it.
[00:44:17.900 --> 00:44:18.900]   Okay.
[00:44:18.900 --> 00:44:22.500]   Because remember what I told you earlier, which is that these things are very expensive,
[00:44:22.500 --> 00:44:23.500]   right?
[00:44:23.500 --> 00:44:28.780]   And as a sole provider, it's very complicated and it's very expensive.
[00:44:28.780 --> 00:44:31.260]   Thankfully, now the crypto market went down.
[00:44:31.260 --> 00:44:37.940]   And so I think it's going to be a little nicer, you know, GPUs, but it did feel at time like
[00:44:37.940 --> 00:44:40.980]   a racket, you know, like we were paying for this GPU.
[00:44:40.980 --> 00:44:46.460]   But the flip side of that is NVIDIA is very good and they're very good, not just because
[00:44:46.460 --> 00:44:47.460]   of the GPU.
[00:44:47.460 --> 00:44:51.620]   So I think the GPU, especially when you come from more of a pie chart, you know, like,
[00:44:51.620 --> 00:44:55.340]   you know, exploration mode, it works well, you know, it's very multipurpose.
[00:44:55.340 --> 00:44:57.140]   I think it is very flexible.
[00:44:57.140 --> 00:44:58.900]   So that worked really well for us.
[00:44:58.900 --> 00:45:02.900]   But the thing also is NVIDIA got the software really, really well.
[00:45:02.900 --> 00:45:04.720]   You know, they really got it right.
[00:45:04.720 --> 00:45:06.380]   And they work with us amazingly well.
[00:45:06.380 --> 00:45:12.580]   They're very competent people to create that, that's, you know, CUDA layer.
[00:45:12.580 --> 00:45:14.020]   And it's hard to replace.
[00:45:14.020 --> 00:45:21.060]   And I'll tell you, like, I mean, I want it and I throw some money at, you know, to other
[00:45:21.060 --> 00:45:23.900]   people to say like, go do it or we'll do it for you.
[00:45:23.900 --> 00:45:28.700]   Like just go, you got to be able to compete, you know, but software is hard and they are
[00:45:28.700 --> 00:45:30.740]   very talented and they do a great job.
[00:45:30.740 --> 00:45:31.740]   And that's why I got them there.
[00:45:31.740 --> 00:45:35.260]   You know, they just have the best software, they have great hardware and have the best
[00:45:35.260 --> 00:45:37.300]   software stack on top of it.
[00:45:37.300 --> 00:45:42.100]   And if you're serious, it's still the best, best in town.
[00:45:42.100 --> 00:45:47.220]   And even if you compare it to like, you know, the TPU, the benchmarks, you know, are comparable,
[00:45:47.220 --> 00:45:50.220]   yet the GPU is way more flexible.
[00:45:50.220 --> 00:45:54.940]   So unless you have some workloads, you know, I think it works well for ads, for Google,
[00:45:54.940 --> 00:45:59.380]   the TPU can be competitive, but for the rest, actually, GPU is still the best game in town
[00:45:59.380 --> 00:46:02.420]   and they have a great software stack on top, you know.
[00:46:02.420 --> 00:46:07.260]   You would think like more specialized systems would work in more specialized cases, wouldn't
[00:46:07.260 --> 00:46:08.260]   you?
[00:46:08.260 --> 00:46:11.200]   It's kind of amazing that the flexible system also seems to function the best for almost
[00:46:11.200 --> 00:46:12.200]   all use cases.
[00:46:12.200 --> 00:46:14.620]   Yeah, but think of this thing, like the challenge we had, right?
[00:46:14.620 --> 00:46:20.980]   So imagine like, okay, you try to design a chip and you design it when the big game in
[00:46:20.980 --> 00:46:27.820]   town or CNN and STMs and, you know, and you know, a lot of, I mean, the recommendations,
[00:46:27.820 --> 00:46:31.100]   a lot of, you know, sparse networks.
[00:46:31.100 --> 00:46:35.740]   And then you wake up three years later and like everything has changed and the game has
[00:46:35.740 --> 00:46:36.740]   changed, you know.
[00:46:36.740 --> 00:46:40.300]   Now it's Transformer and actually like dense network can start to be really relevant to
[00:46:40.300 --> 00:46:43.020]   do also recommendation, you know.
[00:46:43.020 --> 00:46:46.300]   And you design your chip and it takes five years to get it out.
[00:46:46.300 --> 00:46:51.780]   So by the time you get it out, you know, it's already over, which, you know, many people
[00:46:51.780 --> 00:46:53.820]   are done and have done as well, you know.
[00:46:53.820 --> 00:46:56.060]   And so it's very hard.
[00:46:56.060 --> 00:46:58.800]   It's the problem I told you, it's like this early optimization, which is if you don't
[00:46:58.800 --> 00:47:03.700]   keep your options open while still optimizing what you have, you know, like you may be in
[00:47:03.700 --> 00:47:04.700]   a dead end.
[00:47:04.700 --> 00:47:05.700]   Yeah.
[00:47:05.700 --> 00:47:06.700]   Interesting.
[00:47:06.700 --> 00:47:07.700]   Well, cool.
[00:47:07.700 --> 00:47:12.700]   We always end with two questions, but I guess before that, I've just, I'm kind of channeling
[00:47:12.700 --> 00:47:17.900]   all the students that we always get in comments, you know, wherever we post these.
[00:47:17.900 --> 00:47:23.620]   I mean, I think you've had like this very enviable career in machine learning and we
[00:47:23.620 --> 00:47:26.060]   have so many students that use our software and watch these interviews.
[00:47:26.060 --> 00:47:28.620]   Do you have any advice for like students coming out?
[00:47:28.620 --> 00:47:32.300]   Like, what would you work on if you were just sort of like entering the field out of like
[00:47:32.300 --> 00:47:34.260]   undergrad or grad school?
[00:47:34.260 --> 00:47:35.780]   Like how would you think about that?
[00:47:35.780 --> 00:47:41.100]   Well, I would not like, you know, I'm not going to give you a specific, but I'll give
[00:47:41.100 --> 00:47:47.900]   you a little story actually that I got from a guy who used to study ants.
[00:47:47.900 --> 00:47:48.900]   Okay.
[00:47:48.900 --> 00:47:50.900]   So he just died recently, E.O.
[00:47:50.900 --> 00:47:56.860]   Wilson, you know, he invented a really interesting concept around evolution.
[00:47:56.860 --> 00:47:59.860]   And so he wrote a little book, you know, which is a letter to a scientist.
[00:47:59.860 --> 00:48:05.540]   And he says, you know, when I was young, I came out and I was not even, he was in his
[00:48:05.540 --> 00:48:09.100]   PhD and he decided to focus on ants.
[00:48:09.100 --> 00:48:13.780]   And the main thing is at the time that sound like a very crazy idea.
[00:48:13.780 --> 00:48:17.020]   And obviously ants, we know as a society is a very important now.
[00:48:17.020 --> 00:48:20.680]   And he became the world's specialist in it and world renowned.
[00:48:20.680 --> 00:48:25.700]   And so what I tell people, especially in science who come out is like, don't be afraid of like
[00:48:25.700 --> 00:48:31.840]   going for something that you own, that's your own thing, you know, and go for it and be
[00:48:31.840 --> 00:48:35.420]   bold about it, you know, and actually don't assume that everybody has done everything.
[00:48:35.420 --> 00:48:40.260]   You know, there's a lot of opportunity for you to own it and go for it and be focused
[00:48:40.260 --> 00:48:41.260]   on it, you know?
[00:48:41.260 --> 00:48:45.580]   And so that's what I would advise, you know, I think there's very wide space.
[00:48:45.580 --> 00:48:48.220]   There's a lot of space for everybody.
[00:48:48.220 --> 00:48:50.260]   And so be bold, be ambitious.
[00:48:50.260 --> 00:48:51.260]   Fair enough.
[00:48:51.260 --> 00:48:52.260]   All right.
[00:48:52.260 --> 00:48:53.260]   Okay.
[00:48:53.260 --> 00:48:57.740]   So the last two questions are, one is, and this is just like, it seems like you're kind
[00:48:57.740 --> 00:49:02.820]   of doing this, but if you had extra time to work on something, what would it be?
[00:49:02.820 --> 00:49:04.500]   That's what I do now.
[00:49:04.500 --> 00:49:06.460]   I told you I do kite surfing.
[00:49:06.460 --> 00:49:07.460]   Yeah.
[00:49:07.460 --> 00:49:11.820]   But if you were kite surfing all day long, what would you be looking into?
[00:49:11.820 --> 00:49:16.860]   Well, for me, I do two things because, you know, one is, you know, I was a Gundam manager
[00:49:16.860 --> 00:49:19.340]   for the past, you know, like 10 years.
[00:49:19.340 --> 00:49:23.580]   I think the last time I coded was before my company got acquired and I love coding.
[00:49:23.580 --> 00:49:27.620]   So I'm going back to coding, you know, I'm going back to getting my hands dirty, really
[00:49:27.620 --> 00:49:32.660]   understand, you know, as much as my team developed PyTorch, you know, I do really understand
[00:49:32.660 --> 00:49:33.940]   it, do I understand how it works.
[00:49:33.940 --> 00:49:37.300]   And so I spend more time, you know, doing this and it's a lot of fun.
[00:49:37.300 --> 00:49:42.420]   I think our path, you know, you know, just coming out of, out of Tesla, I said the same,
[00:49:42.420 --> 00:49:45.300]   it's just like, oh, my skin is, is cleaner.
[00:49:45.300 --> 00:49:50.660]   I sleep better, you know, like, so dealing with technical problem rather than people
[00:49:50.660 --> 00:49:53.740]   problem is always, you know, a big boost.
[00:49:53.740 --> 00:49:55.940]   That's what I'm doing, you know, really, really stay up to date.
[00:49:55.940 --> 00:49:59.620]   And I think it's really critical to understand, you know, my next stage is, oh, okay, I want
[00:49:59.620 --> 00:50:02.260]   to write a transformer from scratch, you know, what is that?
[00:50:02.260 --> 00:50:03.260]   Nice.
[00:50:03.260 --> 00:50:07.460]   And so the second I'm trying to do is really try to evaluate where the big opportunity
[00:50:07.460 --> 00:50:12.860]   is, you know, and I, I think for me, I feel like, okay, I've done the B2B startup.
[00:50:12.860 --> 00:50:15.860]   I don't want to do another one like this.
[00:50:15.860 --> 00:50:19.060]   And I want to try to see like, what's, what's going to be the big revolution here?
[00:50:19.060 --> 00:50:20.060]   Is there going to be like third discovery?
[00:50:20.060 --> 00:50:21.060]   It's going to be transportation.
[00:50:21.060 --> 00:50:24.220]   It's going to be education.
[00:50:24.220 --> 00:50:27.020]   So I'm going to pick one and I'm going to make a bet.
[00:50:27.020 --> 00:50:28.340]   I'm going to go for it.
[00:50:28.340 --> 00:50:32.580]   And maybe I'll fail, you know, and maybe there's 1% chance I'll succeed, but at this thing
[00:50:32.580 --> 00:50:33.580]   it'll be worth it, you know.
[00:50:33.580 --> 00:50:34.580]   Nice.
[00:50:34.580 --> 00:50:35.580]   I love it.
[00:50:35.580 --> 00:50:36.580]   Okay.
[00:50:36.580 --> 00:50:42.100]   And final question is when, when you think about, you know, taking a model from sort
[00:50:42.100 --> 00:50:49.460]   of research to like deployed in production and useful, like where, where do you see the
[00:50:49.460 --> 00:50:53.100]   major pitfalls or where are the pitfalls that might be surprising to someone that was just
[00:50:53.100 --> 00:50:54.100]   a researcher?
[00:50:54.100 --> 00:50:56.540]   Oh my God, it's so complicated.
[00:50:56.540 --> 00:51:01.060]   You know, like it's actually really, it's something I feel like we don't figure out.
[00:51:01.060 --> 00:51:05.700]   Like I'll reverse the question, you know, which is like, what makes, let's say DevOps
[00:51:05.700 --> 00:51:06.700]   good, right?
[00:51:06.700 --> 00:51:14.060]   You want some things that's, you know, reliable, that scales, that you can test.
[00:51:14.060 --> 00:51:20.740]   And you know, while, you know, testing in AI, it's hard actually.
[00:51:20.740 --> 00:51:21.740]   How do you test?
[00:51:21.740 --> 00:51:24.740]   Actually, you can test like, you know, tests that are very close to the model or you have
[00:51:24.740 --> 00:51:25.740]   downstream tests.
[00:51:25.740 --> 00:51:30.140]   Imagine you change the speech recognition and you have 20 system with 20 layers on top of
[00:51:30.140 --> 00:51:31.140]   that.
[00:51:31.140 --> 00:51:32.140]   How do you test the last system that depends on that?
[00:51:32.140 --> 00:51:33.140]   Reliable.
[00:51:33.140 --> 00:51:37.580]   Well, this systems, I mean, we claim they are deterministic, but they are not actually
[00:51:37.580 --> 00:51:41.860]   a lot of like, and a lot of behaviors are really well, you know, then you cannot actually
[00:51:41.860 --> 00:51:45.660]   completely reproduce, right.
[00:51:45.660 --> 00:51:50.820]   And then scale, this thing keeps scaling, you know, like every, every year at Meta we
[00:51:50.820 --> 00:51:57.580]   were like 10X bigger, you know, and it wreaks havoc in all your assumptions.
[00:51:57.580 --> 00:51:58.980]   It's really, really hard.
[00:51:58.980 --> 00:52:04.060]   You know, it really breaks the, you know, the assumption you want to have to create
[00:52:04.060 --> 00:52:05.060]   this.
[00:52:05.060 --> 00:52:06.060]   They're just not there.
[00:52:06.060 --> 00:52:10.220]   And I don't think we're figuring it out, you know, I think it's still a work in progress.
[00:52:10.220 --> 00:52:11.220]   Awesome.
[00:52:11.220 --> 00:52:12.220]   Well, thanks so much.
[00:52:12.220 --> 00:52:13.220]   This is super fun.
[00:52:13.220 --> 00:52:14.220]   I really appreciate your time.
[00:52:14.220 --> 00:52:15.220]   Thanks for the interview.
[00:52:15.220 --> 00:52:16.220]   Thank you so much, Miguel.
[00:52:16.220 --> 00:52:21.820]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:52:21.820 --> 00:52:26.540]   to the show notes in the description where you can find links to all the papers that
[00:52:26.540 --> 00:52:30.700]   are mentioned, supplemental material, and a transcription that we worked really hard
[00:52:30.700 --> 00:52:31.700]   to produce.
[00:52:31.700 --> 00:52:32.060]   So check it out.
[00:52:32.060 --> 00:52:34.640]   (upbeat music)

