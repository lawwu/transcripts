
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:05.860]   - All right.
[00:00:05.860 --> 00:00:08.500]   Today I have the pleasure of speaking with Bern Hobart,
[00:00:08.500 --> 00:00:11.100]   who's a writer, consultant, and investor,
[00:00:11.100 --> 00:00:13.940]   who writes at diff.substack.com.
[00:00:13.940 --> 00:00:16.940]   That's D-I-F-F.substack.com.
[00:00:16.940 --> 00:00:18.940]   Here's my first question, Bern.
[00:00:18.940 --> 00:00:20.860]   You wrote an article called "Foxes and Hedgehogs,"
[00:00:20.860 --> 00:00:23.260]   and here's the final line in the article.
[00:00:23.260 --> 00:00:25.980]   "If it looks like somebody doesn't have a single big idea,
[00:00:25.980 --> 00:00:28.060]   "they probably do, and it's a good one."
[00:00:28.060 --> 00:00:30.260]   Now, you are somebody who writes every single day,
[00:00:30.260 --> 00:00:31.820]   and every single weekday,
[00:00:31.820 --> 00:00:33.740]   and you're writing about all kinds of things,
[00:00:33.740 --> 00:00:35.860]   in finance, technology, and so on.
[00:00:35.860 --> 00:00:37.500]   And it might seem like you don't have one big idea,
[00:00:37.500 --> 00:00:39.140]   but that's exactly why I should expect you
[00:00:39.140 --> 00:00:40.120]   to have one big idea.
[00:00:40.120 --> 00:00:43.260]   So here's my guess of what your big idea is,
[00:00:43.260 --> 00:00:45.260]   and you tell me if I'm wrong or right.
[00:00:45.260 --> 00:00:46.860]   Basically, most human decisions,
[00:00:46.860 --> 00:00:49.180]   whether they're made by individuals or by institutions,
[00:00:49.180 --> 00:00:54.180]   can be boiled down to some simple financial concepts,
[00:00:54.180 --> 00:00:56.900]   like expected value, optionality, volatility.
[00:00:56.900 --> 00:00:58.900]   And because other people are missing this,
[00:00:58.900 --> 00:01:00.660]   they're not reporting on the important trends,
[00:01:00.660 --> 00:01:02.860]   or they're reporting about them in a way
[00:01:02.860 --> 00:01:05.280]   that misses the long-term impact these trends will have.
[00:01:05.280 --> 00:01:06.280]   How far off am I?
[00:01:06.280 --> 00:01:10.340]   - I think that's a good mental model,
[00:01:10.340 --> 00:01:13.580]   and it's one that I've used a whole lot.
[00:01:13.580 --> 00:01:15.900]   I do think that, like, it's definitely true
[00:01:15.900 --> 00:01:19.580]   that financial concepts can be usefully applied
[00:01:19.580 --> 00:01:20.700]   in a lot of different contexts,
[00:01:20.700 --> 00:01:21.980]   but it's like any other model,
[00:01:21.980 --> 00:01:23.980]   that you want to use the model,
[00:01:23.980 --> 00:01:26.500]   but you also wanna be aware of the deficiencies
[00:01:26.500 --> 00:01:27.340]   in the model.
[00:01:27.340 --> 00:01:29.620]   Like, half the point of the model is to make predictions.
[00:01:29.620 --> 00:01:31.180]   Half the point of the model is to say,
[00:01:31.180 --> 00:01:32.940]   here's a list of assumptions you have to make
[00:01:32.940 --> 00:01:35.340]   in order to make a reasonable prediction.
[00:01:35.340 --> 00:01:37.140]   And if you can't make all those assumptions,
[00:01:37.140 --> 00:01:39.540]   then the model does not actually apply,
[00:01:39.540 --> 00:01:43.380]   or like you should at least not be surprised to be surprised.
[00:01:43.380 --> 00:01:47.620]   So that is, it is definitely a big part of how I think.
[00:01:47.620 --> 00:01:52.060]   I would say maybe the big idea is more of the big question,
[00:01:52.060 --> 00:01:55.380]   which is just how do people coordinate
[00:01:55.380 --> 00:01:57.100]   when they're solving complicated problems?
[00:01:57.100 --> 00:01:59.620]   Because a lot of the interesting problems in the world,
[00:01:59.620 --> 00:02:02.220]   you can't have one lone genius solve them,
[00:02:02.220 --> 00:02:04.740]   and there are various institutions that try to solve them,
[00:02:04.740 --> 00:02:07.420]   but a lot of times the institutional mandate
[00:02:07.420 --> 00:02:09.140]   is not to solve that problem.
[00:02:09.140 --> 00:02:10.060]   It's something else.
[00:02:10.060 --> 00:02:12.300]   And maybe there's a real mandate,
[00:02:12.300 --> 00:02:13.180]   and there's a fictional one.
[00:02:13.180 --> 00:02:16.740]   Like a lot of the mission-driven public companies out there,
[00:02:16.740 --> 00:02:19.580]   they will have their,
[00:02:19.580 --> 00:02:21.740]   they'll have both the mandate of like,
[00:02:21.740 --> 00:02:24.260]   SpaceX, we're gonna go to Mars,
[00:02:24.260 --> 00:02:26.740]   but also we're trying to maximize shareholder value.
[00:02:26.740 --> 00:02:28.420]   And it's never clear which one is actually
[00:02:28.420 --> 00:02:30.420]   the external story that they're just telling you
[00:02:30.420 --> 00:02:33.780]   so that they can really accomplish their internal goals.
[00:02:33.780 --> 00:02:35.860]   It's actually not clear which of those is which.
[00:02:35.860 --> 00:02:38.660]   So, and maybe even within SpaceX,
[00:02:38.660 --> 00:02:39.500]   there are people who think,
[00:02:39.500 --> 00:02:41.300]   okay, the Mars stuff is like,
[00:02:41.300 --> 00:02:43.100]   that's how we recruit good engineers.
[00:02:43.100 --> 00:02:44.860]   That's how we raise a bunch of money.
[00:02:44.860 --> 00:02:45.700]   When we go public,
[00:02:45.700 --> 00:02:46.940]   that's why the stock price will be really high.
[00:02:46.940 --> 00:02:50.540]   But that really SpaceX is trying to maximize
[00:02:50.540 --> 00:02:52.900]   earnings per share on a 10 or 20 year timeframe.
[00:02:52.900 --> 00:02:54.260]   And then there may be other people at SpaceX
[00:02:54.260 --> 00:02:55.100]   who are like, yeah,
[00:02:55.100 --> 00:02:57.140]   everyone thinks that SpaceX is just this company
[00:02:57.140 --> 00:02:58.140]   that's trying to make money.
[00:02:58.140 --> 00:02:59.340]   And of course we will make money,
[00:02:59.340 --> 00:03:01.380]   but the real point is to make humans
[00:03:01.380 --> 00:03:02.540]   an interplanetary species.
[00:03:02.540 --> 00:03:06.580]   So coordination, it's just a really hard problem to solve.
[00:03:06.580 --> 00:03:08.420]   It's a hard problem to think about.
[00:03:08.420 --> 00:03:11.140]   There, like even when you think about
[00:03:11.140 --> 00:03:14.420]   how these different institutions have different goals,
[00:03:14.420 --> 00:03:15.340]   different stated goals,
[00:03:15.340 --> 00:03:17.500]   they may have internal goals that are unstated,
[00:03:17.500 --> 00:03:18.340]   but exist.
[00:03:18.340 --> 00:03:20.140]   And a lot of the time those goals are just,
[00:03:20.140 --> 00:03:22.100]   whoever's there wants to stay there
[00:03:22.100 --> 00:03:24.820]   and wants to get raises and wants to feel important.
[00:03:24.820 --> 00:03:27.700]   And if they fail to accomplish their goals,
[00:03:27.700 --> 00:03:29.900]   but the problem they're working on keeps getting bigger,
[00:03:29.900 --> 00:03:32.260]   they can stay important for a really long time.
[00:03:32.260 --> 00:03:35.860]   So given that there's naturally
[00:03:35.860 --> 00:03:36.980]   just a lot of double talk about this.
[00:03:36.980 --> 00:03:38.500]   And given that if you're coordinating
[00:03:38.500 --> 00:03:39.460]   between different groups of people
[00:03:39.460 --> 00:03:41.500]   who have different goals themselves,
[00:03:41.500 --> 00:03:43.500]   that you sort of need to do some double talk.
[00:03:43.500 --> 00:03:45.620]   It's just a really hard problem to study.
[00:03:45.620 --> 00:03:48.100]   And it ends up being a problem that shows up
[00:03:48.100 --> 00:03:49.540]   in a lot of different domains.
[00:03:49.540 --> 00:03:52.060]   So I've talked about it in finance.
[00:03:52.060 --> 00:03:54.540]   It shows up in tech companies all the time.
[00:03:54.540 --> 00:03:55.900]   It shows up in politics
[00:03:55.900 --> 00:03:59.460]   and like both the politics in the sense of
[00:03:59.460 --> 00:04:01.060]   who gets elected, which bills get passed.
[00:04:01.060 --> 00:04:03.660]   And then in the much broader sense of just
[00:04:03.660 --> 00:04:07.620]   how do humans with intractable desires
[00:04:07.620 --> 00:04:10.260]   resolve them to one party's favor or another,
[00:04:10.260 --> 00:04:11.700]   or just figure out some way
[00:04:11.700 --> 00:04:13.700]   that they can all come to an accommodation.
[00:04:13.700 --> 00:04:17.620]   - Okay, so then just to boil that down
[00:04:17.620 --> 00:04:19.420]   in terms of the question,
[00:04:19.420 --> 00:04:22.580]   is a big idea that you are trying to figure out
[00:04:22.580 --> 00:04:25.540]   what the secret goals of institutions are
[00:04:25.540 --> 00:04:27.820]   that are coordinating on a mass scale?
[00:04:27.820 --> 00:04:30.820]   - Yeah, I guess if you're thinking about
[00:04:30.820 --> 00:04:34.140]   what problem should be solved,
[00:04:34.140 --> 00:04:37.420]   the problem of productivity stagnation is really important
[00:04:37.420 --> 00:04:40.180]   and it affects everybody and it's dire
[00:04:40.180 --> 00:04:42.140]   and it tends to compound over time
[00:04:42.140 --> 00:04:44.700]   because different technologies
[00:04:44.700 --> 00:04:47.020]   cause other technologies to accelerate.
[00:04:47.020 --> 00:04:49.220]   Different social technologies are kind of invisible
[00:04:49.220 --> 00:04:50.180]   until they go away.
[00:04:50.180 --> 00:04:53.420]   Like you don't really know that you were working
[00:04:53.420 --> 00:04:54.980]   at an effective company, for example,
[00:04:54.980 --> 00:04:57.180]   until either it becomes ineffective
[00:04:57.180 --> 00:04:58.020]   or you go somewhere else.
[00:04:58.020 --> 00:05:00.100]   You realize, wait, I can't actually trust
[00:05:00.100 --> 00:05:02.020]   that if I email someone and ask them a question,
[00:05:02.020 --> 00:05:04.980]   they'll actually get back to me with a good answer.
[00:05:04.980 --> 00:05:06.100]   Those kinds of things are only visible
[00:05:06.100 --> 00:05:06.940]   when you don't have them.
[00:05:06.940 --> 00:05:10.620]   So between technological stagnation,
[00:05:10.620 --> 00:05:12.900]   which is pretty visible in the productivity stats
[00:05:12.900 --> 00:05:15.500]   and then social technology stagnation,
[00:05:15.500 --> 00:05:17.180]   which is a lot harder to measure,
[00:05:17.180 --> 00:05:21.140]   but you do get a sense of it just from looking at,
[00:05:21.140 --> 00:05:22.660]   say how the US government functioned
[00:05:22.660 --> 00:05:26.460]   in the 30s, 40s, 50s and 60s versus how it functions today.
[00:05:26.460 --> 00:05:29.940]   It's clear that there's some decline happening there.
[00:05:29.940 --> 00:05:33.100]   And both of those involve a lot of coordination problems.
[00:05:33.100 --> 00:05:38.100]   So studying how people collaborate to accomplish goals
[00:05:38.100 --> 00:05:40.100]   is a way to figure out how we can all collaborate
[00:05:40.100 --> 00:05:43.100]   to accomplish the goal of getting productivity growth
[00:05:43.100 --> 00:05:45.180]   back up to where it used to be.
[00:05:45.180 --> 00:05:46.380]   - Okay, so let's talk about that.
[00:05:46.380 --> 00:05:48.420]   I was planning on asking you about that anyways.
[00:05:48.420 --> 00:05:50.700]   So it's very good that you brought that up.
[00:05:50.700 --> 00:05:53.420]   So you wrote an article where you are bullish
[00:05:53.420 --> 00:05:55.180]   on the claims that the great stagnation is over
[00:05:55.180 --> 00:05:56.540]   based on anecdotal data.
[00:05:56.540 --> 00:05:58.580]   I think the title of the article
[00:05:58.580 --> 00:06:00.620]   was stagnation or something,
[00:06:00.620 --> 00:06:03.140]   and I'll link it in the show notes.
[00:06:03.140 --> 00:06:05.740]   And from what I remember, one of your claims
[00:06:05.740 --> 00:06:07.620]   is that even though you have these sort
[00:06:07.620 --> 00:06:09.940]   of individual developments like mRNA vaccines,
[00:06:09.940 --> 00:06:12.380]   if the regulatory apparatus is not available
[00:06:12.380 --> 00:06:14.060]   for them to get approved and scaled quickly,
[00:06:14.060 --> 00:06:15.860]   then it doesn't matter as much.
[00:06:15.860 --> 00:06:19.220]   I'm wondering if we can quantify that.
[00:06:19.220 --> 00:06:21.660]   So if we look from 1971,
[00:06:21.660 --> 00:06:25.540]   how much higher would total factor productivity be right now
[00:06:25.540 --> 00:06:28.980]   if all the regulations were perfectly in place
[00:06:28.980 --> 00:06:31.980]   to enable productivity to be as high as possible?
[00:06:31.980 --> 00:06:36.260]   - Well, I think it's a mistake to take a time period
[00:06:36.260 --> 00:06:38.700]   and say if we kept things the same,
[00:06:38.700 --> 00:06:41.940]   it's more like if we'd adapted in better ways
[00:06:41.940 --> 00:06:43.900]   that we'd have more productivity growth.
[00:06:43.900 --> 00:06:45.900]   I mean, if you just take the gap
[00:06:45.900 --> 00:06:48.100]   in measured total factor productivity growth,
[00:06:48.100 --> 00:06:50.180]   which is around 1% a year,
[00:06:50.180 --> 00:06:52.620]   you compound that over a 50-year time period.
[00:06:52.620 --> 00:06:56.180]   So you probably get to something like 60 or 70% higher GDP
[00:06:56.180 --> 00:06:57.580]   per capita in the US,
[00:06:57.580 --> 00:07:00.980]   which it's kind of hard to imagine exactly
[00:07:00.980 --> 00:07:01.820]   what that would look like,
[00:07:01.820 --> 00:07:04.260]   because it's not like we would all just live
[00:07:04.260 --> 00:07:06.420]   in 70% bigger houses
[00:07:06.420 --> 00:07:09.540]   and have 70% more cars per household and whatever.
[00:07:09.540 --> 00:07:12.260]   It's more that there would be other additional products
[00:07:12.260 --> 00:07:14.340]   or that people would have longer lifespans
[00:07:14.340 --> 00:07:17.540]   or hopefully that we'd have longer health spans,
[00:07:17.540 --> 00:07:20.580]   which I think is probably more solvable,
[00:07:20.580 --> 00:07:24.420]   definitely more pressing in terms of total human disutility,
[00:07:24.420 --> 00:07:27.380]   more pressing problem than the lifespan issue for now.
[00:07:27.380 --> 00:07:32.500]   So you can sort of ballpark
[00:07:32.500 --> 00:07:33.500]   just what things would look like
[00:07:33.500 --> 00:07:36.660]   if that mid 20th century productivity boom
[00:07:36.660 --> 00:07:37.900]   had gone on forever.
[00:07:37.900 --> 00:07:40.300]   But it is still, you're looking at the data,
[00:07:40.300 --> 00:07:43.260]   which the data can tell you the gap
[00:07:43.260 --> 00:07:45.820]   between this hypothetical possibility
[00:07:45.820 --> 00:07:47.660]   and the reality in this aggregate sense,
[00:07:47.660 --> 00:07:49.180]   but it can't really tell you
[00:07:49.180 --> 00:07:51.740]   what kind of amazing world we would have.
[00:07:51.740 --> 00:07:53.620]   And to figure out that,
[00:07:53.620 --> 00:07:55.620]   you have to look at specific examples,
[00:07:55.620 --> 00:07:58.060]   'cause that whole sweep of productivity growth,
[00:07:58.060 --> 00:08:00.100]   it wasn't just that there's a productivity dial
[00:08:00.100 --> 00:08:01.660]   and someone was turning it up slightly
[00:08:01.660 --> 00:08:02.940]   and then turning it back down.
[00:08:02.940 --> 00:08:05.300]   It was this set of specific inventions
[00:08:05.300 --> 00:08:07.420]   that got developed and deployed
[00:08:07.420 --> 00:08:11.460]   and continued to get more efficient over many decades.
[00:08:11.460 --> 00:08:14.700]   So you'd have to think about the various specific innovations
[00:08:14.700 --> 00:08:16.340]   that could have spread around.
[00:08:16.340 --> 00:08:18.980]   So a really good book to start with there
[00:08:18.980 --> 00:08:20.500]   is "Where's My Flying Car?"
[00:08:20.500 --> 00:08:24.140]   Which is, it's basically this extended,
[00:08:24.140 --> 00:08:26.420]   very detailed, very thoughtful,
[00:08:26.420 --> 00:08:28.620]   very well footnoted rant
[00:08:28.620 --> 00:08:30.580]   from someone who really wants to have a flying car
[00:08:30.580 --> 00:08:32.220]   and he's interested in them.
[00:08:32.220 --> 00:08:35.260]   And he's flown things that are basically flying cars
[00:08:35.260 --> 00:08:38.340]   and he's done all the math on how they would work.
[00:08:38.340 --> 00:08:40.180]   He's looked at the safety concerns.
[00:08:40.180 --> 00:08:43.340]   He has looked at how they would affect things
[00:08:43.340 --> 00:08:44.460]   like the layout of cities,
[00:08:44.460 --> 00:08:46.140]   how they'd affect the nature of travel.
[00:08:46.140 --> 00:08:49.420]   And he makes a really strong case that we should have them
[00:08:49.420 --> 00:08:54.420]   and that they have been, not specifically illegalized,
[00:08:54.420 --> 00:08:56.700]   but just that the regulatory noose has tightened
[00:08:56.700 --> 00:08:59.980]   and it's just gotten a lot harder to fly anything
[00:08:59.980 --> 00:09:03.300]   and much harder than that to build something totally new
[00:09:03.300 --> 00:09:05.500]   and fly it without getting in a whole lot of trouble.
[00:09:05.500 --> 00:09:07.540]   And of course, the trade-off there is
[00:09:07.540 --> 00:09:09.740]   we don't have very many flying car accidents
[00:09:09.740 --> 00:09:11.140]   because we don't have any flying cars.
[00:09:11.140 --> 00:09:12.980]   So there would be downsides
[00:09:12.980 --> 00:09:17.980]   and the early innovators will tend to have more accidents.
[00:09:17.980 --> 00:09:20.740]   If you look at, I'm reading a bit about the early history
[00:09:20.740 --> 00:09:23.900]   of the steam engine, lots of explosions,
[00:09:23.900 --> 00:09:27.420]   lots of times where we basically learned things
[00:09:27.420 --> 00:09:30.300]   about the physical limits of different kinds of materials
[00:09:30.300 --> 00:09:32.860]   by building higher capacity boilers
[00:09:32.860 --> 00:09:34.620]   and then having them explode.
[00:09:34.620 --> 00:09:39.340]   So there's definitely some downside that we are avoiding,
[00:09:39.340 --> 00:09:41.620]   but there's also a lot of upside we're missing.
[00:09:41.620 --> 00:09:42.980]   And that downside is easier to see
[00:09:42.980 --> 00:09:45.540]   because it is weighted to the very beginning
[00:09:45.540 --> 00:09:47.820]   when the new technology doesn't work especially well,
[00:09:47.820 --> 00:09:49.140]   but it can kill people.
[00:09:49.140 --> 00:09:53.340]   And we don't see what things would look like in 20 years
[00:09:53.340 --> 00:09:55.940]   when it's working really well, it's extremely safe
[00:09:55.940 --> 00:09:58.140]   and you have to try pretty hard
[00:09:58.140 --> 00:10:00.620]   to actually kill yourself using it properly.
[00:10:00.620 --> 00:10:02.060]   - Right, yeah.
[00:10:02.060 --> 00:10:03.940]   He especially blames the green movement
[00:10:03.940 --> 00:10:07.140]   and all that sort of the government activity it sponsored
[00:10:07.140 --> 00:10:09.900]   as being a big culprit there in that book.
[00:10:09.900 --> 00:10:11.060]   One of the questions that comes up
[00:10:11.060 --> 00:10:12.380]   when we talk about how regulations
[00:10:12.380 --> 00:10:13.260]   have been slowing progress though,
[00:10:13.260 --> 00:10:17.660]   is obviously there's more than 200 countries in the world.
[00:10:17.660 --> 00:10:20.340]   You could develop flying cars in any one of them,
[00:10:20.340 --> 00:10:23.140]   but what are the odds that you have a sort of correlation
[00:10:23.140 --> 00:10:24.580]   that's so exact between countries
[00:10:24.580 --> 00:10:28.100]   that you managed to kill off the avenues to flying cars?
[00:10:28.100 --> 00:10:29.700]   Is it just that there's very few countries
[00:10:29.700 --> 00:10:30.900]   where something like this is possible
[00:10:30.900 --> 00:10:33.220]   and their regulations are correlated?
[00:10:33.220 --> 00:10:34.780]   What's going on?
[00:10:34.780 --> 00:10:37.020]   - Yeah, so there are a couple of things going on.
[00:10:37.020 --> 00:10:40.340]   One is, like you said, the US is a very rich country
[00:10:40.340 --> 00:10:44.220]   and that allows us to indulge in a lot of weird hobbies
[00:10:44.220 --> 00:10:46.140]   like building flying cars and before that,
[00:10:46.140 --> 00:10:48.980]   building heavier than air flying machines in general
[00:10:48.980 --> 00:10:53.980]   or messing around with the earliest of regular wheeled cars.
[00:10:53.980 --> 00:10:56.740]   It's easier to do that stuff
[00:10:56.740 --> 00:10:58.780]   when you're able to eat three meals a day
[00:10:58.780 --> 00:11:02.060]   and you can have shelter, stuff like that.
[00:11:02.060 --> 00:11:04.580]   So yeah, it doesn't really matter
[00:11:04.580 --> 00:11:07.940]   if it's legal to build something like this in a country
[00:11:07.940 --> 00:11:11.500]   where they don't have the spare wealth to build it.
[00:11:11.500 --> 00:11:13.940]   And for aviation in particular,
[00:11:13.940 --> 00:11:16.420]   it seems like the barrier to entry for countries
[00:11:16.420 --> 00:11:18.340]   is pretty high.
[00:11:18.340 --> 00:11:21.540]   They have, planes have really complicated supply chains.
[00:11:21.540 --> 00:11:25.700]   They're very sensitive to the quality
[00:11:25.700 --> 00:11:27.340]   of a lot of different components.
[00:11:27.340 --> 00:11:28.900]   You basically, like for the larger planes,
[00:11:28.900 --> 00:11:31.300]   so for wide body commercial planes,
[00:11:31.300 --> 00:11:33.580]   you basically have two entities in the world
[00:11:33.580 --> 00:11:36.500]   that make them competitively, Boeing and Airbus.
[00:11:36.500 --> 00:11:39.020]   China has been trying for a really long time
[00:11:39.020 --> 00:11:40.660]   to catch up in that business
[00:11:40.660 --> 00:11:42.700]   and has not been able to do so just yet.
[00:11:42.700 --> 00:11:45.860]   So that is an indicator that it's pretty hard
[00:11:45.860 --> 00:11:48.340]   and or that China's GDP per capita
[00:11:48.340 --> 00:11:49.660]   is not quite at the level
[00:11:49.660 --> 00:11:51.900]   that can support an industry like that.
[00:11:51.900 --> 00:11:56.260]   So it does matter if regulations are strict
[00:11:56.260 --> 00:11:59.340]   in the handful of countries that are pretty rich
[00:11:59.340 --> 00:12:01.660]   and that also have developed capital markets.
[00:12:01.660 --> 00:12:03.380]   That's another thing that you really need
[00:12:03.380 --> 00:12:05.380]   for something like this is that
[00:12:05.380 --> 00:12:06.620]   once someone has the product,
[00:12:06.620 --> 00:12:07.860]   once they've done the proof of concept,
[00:12:07.860 --> 00:12:09.740]   once they've shown that it can work,
[00:12:09.740 --> 00:12:12.900]   it has to scale up and it has network effects.
[00:12:12.900 --> 00:12:14.820]   That is actually one of the exciting things on flying cars
[00:12:14.820 --> 00:12:18.180]   that has started to happen since I read
[00:12:18.180 --> 00:12:20.060]   "Where's My Flying Car" a couple of months ago
[00:12:20.060 --> 00:12:23.700]   is that United Airlines has actually been putting in orders
[00:12:23.700 --> 00:12:25.540]   for basically flying cars.
[00:12:25.540 --> 00:12:28.460]   And they seem to be pretty serious
[00:12:28.460 --> 00:12:30.340]   about messing around with that
[00:12:30.340 --> 00:12:32.020]   and messing around with supersonic travel
[00:12:32.020 --> 00:12:35.540]   and really rethinking the range of speeds
[00:12:35.540 --> 00:12:36.940]   at which human beings should fly
[00:12:36.940 --> 00:12:39.060]   and the range of distances at which they should fly.
[00:12:39.060 --> 00:12:41.540]   So things could be looking better there,
[00:12:41.540 --> 00:12:45.660]   but that's also maybe a sign of how difficult it is
[00:12:45.660 --> 00:12:47.900]   to expand to this industry that you need
[00:12:47.900 --> 00:12:50.820]   one of the big four airlines to back you up
[00:12:50.820 --> 00:12:52.300]   if you're even going to have a hope
[00:12:52.300 --> 00:12:53.460]   of selling these products
[00:12:53.460 --> 00:12:55.420]   and actually selling tickets to passengers
[00:12:55.420 --> 00:12:57.380]   who will then fly in their flying cars.
[00:12:57.380 --> 00:13:00.020]   - Yeah, one of the mysteries though,
[00:13:00.020 --> 00:13:02.100]   is why don't Western companies
[00:13:02.100 --> 00:13:04.740]   with their capital and their technologies,
[00:13:04.740 --> 00:13:05.820]   if they're getting stuck
[00:13:05.820 --> 00:13:08.500]   in the regulatory process in the West,
[00:13:08.500 --> 00:13:10.660]   why don't they just do their experiments
[00:13:10.660 --> 00:13:12.980]   in Honduras or Senegal or something?
[00:13:12.980 --> 00:13:14.860]   And you can imagine this, for example,
[00:13:14.860 --> 00:13:16.940]   with like human challenge trials with COVID,
[00:13:16.940 --> 00:13:19.020]   you're gonna have like some poor country
[00:13:19.020 --> 00:13:22.900]   is completely inoculated to COVID by March or something.
[00:13:22.900 --> 00:13:25.980]   And we're still like dealing with the aftermath.
[00:13:25.980 --> 00:13:27.980]   It's mysterious why that didn't happen.
[00:13:27.980 --> 00:13:31.420]   - I suspect it'll happen next time.
[00:13:31.420 --> 00:13:34.060]   I think like if there's another big pandemic,
[00:13:34.060 --> 00:13:37.260]   I do think that you'll have some people who just say,
[00:13:37.260 --> 00:13:38.780]   we're gonna do human challenge trials
[00:13:38.780 --> 00:13:41.460]   and I'm going to whatever this country is,
[00:13:41.460 --> 00:13:44.100]   I'm gonna bring all the researchers I can
[00:13:44.100 --> 00:13:45.420]   and we're gonna develop a vaccine,
[00:13:45.420 --> 00:13:46.260]   we're gonna test it.
[00:13:46.260 --> 00:13:49.380]   And hopefully we do a good enough job
[00:13:49.380 --> 00:13:53.260]   that it's politically untenable to get us
[00:13:53.260 --> 00:13:54.180]   in legal trouble afterwards.
[00:13:54.180 --> 00:13:57.660]   But there is, so there's this weird coordination
[00:13:57.660 --> 00:13:59.500]   between media on the one hand
[00:13:59.500 --> 00:14:00.740]   and regulators on the other hand,
[00:14:00.740 --> 00:14:05.740]   where whenever someone is breaking these rules,
[00:14:05.740 --> 00:14:10.260]   so like if you try to go around FDA rules,
[00:14:10.260 --> 00:14:11.500]   it's pretty clear that that's what you're trying to do.
[00:14:11.500 --> 00:14:12.740]   It's pretty clear that you were saying,
[00:14:12.740 --> 00:14:14.380]   I don't actually think these rules should apply
[00:14:14.380 --> 00:14:16.260]   and so I'm not going to follow them.
[00:14:16.260 --> 00:14:18.340]   And that gets a lot of attention at the FDA.
[00:14:18.340 --> 00:14:21.620]   And they have a lot of power to make sure
[00:14:21.620 --> 00:14:23.860]   that you do not actually,
[00:14:23.860 --> 00:14:25.260]   that you're never able to sell
[00:14:25.260 --> 00:14:27.420]   whatever the result of that experiment is in the US.
[00:14:27.420 --> 00:14:29.660]   Now, it's not like the FDA is going to say,
[00:14:29.660 --> 00:14:33.100]   if someone in a different country invents a miracle drug
[00:14:33.100 --> 00:14:34.340]   and it works really well
[00:14:34.340 --> 00:14:35.380]   and then they try to sell it in the US,
[00:14:35.380 --> 00:14:37.460]   it's not like the FDA will just flat out refuse to approve it
[00:14:37.460 --> 00:14:39.060]   although they will take a long time.
[00:14:39.060 --> 00:14:42.340]   It's more that if they know that you are specifically
[00:14:42.340 --> 00:14:43.900]   trying to ride around the regulations,
[00:14:43.900 --> 00:14:45.340]   they take that pretty seriously.
[00:14:45.500 --> 00:14:49.060]   And that seems to be just a feature of big institutions
[00:14:49.060 --> 00:14:51.900]   in general, that if they set up some kind of rule
[00:14:51.900 --> 00:14:55.220]   and you thwart it and that gets their attention,
[00:14:55.220 --> 00:14:57.180]   then they're very serious about making sure
[00:14:57.180 --> 00:14:58.660]   that you don't thwart it again.
[00:14:58.660 --> 00:15:00.700]   So it's not just a public sector thing.
[00:15:00.700 --> 00:15:02.420]   Like with Disney,
[00:15:02.420 --> 00:15:04.820]   if you try to make knockoff Disney characters,
[00:15:04.820 --> 00:15:06.180]   you will hear from Disney's lawyers.
[00:15:06.180 --> 00:15:09.540]   If you try to host pirated video games,
[00:15:09.540 --> 00:15:11.140]   you hear from the game publishers.
[00:15:11.140 --> 00:15:13.620]   A lot of big institutions are very serious
[00:15:13.620 --> 00:15:17.660]   about keeping their rules enforced as much as they can,
[00:15:17.660 --> 00:15:21.180]   both because they have those rules because it benefits them
[00:15:21.180 --> 00:15:23.380]   and because it's a source of legitimacy
[00:15:23.380 --> 00:15:26.260]   that they can write the rules and everybody follows them.
[00:15:26.260 --> 00:15:27.740]   - Yeah, and it gets pretty sinister.
[00:15:27.740 --> 00:15:29.300]   I read a book by, I forgot her name,
[00:15:29.300 --> 00:15:31.260]   but she worked at a pharma company
[00:15:31.260 --> 00:15:33.340]   and she wrote a book called "Death by Regulation."
[00:15:33.340 --> 00:15:35.580]   And one of the points she made was,
[00:15:35.580 --> 00:15:37.940]   if anybody actually talked about how many deaths
[00:15:37.940 --> 00:15:41.940]   the FDA causes by just not approving drugs,
[00:15:41.940 --> 00:15:42.900]   if the company says like, "Listen,
[00:15:42.900 --> 00:15:43.740]   "here's the approval process.
[00:15:43.740 --> 00:15:45.340]   "Here's how much it sucked."
[00:15:45.340 --> 00:15:48.020]   That really has an effect on whether future drugs
[00:15:48.020 --> 00:15:49.020]   that they make get approved.
[00:15:49.020 --> 00:15:50.820]   So there's really a silence on that.
[00:15:50.820 --> 00:15:53.740]   Oh, but speaking of the FDA,
[00:15:53.740 --> 00:15:55.900]   here's just one technology I've been interested in.
[00:15:55.900 --> 00:15:58.060]   What are the odds that polygenic scores
[00:15:58.060 --> 00:16:00.260]   for embryo selection get approved by the FDA?
[00:16:00.260 --> 00:16:06.220]   - I suspect that it is,
[00:16:06.220 --> 00:16:09.940]   I mean, I'm definitely not an FDA expert.
[00:16:09.940 --> 00:16:11.820]   So I'm probably the wrong person
[00:16:11.820 --> 00:16:12.980]   to have a strong opinion on this.
[00:16:12.980 --> 00:16:14.860]   My general sense would be,
[00:16:14.860 --> 00:16:18.540]   if you are testing for genetic diseases
[00:16:18.540 --> 00:16:20.260]   and maybe you can broaden the definition
[00:16:20.260 --> 00:16:22.460]   of what constitutes a genetic disease over time,
[00:16:22.460 --> 00:16:26.340]   but that stuff, that seems to be pretty safe.
[00:16:26.340 --> 00:16:30.140]   And then, if you're doing things like,
[00:16:30.140 --> 00:16:33.420]   I want my kid to be tall, not just normal tall,
[00:16:33.420 --> 00:16:35.260]   but like can play in the NBA tall.
[00:16:35.260 --> 00:16:38.260]   And I would pay a million dollars to select the embryos
[00:16:38.260 --> 00:16:40.460]   that give me an NBA qualified kid.
[00:16:40.460 --> 00:16:44.060]   I suspect that that is going to be very hard to approve.
[00:16:44.060 --> 00:16:49.220]   But China's government seems a lot more open
[00:16:49.220 --> 00:16:53.940]   to genetic stuff, both in the genetic research stuff.
[00:16:53.940 --> 00:16:55.620]   Like they were very early buyers
[00:16:55.620 --> 00:16:58.060]   of a lot of genome sequencing equipment.
[00:16:58.060 --> 00:17:03.060]   And they also believe Yao Ming is literally the result
[00:17:03.060 --> 00:17:05.060]   of them telling two very tall people
[00:17:05.060 --> 00:17:06.220]   to get married and have a kid.
[00:17:06.220 --> 00:17:07.940]   And he turned out to be tall.
[00:17:07.940 --> 00:17:12.340]   So we know this genetic stuff can work some of the time.
[00:17:12.340 --> 00:17:16.420]   So that government seems much more open to it.
[00:17:16.420 --> 00:17:18.860]   So I suspect that if that kind of thing does happen
[00:17:18.860 --> 00:17:21.980]   and it's legal, it's probably going to happen in China.
[00:17:21.980 --> 00:17:22.820]   - Right, yeah.
[00:17:22.820 --> 00:17:23.660]   I didn't think about it that way,
[00:17:23.660 --> 00:17:25.780]   that you could just kind of sneak it in through other tests.
[00:17:25.780 --> 00:17:26.980]   But like, you can imagine like,
[00:17:26.980 --> 00:17:30.140]   oh, we're just testing for some nervous system diseases.
[00:17:30.140 --> 00:17:31.900]   And then, oh wow, would you look at that?
[00:17:31.900 --> 00:17:32.780]   They correlate with intelligence.
[00:17:32.780 --> 00:17:35.060]   The next question I have is,
[00:17:35.060 --> 00:17:37.420]   Robin Hansen has written recently and we talked about this.
[00:17:37.420 --> 00:17:40.580]   The idea of just patient long-termism,
[00:17:40.580 --> 00:17:43.260]   which is the idea that like, you park some money
[00:17:43.260 --> 00:17:47.300]   in a compounding fund, and then within a few centuries,
[00:17:47.300 --> 00:17:50.220]   you will be the wealthiest person in the world, right?
[00:17:50.220 --> 00:17:53.260]   If you just assume some sort of like compounding return.
[00:17:53.260 --> 00:17:55.300]   And in fact, one of the things I think you said
[00:17:55.300 --> 00:17:56.700]   at some point about your blog is like,
[00:17:56.700 --> 00:17:59.060]   I'm trying to make your great-grandchildren rich
[00:17:59.060 --> 00:18:01.220]   by just identifying these long-term trends.
[00:18:01.220 --> 00:18:04.220]   As far as we know, this hasn't happened.
[00:18:04.220 --> 00:18:05.060]   I mean, it happened,
[00:18:05.060 --> 00:18:07.420]   like odd cases like Ben Franklin or something.
[00:18:07.420 --> 00:18:10.580]   But like, on a large scale, this really hasn't been tried.
[00:18:10.580 --> 00:18:14.620]   It might be in a temp right now, but what gives?
[00:18:14.620 --> 00:18:17.740]   Like, why is this not a strategy people are using?
[00:18:17.740 --> 00:18:20.980]   - Yeah, so I think it wasn't Piketty's main point,
[00:18:20.980 --> 00:18:22.340]   but Piketty does have a point here
[00:18:22.340 --> 00:18:24.780]   that one of the things that resets this
[00:18:24.780 --> 00:18:28.700]   is war, famine, pestilence, revolution.
[00:18:28.700 --> 00:18:31.580]   Those things tend to knock everyone's net worth
[00:18:31.580 --> 00:18:32.660]   down pretty substantially.
[00:18:32.660 --> 00:18:35.740]   And if your net worth is in financial assets
[00:18:35.740 --> 00:18:37.540]   that are relatively easy to seize,
[00:18:37.540 --> 00:18:38.900]   then it goes down a lot faster.
[00:18:38.900 --> 00:18:41.700]   So that's probably part of what's going on.
[00:18:41.700 --> 00:18:43.380]   I mean, there was that study a while ago
[00:18:43.380 --> 00:18:45.780]   that was looking at the wills,
[00:18:45.780 --> 00:18:47.620]   I think it was like the wills of Florentines
[00:18:47.620 --> 00:18:49.180]   who died in the 15th century
[00:18:49.180 --> 00:18:53.300]   and how the names, the last names that were more prominent
[00:18:53.300 --> 00:18:55.380]   then are actually still more prominent today.
[00:18:55.380 --> 00:18:57.420]   Like the last names that were more associated
[00:18:57.420 --> 00:19:01.620]   with being a doctor in 14th or 15th century Florence,
[00:19:01.620 --> 00:19:03.260]   those people are still more likely to be doctors.
[00:19:03.260 --> 00:19:06.460]   So there is some persistence that actually outlives
[00:19:06.460 --> 00:19:09.580]   the direct loss of wealth.
[00:19:09.580 --> 00:19:12.620]   But I think if you're trying to compound money
[00:19:12.620 --> 00:19:13.540]   for a really long time,
[00:19:13.540 --> 00:19:16.180]   I don't think that the main thing that you wanna focus on
[00:19:16.180 --> 00:19:18.260]   is your annual returns.
[00:19:18.260 --> 00:19:19.340]   I think the thing you wanna focus on
[00:19:19.340 --> 00:19:23.060]   is what are the once a decade, once a century,
[00:19:23.060 --> 00:19:24.540]   and if you're really thinking about it,
[00:19:24.540 --> 00:19:27.100]   once a millennium risks that will turn up
[00:19:27.100 --> 00:19:29.660]   and that can actually push that return to zero.
[00:19:29.660 --> 00:19:32.580]   So I don't know of any financial asset
[00:19:32.580 --> 00:19:35.420]   that you can actually trust in that sense.
[00:19:35.420 --> 00:19:38.100]   The U.S. is a really weird case
[00:19:38.100 --> 00:19:40.500]   because we actually haven't had
[00:19:40.500 --> 00:19:43.980]   any of these massive catastrophic drawdowns
[00:19:43.980 --> 00:19:44.900]   in everybody's wealth
[00:19:44.900 --> 00:19:47.340]   that basically every other country has experienced.
[00:19:47.340 --> 00:19:49.940]   So if you look at it from a U.S. context,
[00:19:49.940 --> 00:19:51.740]   you can actually imagine someone
[00:19:51.740 --> 00:19:54.780]   who put some money in a trust
[00:19:54.780 --> 00:19:58.420]   and they're just buying mostly treasury bonds
[00:19:58.420 --> 00:20:01.860]   and also stocks and that it compounds over time
[00:20:01.860 --> 00:20:03.820]   and ends up being a colossal amount of money.
[00:20:03.820 --> 00:20:06.260]   But there's actually, there's a story in Adam Smith,
[00:20:06.260 --> 00:20:08.380]   the pseudonymous financial commentator,
[00:20:08.380 --> 00:20:11.020]   not the economist, Adam Smith's "Money Game"
[00:20:11.020 --> 00:20:14.140]   where he talks about a guy who did that in the 19th century
[00:20:14.140 --> 00:20:17.780]   and he bought a lot of very reliable blue ship stocks.
[00:20:17.780 --> 00:20:19.820]   And I think he set up his will
[00:20:19.820 --> 00:20:21.780]   so that his heirs just could not get the money
[00:20:21.780 --> 00:20:23.700]   for several generations.
[00:20:23.700 --> 00:20:27.020]   And Smith says that there were years and years
[00:20:27.020 --> 00:20:28.780]   of legal battles and finally the heirs
[00:20:28.780 --> 00:20:30.940]   were able to get access to the trust
[00:20:30.940 --> 00:20:32.580]   and everything was worthless.
[00:20:32.580 --> 00:20:35.260]   It was all like American Alarm Clock Manufacturing Co
[00:20:35.260 --> 00:20:37.220]   that had been bankrupt for 20 years by that point.
[00:20:37.220 --> 00:20:41.740]   So if you're just picking a set of stocks, for example,
[00:20:41.740 --> 00:20:44.220]   over a long time, they'll all go to zero.
[00:20:44.220 --> 00:20:47.940]   Over more intermediate time periods, you can rebalance.
[00:20:47.940 --> 00:20:50.940]   And so that keeps the portfolio value above zero.
[00:20:50.940 --> 00:20:53.500]   But if your country goes through a war
[00:20:53.500 --> 00:20:54.740]   or there's a revolution,
[00:20:54.740 --> 00:20:57.140]   then there's a good chance that all of that value
[00:20:57.140 --> 00:20:57.980]   gets wiped out.
[00:20:57.980 --> 00:21:00.580]   - Yeah, interesting.
[00:21:00.580 --> 00:21:04.300]   This is related to a question that I was talking to
[00:21:04.300 --> 00:21:06.220]   with my last podcast guest.
[00:21:06.220 --> 00:21:09.460]   You've written an article called "Praising Filter Bubbles".
[00:21:09.460 --> 00:21:12.420]   Like this is somewhat related to,
[00:21:12.420 --> 00:21:15.060]   it would be good if there were more cults
[00:21:15.060 --> 00:21:16.300]   and more secret societies.
[00:21:16.300 --> 00:21:18.260]   I was just wondering, I mean, we wouldn't know this
[00:21:18.260 --> 00:21:20.420]   because they're secret societies, but if,
[00:21:20.420 --> 00:21:24.620]   and I'm not saying they're in any way bad or,
[00:21:24.620 --> 00:21:26.980]   but they don't have to be evil or anything,
[00:21:26.980 --> 00:21:28.660]   but like, what would you guess,
[00:21:28.660 --> 00:21:30.540]   how many secret societies do you think there are
[00:21:30.540 --> 00:21:35.540]   that have assets in excess of $1 billion, if any?
[00:21:35.540 --> 00:21:44.060]   - Well, that's a tough question because if you,
[00:21:44.060 --> 00:21:46.900]   so you would need some kind of legal structure
[00:21:46.900 --> 00:21:47.900]   to own the assets, right?
[00:21:47.900 --> 00:21:49.940]   And if it's going to persist for a long time,
[00:21:49.940 --> 00:21:51.180]   you need some system for figuring out
[00:21:51.180 --> 00:21:52.660]   who's going to be in charge and for picking people
[00:21:52.660 --> 00:21:55.660]   who are going to keep that secret society going.
[00:21:55.660 --> 00:21:57.540]   I mean, I wouldn't be surprised if there were organizations
[00:21:57.540 --> 00:21:59.940]   that might've started out trying to be a secret society
[00:21:59.940 --> 00:22:01.180]   that accumulates a bunch of money,
[00:22:01.180 --> 00:22:03.980]   and eventually they end up being run by people
[00:22:03.980 --> 00:22:05.260]   who are just in it for the money.
[00:22:05.260 --> 00:22:07.940]   I mean, maybe a fair number of universities,
[00:22:07.940 --> 00:22:09.740]   like people who started universities,
[00:22:09.740 --> 00:22:11.900]   they often had peculiar ideas
[00:22:11.900 --> 00:22:13.700]   about how human beings should behave
[00:22:13.700 --> 00:22:15.460]   and how to educate people.
[00:22:15.460 --> 00:22:17.860]   Like if you have a bunch of money
[00:22:17.860 --> 00:22:19.340]   and you're thinking about what to do with your life
[00:22:19.340 --> 00:22:20.940]   and you say, you know what I'm going to do?
[00:22:20.940 --> 00:22:23.420]   I am going to exert maximal influence
[00:22:23.420 --> 00:22:25.580]   on people at impressionable ages
[00:22:25.580 --> 00:22:27.460]   and also make these people really important
[00:22:27.460 --> 00:22:28.780]   so that they have a big impact on society.
[00:22:28.780 --> 00:22:32.260]   Like you're already doing a secret society conspiracy.
[00:22:32.260 --> 00:22:34.780]   You're just calling it Harvard or Yale or something.
[00:22:34.780 --> 00:22:39.780]   So you could actually look at modern universities
[00:22:39.780 --> 00:22:42.700]   as having maybe evolved from something closer
[00:22:42.700 --> 00:22:43.660]   to a secret society.
[00:22:43.660 --> 00:22:44.500]   And then they have, of course,
[00:22:44.500 --> 00:22:46.300]   their own little secret societies inside of them,
[00:22:46.300 --> 00:22:49.060]   which have also apparently gotten really boring
[00:22:49.060 --> 00:22:51.180]   and have just become this kind of careerist thing
[00:22:51.180 --> 00:22:52.820]   that you do if you want to get a good job
[00:22:52.820 --> 00:22:54.740]   at an investment bank.
[00:22:54.740 --> 00:22:57.660]   It's hard to keep secret societies alive for a long time.
[00:22:57.660 --> 00:22:59.100]   And I think part of the reason for that
[00:22:59.100 --> 00:23:01.620]   is you do have to keep secrets,
[00:23:01.620 --> 00:23:03.340]   but you also have to reveal secrets.
[00:23:03.340 --> 00:23:08.340]   And if people join it because of one narrative,
[00:23:08.340 --> 00:23:10.700]   and then you tell them once they're pretty high up,
[00:23:10.700 --> 00:23:11.900]   okay, that narrative is totally fake.
[00:23:11.900 --> 00:23:13.420]   Here's what's really going on.
[00:23:13.420 --> 00:23:14.420]   Some of them are real believers.
[00:23:14.420 --> 00:23:17.180]   So you can actually end up with your secret society
[00:23:17.180 --> 00:23:20.140]   failing because it recruited people.
[00:23:20.140 --> 00:23:20.980]   It recruited people who thought
[00:23:20.980 --> 00:23:21.820]   they were the right people for it,
[00:23:21.820 --> 00:23:23.700]   and they turned out to be the wrong people for it.
[00:23:23.700 --> 00:23:25.460]   There's actually a kind of weird example
[00:23:25.460 --> 00:23:28.500]   of this happening in reverse with Al-Qaeda.
[00:23:28.500 --> 00:23:30.140]   So the book, "The Looming Tower,"
[00:23:30.140 --> 00:23:32.340]   that talks about bin Laden's early life
[00:23:32.340 --> 00:23:34.660]   and his early campaigns as a sort of terrorist.
[00:23:34.660 --> 00:23:37.020]   And the impression I got from that
[00:23:37.020 --> 00:23:37.860]   was that a lot of the people
[00:23:37.860 --> 00:23:39.540]   who worked with bin Laden in the '90s
[00:23:39.540 --> 00:23:41.620]   thought he was a loser, but that he had money.
[00:23:41.620 --> 00:23:43.380]   And so they were sort of scamming him
[00:23:43.380 --> 00:23:46.020]   and just telling him, oh yeah,
[00:23:46.020 --> 00:23:48.420]   next year we're definitely doing a terrorist attack.
[00:23:48.420 --> 00:23:51.100]   And then somehow he actually organized something
[00:23:51.100 --> 00:23:54.500]   and was able to do several successful terrorist attacks.
[00:23:54.500 --> 00:23:55.900]   But for a long time,
[00:23:55.900 --> 00:23:57.620]   it looked like a lot of the people involved
[00:23:57.620 --> 00:23:59.820]   just did not think that Al-Qaeda was anything
[00:23:59.820 --> 00:24:03.540]   other than a way to mooch off the bin Laden portion.
[00:24:03.540 --> 00:24:07.460]   So you have a lot of, I guess,
[00:24:07.460 --> 00:24:10.220]   a lot of just randomness in what institutions
[00:24:10.220 --> 00:24:12.860]   try to accomplish and how they try to accomplish it
[00:24:12.860 --> 00:24:14.020]   and who they end up recruiting.
[00:24:14.020 --> 00:24:16.340]   Now, one way around that is families,
[00:24:16.340 --> 00:24:18.780]   because with families, you actually,
[00:24:18.780 --> 00:24:21.660]   if you have some familial goal
[00:24:21.660 --> 00:24:23.500]   and it's gonna take multiple generations,
[00:24:23.500 --> 00:24:27.140]   so you can be more honest with your kids
[00:24:27.140 --> 00:24:29.420]   than with someone you're trying to recruit from the outside
[00:24:29.420 --> 00:24:31.900]   and you could sort of work up to telling them
[00:24:31.900 --> 00:24:33.780]   what the actual plans are.
[00:24:33.780 --> 00:24:37.100]   And I think that with multi-generational wealth,
[00:24:37.100 --> 00:24:39.340]   especially families that got rich first
[00:24:39.340 --> 00:24:40.540]   and then got into politics,
[00:24:40.540 --> 00:24:42.300]   that there's probably some of that going on
[00:24:42.300 --> 00:24:44.300]   where it's not like a huge,
[00:24:44.300 --> 00:24:45.620]   it's not like the Kennedy family
[00:24:45.620 --> 00:24:50.060]   was some huge weird secret society.
[00:24:50.060 --> 00:24:52.100]   It was more like there was a conspiracy
[00:24:52.100 --> 00:24:54.380]   and the conspiracy was get one of the Kennedy kids,
[00:24:54.380 --> 00:24:56.220]   get one of Joseph Kennedy kids elected president,
[00:24:56.220 --> 00:24:57.060]   and then they did it.
[00:24:57.060 --> 00:24:58.860]   And so mission accomplished.
[00:24:58.860 --> 00:25:03.140]   That might be the more viable model.
[00:25:03.140 --> 00:25:06.900]   So if you're trying to count up secret societies
[00:25:06.900 --> 00:25:08.540]   that have secretly accumulated money,
[00:25:08.540 --> 00:25:11.100]   what I would look at is family fortunes
[00:25:11.100 --> 00:25:13.620]   where there's someone who made a bunch of money
[00:25:13.620 --> 00:25:15.420]   and then their kids are doing things
[00:25:15.420 --> 00:25:19.740]   that are more prestigious and more power seeking,
[00:25:19.740 --> 00:25:22.300]   but are not as financially rewarding.
[00:25:22.300 --> 00:25:25.020]   And probably some of those basically count
[00:25:25.020 --> 00:25:29.420]   as secret societies in some approximate sense.
[00:25:29.420 --> 00:25:30.860]   I don't have a good way to speculate
[00:25:30.860 --> 00:25:32.900]   on how many secret societies there are,
[00:25:32.900 --> 00:25:34.620]   how powerful they are, et cetera.
[00:25:34.620 --> 00:25:36.300]   I mean, either there aren't very many
[00:25:36.300 --> 00:25:38.020]   or they're really good at being secretive.
[00:25:38.020 --> 00:25:42.700]   So I guess one way to say that the population of them
[00:25:42.700 --> 00:25:43.540]   is probably pretty low
[00:25:43.540 --> 00:25:48.540]   is that we don't have a lot of dark matter to explain.
[00:25:48.540 --> 00:25:50.580]   Like there are not a lot of times
[00:25:50.580 --> 00:25:53.340]   where we look at what happens in the world
[00:25:53.340 --> 00:25:54.180]   and we have to think,
[00:25:54.180 --> 00:25:55.780]   wow, there are really dark forces at work.
[00:25:55.780 --> 00:25:56.620]   There are a lot more times
[00:25:56.620 --> 00:25:57.460]   where you can look at the world and say,
[00:25:57.460 --> 00:26:00.060]   wow, there are pretty incompetent forces at work
[00:26:00.060 --> 00:26:01.260]   and nobody knows what they're doing
[00:26:01.260 --> 00:26:02.180]   and there's nobody in charge,
[00:26:02.180 --> 00:26:05.980]   but you rarely see all the chess pieces
[00:26:05.980 --> 00:26:07.100]   moving in a really clever way.
[00:26:07.100 --> 00:26:10.140]   You just sort of see the chess pieces moving at random.
[00:26:10.140 --> 00:26:10.980]   - Right.
[00:26:10.980 --> 00:26:12.780]   I mean, the one thing that shakes my view on that
[00:26:12.780 --> 00:26:14.260]   is like UFOs, right?
[00:26:14.260 --> 00:26:15.780]   It just like, either they're aliens
[00:26:15.780 --> 00:26:18.180]   or it's like a military thing or just like an air,
[00:26:18.180 --> 00:26:19.260]   but one of the alternatives,
[00:26:19.260 --> 00:26:22.340]   like it slightly nudges your prior on secret societies up
[00:26:22.340 --> 00:26:25.260]   because like maybe that's what's confusing the military.
[00:26:25.260 --> 00:26:27.860]   One example, by the way, of like a familiar secret society,
[00:26:27.860 --> 00:26:29.420]   it's not even a secret society.
[00:26:29.420 --> 00:26:33.500]   I was reading Andrew Roberts' biography of Churchill
[00:26:33.500 --> 00:26:35.500]   and his dad was a minister
[00:26:35.500 --> 00:26:37.500]   and he was planning on becoming, sorry,
[00:26:37.500 --> 00:26:41.500]   a parliament, whatever somebody in the parliament is called,
[00:26:41.500 --> 00:26:44.180]   and he was planning on becoming PM and he just died early
[00:26:44.180 --> 00:26:47.140]   and basically Churchill, Winston Churchill,
[00:26:47.140 --> 00:26:49.100]   just adopted his dad's entire platform
[00:26:49.100 --> 00:26:50.300]   even after his dad died.
[00:26:50.300 --> 00:26:53.460]   Okay, you wrote an essay
[00:26:53.460 --> 00:26:55.580]   called "Optionalities for Enumerate Cowards"
[00:26:55.580 --> 00:26:58.380]   and it really reminded me of this little passage
[00:26:58.380 --> 00:27:00.980]   from William Dershowitz's book, "Excellent Sheep,"
[00:27:00.980 --> 00:27:01.940]   which you wrote about, you know,
[00:27:01.940 --> 00:27:04.860]   the caliber of students that are going to college nowadays.
[00:27:04.860 --> 00:27:06.820]   Anyways, I just want you to get your reaction
[00:27:06.820 --> 00:27:07.740]   to this passage.
[00:27:07.740 --> 00:27:10.420]   So he writes, "Yale students," he said, "are like stem cells.
[00:27:10.420 --> 00:27:11.700]   They can be anything in the world,
[00:27:11.700 --> 00:27:14.420]   so they try to delay for as long as possible
[00:27:14.420 --> 00:27:15.580]   the moment when they have to become
[00:27:15.580 --> 00:27:17.180]   just one thing in particular.
[00:27:17.180 --> 00:27:20.420]   Possibility paradoxically becomes limitation."
[00:27:20.420 --> 00:27:22.700]   I guess I'll get your reaction to that generically,
[00:27:22.700 --> 00:27:23.660]   but I'm curious,
[00:27:23.660 --> 00:27:26.060]   to the extent that that quote reflects something true,
[00:27:26.060 --> 00:27:29.660]   is it post-election in the sense of, you know,
[00:27:29.660 --> 00:27:30.980]   the people who get out of Yale
[00:27:30.980 --> 00:27:32.540]   are just like become risk averse,
[00:27:32.540 --> 00:27:34.420]   or is it just the people who go to Yale in the first place
[00:27:34.420 --> 00:27:35.740]   and getting generic degrees?
[00:27:35.740 --> 00:27:37.540]   Are they risk averse in the first place?
[00:27:37.540 --> 00:27:39.180]   They are looking for an option, basically,
[00:27:39.180 --> 00:27:42.020]   in the form of the insurance that Yale University offers.
[00:27:42.020 --> 00:27:43.180]   What's going on?
[00:27:43.180 --> 00:27:46.740]   - Well, I would say it's probably a lot of selection effect
[00:27:46.740 --> 00:27:49.700]   because I do encounter a lot of really bright people
[00:27:49.700 --> 00:27:51.140]   who did not go to elite schools
[00:27:51.140 --> 00:27:53.180]   and did really interesting things instead.
[00:27:53.180 --> 00:27:55.620]   And so I don't actually,
[00:27:55.620 --> 00:27:57.900]   I don't know that the schools really make people conformists,
[00:27:57.900 --> 00:28:01.860]   but definitely if you can get into a really good school
[00:28:01.860 --> 00:28:04.060]   and then you can go straight from there
[00:28:04.060 --> 00:28:05.700]   into consulting or banking,
[00:28:05.700 --> 00:28:08.020]   or I think at this point, big tech,
[00:28:08.020 --> 00:28:09.700]   it is a pretty conformist thing to do.
[00:28:09.700 --> 00:28:12.300]   Like no one is going to think you made a bad decision.
[00:28:12.300 --> 00:28:13.860]   Whereas if you have that opportunity
[00:28:13.860 --> 00:28:14.980]   and you go do something else,
[00:28:14.980 --> 00:28:16.740]   then a lot of people are going to wonder
[00:28:16.740 --> 00:28:17.740]   what's really going on.
[00:28:17.740 --> 00:28:22.700]   So I do think that it selects for some risk aversion.
[00:28:22.700 --> 00:28:25.660]   And some of that is just that it's incredibly competitive
[00:28:25.660 --> 00:28:27.940]   to get into these schools.
[00:28:27.940 --> 00:28:29.620]   There's a lot of randomness at the top now,
[00:28:29.620 --> 00:28:33.180]   and that is partly just a feature of competition
[00:28:33.180 --> 00:28:35.060]   that the fiercer competition gets
[00:28:35.060 --> 00:28:36.940]   and the more stratified things are,
[00:28:36.940 --> 00:28:39.860]   the more likely it is that any one person is where they are
[00:28:39.860 --> 00:28:43.500]   because they were a little bit underslept on test day
[00:28:43.500 --> 00:28:45.660]   or got a couple of answers right or something like that.
[00:28:45.660 --> 00:28:48.500]   It's a lot more skewed to randomness at the high end,
[00:28:48.500 --> 00:28:50.100]   which actually would make people more risk averse
[00:28:50.100 --> 00:28:53.180]   because if they got into a worse school
[00:28:53.180 --> 00:28:54.700]   and they really deserve to,
[00:28:54.700 --> 00:28:57.220]   then they're stuck with this feeling of,
[00:28:57.220 --> 00:28:59.380]   it's hard to attain things.
[00:28:59.380 --> 00:29:02.300]   And so it is worth going on the safer path.
[00:29:02.300 --> 00:29:04.660]   Whereas if they get into a really good school
[00:29:04.660 --> 00:29:06.340]   that is better than they expected,
[00:29:06.340 --> 00:29:09.460]   then they have this sense of,
[00:29:09.460 --> 00:29:12.260]   as long as I stay on this exact path,
[00:29:12.260 --> 00:29:16.260]   I am actually continuing to be 99.9th percentile
[00:29:16.260 --> 00:29:18.340]   instead of being stuck at 99.5th.
[00:29:18.340 --> 00:29:21.660]   And so that is the best life I could expect to have.
[00:29:21.660 --> 00:29:24.980]   It is really hard to force people to have agency.
[00:29:24.980 --> 00:29:26.740]   It's really hard to force people to take risks
[00:29:26.740 --> 00:29:31.740]   except in these kind of simulated fake sort of ways.
[00:29:32.100 --> 00:29:34.780]   But it is an important thing for people to do.
[00:29:34.780 --> 00:29:38.140]   And that essay, I'm very happy with it
[00:29:38.140 --> 00:29:40.780]   in part because it was fun to write something bombastic
[00:29:40.780 --> 00:29:43.780]   and in part because I think it's true that,
[00:29:43.780 --> 00:29:45.820]   and it's like, that's actually a fun one
[00:29:45.820 --> 00:29:47.700]   from a financial modeling perspective
[00:29:47.700 --> 00:29:49.580]   because one of the things that I realized
[00:29:49.580 --> 00:29:51.620]   when I was thinking about optionality
[00:29:51.620 --> 00:29:54.300]   and like optionality in your life,
[00:29:54.300 --> 00:29:56.260]   being able to choose a lot of different life paths
[00:29:56.260 --> 00:29:58.380]   and not having to be stuck with one thing
[00:29:58.380 --> 00:30:00.700]   is that in financial markets,
[00:30:00.700 --> 00:30:03.300]   you can buy optionality very directly.
[00:30:03.300 --> 00:30:04.580]   You buy stock options.
[00:30:04.580 --> 00:30:06.940]   And it turns out that the converse of that,
[00:30:06.940 --> 00:30:08.220]   selling people optionality
[00:30:08.220 --> 00:30:10.060]   is a generally profitable strategy.
[00:30:10.060 --> 00:30:12.020]   It does have drawdowns during a market crash,
[00:30:12.020 --> 00:30:15.340]   but in general, you make more money over time
[00:30:15.340 --> 00:30:17.220]   by systematically selling optionality
[00:30:17.220 --> 00:30:18.940]   rather than by systematically buying it.
[00:30:18.940 --> 00:30:20.380]   And I suspect that something like that
[00:30:20.380 --> 00:30:21.980]   is probably true in the real world,
[00:30:21.980 --> 00:30:24.660]   especially because there are very few people
[00:30:24.660 --> 00:30:26.820]   who are explicitly saying,
[00:30:26.820 --> 00:30:28.500]   I'm trying to minimize my optionality.
[00:30:28.500 --> 00:30:32.340]   I'm trying to cut off options and not have choices.
[00:30:32.340 --> 00:30:35.740]   So given that there are some decisions you can make
[00:30:35.740 --> 00:30:40.100]   that do require you to forego other decisions,
[00:30:40.100 --> 00:30:42.420]   I suspect that those decisions are going to be good,
[00:30:42.420 --> 00:30:43.420]   or they're going to be better for you
[00:30:43.420 --> 00:30:44.780]   just because so many people
[00:30:44.780 --> 00:30:46.620]   are looking for maximum optionality,
[00:30:46.620 --> 00:30:48.420]   so they're not taking them.
[00:30:48.420 --> 00:30:49.860]   - Yeah, yeah.
[00:30:49.860 --> 00:30:51.980]   One way of reading Tyler Cowen's "Complacent Class"
[00:30:51.980 --> 00:30:54.380]   is basically just a way of like our preferences,
[00:30:54.380 --> 00:30:56.500]   our preferences for optionality have grown a lot.
[00:30:56.500 --> 00:30:59.300]   You can think of it in terms of not starting a business,
[00:30:59.300 --> 00:31:01.740]   not having a family early, at least.
[00:31:01.740 --> 00:31:03.780]   That's just a way of preserving your optionality.
[00:31:03.780 --> 00:31:05.500]   What is the explanation for,
[00:31:05.500 --> 00:31:07.500]   maybe it's because we're wealthier,
[00:31:07.500 --> 00:31:10.500]   maybe it's because of the demographics of the society.
[00:31:10.500 --> 00:31:12.340]   What reason explains,
[00:31:12.340 --> 00:31:14.740]   if it's true that we're a society
[00:31:14.740 --> 00:31:16.940]   where we value optionality more than we did before,
[00:31:16.940 --> 00:31:17.900]   what explains that?
[00:31:17.900 --> 00:31:21.340]   - Yeah, I think a lot of it is this wealth effect
[00:31:21.340 --> 00:31:25.140]   that in general, money is a way to get freedom
[00:31:25.140 --> 00:31:27.980]   in individual ways and then in aggregate ways.
[00:31:27.980 --> 00:31:30.540]   And certainly you have a lot less optionality
[00:31:30.540 --> 00:31:32.340]   if you're living in a subsistence society
[00:31:32.340 --> 00:31:34.980]   where your choices are either do something
[00:31:34.980 --> 00:31:36.380]   that gets you fed or starve.
[00:31:36.380 --> 00:31:38.580]   So not a lot of optionality there.
[00:31:38.580 --> 00:31:42.860]   So we definitely have more choices as we get richer.
[00:31:42.860 --> 00:31:45.740]   And I think one of the issues is that
[00:31:45.740 --> 00:31:48.940]   we are partly wired to be more satisfied
[00:31:48.940 --> 00:31:52.140]   with actually making choices and pursuing specific goals
[00:31:52.140 --> 00:31:53.940]   than with making the choice
[00:31:53.940 --> 00:31:55.820]   to not pursue any specific goals.
[00:31:55.820 --> 00:32:00.940]   I don't really know what the deeper, darker meaning
[00:32:00.940 --> 00:32:04.100]   of this pursuit of optionality is.
[00:32:04.100 --> 00:32:07.260]   I just, I know a lot of people talk about it explicitly.
[00:32:07.260 --> 00:32:09.620]   I know a lot of people talk about it implicitly.
[00:32:09.620 --> 00:32:12.940]   And it seems like there's this search for optionality
[00:32:12.940 --> 00:32:14.620]   that takes place on a kind of fractal level
[00:32:14.620 --> 00:32:18.700]   where not only do people want to choose a job
[00:32:18.700 --> 00:32:20.740]   with lots of different kinds of exit opportunities
[00:32:20.740 --> 00:32:22.060]   depending on what they want in the future,
[00:32:22.060 --> 00:32:24.740]   but they'll try to make their plans
[00:32:24.740 --> 00:32:26.580]   to have lots of different plans
[00:32:26.580 --> 00:32:29.060]   so that if they decide they don't feel like
[00:32:29.060 --> 00:32:30.620]   going out this night,
[00:32:30.620 --> 00:32:32.020]   they have an option to go out the next night
[00:32:32.020 --> 00:32:33.020]   and maybe they'll skip that
[00:32:33.020 --> 00:32:34.420]   and go out the next night and so on.
[00:32:34.420 --> 00:32:36.540]   And you just, you get less done
[00:32:36.540 --> 00:32:38.100]   if you have lots of opportunities
[00:32:38.100 --> 00:32:40.940]   and you could easily forego any one of them.
[00:32:40.940 --> 00:32:43.700]   But there just aren't that many people
[00:32:43.700 --> 00:32:47.860]   who succeeded in a memorable way
[00:32:47.860 --> 00:32:50.060]   because they kept all of their options open.
[00:32:50.060 --> 00:32:51.420]   I think the closest to that you can get
[00:32:51.420 --> 00:32:53.540]   is that there have been a number of people
[00:32:53.540 --> 00:32:55.820]   who made their money in smart deals
[00:32:55.820 --> 00:32:57.780]   where they did manage to control the downside.
[00:32:57.780 --> 00:33:00.340]   But to take the optionality example,
[00:33:00.340 --> 00:33:02.620]   that is more like being a structured products trader
[00:33:02.620 --> 00:33:03.660]   or being an options trader
[00:33:03.660 --> 00:33:05.700]   who is trying to construct
[00:33:05.700 --> 00:33:07.660]   this pretty elaborate payoff structure
[00:33:07.660 --> 00:33:11.420]   where you get to control your losses on one side
[00:33:11.420 --> 00:33:14.340]   and then you're maximizing your payoff on some other side.
[00:33:14.340 --> 00:33:18.260]   And that is a lot more complicated than just optionality.
[00:33:18.260 --> 00:33:21.260]   That is, you're not buying it wholesale.
[00:33:21.260 --> 00:33:23.060]   You're actually picking and choosing
[00:33:23.060 --> 00:33:24.940]   which options you care about and which options you don't.
[00:33:24.940 --> 00:33:27.620]   And that's, I think that is something worth keeping in mind.
[00:33:27.620 --> 00:33:29.260]   And maybe I didn't emphasize enough in the essay
[00:33:29.260 --> 00:33:31.820]   that I'm not saying you should always choose
[00:33:31.820 --> 00:33:33.260]   the minimum optionality choice.
[00:33:33.260 --> 00:33:36.540]   Like, you could read that essay
[00:33:36.540 --> 00:33:38.380]   and immediately sign up for the military
[00:33:38.380 --> 00:33:40.260]   or join a religious order or something
[00:33:40.260 --> 00:33:44.420]   and have no more choices in your life for a long period.
[00:33:44.420 --> 00:33:46.020]   I'm not saying you should immediately seize
[00:33:46.020 --> 00:33:47.540]   the opportunity to give up options.
[00:33:47.540 --> 00:33:50.060]   I'm saying that you should be very judicious about them
[00:33:50.060 --> 00:33:53.060]   and that, going back to the markets example,
[00:33:53.060 --> 00:33:55.420]   if markets are not perfectly efficient,
[00:33:55.420 --> 00:33:57.020]   but they are pretty efficient,
[00:33:57.020 --> 00:33:59.500]   you should expect that you can get a superior payoff,
[00:33:59.500 --> 00:34:01.700]   but only if you are actually doing pretty good work
[00:34:01.700 --> 00:34:03.420]   and only if you're doing something
[00:34:03.420 --> 00:34:04.900]   you're actually pretty good at.
[00:34:04.900 --> 00:34:05.740]   - Yeah.
[00:34:05.740 --> 00:34:08.100]   I mean, like some of the risk aversion in society
[00:34:08.100 --> 00:34:10.300]   can be explained by optionality alone.
[00:34:10.300 --> 00:34:13.140]   It's just somewhat conformity.
[00:34:13.140 --> 00:34:14.700]   You can think in terms of like,
[00:34:14.700 --> 00:34:16.820]   one of the examples I like is,
[00:34:16.820 --> 00:34:18.260]   think about somebody who becomes a dentist, right?
[00:34:18.260 --> 00:34:19.980]   It takes like, what, 12 years or something,
[00:34:19.980 --> 00:34:21.900]   like $500,000 or more in student loans.
[00:34:21.900 --> 00:34:23.100]   And it's a very specific thing to do.
[00:34:23.100 --> 00:34:24.460]   You're getting rid of options.
[00:34:24.460 --> 00:34:25.740]   What if you just spend those 12 years
[00:34:25.740 --> 00:34:26.620]   and not that much money,
[00:34:26.620 --> 00:34:28.260]   just trying to start like 12 businesses,
[00:34:28.260 --> 00:34:29.100]   each one, every year.
[00:34:29.100 --> 00:34:31.700]   See if it fails or you just move on.
[00:34:31.700 --> 00:34:34.460]   In some sense, that option or that choice,
[00:34:34.460 --> 00:34:35.500]   you have more optionality,
[00:34:35.500 --> 00:34:36.940]   but people are still less willing to do that
[00:34:36.940 --> 00:34:37.780]   for some reason.
[00:34:37.780 --> 00:34:40.580]   Oh, you mentioned that, you know,
[00:34:40.580 --> 00:34:43.380]   people trying to get accredited in ways that, you know,
[00:34:43.380 --> 00:34:47.540]   there's a group of people who are doing cool things,
[00:34:47.540 --> 00:34:48.380]   but they, you know,
[00:34:48.380 --> 00:34:50.540]   they're not trying to get accredited by elite universities.
[00:34:50.540 --> 00:34:53.780]   Is that, are people acting irrationally
[00:34:53.780 --> 00:34:56.260]   by trying to get accredited from elite universities?
[00:34:56.260 --> 00:34:57.660]   You can look at your example.
[00:34:57.660 --> 00:35:00.340]   I mean, you know, you got a well accredited,
[00:35:00.340 --> 00:35:03.220]   but just by being a very excellent SEO writer.
[00:35:03.220 --> 00:35:05.100]   And in some sense,
[00:35:05.100 --> 00:35:07.060]   maybe more people should be trying to do that, right?
[00:35:07.060 --> 00:35:10.540]   Like you're getting a better credential by doing that.
[00:35:10.540 --> 00:35:12.780]   - Yeah, I would still,
[00:35:12.780 --> 00:35:15.580]   I still view elite higher education
[00:35:15.580 --> 00:35:18.140]   as probably a pretty good deal
[00:35:18.140 --> 00:35:19.540]   for the individuals who are doing it.
[00:35:19.540 --> 00:35:21.540]   I just think it's a bad deal for society
[00:35:21.540 --> 00:35:23.020]   that so many people are pursuing it.
[00:35:23.020 --> 00:35:24.940]   So like, yeah, if you have the opportunity
[00:35:24.940 --> 00:35:26.540]   to go to a great school
[00:35:26.540 --> 00:35:29.060]   and you're considering just not doing that
[00:35:29.060 --> 00:35:30.900]   and figuring out what to do instead,
[00:35:30.900 --> 00:35:31.740]   that is probably,
[00:35:31.740 --> 00:35:33.220]   you should probably just go to the school.
[00:35:33.220 --> 00:35:37.340]   But if you are going to a really good school
[00:35:37.340 --> 00:35:38.940]   and you see a particular opportunity
[00:35:38.940 --> 00:35:40.620]   that you want to seize right now,
[00:35:40.620 --> 00:35:43.220]   and you know that that entails dropping out.
[00:35:43.220 --> 00:35:45.460]   And like, if there's one specific reason,
[00:35:45.460 --> 00:35:49.260]   not to go, then that is when you should not go
[00:35:49.260 --> 00:35:50.100]   or at least drop out.
[00:35:50.100 --> 00:35:51.660]   And you still have optionality there.
[00:35:51.660 --> 00:35:55.660]   Like, I think last time I looked at this,
[00:35:55.660 --> 00:35:58.140]   both Mark Zuckerberg and Bill Gates
[00:35:58.140 --> 00:36:00.340]   are technically on,
[00:36:00.340 --> 00:36:02.260]   they're on some kind of like hiatus,
[00:36:02.260 --> 00:36:03.460]   sabbatical thing from Harvard.
[00:36:03.460 --> 00:36:05.460]   Like they could just enroll,
[00:36:05.460 --> 00:36:07.900]   I think next semester and start taking classes again
[00:36:07.900 --> 00:36:08.740]   if they wanted to finish.
[00:36:08.740 --> 00:36:13.180]   So, and I guess maybe that is an example
[00:36:13.180 --> 00:36:16.300]   of just how far the preference for optionality goes
[00:36:16.300 --> 00:36:19.860]   that even like once you get into a good school,
[00:36:19.860 --> 00:36:22.460]   like they actually won't let you drop out by default.
[00:36:22.460 --> 00:36:24.420]   You have to ask them if,
[00:36:24.420 --> 00:36:26.060]   you have to ask them if when you leave,
[00:36:26.060 --> 00:36:29.180]   they will not let you back in or they just will.
[00:36:29.180 --> 00:36:32.500]   But yeah, in general, going to a good school,
[00:36:32.500 --> 00:36:33.860]   like it's a good credential
[00:36:33.860 --> 00:36:35.180]   and it's just, it's hard for someone
[00:36:35.180 --> 00:36:37.540]   who is say 22 years old,
[00:36:37.540 --> 00:36:39.660]   it is hard for them to do something
[00:36:39.660 --> 00:36:41.460]   that they expect to succeed in
[00:36:41.460 --> 00:36:43.260]   that is more impressive than
[00:36:43.260 --> 00:36:44.660]   getting a bachelor's degree from Harvard.
[00:36:44.660 --> 00:36:45.500]   That is just like,
[00:36:45.500 --> 00:36:46.980]   you can do things that are more impressive than that,
[00:36:46.980 --> 00:36:50.780]   but your expected outcome is probably lower,
[00:36:50.780 --> 00:36:52.060]   your median outcome is lower.
[00:36:52.060 --> 00:36:54.940]   On the other hand, if you do succeed in something else
[00:36:54.940 --> 00:36:58.980]   and you also turn down the easy prestige,
[00:36:58.980 --> 00:37:01.980]   the easy credential, then that's even more impressive.
[00:37:01.980 --> 00:37:03.740]   And it's not just externally,
[00:37:03.740 --> 00:37:05.620]   I think there's something really important internally
[00:37:05.620 --> 00:37:09.460]   about having this experience of having an easy way
[00:37:09.460 --> 00:37:11.500]   and choosing to do something else,
[00:37:11.500 --> 00:37:13.180]   even though it's much more risky,
[00:37:13.180 --> 00:37:16.540]   that you know that you have slain the optionality dragon
[00:37:16.540 --> 00:37:19.340]   and it's other heads are gonna grow back at some point,
[00:37:19.340 --> 00:37:21.940]   but you have actually fought it one
[00:37:21.940 --> 00:37:24.700]   and that's gonna stick with you.
[00:37:24.700 --> 00:37:25.820]   - Yeah, yeah.
[00:37:25.820 --> 00:37:28.860]   One of my previous guests, Scott Young,
[00:37:28.860 --> 00:37:30.540]   he's famous for basically,
[00:37:30.540 --> 00:37:31.540]   I don't know if you heard about this,
[00:37:31.540 --> 00:37:33.500]   but he did something called the MIT Challenge,
[00:37:33.500 --> 00:37:35.420]   where instead of going to MIT,
[00:37:35.420 --> 00:37:36.940]   he just, they have an open course,
[00:37:36.940 --> 00:37:38.780]   MIT has like all their classes online.
[00:37:38.780 --> 00:37:39.620]   He just did a thing.
[00:37:39.620 --> 00:37:41.300]   I'm gonna get a four years bachelor's education
[00:37:41.300 --> 00:37:42.980]   in computer science for MIT
[00:37:42.980 --> 00:37:44.780]   by just doing the online coursework.
[00:37:44.780 --> 00:37:48.380]   And if I get above an F on the final exam,
[00:37:48.380 --> 00:37:49.980]   which I self administer,
[00:37:49.980 --> 00:37:51.940]   then I consider myself passing the class.
[00:37:51.940 --> 00:37:52.780]   And he does this.
[00:37:52.780 --> 00:37:54.100]   I mean, in some sense,
[00:37:54.100 --> 00:37:55.700]   given the fact that it's truncated,
[00:37:55.700 --> 00:37:58.380]   he did less work than getting an MIT education would be,
[00:37:58.380 --> 00:38:00.740]   but he got tons of job offers from that.
[00:38:00.740 --> 00:38:02.740]   Maybe people just aren't creative enough.
[00:38:02.740 --> 00:38:07.780]   Finding a way to get a good signal to employers
[00:38:07.780 --> 00:38:08.940]   is not an efficient market.
[00:38:08.940 --> 00:38:11.020]   It's just that people, 19 year olds,
[00:38:11.020 --> 00:38:11.860]   you wouldn't expect them
[00:38:11.860 --> 00:38:13.460]   to pick up $100 bills on the floor.
[00:38:13.460 --> 00:38:17.260]   - Yeah, I think one thing to note on that
[00:38:17.260 --> 00:38:21.580]   is that a lot of people who try that
[00:38:21.580 --> 00:38:23.540]   will actually find that they've really overestimated
[00:38:23.540 --> 00:38:24.540]   their willpower.
[00:38:24.540 --> 00:38:26.340]   And I think this is actually one of the functions
[00:38:26.340 --> 00:38:27.580]   that a school like MIT serves
[00:38:27.580 --> 00:38:30.340]   is just to consistently motivate you
[00:38:30.340 --> 00:38:31.900]   both by surrounding you with peers
[00:38:31.900 --> 00:38:34.380]   and by giving you professors who are very smart
[00:38:34.380 --> 00:38:35.500]   and whose judgment you respect.
[00:38:35.500 --> 00:38:37.020]   And so if your professor tells you,
[00:38:37.020 --> 00:38:38.460]   this is C minus work,
[00:38:38.460 --> 00:38:40.300]   you actually feel bad and you try to do better.
[00:38:40.300 --> 00:38:41.140]   Whereas with yourself,
[00:38:41.140 --> 00:38:42.900]   it's a lot harder to motivate yourself.
[00:38:42.900 --> 00:38:43.740]   You can do it,
[00:38:43.740 --> 00:38:48.260]   but you will often have to pick your own.
[00:38:48.260 --> 00:38:50.700]   You have to pick your own topics to study to do that.
[00:38:50.700 --> 00:38:52.180]   And then there's no credential.
[00:38:52.180 --> 00:38:53.020]   There's not even like,
[00:38:53.020 --> 00:38:54.380]   I could have gotten this credential,
[00:38:54.380 --> 00:38:56.100]   but I didn't go to this particular school.
[00:38:56.100 --> 00:38:57.940]   So I'm equipped to do the coursework,
[00:38:57.940 --> 00:38:58.900]   but I did it somewhere else.
[00:38:58.900 --> 00:39:02.580]   It's more like, this is not really a thing,
[00:39:02.580 --> 00:39:04.100]   but it's a thing I'm actually really good at.
[00:39:04.100 --> 00:39:08.740]   And so you have to both do the work
[00:39:08.740 --> 00:39:12.740]   and then sell people on treating that work as credible.
[00:39:12.740 --> 00:39:15.140]   - Yeah, so I mean, speaking of that,
[00:39:15.140 --> 00:39:16.540]   you wrote an essay called,
[00:39:16.540 --> 00:39:17.580]   I forget the title exactly,
[00:39:17.580 --> 00:39:19.540]   but it's basically the idea is that introverts
[00:39:19.540 --> 00:39:21.580]   will be more influential in the future.
[00:39:21.580 --> 00:39:24.300]   And if you think about the big five personality traits,
[00:39:24.300 --> 00:39:27.980]   like introversion is one that will be more valuable.
[00:39:27.980 --> 00:39:29.220]   I mean, given the fact,
[00:39:29.220 --> 00:39:31.900]   I mean, Tyler Cowen makes this point in "Average is Over"
[00:39:31.900 --> 00:39:33.180]   that he expects conscientiousness
[00:39:33.180 --> 00:39:35.460]   to become a highly valuable trade in the future.
[00:39:35.460 --> 00:39:37.180]   In that essay, I think you basically said
[00:39:37.180 --> 00:39:39.300]   that actually conscientiousness might become less important,
[00:39:39.300 --> 00:39:41.420]   or at least you didn't emphasize it.
[00:39:41.420 --> 00:39:44.460]   I mean, comparing conscientiousness to introversion,
[00:39:44.460 --> 00:39:48.380]   I mean, obviously you have things like self-education,
[00:39:48.380 --> 00:39:49.620]   but there's just so many other things
[00:39:49.620 --> 00:39:53.540]   where given that you can get more returns
[00:39:53.540 --> 00:39:56.540]   by doing clever and more things,
[00:39:56.540 --> 00:39:58.540]   scaling them up to a higher level.
[00:39:58.540 --> 00:40:00.540]   And maybe conscientiousness is a more important trade
[00:40:00.540 --> 00:40:01.500]   in the new economy.
[00:40:03.140 --> 00:40:06.460]   - Well, conscientiousness is really important
[00:40:06.460 --> 00:40:09.300]   if you have goals that are set by somebody else,
[00:40:09.300 --> 00:40:11.660]   and it's really what lets you achieve those goals.
[00:40:11.660 --> 00:40:14.460]   And so I would rate myself as not very conscientious at all
[00:40:14.460 --> 00:40:18.500]   because I'm pretty bad at fulfilling totally external goals,
[00:40:18.500 --> 00:40:20.780]   like almost deliberately bad at it,
[00:40:20.780 --> 00:40:24.380]   but I do work a lot on things that I'm interested in.
[00:40:24.380 --> 00:40:27.540]   So what, and this may be,
[00:40:27.540 --> 00:40:29.620]   it may be almost a terminological difference
[00:40:29.620 --> 00:40:31.420]   on what conscientiousness is,
[00:40:31.420 --> 00:40:34.340]   because I would view it as
[00:40:34.340 --> 00:40:36.980]   if someone asks you to do something
[00:40:36.980 --> 00:40:39.820]   and it's some arbitrary selected thing,
[00:40:39.820 --> 00:40:41.580]   arbitrary thing that they select,
[00:40:41.580 --> 00:40:43.500]   are you likely to get it done?
[00:40:43.500 --> 00:40:44.620]   And maybe that's likely to get it done
[00:40:44.620 --> 00:40:45.620]   conditional on promising to,
[00:40:45.620 --> 00:40:47.780]   or conditional on being expected to or whatever,
[00:40:47.780 --> 00:40:50.340]   but just it's a measure of that expectation.
[00:40:50.340 --> 00:40:54.420]   And that people have low expectations for me
[00:40:54.420 --> 00:40:56.220]   on many of those things.
[00:40:56.220 --> 00:40:59.380]   But if conscientiousness is just,
[00:40:59.380 --> 00:41:02.820]   are you willing to put in effort to accomplish something,
[00:41:02.820 --> 00:41:05.820]   then that's something that I am willing to do.
[00:41:05.820 --> 00:41:09.340]   So my view, and this is kind of an averages overview,
[00:41:09.340 --> 00:41:14.340]   is that as the cost of communication gets lower,
[00:41:14.340 --> 00:41:17.020]   as transaction costs decline,
[00:41:17.020 --> 00:41:19.580]   and as more of the economy gets mediated through software,
[00:41:19.580 --> 00:41:21.660]   there are just going to be more specialized jobs.
[00:41:21.660 --> 00:41:24.820]   And if you are really interested in one of them
[00:41:24.820 --> 00:41:26.940]   and you're just naturally interested,
[00:41:26.940 --> 00:41:28.220]   you would do it for free,
[00:41:28.220 --> 00:41:30.460]   then you don't need to be especially conscientious
[00:41:30.460 --> 00:41:31.460]   to succeed in it.
[00:41:31.460 --> 00:41:34.460]   I guess there's another side to the conscientiousness thing.
[00:41:34.460 --> 00:41:38.060]   And I wish I knew what Tyler Cowen's exact argument is,
[00:41:38.060 --> 00:41:42.660]   because another reason that you might actually want to,
[00:41:42.660 --> 00:41:44.940]   you might think conscientiousness would pay better
[00:41:44.940 --> 00:41:46.980]   is that entertainment is getting so much better
[00:41:46.980 --> 00:41:48.940]   and is getting much more addictive
[00:41:48.940 --> 00:41:51.140]   and it's getting better at marketing itself to you.
[00:41:51.140 --> 00:41:54.780]   So just being able to resist the siren song
[00:41:54.780 --> 00:41:56.420]   of Netflix and video games
[00:41:56.420 --> 00:41:58.460]   is becoming a more valuable skill.
[00:41:58.460 --> 00:42:01.380]   But then again, there are times
[00:42:01.380 --> 00:42:03.740]   when I will just be messing around and waste my time
[00:42:03.740 --> 00:42:06.420]   and I'll realize that there's something more interesting
[00:42:06.420 --> 00:42:07.260]   I could be working on
[00:42:07.260 --> 00:42:09.820]   and it would actually produce something tangible
[00:42:09.820 --> 00:42:11.420]   that subscribers would pay for
[00:42:11.420 --> 00:42:12.420]   and that I would be proud of.
[00:42:12.420 --> 00:42:16.900]   And so I get Apple Q and get back to work.
[00:42:16.900 --> 00:42:19.060]   - Yeah, that's surprising
[00:42:19.060 --> 00:42:21.140]   that you wouldn't consider yourself especially conscientious.
[00:42:21.140 --> 00:42:22.540]   I mean, the distinction makes sense,
[00:42:22.540 --> 00:42:24.540]   but given the fact that you produce
[00:42:24.540 --> 00:42:26.500]   a lengthy loose seller every day.
[00:42:26.500 --> 00:42:30.100]   The argument that Tyler Cowen makes is,
[00:42:30.100 --> 00:42:31.060]   I mean, obviously self-education.
[00:42:31.060 --> 00:42:32.340]   Another point he makes is that,
[00:42:32.340 --> 00:42:34.860]   businesses are bigger now, you have global supply chains.
[00:42:34.860 --> 00:42:37.580]   So if you have a person who's kind of a rebel without a cause
[00:42:37.580 --> 00:42:39.500]   or even a rebel with a cause,
[00:42:39.500 --> 00:42:44.500]   just being unreliable and being volatile in some sense
[00:42:44.500 --> 00:42:47.100]   makes you much more dangerous
[00:42:47.100 --> 00:42:48.580]   than you would have been in the past
[00:42:48.580 --> 00:42:50.580]   because you can mess up more things.
[00:42:50.580 --> 00:42:52.220]   I don't exactly know what that means
[00:42:52.220 --> 00:42:53.140]   'cause I've never interacted
[00:42:53.140 --> 00:42:54.860]   with these like global supply chains myself,
[00:42:54.860 --> 00:42:56.780]   but that's just what I remember.
[00:42:56.780 --> 00:43:00.500]   - Yeah, well, I wonder about that
[00:43:00.500 --> 00:43:03.260]   because I suspect that some of the people at Apple
[00:43:03.260 --> 00:43:05.580]   who are managing things like,
[00:43:05.580 --> 00:43:08.220]   managing their response to COVID or whatever,
[00:43:08.220 --> 00:43:09.540]   I suspect that they're doing this in part
[00:43:09.540 --> 00:43:10.980]   because it's a really fun challenge.
[00:43:10.980 --> 00:43:12.540]   Like I'm sure they're paid well and everything
[00:43:12.540 --> 00:43:14.740]   and I'm sure that's a big motivator too.
[00:43:14.740 --> 00:43:19.220]   But it actually sounds kind of like a fun puzzle to say,
[00:43:19.220 --> 00:43:22.700]   if Shenzhen shuts down
[00:43:22.700 --> 00:43:26.460]   and most of your manufacturing is close to Shenzhen,
[00:43:26.460 --> 00:43:27.700]   what do you do right now?
[00:43:27.700 --> 00:43:28.540]   What do you do in a week?
[00:43:28.540 --> 00:43:29.540]   What do you do in a month?
[00:43:29.540 --> 00:43:31.620]   And by the way, what is your plan for making sure
[00:43:31.620 --> 00:43:33.340]   that we never have this problem again?
[00:43:33.340 --> 00:43:36.260]   So there could be just some general appeal.
[00:43:36.260 --> 00:43:38.700]   Like "Factorio" is a very popular video game
[00:43:38.700 --> 00:43:40.260]   and the whole point of it
[00:43:40.260 --> 00:43:42.580]   is managing a complicated supply chain
[00:43:42.580 --> 00:43:46.140]   and managing it around both passively
[00:43:46.140 --> 00:43:48.580]   just increasing production
[00:43:48.580 --> 00:43:52.100]   and also actively managing to deal with various crises
[00:43:52.100 --> 00:43:52.940]   that interfere with it.
[00:43:52.940 --> 00:43:55.060]   So I think there are people,
[00:43:55.060 --> 00:43:57.100]   I mean, maybe the people who are really into that
[00:43:57.100 --> 00:43:58.300]   are all playing "Factorio"
[00:43:58.300 --> 00:44:00.220]   and not actually getting jobs at Apple,
[00:44:00.220 --> 00:44:02.700]   but there's some fun to that.
[00:44:02.700 --> 00:44:06.380]   And I suspect they're actually pretty proud of what they do.
[00:44:06.380 --> 00:44:08.980]   And it's probably pretty,
[00:44:08.980 --> 00:44:10.820]   I mean, maybe it's not fun in the moment,
[00:44:10.820 --> 00:44:13.940]   but I suspect that when a lot of the Apple supply chain
[00:44:13.940 --> 00:44:18.300]   people look back on the first calendar quarter of 2020,
[00:44:18.300 --> 00:44:22.900]   they think of that as just a really, really cool experience.
[00:44:22.900 --> 00:44:26.020]   Not a fun one, but a gratifying one in retrospect.
[00:44:26.020 --> 00:44:27.460]   - Yeah, interesting.
[00:44:27.460 --> 00:44:28.740]   You wrote an article saying
[00:44:28.740 --> 00:44:31.140]   that investing is rationality dojo.
[00:44:31.140 --> 00:44:35.540]   I'm curious why then the subculture of rationality
[00:44:35.540 --> 00:44:37.380]   isn't more centered around finance
[00:44:37.380 --> 00:44:39.860]   than it is around programming.
[00:44:39.860 --> 00:44:42.900]   Maybe debugging is also rationality dojo,
[00:44:42.900 --> 00:44:45.500]   but it would seem that if this community
[00:44:45.500 --> 00:44:47.020]   were gonna be centered in some industry,
[00:44:47.020 --> 00:44:48.420]   it would be finance.
[00:44:48.420 --> 00:44:50.940]   - That's a good question.
[00:44:50.940 --> 00:44:54.100]   I have noticed, I mean, I would hang out
[00:44:54.100 --> 00:44:56.900]   with some of the rationalists in New York
[00:44:56.900 --> 00:44:58.100]   and a lot of them are in finance,
[00:44:58.100 --> 00:45:00.220]   but then again, I'm hanging out with people in New York.
[00:45:00.220 --> 00:45:03.100]   So of course, a lot of them are already in finance.
[00:45:03.100 --> 00:45:05.740]   I think the rationalist subculture
[00:45:05.740 --> 00:45:08.260]   does skew a little more to finance
[00:45:08.260 --> 00:45:11.340]   than just is representative.
[00:45:11.340 --> 00:45:12.660]   But you're right, it's definitely more
[00:45:12.660 --> 00:45:14.180]   of a programming thing.
[00:45:14.180 --> 00:45:18.420]   One possibility is that the rationalists are,
[00:45:18.420 --> 00:45:22.940]   they do talk about the center of their subculture
[00:45:22.940 --> 00:45:24.180]   was less wrong.
[00:45:24.180 --> 00:45:25.540]   And so they're clearly not saying
[00:45:25.540 --> 00:45:26.540]   we're going to be perfectly right.
[00:45:26.540 --> 00:45:29.860]   They're saying we're going to identify these flaws one by one
[00:45:29.860 --> 00:45:31.660]   and improve them over time.
[00:45:31.660 --> 00:45:33.700]   They're not trying to reach
[00:45:33.700 --> 00:45:36.100]   some kind of perfect understanding.
[00:45:36.100 --> 00:45:37.420]   At least, I think most of them are.
[00:45:37.420 --> 00:45:39.780]   Most of them would say that that's not the end state
[00:45:39.780 --> 00:45:41.500]   that they expect to achieve.
[00:45:41.500 --> 00:45:44.900]   Maybe it just, maybe this approach, though,
[00:45:44.900 --> 00:45:47.980]   of believing that you can solve a lot of these problems
[00:45:47.980 --> 00:45:49.820]   and, or believing that you can improve
[00:45:49.820 --> 00:45:50.660]   on a lot of these problems,
[00:45:50.660 --> 00:45:53.460]   maybe that is just more of a programmer kind of attitude.
[00:45:53.460 --> 00:45:57.580]   And like, I've definitely, I used to program.
[00:45:57.580 --> 00:45:59.260]   I don't really do it anymore.
[00:45:59.260 --> 00:46:04.060]   And it is also very much an exercise in rationality
[00:46:04.060 --> 00:46:05.420]   and it's also a more introverted one.
[00:46:05.420 --> 00:46:07.300]   So in finance, part of what you're doing
[00:46:07.300 --> 00:46:09.220]   is you are trying to model other people's behavior.
[00:46:09.220 --> 00:46:12.420]   You're trying to figure out not just why am I right,
[00:46:12.420 --> 00:46:15.100]   but why is the person on the other side of the trade wrong?
[00:46:15.100 --> 00:46:17.380]   And that can be sometimes more valuable
[00:46:17.380 --> 00:46:19.860]   than knowing, being confident that you're right,
[00:46:19.860 --> 00:46:22.060]   especially if they're wrong in the sense
[00:46:22.060 --> 00:46:25.100]   that they have some kind of balance sheet constraint
[00:46:25.100 --> 00:46:26.020]   or regulatory constraint
[00:46:26.020 --> 00:46:27.700]   or some kind of institutional constraint
[00:46:27.700 --> 00:46:30.500]   that keeps them from doing the obvious reasonable thing.
[00:46:30.500 --> 00:46:33.700]   So if you, you might think of that,
[00:46:33.700 --> 00:46:37.100]   like one example of that would be that small cap stocks
[00:46:37.100 --> 00:46:39.340]   tend to have less efficient pricing than large stocks.
[00:46:39.340 --> 00:46:40.380]   And one of the reasons for that
[00:46:40.380 --> 00:46:44.500]   is that a lot of financial institutions have a size cutoff
[00:46:44.500 --> 00:46:45.740]   where they just tell analysts,
[00:46:45.740 --> 00:46:46.900]   please don't look at anything
[00:46:46.900 --> 00:46:48.660]   that's worth under $5 billion
[00:46:48.660 --> 00:46:50.940]   because we won't be able to take a meaningful position
[00:46:50.940 --> 00:46:52.260]   so it's not worth your time.
[00:46:52.260 --> 00:46:54.300]   Whereas you can take a very large position
[00:46:54.300 --> 00:46:57.380]   in Google or Amazon, and if it moves 1%,
[00:46:57.380 --> 00:46:59.180]   then that is more money than you make
[00:46:59.180 --> 00:47:01.740]   in the tiny, tiny position, the tiny, tiny stock.
[00:47:01.740 --> 00:47:04.180]   So there is a market inefficiency there
[00:47:04.180 --> 00:47:05.460]   where a lot of the talent is focused
[00:47:05.460 --> 00:47:08.420]   on getting more precise valuations for big companies
[00:47:08.420 --> 00:47:11.100]   rather than finding pretty obvious opportunities
[00:47:11.100 --> 00:47:12.700]   in small ones.
[00:47:12.700 --> 00:47:15.740]   But the, like debugging as a rationality exercise
[00:47:15.740 --> 00:47:17.620]   is interesting because you, when you debug,
[00:47:17.620 --> 00:47:18.900]   you're trying to outsmart yourself.
[00:47:18.900 --> 00:47:20.660]   You were trying to, you're looking at a situation
[00:47:20.660 --> 00:47:23.140]   where you were, I forget what the quote is,
[00:47:23.140 --> 00:47:24.780]   like if you are, I think the line,
[00:47:24.780 --> 00:47:26.620]   there's some line about cleverness in code
[00:47:26.620 --> 00:47:29.100]   where it's like if you, debugging is twice as hard
[00:47:29.100 --> 00:47:31.740]   as writing the code in the first place.
[00:47:31.740 --> 00:47:33.940]   So if you write the cleverest code you can,
[00:47:33.940 --> 00:47:36.780]   then by definition, you are not smart enough
[00:47:36.780 --> 00:47:38.140]   to figure out what's wrong with it.
[00:47:38.140 --> 00:47:41.060]   And that's, that seems largely true.
[00:47:41.060 --> 00:47:44.180]   So it is an exercise in humility
[00:47:44.180 --> 00:47:45.900]   and also in self-knowledge.
[00:47:45.900 --> 00:47:48.020]   I mean, maybe finance is like,
[00:47:48.020 --> 00:47:50.500]   is the extrovert rationality dojo
[00:47:50.500 --> 00:47:53.420]   and then debugging is the introvert rationality dojo.
[00:47:53.420 --> 00:47:55.620]   Although all the finance people I knew
[00:47:55.620 --> 00:47:58.780]   in less wrong-ish circles,
[00:47:58.780 --> 00:48:00.460]   they were much more likely to be quants.
[00:48:00.460 --> 00:48:02.700]   I don't think there was a single investment banker
[00:48:02.700 --> 00:48:04.180]   in the entire crowd.
[00:48:04.180 --> 00:48:05.860]   - Yeah, you wrote an article about rationality
[00:48:05.860 --> 00:48:09.140]   saw COVID coming much more so than other subcultures.
[00:48:09.140 --> 00:48:10.540]   One of the things you wrote in an article,
[00:48:10.540 --> 00:48:11.980]   and I really didn't know what this meant.
[00:48:11.980 --> 00:48:13.940]   So I really wanted to get a clarification.
[00:48:13.940 --> 00:48:16.340]   You said, it's this attachment
[00:48:16.340 --> 00:48:18.140]   to the factual consequences of predictions
[00:48:18.140 --> 00:48:20.260]   and detachment from their social consequences
[00:48:20.260 --> 00:48:21.620]   that makes rationality is relatively good
[00:48:21.620 --> 00:48:22.780]   at predicting the future,
[00:48:22.780 --> 00:48:24.980]   but comparatively worse at influencing it.
[00:48:24.980 --> 00:48:27.260]   They can spend political capital in cost-effective ways,
[00:48:27.260 --> 00:48:28.620]   but earning it often requires
[00:48:28.620 --> 00:48:30.580]   perversely counter-rational decisions.
[00:48:30.580 --> 00:48:31.900]   What did you mean by that?
[00:48:32.700 --> 00:48:37.140]   - Yeah, so part of, and this is not something
[00:48:37.140 --> 00:48:39.140]   that the rationalist subculture strictly advocates,
[00:48:39.140 --> 00:48:40.860]   but it is kind of a cultural norm
[00:48:40.860 --> 00:48:43.540]   is just being honest and direct and blunt.
[00:48:43.540 --> 00:48:45.860]   And it's really refreshing
[00:48:45.860 --> 00:48:48.140]   that people will just tell you what they're thinking
[00:48:48.140 --> 00:48:50.460]   instead of trying to coddle you
[00:48:50.460 --> 00:48:55.100]   or trying to be more polite about it.
[00:48:55.100 --> 00:48:59.220]   But it does mean that they're doing something
[00:48:59.220 --> 00:49:00.740]   that is orthogonal to politics.
[00:49:00.740 --> 00:49:02.980]   So if it has a political implication,
[00:49:02.980 --> 00:49:06.300]   it's usually that they're making something
[00:49:06.300 --> 00:49:08.180]   that in political terms is a mistake.
[00:49:08.180 --> 00:49:12.580]   So rationalists, they, you know,
[00:49:12.580 --> 00:49:14.260]   I guess it's hard to articulate
[00:49:14.260 --> 00:49:16.100]   exactly how this played out with COVID,
[00:49:16.100 --> 00:49:18.340]   but I did get the sense, like part of the problem
[00:49:18.340 --> 00:49:19.980]   is they take ideas seriously, right?
[00:49:19.980 --> 00:49:23.660]   They think about, they extrapolate things
[00:49:23.660 --> 00:49:25.020]   to their natural endpoint,
[00:49:25.020 --> 00:49:27.500]   and they're willing to bite a lot of bullets.
[00:49:27.500 --> 00:49:31.460]   They're willing to say that if something is weird,
[00:49:31.460 --> 00:49:34.500]   but factually true, then they believe it.
[00:49:34.500 --> 00:49:36.380]   And they're also willing to loudly affirm it,
[00:49:36.380 --> 00:49:38.220]   which is, again, nice, refreshing.
[00:49:38.220 --> 00:49:40.260]   A lot of people just keep their real beliefs to themselves,
[00:49:40.260 --> 00:49:42.460]   not so with the rationalists.
[00:49:42.460 --> 00:49:44.620]   But it also means that the population
[00:49:44.620 --> 00:49:47.260]   that was warning about COVID early,
[00:49:47.260 --> 00:49:49.980]   it's also the set of people who are talking about,
[00:49:49.980 --> 00:49:51.940]   I don't know, Bitcoin is going to save the world,
[00:49:51.940 --> 00:49:54.060]   and cryogenics is really important,
[00:49:54.060 --> 00:49:55.580]   and we live in a simulation.
[00:49:55.580 --> 00:49:59.180]   Like, they have a lot of weird beliefs.
[00:49:59.180 --> 00:50:00.740]   Some of these beliefs are probably true.
[00:50:00.740 --> 00:50:02.500]   Some of them are probably false.
[00:50:02.500 --> 00:50:04.780]   They are generally beliefs that make sense
[00:50:04.780 --> 00:50:06.380]   if you accept a certain set of premises,
[00:50:06.380 --> 00:50:08.060]   and they don't if you don't.
[00:50:08.060 --> 00:50:10.980]   And believing that COVID was going to be a big problem
[00:50:10.980 --> 00:50:13.060]   made perfect sense if you accepted
[00:50:13.060 --> 00:50:15.700]   that it has an R naught of two or higher.
[00:50:15.700 --> 00:50:17.940]   And at the time, we thought the fatality rate
[00:50:17.940 --> 00:50:20.460]   was about, case fatality rate was about 2%.
[00:50:20.460 --> 00:50:22.940]   All you had to do was start compounding
[00:50:22.940 --> 00:50:25.260]   some numbers in your head, and you quickly realized,
[00:50:25.260 --> 00:50:26.460]   okay, this is going to be everywhere,
[00:50:26.460 --> 00:50:28.300]   and it's going to get a lot of people sick.
[00:50:28.300 --> 00:50:30.740]   And if it broke out in a city,
[00:50:30.740 --> 00:50:32.940]   and it was right after the Chinese New Year
[00:50:32.940 --> 00:50:35.180]   that it got shut down, that the city got shut down,
[00:50:35.180 --> 00:50:36.780]   that means a bunch of people got exposed
[00:50:36.780 --> 00:50:38.180]   and then went all over China.
[00:50:38.180 --> 00:50:41.060]   And since flights are still going between China and the US,
[00:50:41.060 --> 00:50:44.060]   and China and Europe, then it's going to be everywhere.
[00:50:44.060 --> 00:50:48.060]   So they were not making any big inferential leaps,
[00:50:48.060 --> 00:50:51.780]   but because they've also, they're willing to say things like
[00:50:51.780 --> 00:50:53.220]   if you think it's theoretically possible
[00:50:53.220 --> 00:50:55.540]   that a computer can simulate
[00:50:55.540 --> 00:50:56.940]   what feels like the real world,
[00:50:56.940 --> 00:51:00.220]   and if you think that there's some chance of that happening,
[00:51:00.220 --> 00:51:02.340]   and if you know how computers work,
[00:51:02.340 --> 00:51:05.460]   then you think that given some non-zero chance
[00:51:05.460 --> 00:51:07.820]   of it happening, the odds of you living in the real world
[00:51:07.820 --> 00:51:10.660]   versus a simulated world are vanishingly small.
[00:51:10.660 --> 00:51:11.940]   I mean, I guess that assumes
[00:51:11.940 --> 00:51:13.420]   that there's only one real world,
[00:51:13.420 --> 00:51:16.860]   but I guess it assumes there's one real world
[00:51:16.860 --> 00:51:18.620]   where it is possible to do this,
[00:51:18.620 --> 00:51:20.860]   and not an infinite series of real worlds
[00:51:20.900 --> 00:51:23.580]   where it is, for whatever reason, not possible to.
[00:51:23.580 --> 00:51:26.860]   Anyway, it's also something that you derive
[00:51:26.860 --> 00:51:28.580]   from taking ideas seriously, or with cryogenics.
[00:51:28.580 --> 00:51:32.300]   If you think that there is value to living
[00:51:32.300 --> 00:51:33.260]   and continuing to live,
[00:51:33.260 --> 00:51:34.700]   and you think that there's some chance
[00:51:34.700 --> 00:51:36.980]   that you can be brought back to life
[00:51:36.980 --> 00:51:40.260]   if your head is frozen, you will do it.
[00:51:40.260 --> 00:51:43.820]   It's like, given the upside, it's actually kind of cheap,
[00:51:43.820 --> 00:51:46.260]   especially if you do the life insurance thing to pay for it.
[00:51:46.260 --> 00:51:49.980]   So they're being very straightforwardly reasonable,
[00:51:49.980 --> 00:51:53.100]   but it means that anyone who was in the rationalist community
[00:51:53.100 --> 00:51:54.580]   and is worrying about COVID,
[00:51:54.580 --> 00:51:56.540]   you could immediately link it to other crazy things
[00:51:56.540 --> 00:51:57.940]   that they had told people
[00:51:57.940 --> 00:52:00.660]   that everyone just thought were absurd.
[00:52:00.660 --> 00:52:02.860]   And the rationalist will argue against,
[00:52:02.860 --> 00:52:05.060]   it's like saying something is absurd
[00:52:05.060 --> 00:52:06.780]   is not the same as saying it's false.
[00:52:06.780 --> 00:52:09.340]   It's just saying, it's more like saying,
[00:52:09.340 --> 00:52:10.860]   if this is true, it's really important,
[00:52:10.860 --> 00:52:15.020]   but the normal heuristic is this is absurd, therefore false,
[00:52:15.020 --> 00:52:18.580]   and therefore you were low status for believing in it.
[00:52:18.580 --> 00:52:19.980]   - Yeah, yeah.
[00:52:19.980 --> 00:52:21.660]   You wrote an essay called "Read More,"
[00:52:21.660 --> 00:52:23.420]   where you say that you're more likely
[00:52:23.420 --> 00:52:27.780]   to get valuable content that's fitted to your niche
[00:52:27.780 --> 00:52:29.580]   or whatever you would find valuable
[00:52:29.580 --> 00:52:32.580]   because writing has a lower production cost,
[00:52:32.580 --> 00:52:35.500]   so people have a higher,
[00:52:35.500 --> 00:52:38.980]   it's easier for people to produce good content.
[00:52:38.980 --> 00:52:41.380]   If you extend this logic,
[00:52:41.380 --> 00:52:43.980]   Twitter has an even lower production cost than a book,
[00:52:43.980 --> 00:52:45.780]   so then should I be reading tweets?
[00:52:45.780 --> 00:52:48.900]   Is there like a golden spot of fixed costs
[00:52:48.900 --> 00:52:51.540]   that a newsletter or a book has
[00:52:51.540 --> 00:52:53.220]   that it's not a movie on one end,
[00:52:53.220 --> 00:52:54.940]   but it's not a tweet on the other?
[00:52:54.940 --> 00:52:59.980]   - Yeah, it is true that if you extrapolate
[00:52:59.980 --> 00:53:00.820]   that part of the argument,
[00:53:00.820 --> 00:53:03.820]   you do conclude that tweets are good
[00:53:03.820 --> 00:53:06.660]   and that watching live streams is even better.
[00:53:06.660 --> 00:53:10.140]   So there are different kinds of content
[00:53:10.140 --> 00:53:11.260]   and there are different production costs.
[00:53:11.260 --> 00:53:15.100]   And what I was focused on was the kind of content
[00:53:15.100 --> 00:53:17.620]   where someone is trying to deeply understand a topic
[00:53:17.620 --> 00:53:18.580]   and explain it.
[00:53:18.580 --> 00:53:20.060]   And in that kind of content,
[00:53:20.060 --> 00:53:21.100]   it's hard to do that in a tweet.
[00:53:21.100 --> 00:53:22.740]   You can definitely tweet something
[00:53:22.740 --> 00:53:24.700]   that is koanic and insightful,
[00:53:24.700 --> 00:53:26.700]   but the hit rate on those is pretty low.
[00:53:26.700 --> 00:53:32.020]   So it's partly a comparison of learning,
[00:53:32.020 --> 00:53:34.980]   learning, say, history or math
[00:53:34.980 --> 00:53:37.260]   or learning about a new technology
[00:53:37.260 --> 00:53:39.580]   from watching YouTube videos on it
[00:53:39.580 --> 00:53:41.660]   versus, or watching documentaries on Netflix
[00:53:41.660 --> 00:53:43.140]   or on Disney+ or whatever,
[00:53:43.140 --> 00:53:45.140]   versus learning about it by reading.
[00:53:45.140 --> 00:53:47.940]   And my argument was that there's a long tail of things
[00:53:47.940 --> 00:53:50.060]   where it is actually worth the time and effort
[00:53:50.060 --> 00:53:51.660]   to make a book and it's not worth the time
[00:53:51.660 --> 00:53:53.860]   and effort to make a documentary.
[00:53:53.860 --> 00:53:55.900]   And you can also think of it in terms
[00:53:55.900 --> 00:53:58.780]   of hours of research required,
[00:53:58.780 --> 00:54:00.180]   hours of production required
[00:54:00.180 --> 00:54:02.620]   versus hours of content consumption,
[00:54:02.620 --> 00:54:04.780]   where making a documentary,
[00:54:04.780 --> 00:54:06.820]   it requires a lot of people to spend a lot of time
[00:54:06.820 --> 00:54:09.100]   and they're gonna cut it down to an hour.
[00:54:09.100 --> 00:54:12.820]   And with a book, you, A, books,
[00:54:12.820 --> 00:54:14.380]   you will usually spend more time reading a book
[00:54:14.380 --> 00:54:17.100]   than you would spend watching a movie.
[00:54:17.100 --> 00:54:18.260]   I guess there are exceptions on both ends,
[00:54:18.260 --> 00:54:19.740]   but that's a generalization.
[00:54:19.740 --> 00:54:22.900]   And the production time is just typing
[00:54:22.900 --> 00:54:24.580]   and that can happen pretty quickly.
[00:54:24.580 --> 00:54:26.220]   There's a lot of reading that goes into it
[00:54:26.220 --> 00:54:29.100]   and that can take an arbitrarily long amount of time,
[00:54:29.100 --> 00:54:30.740]   depending on how thorough you wanna be,
[00:54:30.740 --> 00:54:33.620]   but you're also, that reading process
[00:54:33.620 --> 00:54:35.860]   is really a process of winnowing ideas down.
[00:54:35.860 --> 00:54:38.460]   Like it's, so I'm working on a book with a friend
[00:54:38.460 --> 00:54:41.660]   and I've increasingly come to this view
[00:54:41.660 --> 00:54:46.220]   that the writing process happens while I'm reading.
[00:54:46.220 --> 00:54:47.900]   And then what I'm actually doing
[00:54:47.900 --> 00:54:49.140]   when I sit down and start typing
[00:54:49.140 --> 00:54:54.140]   is more like it's a more, you know, just taking dictations.
[00:54:54.140 --> 00:54:57.060]   I have the general thoughts in my head,
[00:54:57.060 --> 00:54:58.780]   the general connections between different topics.
[00:54:58.780 --> 00:55:00.300]   I've highlighted the things I really wanna talk about
[00:55:00.300 --> 00:55:02.820]   in the books and I'm just applying
[00:55:02.820 --> 00:55:05.100]   a pretty coherent narrative to it.
[00:55:05.100 --> 00:55:07.140]   And that part's not super hard.
[00:55:07.140 --> 00:55:10.340]   So the book is going to be mostly,
[00:55:10.340 --> 00:55:12.700]   most of the effort is reading and learning.
[00:55:12.700 --> 00:55:15.580]   And then a relatively small fraction of that
[00:55:15.580 --> 00:55:18.300]   is actually producing the end artifact.
[00:55:18.300 --> 00:55:21.340]   Whereas with a movie, you can imagine a documentary
[00:55:21.340 --> 00:55:23.460]   where there are more person hours spent
[00:55:23.460 --> 00:55:25.420]   on the production process, including, you know,
[00:55:25.420 --> 00:55:27.260]   filming it and then editing it
[00:55:27.260 --> 00:55:28.860]   and managing all the equipment
[00:55:28.860 --> 00:55:31.780]   and just getting in all the travel and whatever else,
[00:55:31.780 --> 00:55:34.380]   that that could actually end up taking more time
[00:55:34.380 --> 00:55:38.860]   than the preparatory research to come up with the idea
[00:55:38.860 --> 00:55:41.780]   and figure out what the narrative of the documentary will be.
[00:55:41.780 --> 00:55:42.660]   - Isn't the logic there?
[00:55:42.660 --> 00:55:44.820]   I mean, most of the time that goes into the podcast,
[00:55:44.820 --> 00:55:48.140]   for me, it's just me preparing and researching my guests.
[00:55:48.140 --> 00:55:50.460]   Is the logic there then say that like podcasts
[00:55:50.460 --> 00:55:53.700]   are as valid a medium of consuming content?
[00:55:53.700 --> 00:55:56.380]   - Well, so podcasts have a totally different dynamic
[00:55:56.380 --> 00:55:57.780]   because there's more back and forth.
[00:55:57.780 --> 00:56:00.420]   So a book is more analogous to a lecture.
[00:56:00.420 --> 00:56:03.100]   And a lot of these documentaries and YouTube tutorials
[00:56:03.100 --> 00:56:05.020]   are also basically a lecture.
[00:56:05.020 --> 00:56:06.260]   The podcast is a dialogue.
[00:56:06.260 --> 00:56:08.060]   So there could be tangents
[00:56:08.060 --> 00:56:09.660]   and both sides are contributing things.
[00:56:09.660 --> 00:56:11.740]   And one person will say something
[00:56:11.740 --> 00:56:12.780]   that leads to an interesting question,
[00:56:12.780 --> 00:56:14.580]   leads to another tangent that leads to something else.
[00:56:14.580 --> 00:56:16.220]   So it's creating more connections.
[00:56:16.220 --> 00:56:17.700]   It's basically, you know,
[00:56:17.700 --> 00:56:20.300]   we both have different clusters of ideas in our heads
[00:56:20.300 --> 00:56:21.580]   and different things that we wanna talk about.
[00:56:21.580 --> 00:56:24.820]   And because they're different clusters,
[00:56:24.820 --> 00:56:25.780]   there's some overlap
[00:56:25.780 --> 00:56:27.660]   and then there are some totally separate things.
[00:56:27.660 --> 00:56:30.300]   Because of that, the connections that can happen
[00:56:31.420 --> 00:56:33.460]   between ideas in a dialogue
[00:56:33.460 --> 00:56:35.300]   are going to be a lot broader than the connections
[00:56:35.300 --> 00:56:37.860]   that could happen with one person writing a book.
[00:56:37.860 --> 00:56:40.140]   But a book will probably get more depth.
[00:56:40.140 --> 00:56:42.980]   And that's just, it's a trade-off that different media have.
[00:56:42.980 --> 00:56:45.300]   So it would be hard to do a podcast
[00:56:45.300 --> 00:56:48.020]   where you're talking to one person
[00:56:48.020 --> 00:56:49.940]   and you're actually trying to develop, you know,
[00:56:49.940 --> 00:56:50.780]   a theory of history.
[00:56:50.780 --> 00:56:52.660]   You're trying to develop, you know,
[00:56:52.660 --> 00:56:53.660]   you're trying to decide, okay,
[00:56:53.660 --> 00:56:54.900]   is inflation transitory or not?
[00:56:54.900 --> 00:56:56.460]   You could have a really interesting discussion
[00:56:56.460 --> 00:56:57.900]   and lead to a lot of ideas.
[00:56:57.900 --> 00:57:01.100]   But I think if you wanted to really answer the question of,
[00:57:01.100 --> 00:57:02.940]   are we gonna have hyperinflation, for example,
[00:57:02.940 --> 00:57:05.020]   you probably wanna be of the mind
[00:57:05.020 --> 00:57:07.700]   that you are writing a book, making the case yes or no.
[00:57:07.700 --> 00:57:08.540]   - Yep, yep.
[00:57:08.540 --> 00:57:09.380]   Yeah, that makes sense.
[00:57:09.380 --> 00:57:11.580]   A related concept that you've talked about
[00:57:11.580 --> 00:57:12.580]   is the convexity of knowledge.
[00:57:12.580 --> 00:57:15.220]   Basically, there's increasing returns to learning more.
[00:57:15.220 --> 00:57:18.260]   Doesn't that imply that, for example,
[00:57:18.260 --> 00:57:19.500]   the next book you're gonna read,
[00:57:19.500 --> 00:57:20.860]   you're gonna derive more value from it
[00:57:20.860 --> 00:57:22.820]   than the next book I'm gonna read?
[00:57:22.820 --> 00:57:25.020]   Where, in fact, it would seem to me pretty clear
[00:57:25.020 --> 00:57:27.060]   that, like, I'm probably gonna gain a lot more value
[00:57:27.060 --> 00:57:28.860]   just because I know a lot less.
[00:57:28.860 --> 00:57:30.540]   Or maybe is this the wrong way of thinking about it,
[00:57:30.540 --> 00:57:32.020]   that you gain more knowledge,
[00:57:32.020 --> 00:57:33.860]   but since you already have so much knowledge,
[00:57:33.860 --> 00:57:36.500]   knowledge is worth less to you, so utility is lower?
[00:57:36.500 --> 00:57:40.420]   - So we're sort of trying to apply utility functions
[00:57:40.420 --> 00:57:43.180]   to things that are hard to quantify in the first place.
[00:57:43.180 --> 00:57:46.460]   What I would say is that it's not just about
[00:57:46.460 --> 00:57:48.420]   reading the book, it's also about retaining it.
[00:57:48.420 --> 00:57:51.140]   And that's really where the convexity kicks in,
[00:57:51.140 --> 00:57:54.500]   is that there are things that make more sense in context
[00:57:54.500 --> 00:57:55.820]   when you have more context.
[00:57:55.820 --> 00:57:58.300]   And they also make the context more memorable
[00:57:58.300 --> 00:57:59.900]   and they make it have more sense.
[00:57:59.900 --> 00:58:01.460]   I think Paul Graham actually wrote about this
[00:58:01.460 --> 00:58:03.380]   a long time ago, where he's talking about learning history
[00:58:03.380 --> 00:58:05.500]   and he's like, when you read one history book,
[00:58:05.500 --> 00:58:08.740]   you just have this series of dates and of names
[00:58:08.740 --> 00:58:12.220]   and you don't have any, like, you have whatever connections
[00:58:12.220 --> 00:58:13.700]   the historian is drawing between these things,
[00:58:13.700 --> 00:58:18.060]   but there's no broader context that you're putting it in.
[00:58:18.060 --> 00:58:21.060]   But then when you start to read multiple books
[00:58:21.060 --> 00:58:22.260]   about the same time period,
[00:58:22.260 --> 00:58:25.260]   and maybe one of them is a political history,
[00:58:25.260 --> 00:58:27.700]   and then maybe one of them is a biography
[00:58:27.700 --> 00:58:28.660]   of one particular person,
[00:58:28.660 --> 00:58:30.660]   and then one of them is a history of technology changes
[00:58:30.660 --> 00:58:31.500]   at that time,
[00:58:31.500 --> 00:58:33.620]   you start to see different things connect together.
[00:58:33.620 --> 00:58:35.220]   And one of the things that makes it really memorable
[00:58:35.220 --> 00:58:37.780]   is when you see things that you know are important
[00:58:37.780 --> 00:58:38.980]   that someone missed,
[00:58:38.980 --> 00:58:42.180]   because now you are actually having a little dialogue
[00:58:42.180 --> 00:58:45.580]   with the author where you are, I don't know,
[00:58:45.580 --> 00:58:48.620]   you're reading about, I read a biography of Deng Xiaoping,
[00:58:48.620 --> 00:58:50.700]   and one of the striking things in that book
[00:58:50.700 --> 00:58:53.180]   was that it mentions container ships once,
[00:58:53.180 --> 00:58:54.620]   and it's a totally offhand mention.
[00:58:54.620 --> 00:58:57.060]   It's just Chinese delegation went to a German port city,
[00:58:57.060 --> 00:59:00.020]   they saw containers being loaded on ships.
[00:59:00.020 --> 00:59:02.500]   But there've been other things I've read
[00:59:02.500 --> 00:59:04.500]   that argued that China's industrialization
[00:59:04.500 --> 00:59:06.540]   was massively driven by containerization,
[00:59:06.540 --> 00:59:10.740]   because it meant that you could import intermediate goods,
[00:59:10.740 --> 00:59:12.540]   process them, export intermediate goods.
[00:59:12.540 --> 00:59:14.580]   You could have these really complicated supply chains
[00:59:14.580 --> 00:59:17.260]   because the cost of shipping things on one more hop
[00:59:17.260 --> 00:59:18.500]   was a lot lower.
[00:59:18.500 --> 00:59:20.580]   So it meant that countries with cheap labor
[00:59:20.580 --> 00:59:21.940]   had more ways that they could start
[00:59:21.940 --> 00:59:24.260]   slotting themselves into the supply chain.
[00:59:24.260 --> 00:59:25.700]   The book was still really good,
[00:59:25.700 --> 00:59:29.380]   but in a lot of it was more focused on the political history
[00:59:29.380 --> 00:59:31.940]   and internal Communist Party deliberations
[00:59:31.940 --> 00:59:35.060]   rather than the broad economic sweep.
[00:59:35.060 --> 00:59:37.340]   But it was memorable to me
[00:59:37.340 --> 00:59:40.300]   because it was something where I felt like I actually,
[00:59:40.300 --> 00:59:42.340]   the author knew a lot more than me about China,
[00:59:42.340 --> 00:59:43.980]   and there was something I knew about China
[00:59:43.980 --> 00:59:45.460]   that the author did not know.
[00:59:45.460 --> 00:59:47.580]   And that's a really fun feeling,
[00:59:47.580 --> 00:59:50.620]   and it just, it made the rest of the book more memorable.
[00:59:50.620 --> 00:59:51.780]   - All right, I'm glad you brought that up,
[00:59:51.780 --> 00:59:54.940]   'cause that's relevant to a question I wanted to ask.
[00:59:54.940 --> 00:59:57.540]   Like say, just consider I have like zero knowledge
[00:59:57.540 --> 00:59:58.700]   in comparison to you, right?
[00:59:58.700 --> 01:00:00.940]   Which is like almost close to true.
[01:00:00.940 --> 01:00:04.500]   Then if you're recommending what kinds of books to read,
[01:00:04.500 --> 01:00:05.500]   take that example.
[01:00:05.500 --> 01:00:08.140]   Maybe I can read like the box about like shipping.
[01:00:08.140 --> 01:00:10.220]   Maybe, so that's like very contemporary and very specific.
[01:00:10.220 --> 01:00:12.540]   Maybe I can read like a biography of a contemporary figure
[01:00:12.540 --> 01:00:14.580]   like Deng Xiaoping, or like, not contemporary,
[01:00:14.580 --> 01:00:15.580]   but you know what I mean.
[01:00:15.580 --> 01:00:17.700]   Maybe I can read an ancient Chinese classic.
[01:00:17.700 --> 01:00:19.740]   Maybe I can read like the three-body problem.
[01:00:19.740 --> 01:00:22.220]   Like what is Chinese fiction like?
[01:00:22.220 --> 01:00:24.260]   If I'm trying to understand a topic like China,
[01:00:24.260 --> 01:00:26.420]   which of those categories, or any topic,
[01:00:26.420 --> 01:00:27.900]   which of those categories is like,
[01:00:27.900 --> 01:00:29.180]   this is the highest utility,
[01:00:29.180 --> 01:00:31.340]   and then these are the supplementary texts?
[01:00:31.340 --> 01:00:34.420]   - I don't have a good answer,
[01:00:34.420 --> 01:00:37.140]   because what I usually do is buy a bunch
[01:00:37.140 --> 01:00:38.940]   of somewhat random books.
[01:00:38.940 --> 01:00:42.420]   And it's partly a convexity thing,
[01:00:42.420 --> 01:00:43.660]   that there are things that I'd heard of,
[01:00:43.660 --> 01:00:46.500]   and that I knew I would read at some point.
[01:00:46.500 --> 01:00:47.940]   And when I decided to read about a topic,
[01:00:47.940 --> 01:00:49.220]   that's the time to read them.
[01:00:49.220 --> 01:00:52.300]   So it is hard to give a really good answer to that,
[01:00:52.300 --> 01:00:55.780]   other than just, I would try to buy a smattering
[01:00:55.780 --> 01:00:59.300]   of books that take different angles on the same topic.
[01:00:59.300 --> 01:01:02.620]   And fiction that is written in the time period
[01:01:02.620 --> 01:01:03.980]   that you are interested in,
[01:01:03.980 --> 01:01:06.420]   or that is roughly about the topic you're interested in,
[01:01:06.420 --> 01:01:08.300]   can also be really effective.
[01:01:08.300 --> 01:01:10.220]   Authors just, they have an eye for some details
[01:01:10.220 --> 01:01:11.300]   that other people will miss.
[01:01:11.300 --> 01:01:14.260]   And sometimes, just the specific tangible things
[01:01:14.260 --> 01:01:17.940]   that show up in fiction that don't show up elsewhere,
[01:01:17.940 --> 01:01:19.580]   they just add a lot more texture.
[01:01:19.580 --> 01:01:24.220]   So you should probably do all the above, actually.
[01:01:24.220 --> 01:01:26.220]   You should just have a giant stack of books
[01:01:26.220 --> 01:01:28.860]   on whatever topic you're getting into right now.
[01:01:28.860 --> 01:01:30.060]   - Gotcha.
[01:01:30.060 --> 01:01:31.580]   Now, the favorite essay you wrote,
[01:01:31.580 --> 01:01:32.980]   the favorite essay of mine that you wrote,
[01:01:32.980 --> 01:01:35.180]   was "The Middle Income Trap,"
[01:01:35.180 --> 01:01:37.620]   where you compare the process by which countries
[01:01:37.620 --> 01:01:41.180]   get stuck in a middle income, second class tier,
[01:01:41.180 --> 01:01:43.380]   and how their careers get stuck in that place.
[01:01:43.380 --> 01:01:45.460]   And obviously, it's directly relevant
[01:01:45.460 --> 01:01:47.100]   to the considerations I have.
[01:01:49.180 --> 01:01:50.420]   In the book "How Asia Works,"
[01:01:50.420 --> 01:01:53.300]   the author talks about how many of these countries
[01:01:53.300 --> 01:01:55.860]   created terrorists basically to create internal knowledge
[01:01:55.860 --> 01:01:57.740]   about how to produce highly valuable things
[01:01:57.740 --> 01:01:59.380]   that were really shitty at first.
[01:01:59.380 --> 01:02:01.660]   Is there an analogous strategy here
[01:02:01.660 --> 01:02:03.740]   where you basically learn to do things
[01:02:03.740 --> 01:02:05.340]   that are not your comparative advantage,
[01:02:05.340 --> 01:02:06.740]   like you're really shitty at them,
[01:02:06.740 --> 01:02:08.540]   but the goal is eventually,
[01:02:08.540 --> 01:02:10.660]   this is something that differentiates you?
[01:02:10.660 --> 01:02:12.060]   Or maybe is there a better strategy
[01:02:12.060 --> 01:02:13.260]   to escape the middle income trap?
[01:02:13.260 --> 01:02:14.900]   What do you think?
[01:02:14.900 --> 01:02:17.540]   - Yeah, I definitely think that picking up skills
[01:02:17.540 --> 01:02:19.860]   that are adjacent to what you do
[01:02:19.860 --> 01:02:22.420]   and that you're not good at is really valuable.
[01:02:22.420 --> 01:02:24.460]   If nothing else, if you're successful,
[01:02:24.460 --> 01:02:26.180]   you will end up delegating some of those tasks
[01:02:26.180 --> 01:02:28.340]   to other people, and it is very helpful
[01:02:28.340 --> 01:02:30.420]   to know roughly how hard they are
[01:02:30.420 --> 01:02:31.860]   rather than just having to guess.
[01:02:31.860 --> 01:02:35.540]   So if you have a job where you're not doing any programming,
[01:02:35.540 --> 01:02:36.620]   but you're working with programmers,
[01:02:36.620 --> 01:02:39.420]   you should probably learn to program even badly
[01:02:39.420 --> 01:02:41.660]   just so you can have some intelligent opinions,
[01:02:41.660 --> 01:02:43.500]   and at least so that you know
[01:02:43.500 --> 01:02:45.860]   which timelines are totally fictional
[01:02:45.860 --> 01:02:46.860]   and which ones are not.
[01:02:46.860 --> 01:02:49.660]   And it gives you ways to ask better questions.
[01:02:49.660 --> 01:02:53.020]   But a lot of that essay, it was,
[01:02:53.020 --> 01:02:55.260]   so one of the escapes from the middle income trap
[01:02:55.260 --> 01:02:56.380]   that I think Stilwell talks about
[01:02:56.380 --> 01:03:00.060]   is that countries either built branded products
[01:03:00.060 --> 01:03:02.140]   that were popular around the world,
[01:03:02.140 --> 01:03:04.380]   they could export and sell at a premium price,
[01:03:04.380 --> 01:03:06.660]   or they developed indigenous technologies
[01:03:06.660 --> 01:03:08.460]   that other places just could not match.
[01:03:08.460 --> 01:03:12.620]   And that's sometimes a process
[01:03:12.620 --> 01:03:13.980]   that blots back and forth.
[01:03:13.980 --> 01:03:15.900]   Like there was a time when it was really hard
[01:03:15.900 --> 01:03:18.940]   for US automakers to compete with Japanese automakers,
[01:03:18.940 --> 01:03:23.380]   and everyone seems to be a lot closer to parity right now.
[01:03:23.380 --> 01:03:25.620]   So it was a temporary competitive advantage,
[01:03:25.620 --> 01:03:26.980]   but it was a big one for Japan,
[01:03:26.980 --> 01:03:30.020]   and it certainly helped their economy continue to grow.
[01:03:30.020 --> 01:03:34.660]   But then there are all these intermediate industrial goods
[01:03:34.660 --> 01:03:35.860]   that Japan produces.
[01:03:35.860 --> 01:03:37.140]   There are just a lot of supply chains
[01:03:37.140 --> 01:03:39.260]   that happen to pass through Japan,
[01:03:39.260 --> 01:03:42.700]   and the final assembly is in somewhere like China or Vietnam,
[01:03:42.700 --> 01:03:45.140]   and the end product is sold in the US or Europe.
[01:03:45.140 --> 01:03:47.140]   But there's some essential component
[01:03:47.140 --> 01:03:50.380]   that is still only made by a Japanese company,
[01:03:50.380 --> 01:03:52.980]   and it's hard to match anywhere else.
[01:03:52.980 --> 01:03:54.820]   And it's a bunch of random stuff.
[01:03:54.820 --> 01:03:56.420]   Like there's a lot of stuff
[01:03:56.420 --> 01:03:59.700]   in the semiconductor supply chain where there will be,
[01:03:59.700 --> 01:04:01.900]   the step is like a hundred different things
[01:04:01.900 --> 01:04:03.620]   that you have to do or 500 things you have to do
[01:04:03.620 --> 01:04:07.580]   to go from here is silicon to here is a chip
[01:04:07.580 --> 01:04:09.900]   that actually works and your computer works now.
[01:04:09.900 --> 01:04:13.420]   And sometimes one or two or five or 10 of those steps
[01:04:13.420 --> 01:04:16.500]   will be monopolized by some company in Japan.
[01:04:16.500 --> 01:04:21.380]   So that is the escape is to have some set of skills
[01:04:21.380 --> 01:04:23.620]   that you are pretty confident nobody can match
[01:04:23.620 --> 01:04:25.500]   and that people know that you have.
[01:04:25.500 --> 01:04:27.380]   And that allows you to compete
[01:04:27.380 --> 01:04:28.580]   on something other than price.
[01:04:28.580 --> 01:04:30.580]   And competing on price is fine when you're young
[01:04:30.580 --> 01:04:32.460]   because your cost of living is low,
[01:04:32.460 --> 01:04:35.020]   and you'll learn a lot by getting thrown into jobs
[01:04:35.020 --> 01:04:36.540]   that you are only qualified for in the sense
[01:04:36.540 --> 01:04:38.900]   that you are far cheaper than everyone else could do it.
[01:04:38.900 --> 01:04:42.500]   But you can't be cheaper than,
[01:04:42.500 --> 01:04:45.020]   you can't get rich being cheaper than average to hire.
[01:04:45.020 --> 01:04:48.100]   So at some point you have to do something different.
[01:04:48.100 --> 01:04:48.940]   - Gotcha.
[01:04:48.940 --> 01:04:51.260]   And my final question is usually always,
[01:04:51.260 --> 01:04:54.460]   what is one piece of advice you would give to somebody
[01:04:54.460 --> 01:04:55.300]   by age?
[01:04:55.300 --> 01:04:56.140]   I mean, we already talked about some of the things,
[01:04:56.140 --> 01:04:57.780]   so this might be a redundant question.
[01:04:57.780 --> 01:05:00.060]   So, I mean, we talked about avoid that rationality,
[01:05:00.060 --> 01:05:03.180]   read more, differentiate yourself by,
[01:05:03.180 --> 01:05:06.220]   and use that to avoid the middle income trap.
[01:05:06.220 --> 01:05:08.580]   Is there anything else that you would recommend?
[01:05:09.660 --> 01:05:12.740]   - Yeah, maybe a synthesis of a lot of those things
[01:05:12.740 --> 01:05:16.180]   is that because books are cheap
[01:05:16.180 --> 01:05:17.780]   and because the internet
[01:05:17.780 --> 01:05:19.820]   is such a wonderful distribution mechanism,
[01:05:19.820 --> 01:05:23.660]   it is really not hard if you were determined to be,
[01:05:23.660 --> 01:05:24.820]   if you pick a narrow enough topic
[01:05:24.820 --> 01:05:28.420]   to be close to one of the world's leading experts on it
[01:05:28.420 --> 01:05:29.660]   in a fairly short timeframe.
[01:05:29.660 --> 01:05:32.020]   So you have to pick a topic that is basically narrow enough
[01:05:32.020 --> 01:05:33.620]   that nobody has written a dissertation about it,
[01:05:33.620 --> 01:05:34.620]   which can be hard.
[01:05:34.620 --> 01:05:36.860]   But the next best thing is you can be
[01:05:36.860 --> 01:05:39.540]   the world's public facing expert on this
[01:05:39.540 --> 01:05:41.300]   by just reading a bunch of academic papers
[01:05:41.300 --> 01:05:42.740]   from people who know more than you,
[01:05:42.740 --> 01:05:44.820]   diving through what they cite, reading that stuff too,
[01:05:44.820 --> 01:05:47.140]   and consolidating it into something
[01:05:47.140 --> 01:05:49.540]   that you can put on Substack or somewhere.
[01:05:49.540 --> 01:05:52.060]   And I think it's a valuable exercise,
[01:05:52.060 --> 01:05:55.460]   both because it gives you some body of work
[01:05:55.460 --> 01:05:56.900]   you could point to to say,
[01:05:56.900 --> 01:05:58.700]   I am willing to work hard on projects,
[01:05:58.700 --> 01:06:02.100]   I'm able to find interesting and insightful things,
[01:06:02.100 --> 01:06:06.780]   and able to learn a lot and synthesize it in a way
[01:06:06.780 --> 01:06:09.300]   that helps other people learn too.
[01:06:09.300 --> 01:06:13.820]   But it's useful because one of the things
[01:06:13.820 --> 01:06:16.740]   that I learned from these various deep dives
[01:06:16.740 --> 01:06:18.860]   that I've done on different industries,
[01:06:18.860 --> 01:06:20.220]   different countries, different companies,
[01:06:20.220 --> 01:06:22.140]   is that the world is just much more complicated
[01:06:22.140 --> 01:06:23.220]   than I thought.
[01:06:23.220 --> 01:06:26.540]   And it's very easy to say that,
[01:06:26.540 --> 01:06:29.620]   and it's very hard to internalize it.
[01:06:29.620 --> 01:06:33.780]   And I think really the only way to internalize it
[01:06:33.780 --> 01:06:36.580]   is to realize repeatedly that you were wrong about something
[01:06:36.580 --> 01:06:37.940]   because you had oversimplified it.
[01:06:37.940 --> 01:06:40.660]   And often it's not just that you were wrong,
[01:06:40.660 --> 01:06:43.220]   it's that the first source you read had some theory
[01:06:43.220 --> 01:06:45.500]   and that theory was later overturned.
[01:06:45.500 --> 01:06:47.740]   So you wanna get to the point
[01:06:47.740 --> 01:06:50.700]   where you can look back three months
[01:06:50.700 --> 01:06:52.540]   and realize you were dumb about something
[01:06:52.540 --> 01:06:54.060]   that you thought you knew a whole lot about.
[01:06:54.060 --> 01:06:55.460]   And at some point
[01:06:55.460 --> 01:06:57.380]   when you're no longer realizing you're dumb,
[01:06:57.380 --> 01:07:01.260]   but you're starting to just have more open-ended questions
[01:07:01.260 --> 01:07:02.780]   that you can't really get answers to,
[01:07:02.780 --> 01:07:05.740]   then that's a good time to move on to the next big topic.
[01:07:05.740 --> 01:07:08.500]   But that is a worthwhile exercise.
[01:07:08.500 --> 01:07:11.460]   And there's so much material out there.
[01:07:11.460 --> 01:07:15.420]   There are lots of aspects of economic history
[01:07:15.420 --> 01:07:18.220]   that I think are worth revisiting right now
[01:07:18.220 --> 01:07:22.500]   because we're either at a time that is strange
[01:07:22.500 --> 01:07:24.140]   because growth has slowed down,
[01:07:24.140 --> 01:07:25.820]   it's gonna keep being slow forever,
[01:07:25.820 --> 01:07:28.660]   which hasn't really happened before
[01:07:28.660 --> 01:07:31.180]   without some kind of big natural disaster.
[01:07:31.180 --> 01:07:36.180]   Like we basically had growth that continuously accelerated
[01:07:36.180 --> 01:07:40.220]   albeit at a slow pace from pretty much the dawn of time
[01:07:40.220 --> 01:07:42.420]   through the 1970s.
[01:07:42.420 --> 01:07:44.740]   Yeah, and then it slowed down.
[01:07:44.740 --> 01:07:47.220]   Different countries have had different ebbs
[01:07:47.220 --> 01:07:48.060]   and flows of growth,
[01:07:48.060 --> 01:07:49.300]   but overall that's been the story.
[01:07:49.300 --> 01:07:51.980]   It's just gradual growth, gradual acceleration.
[01:07:51.980 --> 01:07:53.940]   So we're either at a unique time period
[01:07:53.940 --> 01:07:55.380]   because that's stopping
[01:07:55.380 --> 01:07:59.700]   and economies total factor productivity
[01:07:59.700 --> 01:08:00.860]   grew and accelerated for a while
[01:08:00.860 --> 01:08:02.740]   and then it stopped growing so much.
[01:08:02.740 --> 01:08:04.140]   And that's just where we are.
[01:08:04.140 --> 01:08:04.980]   That's interesting.
[01:08:04.980 --> 01:08:06.340]   It'd be interesting to figure out why that is.
[01:08:06.340 --> 01:08:08.340]   Or we're at an interesting time in history
[01:08:08.340 --> 01:08:10.180]   because there will be specific technologies
[01:08:10.180 --> 01:08:12.660]   that actually do accelerate productivity growth.
[01:08:12.660 --> 01:08:15.300]   And it's really worth it to know what those are.
[01:08:15.300 --> 01:08:16.140]   'Cause if you go back
[01:08:16.140 --> 01:08:18.340]   and you look at the specific technologies
[01:08:18.340 --> 01:08:21.340]   that say led the first industrial revolution
[01:08:21.340 --> 01:08:22.940]   or led the second industrial revolution
[01:08:22.940 --> 01:08:26.420]   or led the information technology revolution,
[01:08:26.420 --> 01:08:27.860]   they are all associated with people
[01:08:27.860 --> 01:08:29.340]   making lots and lots of money.
[01:08:29.340 --> 01:08:32.140]   And even the people who didn't get extremely rich,
[01:08:32.140 --> 01:08:33.300]   they had really interesting lives.
[01:08:33.300 --> 01:08:35.900]   So it is very worth figuring out
[01:08:35.900 --> 01:08:38.140]   if we're in year 10
[01:08:38.140 --> 01:08:40.740]   of another one of those 50 year deployment cycles.
[01:08:40.740 --> 01:08:42.380]   - Yeah, final question.
[01:08:42.380 --> 01:08:44.020]   I mean, I kind of asked this earlier
[01:08:44.020 --> 01:08:45.380]   when I asked what kinds of books to read,
[01:08:45.380 --> 01:08:46.220]   but when you're trying to figure out
[01:08:46.220 --> 01:08:47.060]   the answer to that question,
[01:08:47.060 --> 01:08:47.900]   which I'm very much interested in,
[01:08:47.900 --> 01:08:49.660]   other than reading your newsletter,
[01:08:49.660 --> 01:08:51.220]   should you be looking at,
[01:08:51.220 --> 01:08:52.860]   you know, like doing a deep dive
[01:08:52.860 --> 01:08:54.380]   in how something in the world works today?
[01:08:54.380 --> 01:08:55.220]   Are you trying to understand,
[01:08:55.220 --> 01:08:56.860]   should you be trying to understand history?
[01:08:56.860 --> 01:09:00.820]   Like what is the most relevant lens to be looking at here?
[01:09:00.820 --> 01:09:03.460]   - History is a lot easier
[01:09:03.460 --> 01:09:06.660]   because we've seen how things played out.
[01:09:06.660 --> 01:09:09.660]   If you're looking at things that are contemporary trends,
[01:09:09.660 --> 01:09:10.580]   you have a couple of problems.
[01:09:10.580 --> 01:09:12.260]   One is that a lot of people writing about them
[01:09:12.260 --> 01:09:13.540]   have an agenda
[01:09:13.540 --> 01:09:14.900]   because they either want something to happen
[01:09:14.900 --> 01:09:16.340]   or they don't want it to happen.
[01:09:16.340 --> 01:09:17.860]   There's also the meta agenda thing
[01:09:17.860 --> 01:09:20.220]   where the people who are really busy making it happen
[01:09:20.220 --> 01:09:21.380]   don't have time to write books.
[01:09:21.380 --> 01:09:23.660]   And the people who got left behind one way or another,
[01:09:23.660 --> 01:09:24.500]   they do have time.
[01:09:24.500 --> 01:09:26.220]   So there are a lot of selection effects
[01:09:26.220 --> 01:09:27.340]   that make the data noisier.
[01:09:27.340 --> 01:09:29.180]   On the other hand, there's a lot more data.
[01:09:29.180 --> 01:09:30.940]   And if you go back for,
[01:09:30.940 --> 01:09:32.660]   if you go back and look at earlier historical events,
[01:09:32.660 --> 01:09:35.420]   you may not be able to find much at all.
[01:09:35.420 --> 01:09:37.660]   Or, you know, you'll find a handful of sources
[01:09:37.660 --> 01:09:39.500]   and you've exhausted your material.
[01:09:39.500 --> 01:09:42.700]   So you do have that trade off.
[01:09:42.700 --> 01:09:43.940]   I go back and forth.
[01:09:43.940 --> 01:09:46.740]   So I, and it's not a deliberate thing,
[01:09:46.740 --> 01:09:48.980]   but I've noticed over time
[01:09:48.980 --> 01:09:50.820]   that I will generally spend
[01:09:50.820 --> 01:09:52.180]   some time writing about contemporary things,
[01:09:52.180 --> 01:09:56.180]   some time writing about historical analogy to those things.
[01:09:56.180 --> 01:09:57.860]   Sometimes I will, like right now,
[01:09:57.860 --> 01:09:59.660]   I'm doing a bunch of reading on central banks,
[01:09:59.660 --> 01:10:01.100]   how those came about, et cetera.
[01:10:01.100 --> 01:10:03.260]   And the original reason for that
[01:10:03.260 --> 01:10:06.260]   was just trying to figure out some questions I had
[01:10:06.260 --> 01:10:07.260]   about quantitative easing
[01:10:07.260 --> 01:10:08.740]   for which I don't have good answers.
[01:10:08.740 --> 01:10:10.620]   But I'm going back to the beginning
[01:10:10.620 --> 01:10:12.620]   and reading about how the Fed was put together,
[01:10:12.620 --> 01:10:14.180]   all the debates around that,
[01:10:14.180 --> 01:10:17.060]   what the financial system looked like without central banks
[01:10:17.060 --> 01:10:21.420]   and how the central banks evolved over time
[01:10:21.420 --> 01:10:23.060]   in their powers and their mandate
[01:10:23.060 --> 01:10:24.660]   and their institutional cultures.
[01:10:24.660 --> 01:10:28.020]   So that is very much a present driven,
[01:10:28.020 --> 01:10:30.020]   but history focused project.
[01:10:30.020 --> 01:10:31.700]   So I don't, maybe there's not a good answer.
[01:10:31.700 --> 01:10:33.700]   Like history, you know, history is still happening.
[01:10:33.700 --> 01:10:37.020]   And so you're technically reading history
[01:10:37.020 --> 01:10:39.660]   if you read something that was written yesterday.
[01:10:39.660 --> 01:10:41.420]   If you read it with a,
[01:10:41.420 --> 01:10:45.220]   you try to read it with a pseudo historian's perspective
[01:10:45.220 --> 01:10:46.860]   where you're trying to figure out
[01:10:46.860 --> 01:10:49.940]   what people will think about this in 50 or a hundred years
[01:10:49.940 --> 01:10:51.460]   when the debate is largely settled,
[01:10:51.460 --> 01:10:53.860]   or at least in 50 or a hundred years
[01:10:53.860 --> 01:10:55.900]   when they're still debating this in history departments,
[01:10:55.900 --> 01:10:57.500]   what will be the main schools of thought
[01:10:57.500 --> 01:10:58.820]   and what will be the strengths and weaknesses
[01:10:58.820 --> 01:11:00.060]   of those schools of thought.
[01:11:00.060 --> 01:11:01.220]   That is probably the right attitude
[01:11:01.220 --> 01:11:02.980]   to have court-smart events.
[01:11:02.980 --> 01:11:03.820]   - Awesome.
[01:11:03.820 --> 01:11:05.660]   Bernd, thanks so much for your time.
[01:11:05.660 --> 01:11:07.100]   - Anytime, thanks.
[01:11:07.100 --> 01:11:09.700]   (upbeat music)
[01:11:09.700 --> 01:11:13.100]   (upbeat music continues)
[01:11:13.100 --> 01:11:16.500]   (upbeat music continues)
[01:11:16.500 --> 01:11:19.900]   (upbeat music continues)
[01:11:20.140 --> 01:11:23.540]   (upbeat music continues)
[01:11:23.540 --> 01:11:26.940]   (upbeat music continues)
[01:11:26.940 --> 01:11:30.340]   (upbeat music continues)
[01:11:30.340 --> 01:11:32.340]   (upbeat music)

