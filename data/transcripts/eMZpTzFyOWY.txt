
[00:00:00.000 --> 00:00:06.240]   All right.
[00:00:06.240 --> 00:00:11.680]   I'm so excited to be here with Peter, the VP of Product and Partnerships at OpenAI,
[00:00:11.680 --> 00:00:18.960]   a day after the GPT-4 launch, one of the most exciting launches I think ever in Silicon
[00:00:18.960 --> 00:00:19.960]   Valley.
[00:00:19.960 --> 00:00:20.960]   So congratulations, Peter.
[00:00:20.960 --> 00:00:21.960]   Thanks so much.
[00:00:21.960 --> 00:00:22.960]   Thanks for having me.
[00:00:22.960 --> 00:00:23.960]   Super excited to chat.
[00:00:23.960 --> 00:00:24.960]   All right.
[00:00:24.960 --> 00:00:30.760]   So I know you're busy and we have limited time and we crowdsourced way too many questions.
[00:00:30.760 --> 00:00:33.760]   So I'm just going to rapid fire at you if that's all right.
[00:00:33.760 --> 00:00:34.760]   Yeah, let's do it.
[00:00:34.760 --> 00:00:35.760]   Let's do it.
[00:00:35.760 --> 00:00:39.640]   So first question is, what are some specific applications of GPT-4 that you and your team
[00:00:39.640 --> 00:00:42.880]   are most enthusiastic about?
[00:00:42.880 --> 00:00:48.240]   I think there are tons of them, but just to call out a few, one thing that I really thought
[00:00:48.240 --> 00:00:56.360]   was super cool was Be My Eyes, who is using the visual inputs to GPT-4 to help visually
[00:00:56.360 --> 00:00:57.360]   impaired people.
[00:00:57.360 --> 00:01:01.280]   I mean, it's like, computer vision used to be this thing where you did object recognition
[00:01:01.280 --> 00:01:03.280]   and so on, and suddenly you can do much more.
[00:01:03.280 --> 00:01:06.320]   You can ask, "Hey, is something wrong in my outfit?
[00:01:06.320 --> 00:01:07.320]   Something I should fix?"
[00:01:07.320 --> 00:01:09.440]   Or, "I have these ingredients.
[00:01:09.440 --> 00:01:10.440]   What should I cook?"
[00:01:10.440 --> 00:01:17.020]   It opens up a whole new range of applications.
[00:01:17.020 --> 00:01:18.200]   And you're originally a vision guy, right?
[00:01:18.200 --> 00:01:21.680]   That must be wild to see this approach work so well.
[00:01:21.680 --> 00:01:22.680]   That's right.
[00:01:22.680 --> 00:01:23.680]   It's sort of crazy.
[00:01:23.680 --> 00:01:24.680]   All we did was language models.
[00:01:24.680 --> 00:01:25.680]   Awesome.
[00:01:25.680 --> 00:01:26.680]   Okay.
[00:01:26.680 --> 00:01:32.040]   Does a lot of prompt engineering become redundant given the larger context length and the ability
[00:01:32.040 --> 00:01:35.240]   to supply more in-context examples in the prompt?
[00:01:35.240 --> 00:01:36.240]   Oh yeah.
[00:01:36.240 --> 00:01:37.880]   This is super interesting.
[00:01:37.880 --> 00:01:43.960]   I sort of feel like, I keep on thinking that something is both right and wrong with prompt
[00:01:43.960 --> 00:01:44.960]   engineering.
[00:01:44.960 --> 00:01:48.260]   Because ultimately, prompt engineering, when we should think about it, we should think
[00:01:48.260 --> 00:01:52.460]   about it like, how good are you at this specifying task?
[00:01:52.460 --> 00:01:56.180]   Good managers should be able to tell their employee, whatever, "This is your job.
[00:01:56.180 --> 00:01:57.340]   This is what you work on."
[00:01:57.340 --> 00:01:59.340]   There shouldn't be super much ambiguity.
[00:01:59.340 --> 00:02:02.780]   And that's usually what prompt engineering is about.
[00:02:02.780 --> 00:02:07.420]   But this ability to just have a conversation with the model and fix it, like, "No, no,
[00:02:07.420 --> 00:02:08.420]   no.
[00:02:08.420 --> 00:02:11.020]   That was too formal," or, "No, make it a little upbeat.
[00:02:11.020 --> 00:02:12.020]   Add a joke here."
[00:02:12.480 --> 00:02:16.000]   That sort of interaction, that's how I think we would think about language models, much
[00:02:16.000 --> 00:02:20.240]   more than adding lots of examples or writing very specified instructions.
[00:02:20.240 --> 00:02:24.960]   Let's just have a conversation with the model and it would pick up on what I want over time.
[00:02:24.960 --> 00:02:25.960]   Totally.
[00:02:25.960 --> 00:02:26.960]   Makes sense.
[00:02:26.960 --> 00:02:27.960]   All right.
[00:02:27.960 --> 00:02:31.060]   Now, here's a question we got the most in different forms, but why weren't you able
[00:02:31.060 --> 00:02:36.560]   to keep the training of GPT-4 on more up-to-date data?
[00:02:36.560 --> 00:02:38.300]   Oh yeah.
[00:02:38.300 --> 00:02:42.000]   That's such an interesting question.
[00:02:42.000 --> 00:02:46.740]   So ultimately, training these models takes a long time.
[00:02:46.740 --> 00:02:49.200]   So that's ultimately the answer.
[00:02:49.200 --> 00:02:54.500]   I can't tell you exactly how long it took to train this, but there's processes here
[00:02:54.500 --> 00:02:58.840]   of at some point you're doing testing on certain kinds of data, and then you want to lock down
[00:02:58.840 --> 00:03:02.280]   that data and not change it too much.
[00:03:02.280 --> 00:03:05.800]   So you can really trust that when you run the big training run, nothing's going to go
[00:03:05.800 --> 00:03:06.800]   wrong.
[00:03:06.800 --> 00:03:10.160]   That just means that you're going to have stale data at some point.
[00:03:10.160 --> 00:03:16.480]   Obviously, getting fresh data into the models is a pretty big question that we get over
[00:03:16.480 --> 00:03:17.480]   and over and over again.
[00:03:17.480 --> 00:03:22.040]   So it's something that's definitely on top of our mind to fix going forward.
[00:03:22.040 --> 00:03:27.200]   But ultimately, it just takes time to train these models.
[00:03:27.200 --> 00:03:28.200]   Okay.
[00:03:28.200 --> 00:03:29.580]   All right.
[00:03:29.580 --> 00:03:34.040]   Considering that GPT-4 is multimodal now, do you imagine future foundation models to
[00:03:34.040 --> 00:03:36.480]   be multimodal by design?
[00:03:36.480 --> 00:03:39.960]   And I guess, do you have any comments on what the biggest blockers were when introducing
[00:03:39.960 --> 00:03:41.960]   the multimodal component?
[00:03:41.960 --> 00:03:46.720]   Yeah, that's a good question.
[00:03:46.720 --> 00:03:52.260]   I definitely think that multimodal is sort of the future.
[00:03:52.260 --> 00:03:56.080]   You just give the models abilities to understand many more concepts.
[00:03:56.080 --> 00:04:02.920]   They kind of understand things like physics concepts based on looking at images, or they
[00:04:02.920 --> 00:04:07.040]   can understand, you can imagine eventually these models dealing with more modalities
[00:04:07.040 --> 00:04:08.440]   like audio and Sonic.
[00:04:08.440 --> 00:04:17.440]   It's pretty clear that the future here is just more and more multimodal models.
[00:04:17.440 --> 00:04:22.920]   Some components are already there today, like with Whisper, you can take audio and turn
[00:04:22.920 --> 00:04:25.400]   it into text and so on.
[00:04:25.400 --> 00:04:28.360]   All of these things eventually will get connected.
[00:04:28.360 --> 00:04:36.120]   I think in terms of challenges, I will confess here that I wasn't on the research team that
[00:04:36.120 --> 00:04:39.600]   did a lot of the amazing work to get GPT-4 working.
[00:04:39.600 --> 00:04:49.400]   But I just know that going from small models to these enormous models that GPT-4 is, that's
[00:04:49.400 --> 00:04:51.120]   a huge challenge in itself.
[00:04:51.120 --> 00:04:55.480]   You need to do a lot of science to understand on small models to see, here's how good it
[00:04:55.480 --> 00:04:59.120]   is and here's how good it's going to be when we scale it up.
[00:04:59.120 --> 00:05:08.600]   And that just requires a lot of very detailed investigations and so on, and a lot of just
[00:05:08.600 --> 00:05:10.560]   hard work, which that team did.
[00:05:10.560 --> 00:05:11.560]   Totally.
[00:05:11.560 --> 00:05:15.720]   Were there any new specific challenges with GPT-4 different than previous models, or is
[00:05:15.720 --> 00:05:22.040]   it just trying to get even another level of scale?
[00:05:22.040 --> 00:05:30.440]   Ultimately, it is just another level of scale, but I would say that we work in all aspects
[00:05:30.440 --> 00:05:34.920]   of that, whether it's data, whether it's how to most efficiently use the compute, architectures
[00:05:34.920 --> 00:05:42.520]   of models, and all of these things are things that we tune and tweak continuously.
[00:05:42.520 --> 00:05:46.600]   But ultimately, a big part of it is really being able to scale.
[00:05:46.600 --> 00:05:50.400]   And as you scale, every order of magnitude, there's just more and more things that break,
[00:05:50.400 --> 00:05:52.480]   even in your infrastructure.
[00:05:52.480 --> 00:05:59.320]   And so we have just an incredibly talented infrastructure engineering team that is made
[00:05:59.320 --> 00:06:04.240]   up of both engineers and researchers that have built this incredible system for being
[00:06:04.240 --> 00:06:05.520]   able to train these models.
[00:06:05.520 --> 00:06:09.200]   And there's just so many little things that you need to get right, because if you have
[00:06:09.200 --> 00:06:12.600]   a bug somewhere, it's going to propagate immediately.
[00:06:12.600 --> 00:06:17.600]   And so I think one of the biggest mantras, essentially, that team has is, "Don't have
[00:06:17.600 --> 00:06:22.840]   bugs," which seems like the most hardest thing to do as a software engineer.
[00:06:22.840 --> 00:06:25.640]   Good advice, though.
[00:06:25.640 --> 00:06:28.760]   Don't have bugs.
[00:06:28.760 --> 00:06:30.640]   So here's another question.
[00:06:30.640 --> 00:06:37.360]   I don't even know if it's fully formed, but do you have a sense or a guess of the order
[00:06:37.360 --> 00:06:41.920]   of magnitude of experiments running in a typical day, like in the weights and biases notion
[00:06:41.920 --> 00:06:44.120]   of experiments, like training something?
[00:06:44.120 --> 00:06:50.200]   Is it like a handful or millions or what do you think it is?
[00:06:50.200 --> 00:06:51.200]   It's a good question.
[00:06:51.200 --> 00:06:56.040]   I would say there's probably on the order of like, I don't know, like somewhere between
[00:06:56.040 --> 00:07:00.300]   100 and 200 people essentially doing some sort of research at OpenAI.
[00:07:00.300 --> 00:07:06.320]   And each of them are probably running on the order of a few experiments per week.
[00:07:06.320 --> 00:07:09.640]   So that probably makes it like multiple hundreds experiments per week.
[00:07:09.640 --> 00:07:10.640]   Some of them will take...
[00:07:10.640 --> 00:07:14.720]   Some experiments take weeks, some experiments take hours, right?
[00:07:14.720 --> 00:07:17.160]   But it's sort of like, that's probably the order of magnitude.
[00:07:17.160 --> 00:07:18.160]   Got it.
[00:07:18.160 --> 00:07:19.160]   Okay.
[00:07:19.160 --> 00:07:22.720]   To the extent that you can comment, what was the most useful type or source of data for
[00:07:22.720 --> 00:07:23.720]   training GPT-4?
[00:07:23.720 --> 00:07:33.320]   Oh, I mean, I feel like it's just like the text on the internet is probably the answer
[00:07:33.320 --> 00:07:34.320]   here.
[00:07:34.320 --> 00:07:37.440]   You know, these models are just incredibly data hungry.
[00:07:37.440 --> 00:07:41.160]   So just finding more and more and more of this, like we have a whole team that's just
[00:07:41.160 --> 00:07:45.840]   like basically focused on like, how can we bring more data into these models?
[00:07:45.840 --> 00:07:50.840]   So I think at this point, it's like almost to some extent, it's like less about the particular
[00:07:50.840 --> 00:07:53.760]   data and more about more data, you know?
[00:07:53.760 --> 00:07:57.040]   So that's probably how I would think about it.
[00:07:57.040 --> 00:07:58.360]   Is there any particular task?
[00:07:58.360 --> 00:08:03.800]   I mean, I saw all these tasks that you showed GPT-4 getting better at.
[00:08:03.800 --> 00:08:08.880]   Is there any task where like, you know, it's been surprisingly hard given, you know, how
[00:08:08.880 --> 00:08:13.520]   impressive like all these, like every version of LLMs is, anything stand out as like you
[00:08:13.520 --> 00:08:17.160]   would have thought it would have gotten solved by an earlier version?
[00:08:17.160 --> 00:08:19.480]   Oh, that's a good question.
[00:08:19.480 --> 00:08:23.440]   I mean, it is still surprising to me.
[00:08:23.440 --> 00:08:29.960]   I feel like at this point, like GPT-4 is quite good at just doing mental arithmetic and stuff
[00:08:29.960 --> 00:08:30.960]   like that.
[00:08:30.960 --> 00:08:34.400]   But like, you know, it took all the way to GPT-4 to get it to be fairly robust at that.
[00:08:34.400 --> 00:08:38.600]   It would just make these silly mistakes for even up to 3.5, you know?
[00:08:38.600 --> 00:08:42.560]   And it's still, just to be clear, it still makes some silly mistakes.
[00:08:42.560 --> 00:08:47.360]   But you know, it's like, the way I look at it, it's kind of weird, almost like a weird
[00:08:47.360 --> 00:08:48.360]   savant in some sense, right?
[00:08:48.360 --> 00:08:52.360]   Like it has these amazing capabilities on some dimensions and then makes these like
[00:08:52.360 --> 00:08:58.040]   incredibly stupid mistakes that a human just wouldn't make unless the human is drunk or
[00:08:58.040 --> 00:08:59.040]   something, right?
[00:08:59.040 --> 00:09:02.160]   That's sort of like, that's what it feels like sometimes with the model is like, you
[00:09:02.160 --> 00:09:06.320]   kind of never know, did you get the kind of early morning version or like the late at
[00:09:06.320 --> 00:09:08.200]   night after multiple drinks version?
[00:09:08.200 --> 00:09:14.240]   And like, the weirdest thing about this is that sometimes like even like prompting the
[00:09:14.240 --> 00:09:15.760]   model, it seems to matter.
[00:09:15.760 --> 00:09:19.720]   Like if you tell the model, "Hey, you're a genius at math," it would do better at math
[00:09:19.720 --> 00:09:22.040]   than if you tell it's like, "You're not very good at math."
[00:09:22.040 --> 00:09:23.040]   Right?
[00:09:23.040 --> 00:09:24.040]   Like it's-
[00:09:24.040 --> 00:09:26.800]   So literally, you can't like prod it, like you can't get it riled up and then make it
[00:09:26.800 --> 00:09:27.800]   work.
[00:09:27.800 --> 00:09:28.800]   Exactly.
[00:09:28.800 --> 00:09:33.840]   And so the model is like, it's still very vulnerable to this kind of stuff.
[00:09:33.840 --> 00:09:38.480]   So but I think that's like, you know, that's one thing I think it is.
[00:09:38.480 --> 00:09:47.000]   I mean, it's like, it's also funny, you might have seen in Greg Brockman's demo that he
[00:09:47.000 --> 00:09:52.520]   did like, you know, the whole thing about like summarizing with a single letter, the
[00:09:52.520 --> 00:09:54.680]   same letter starting every word.
[00:09:54.680 --> 00:09:58.560]   I mean, it's kind of, you know, it's sort of weird how it just happens that once you
[00:09:58.560 --> 00:10:01.360]   reach a certain scale, it kind of just gets it.
[00:10:01.360 --> 00:10:04.960]   And I think there's a number of those where it's just like, it's kind of weird how you
[00:10:04.960 --> 00:10:09.520]   just, you know, I think it's kind of called grokking, this phenomenon where you train
[00:10:09.520 --> 00:10:11.160]   and train and train, it doesn't do very well.
[00:10:11.160 --> 00:10:12.480]   And then suddenly, it just gets it.
[00:10:12.480 --> 00:10:16.160]   Something kind of, it forms some sort of representation and just kind of gets it.
[00:10:16.160 --> 00:10:18.200]   And so there's been a few of those.
[00:10:18.200 --> 00:10:21.120]   And it's like, I think it's always surprising when they happen.
[00:10:21.120 --> 00:10:23.080]   And you're like, "Why didn't it happen earlier?"
[00:10:23.080 --> 00:10:25.000]   You know, I don't know.
[00:10:25.000 --> 00:10:26.000]   Interesting.
[00:10:26.000 --> 00:10:27.000]   Cool.
[00:10:27.000 --> 00:10:31.040]   Okay, here's an interesting question, maybe useful for us at Wits and Biases.
[00:10:31.040 --> 00:10:35.040]   What kind of visualization tools would be most helpful as we continue to build on these
[00:10:35.040 --> 00:10:36.040]   models?
[00:10:36.040 --> 00:10:38.680]   Or do you have any ideas of what might be helpful for people building on top of these
[00:10:38.680 --> 00:10:39.680]   models?
[00:10:39.680 --> 00:10:42.040]   Oh, yeah, that's such a great question.
[00:10:42.040 --> 00:10:47.480]   Because I think this is like kind of evolving more and more as these models get better and
[00:10:47.480 --> 00:10:48.480]   better.
[00:10:48.480 --> 00:10:52.820]   You know, it used to be the case where I think kind of all you cared about was really like
[00:10:52.820 --> 00:10:54.600]   perplexity and so on as you were training it.
[00:10:54.600 --> 00:10:56.400]   It's like that was mostly what you were optimizing.
[00:10:56.400 --> 00:11:02.680]   But I think we're getting to a point now like where you kind of, you really want to visualize
[00:11:02.680 --> 00:11:06.800]   how like dig into kind of the mistakes that the models are doing.
[00:11:06.800 --> 00:11:10.920]   And so like you often, we move to a world where I think you can get really far with
[00:11:10.920 --> 00:11:17.680]   like prompting and whether it's like through conversations or a single kind of prompt.
[00:11:17.680 --> 00:11:21.960]   And then, but then you will get to some point where you can kind of, you can fine tune these
[00:11:21.960 --> 00:11:23.320]   models to with even more data.
[00:11:23.320 --> 00:11:27.520]   And at that point, I think the paradigm now is that you don't need a ton of data, but
[00:11:27.520 --> 00:11:30.760]   you need to make sure that the data is kind of really, really high quality.
[00:11:30.760 --> 00:11:34.560]   So understanding kind of a way to see like where are mistakes made?
[00:11:34.560 --> 00:11:38.080]   How like how do you automatically kind of build, you know, we ship this open source
[00:11:38.080 --> 00:11:43.800]   library called OpenAI evals that kind of makes it easy for you to build automated evals.
[00:11:43.800 --> 00:11:48.640]   So I think like that would be a big part of the workflow and allowing people to see overall
[00:11:48.640 --> 00:11:53.080]   trends but dig into the examples, because oftentimes what we find that at the end of
[00:11:53.080 --> 00:11:55.760]   the day, like examples are under specified and so on.
[00:11:55.760 --> 00:12:01.080]   And so you just need to be so way, way more careful now that like the models are so good
[00:12:01.080 --> 00:12:06.080]   that like you're not seeing weird stuff because the model is bad versus like you just had
[00:12:06.080 --> 00:12:08.440]   a mistake in your data set.
[00:12:08.440 --> 00:12:10.440]   Makes sense.
[00:12:10.440 --> 00:12:11.440]   Okay.
[00:12:11.440 --> 00:12:17.120]   When, so I guess, here's, I guess the summary of this question is how many people worked
[00:12:17.120 --> 00:12:18.120]   on GPD for?
[00:12:18.120 --> 00:12:21.160]   How many NML engineers did it take to train this thing?
[00:12:21.160 --> 00:12:23.400]   Oh, that's a good question.
[00:12:23.400 --> 00:12:29.480]   I mean, it's just like, I feel like all of OpenAI worked on this one in one way or another,
[00:12:29.480 --> 00:12:34.840]   because like we have this whole kind of process, like a big, like, you know, all the data set
[00:12:34.840 --> 00:12:39.320]   work that goes into it, all of the infrastructure engineering that built the data centers that
[00:12:39.320 --> 00:12:45.360]   kind of built the infrastructure on top of those data centers, training these models,
[00:12:45.360 --> 00:12:49.760]   then all the kind of science and stuff that went into actually the pre-training.
[00:12:49.760 --> 00:12:54.960]   And then we do this kind of fine tuning on top with RL from human feedback, you know,
[00:12:54.960 --> 00:13:00.160]   so I feel like at the end of the day, it's got to be like probably out of the 300 plus
[00:13:00.160 --> 00:13:06.400]   people at OpenAI, probably at least two thirds of the company worked on some aspect of this.
[00:13:06.400 --> 00:13:07.400]   Okay.
[00:13:07.400 --> 00:13:12.080]   And final question, since this is our user conference and you are such a high profile
[00:13:12.080 --> 00:13:16.120]   user, OpenAI, I'm curious if you could say a little bit about how OpenAI uses Weights
[00:13:16.120 --> 00:13:20.680]   and Biases and if you have a favorite feature or part of Weights and Biases, we'd love to
[00:13:20.680 --> 00:13:21.680]   know about that.
[00:13:21.680 --> 00:13:27.360]   Yeah, I mean, we use it for pretty much all of our model training.
[00:13:27.360 --> 00:13:31.920]   So just like, you know, just tracking them.
[00:13:31.920 --> 00:13:35.840]   I think there's a lot of kind of just sharing of like, you know, the fact that you can easily
[00:13:35.840 --> 00:13:40.720]   share training runs and stuff like that, it's a super used feature.
[00:13:40.720 --> 00:13:45.720]   But I think one thing that, you know, these days I do way, way less of that sort of work.
[00:13:45.720 --> 00:13:49.320]   So one of the features I really like is kind of the ability to kind of have reports and
[00:13:49.320 --> 00:13:54.000]   so on where people, so we use that quite heavily, depends a little bit on the team, but a number
[00:13:54.000 --> 00:13:57.600]   of teams are using that quite heavily to kind of really have a clear hypothesis.
[00:13:57.600 --> 00:14:04.200]   Here's like the hypothesis, here are the experiments that were run to kind of validate or invalidate
[00:14:04.200 --> 00:14:06.080]   that hypothesis, here's the conclusion.
[00:14:06.080 --> 00:14:10.320]   You have all these like mini scientific papers, essentially, on all of the stuff that's happening
[00:14:10.320 --> 00:14:14.640]   at OpenAI, which is like incredibly interesting to kind of follow along with.
[00:14:14.640 --> 00:14:15.640]   Fantastic.
[00:14:15.640 --> 00:14:16.640]   That sounds very interesting.
[00:14:16.640 --> 00:14:18.160]   Thank you so much, Peter.
[00:14:18.160 --> 00:14:22.680]   It's really, really nice to have your time, especially so close to after your big launch
[00:14:22.680 --> 00:14:24.200]   and congratulations.
[00:14:24.200 --> 00:14:27.640]   We're loving using it internally here.
[00:14:27.640 --> 00:14:28.640]   Thank you so much for having me.
[00:14:28.640 --> 00:14:32.640]   This was super fun and can't wait to see what you all build with Dppd4.
[00:14:32.640 --> 00:14:33.640]   Awesome.
[00:14:33.640 --> 00:14:34.640]   Take care.
[00:14:34.640 --> 00:14:38.000]   [MUSIC PLAYING]

