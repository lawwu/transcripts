
[00:00:00.000 --> 00:00:04.280]   All right, yeah, so thanks a lot, Charles, for having me.
[00:00:04.280 --> 00:00:06.120]   I'm excited to talk about this work.
[00:00:06.120 --> 00:00:09.480]   It's a new approach to unsupervised representation
[00:00:09.480 --> 00:00:13.760]   learning with specific application to disentanglement.
[00:00:13.760 --> 00:00:16.840]   I worked on it with these people named on the slide.
[00:00:16.840 --> 00:00:19.280]   And like Charles said, I'm at Matthias Betke's lab
[00:00:19.280 --> 00:00:21.920]   at the University of TÃ¼bingen.
[00:00:21.920 --> 00:00:24.080]   OK, so let's get started.
[00:00:24.080 --> 00:00:25.960]   I'm just going to-- just as a quick overview,
[00:00:25.960 --> 00:00:28.400]   I'm going to spend some time talking about disentanglement.
[00:00:28.400 --> 00:00:29.080]   What is it?
[00:00:29.080 --> 00:00:30.040]   Why is it important?
[00:00:30.040 --> 00:00:31.120]   Where does it come from?
[00:00:31.120 --> 00:00:33.160]   What has been done so far?
[00:00:33.160 --> 00:00:35.480]   And then I'll go into our approach for disentanglement,
[00:00:35.480 --> 00:00:38.800]   which includes introducing new data sets intended
[00:00:38.800 --> 00:00:43.160]   to move the field towards more natural benchmarks.
[00:00:43.160 --> 00:00:45.720]   So as Charles said, my background is in business science.
[00:00:45.720 --> 00:00:47.920]   So I like to think about the task of disentanglement
[00:00:47.920 --> 00:00:51.240]   in the context of the human visual system.
[00:00:51.240 --> 00:00:53.520]   Generally, our task is to disentangle
[00:00:53.520 --> 00:00:55.720]   the original sources from the mixed signal that
[00:00:55.720 --> 00:00:57.360]   comes to our eye.
[00:00:57.360 --> 00:00:59.560]   And this is an extremely difficult task.
[00:00:59.560 --> 00:01:02.640]   One example of why is that our eyes have a 2D array
[00:01:02.640 --> 00:01:05.280]   of receptors, and the world is in 3D.
[00:01:05.280 --> 00:01:07.520]   So for any image that would be projected under our eye,
[00:01:07.520 --> 00:01:10.360]   there's an infinite number of scenarios
[00:01:10.360 --> 00:01:13.520]   that could lead to that image.
[00:01:13.520 --> 00:01:16.160]   And this is also going to be true for disentangling
[00:01:16.160 --> 00:01:20.120]   from digital images or videos.
[00:01:20.120 --> 00:01:23.120]   And so in disentanglement, we have what we call sources.
[00:01:23.120 --> 00:01:25.720]   And some example of these sources are highlighted in red.
[00:01:25.720 --> 00:01:26.760]   They're pretty intuitive.
[00:01:26.760 --> 00:01:28.960]   They're basically just like the high level descriptors
[00:01:28.960 --> 00:01:32.120]   that one would use to explain a scene.
[00:01:32.120 --> 00:01:35.000]   And so for one observation that we
[00:01:35.000 --> 00:01:37.680]   can use to our advantage in the natural world
[00:01:37.680 --> 00:01:39.280]   is that certain properties of the world
[00:01:39.280 --> 00:01:41.440]   are generally persistent while others change.
[00:01:41.440 --> 00:01:44.280]   For example, if the tiger moves, it's still a tiger.
[00:01:44.280 --> 00:01:46.160]   Or another example is as the day progresses,
[00:01:46.160 --> 00:01:49.400]   the lighting changes, the visual signals might change a lot,
[00:01:49.400 --> 00:01:51.480]   but the tree is still a tree.
[00:01:51.480 --> 00:01:53.720]   And so you might notice that the two examples I just gave
[00:01:53.720 --> 00:01:55.640]   involve things that change over time.
[00:01:55.640 --> 00:01:57.600]   And that's going to become important later
[00:01:57.600 --> 00:01:59.680]   when I explain our model.
[00:01:59.680 --> 00:02:02.440]   So ultimately, I'm interested in how the brain figures this out.
[00:02:02.440 --> 00:02:03.940]   None of the work that I'm going to present today
[00:02:03.940 --> 00:02:06.200]   is actually a proposal for specific computation
[00:02:06.200 --> 00:02:07.200]   in the brain.
[00:02:07.200 --> 00:02:10.600]   We do provide a partial solution to this problem that exploits
[00:02:10.600 --> 00:02:12.520]   the statistics of the world.
[00:02:12.520 --> 00:02:14.960]   And we demonstrate via an engineered approach
[00:02:14.960 --> 00:02:17.480]   that it's at least plausible that the brain could exploit
[00:02:17.480 --> 00:02:19.920]   such statistics as well.
[00:02:19.920 --> 00:02:21.400]   So as with any problem in science,
[00:02:21.400 --> 00:02:23.240]   the first thing we want to do is simplify it
[00:02:23.240 --> 00:02:27.080]   as possible and define it in more concrete terms.
[00:02:27.080 --> 00:02:29.080]   So that's what I'm going to do here.
[00:02:29.080 --> 00:02:31.680]   On this slide in black, I have the steps
[00:02:31.680 --> 00:02:34.280]   that we'd be taking that are associated with disentanglement.
[00:02:34.280 --> 00:02:37.000]   In red, I have examples of what those steps might be.
[00:02:37.000 --> 00:02:38.720]   And in blue, I've assigned some variables
[00:02:38.720 --> 00:02:41.240]   to make it easier to talk about later.
[00:02:41.240 --> 00:02:43.560]   So first, let's talk about the generator or the world.
[00:02:43.560 --> 00:02:46.600]   We have our sources, which I'm calling S.
[00:02:46.600 --> 00:02:48.760]   And so in the case of, for example,
[00:02:48.760 --> 00:02:50.800]   fake generated data, like in video games,
[00:02:50.800 --> 00:02:52.600]   we know exactly what those sources are.
[00:02:52.600 --> 00:02:54.120]   We have exact information for everything
[00:02:54.120 --> 00:02:57.000]   we need to make an image, like lighting and position,
[00:02:57.000 --> 00:02:59.000]   pose, identity.
[00:02:59.000 --> 00:03:02.760]   For real world data, natural images and videos,
[00:03:02.760 --> 00:03:04.600]   we don't know what all the sources are.
[00:03:04.600 --> 00:03:06.160]   We have to only guess.
[00:03:06.160 --> 00:03:08.360]   And these sources are mixed with the ground truth
[00:03:08.360 --> 00:03:09.160]   generator model.
[00:03:09.160 --> 00:03:11.000]   So for video games, it would be a graphics engine.
[00:03:11.000 --> 00:03:12.440]   And for natural signals, it's just
[00:03:12.440 --> 00:03:15.040]   physical processes, like bouncing around,
[00:03:15.040 --> 00:03:17.000]   interacting with things in the world.
[00:03:17.000 --> 00:03:18.880]   And this mixture is a complicated process,
[00:03:18.880 --> 00:03:20.920]   produces these entangled signals,
[00:03:20.920 --> 00:03:22.680]   which we could think of as images.
[00:03:22.680 --> 00:03:26.040]   And then our goal is to try to disentangle that.
[00:03:26.040 --> 00:03:29.880]   So we want to learn something about this world process.
[00:03:29.880 --> 00:03:32.880]   One common next step is to use an encoder.
[00:03:32.880 --> 00:03:35.920]   So we want to recover sources from the mixed data
[00:03:35.920 --> 00:03:39.720]   and produce a disentangled code, which we'll call latent code.
[00:03:39.720 --> 00:03:43.160]   Many approaches also try to learn to generate new samples
[00:03:43.160 --> 00:03:44.880]   from the latent code.
[00:03:44.880 --> 00:03:47.360]   And so they learn to approximate the original ground truth
[00:03:47.360 --> 00:03:51.800]   generator and generate data samples.
[00:03:51.800 --> 00:03:54.440]   So in our model, what we're going to learn,
[00:03:54.440 --> 00:03:57.560]   or things we could learn, are the functions f and g,
[00:03:57.560 --> 00:04:00.200]   which could be neural networks or other algorithms.
[00:04:00.200 --> 00:04:02.680]   In our case, they're going to be neural networks.
[00:04:02.680 --> 00:04:04.040]   And there's a few criteria that we
[00:04:04.040 --> 00:04:06.920]   might use to check to see if f and g are correct.
[00:04:06.920 --> 00:04:09.680]   One is, does my latent code match my original sources?
[00:04:09.680 --> 00:04:11.600]   So if we have access to the original sources,
[00:04:11.600 --> 00:04:13.880]   we can just check that directly.
[00:04:13.880 --> 00:04:16.800]   Does my encoder match the inverse of the ground truth
[00:04:16.800 --> 00:04:17.720]   generator?
[00:04:17.720 --> 00:04:19.560]   That's times that g star negative 1
[00:04:19.560 --> 00:04:21.680]   means I'm looking at the inverse of that generator.
[00:04:21.680 --> 00:04:24.440]   I'm able to undo that mixing process.
[00:04:24.440 --> 00:04:26.760]   Or does my generator match the ground truth generator?
[00:04:26.760 --> 00:04:28.360]   And an easy way to check that is just,
[00:04:28.360 --> 00:04:31.760]   does my generated data look like the original ground truth data?
[00:04:31.760 --> 00:04:33.720]   So if I generate a data set x, does it
[00:04:33.720 --> 00:04:36.960]   look like the data set x star?
[00:04:36.960 --> 00:04:39.460]   Another common way to check that the latent code is sensible
[00:04:39.460 --> 00:04:43.040]   is to just purposefully vary the latent code z
[00:04:43.040 --> 00:04:45.360]   and look at what happens in the generated images.
[00:04:45.360 --> 00:04:47.760]   So for example, if z is a vector of 10 numbers,
[00:04:47.760 --> 00:04:49.960]   and I want to vary just one of those numbers,
[00:04:49.960 --> 00:04:52.260]   and I see that in my image, the size of an object
[00:04:52.260 --> 00:04:55.120]   changes without anything else changing,
[00:04:55.120 --> 00:04:58.920]   then I can assign that specific number in z a size label.
[00:04:58.920 --> 00:05:04.280]   So there's a lot of different approaches to disentanglement.
[00:05:04.280 --> 00:05:08.000]   And these different approaches focus on different portions
[00:05:08.000 --> 00:05:10.040]   of this task.
[00:05:10.040 --> 00:05:12.680]   So for example, ICA, independent component analysis,
[00:05:12.680 --> 00:05:16.600]   is a really common one that dates back to the '80s.
[00:05:16.600 --> 00:05:18.560]   And they only focus on the demixing part,
[00:05:18.560 --> 00:05:20.320]   the disentangling part.
[00:05:20.320 --> 00:05:22.220]   So the upsides of this type of algorithm
[00:05:22.220 --> 00:05:26.720]   is it's easier than considering the whole problem,
[00:05:26.720 --> 00:05:28.600]   both in terms of the mathematical description
[00:05:28.600 --> 00:05:31.000]   and the applications.
[00:05:31.000 --> 00:05:33.600]   It really narrows down the true problem of disentanglement
[00:05:33.600 --> 00:05:35.740]   to the question of, can we undo the process that
[00:05:35.740 --> 00:05:37.600]   happens in the real world?
[00:05:37.600 --> 00:05:39.760]   And another thing is it was proven a long time ago
[00:05:39.760 --> 00:05:42.000]   that it can produce identifiable solutions, which
[00:05:42.000 --> 00:05:44.480]   I'm going to describe more in a few slides.
[00:05:44.480 --> 00:05:46.880]   However, some downsides are, one, we don't know--
[00:05:46.880 --> 00:05:48.400]   we don't have a good way of verifying
[00:05:48.400 --> 00:05:50.700]   that we've preserved all the information.
[00:05:50.700 --> 00:05:54.040]   So we don't really have a good way of checking that z has
[00:05:54.040 --> 00:05:56.720]   all the information in x, whereas if we were regenerating
[00:05:56.720 --> 00:05:59.120]   data samples, then we would know for sure.
[00:05:59.120 --> 00:06:00.520]   And we also don't have a good way
[00:06:00.520 --> 00:06:02.680]   of generating new data, which has a lot of its own
[00:06:02.680 --> 00:06:05.880]   interesting applications.
[00:06:05.880 --> 00:06:09.200]   So ICA has been around for a very long time.
[00:06:09.200 --> 00:06:11.960]   And so there's a bunch of places where ICA gets used.
[00:06:11.960 --> 00:06:15.640]   In industry, so some classic applications of ICA
[00:06:15.640 --> 00:06:17.280]   are the cocktail party problems.
[00:06:17.280 --> 00:06:19.120]   This is where you have a cocktail party,
[00:06:19.120 --> 00:06:20.360]   a bunch of people are talking.
[00:06:20.360 --> 00:06:22.680]   You have a couple of microphones in the room.
[00:06:22.680 --> 00:06:24.480]   And you're recording all the conversations.
[00:06:24.480 --> 00:06:27.080]   And so the conversations are getting mixed.
[00:06:27.080 --> 00:06:29.160]   And then the goal is to de-mix that conversation
[00:06:29.160 --> 00:06:32.520]   or disentangle it so that the output is individual persons,
[00:06:32.520 --> 00:06:33.880]   what they're saying.
[00:06:33.880 --> 00:06:36.180]   Another example of ICA is estimating
[00:06:36.180 --> 00:06:38.000]   the underlying structure of natural images.
[00:06:38.000 --> 00:06:40.560]   So people have shown that you can take a natural image
[00:06:40.560 --> 00:06:42.680]   as input, and the output is estimates
[00:06:42.680 --> 00:06:45.560]   of the core atomic elements that could
[00:06:45.560 --> 00:06:49.400]   be used to construct those natural images,
[00:06:49.400 --> 00:06:50.080]   visual elements.
[00:06:50.080 --> 00:06:54.640]   I don't mean literal atomic elements.
[00:06:54.640 --> 00:06:56.480]   So another interesting application
[00:06:56.480 --> 00:06:58.560]   that I thought would be relevant with the recent--
[00:06:58.560 --> 00:07:00.560]   or topical with the recent news on Neuralink
[00:07:00.560 --> 00:07:02.040]   is to look at brain data.
[00:07:02.040 --> 00:07:04.120]   So this is not the same type of data as Neuralink,
[00:07:04.120 --> 00:07:06.560]   but still interesting nonetheless.
[00:07:06.560 --> 00:07:10.000]   And the reason why this is interesting is because it
[00:07:10.000 --> 00:07:10.800]   allows me to--
[00:07:10.800 --> 00:07:13.280]   it has relation to identifiability.
[00:07:13.280 --> 00:07:16.520]   So the task here is we have a EEG array.
[00:07:16.520 --> 00:07:18.600]   So it's just like a thing that goes in your head.
[00:07:18.600 --> 00:07:20.080]   And it records brain activity.
[00:07:20.080 --> 00:07:22.360]   And the brain activity is coming from neurons.
[00:07:22.360 --> 00:07:24.280]   Neurons are producing electrical signals.
[00:07:24.280 --> 00:07:25.520]   But they're getting mixed up.
[00:07:25.520 --> 00:07:26.900]   They're going through brain data,
[00:07:26.900 --> 00:07:29.040]   through your skull, et cetera.
[00:07:29.040 --> 00:07:31.480]   And so if we train a disentangling model on ICA,
[00:07:31.480 --> 00:07:34.720]   the goal is to get out relevant signals that
[00:07:34.720 --> 00:07:39.120]   are relevant to cognition or action or whatever.
[00:07:39.120 --> 00:07:42.880]   And so identifiability is important in this case.
[00:07:42.880 --> 00:07:44.480]   What identifiability means is that we
[00:07:44.480 --> 00:07:47.240]   can guarantee mathematically that the solution it finds
[00:07:47.240 --> 00:07:49.000]   is the correct solution.
[00:07:49.000 --> 00:07:51.160]   And there's different amounts of identifiability
[00:07:51.160 --> 00:07:54.560]   from an exact match to a match up to permutations
[00:07:54.560 --> 00:07:57.640]   or up to some linear operation.
[00:07:57.640 --> 00:07:59.640]   It was shown a long time ago that ICA
[00:07:59.640 --> 00:08:03.320]   can do identifiable disentanglement when
[00:08:03.320 --> 00:08:04.920]   the process is linear.
[00:08:04.920 --> 00:08:06.680]   But it's actually impossible to do it
[00:08:06.680 --> 00:08:09.800]   when the process is nonlinear for the standard ICA
[00:08:09.800 --> 00:08:12.080]   algorithm.
[00:08:12.080 --> 00:08:16.560]   And so there have been solutions proposed for quite a while
[00:08:16.560 --> 00:08:18.040]   for nonlinear ICA.
[00:08:18.040 --> 00:08:19.600]   But only in the last five years or so
[00:08:19.600 --> 00:08:23.720]   have they really started making advancements to realistic data.
[00:08:23.720 --> 00:08:25.280]   And these solutions assume that you
[00:08:25.280 --> 00:08:29.080]   have some additional information that we didn't
[00:08:29.080 --> 00:08:31.120]   need with regular ICA.
[00:08:31.120 --> 00:08:33.080]   So with linear ICA, we can solve the problem just
[00:08:33.080 --> 00:08:35.200]   with random samples of the data.
[00:08:35.200 --> 00:08:38.080]   But with nonlinear ICA, we need more information like a label
[00:08:38.080 --> 00:08:41.240]   or a time step or some guarantee that a certain number of sources
[00:08:41.240 --> 00:08:43.960]   are only changing from one time point to another.
[00:08:43.960 --> 00:08:49.680]   So another alternative method for doing disentanglement
[00:08:49.680 --> 00:08:53.040]   is with generative adversarial networks or GANs.
[00:08:53.040 --> 00:08:56.120]   And so these look at the second half of the problem only.
[00:08:56.120 --> 00:08:58.960]   They just generate data from random samples
[00:08:58.960 --> 00:09:02.480]   of a latent code and then check that that data looks realistic.
[00:09:02.480 --> 00:09:04.600]   So in practice, these are the most successful
[00:09:04.600 --> 00:09:06.320]   in terms of large-scale data sets.
[00:09:06.320 --> 00:09:09.800]   They can work on high-definition images, much higher-scale data
[00:09:09.800 --> 00:09:11.480]   sets than ICA.
[00:09:11.480 --> 00:09:13.400]   But they're difficult to construct a good metric
[00:09:13.400 --> 00:09:15.800]   for assessing disentanglement because you've completely
[00:09:15.800 --> 00:09:17.680]   disregarded the original ground truth
[00:09:17.680 --> 00:09:20.120]   sources in the whole framework.
[00:09:20.120 --> 00:09:22.520]   And so there's no notion of identifiability.
[00:09:22.520 --> 00:09:25.400]   And therefore, the model itself is less interpretable.
[00:09:25.400 --> 00:09:26.920]   And another important difference is
[00:09:26.920 --> 00:09:29.360]   that we can't actually apply it to the same tasks
[00:09:29.360 --> 00:09:32.520]   that we could apply ICA to.
[00:09:32.520 --> 00:09:36.680]   I can't give you data and have you pass it through your encoder
[00:09:36.680 --> 00:09:38.080]   and give me sources.
[00:09:38.080 --> 00:09:44.080]   So the third approach is the one that we used,
[00:09:44.080 --> 00:09:45.720]   which is to use autoencoders.
[00:09:45.720 --> 00:09:48.440]   So the autoencoder approach uses all the steps above.
[00:09:48.440 --> 00:09:51.200]   And the upsides is, in theory, all of the same upsides
[00:09:51.200 --> 00:09:52.560]   as the other two.
[00:09:52.560 --> 00:09:55.520]   However, generally, it works in practice
[00:09:55.520 --> 00:09:58.600]   on larger-scale data than ICA but smaller-scale data
[00:09:58.600 --> 00:10:00.640]   than GANs.
[00:10:00.640 --> 00:10:02.880]   And until very recently, there were not any approaches
[00:10:02.880 --> 00:10:07.880]   to prove identifiability for autoencoder-based models.
[00:10:07.880 --> 00:10:10.560]   But some recent work has shown that you
[00:10:10.560 --> 00:10:14.080]   can produce an unidentifiable disentanglement code.
[00:10:14.080 --> 00:10:15.840]   I don't have time to go into that now.
[00:10:15.840 --> 00:10:17.420]   But I'd encourage people to check out
[00:10:17.420 --> 00:10:22.040]   our paper or the references on the screen to look into those.
[00:10:22.040 --> 00:10:23.760]   So our approach is using an autoencoder.
[00:10:23.760 --> 00:10:28.440]   So I'm going to spend the rest of my time focusing on that.
[00:10:28.440 --> 00:10:29.840]   OK, so that's the background.
[00:10:29.840 --> 00:10:32.440]   Now I'm going to go into our approach, which
[00:10:32.440 --> 00:10:34.240]   we'll start with looking at benchmarks
[00:10:34.240 --> 00:10:37.240]   that are used for this task.
[00:10:37.240 --> 00:10:40.720]   So disentanglement task is best done
[00:10:40.720 --> 00:10:43.400]   when you have a metric that you can
[00:10:43.400 --> 00:10:45.240]   use to assess the performance.
[00:10:45.240 --> 00:10:47.600]   And so a big advancement was made recently
[00:10:47.600 --> 00:10:50.520]   in this with the disentanglement library.
[00:10:50.520 --> 00:10:53.280]   And so this library, they collected
[00:10:53.280 --> 00:10:56.920]   a bunch of different generative graphics engines.
[00:10:56.920 --> 00:11:00.720]   And so that means we know the exact parameters
[00:11:00.720 --> 00:11:02.760]   used to generate images.
[00:11:02.760 --> 00:11:08.320]   And it allows us to be able to verify the ground truth
[00:11:08.320 --> 00:11:12.400]   generating sources with what comes out of the model.
[00:11:12.400 --> 00:11:16.120]   So as I alluded to before, it's been known for like 20 years
[00:11:16.120 --> 00:11:18.080]   or so that it's not possible to perform
[00:11:18.080 --> 00:11:20.320]   identifiable disentanglement when you're only getting
[00:11:20.320 --> 00:11:22.440]   independent image samples.
[00:11:22.440 --> 00:11:25.240]   And so what we have done is achieved
[00:11:25.240 --> 00:11:26.640]   this identifiable disentanglement
[00:11:26.640 --> 00:11:30.240]   by extending these data sets into the time domain
[00:11:30.240 --> 00:11:34.080]   and then using time information to constrain our model using
[00:11:34.080 --> 00:11:38.680]   the statistics that we found to constrain our model.
[00:11:38.680 --> 00:11:40.360]   So when extending it to the time domain,
[00:11:40.360 --> 00:11:43.040]   we asked the question of what time statistics
[00:11:43.040 --> 00:11:43.800]   do we want to use?
[00:11:43.800 --> 00:11:46.400]   And we decided to try to use real data.
[00:11:46.400 --> 00:11:49.440]   So the first data set that we use is YouTube.
[00:11:49.440 --> 00:11:52.160]   So we pulled videos straight from YouTube.
[00:11:52.160 --> 00:11:57.080]   And then this other group uses a pre-training image segmentation
[00:11:57.080 --> 00:12:00.440]   algorithm to extract these binary masks.
[00:12:00.440 --> 00:12:04.320]   And so we take the masks and we record the measure, the scale,
[00:12:04.320 --> 00:12:06.560]   and position data of the masks.
[00:12:06.560 --> 00:12:08.680]   And then we construct our own sprite data
[00:12:08.680 --> 00:12:11.160]   set using the measurements that we have.
[00:12:11.160 --> 00:12:13.560]   And we call that natural sprites.
[00:12:13.560 --> 00:12:16.520]   So natural sprites are images generating using the Sprite
[00:12:16.520 --> 00:12:18.240]   World graphics engine.
[00:12:18.240 --> 00:12:20.040]   So they're simple, well-controlled objects.
[00:12:20.040 --> 00:12:22.800]   We know all the parameters, like the shape and orientation
[00:12:22.800 --> 00:12:24.120]   and everything.
[00:12:24.120 --> 00:12:27.560]   But we've added time component to the--
[00:12:27.560 --> 00:12:28.560]   made them videos.
[00:12:28.560 --> 00:12:30.720]   And these statistics of this time
[00:12:30.720 --> 00:12:34.200]   match exactly the statistics from these YouTube videos.
[00:12:34.200 --> 00:12:35.840]   So this is a step, the first step,
[00:12:35.840 --> 00:12:40.960]   towards running this problem on realistic videos.
[00:12:40.960 --> 00:12:45.800]   As a next step, we used the kitty data set.
[00:12:45.800 --> 00:12:48.240]   So this is masks extracted directly
[00:12:48.240 --> 00:12:51.120]   from self-driving car cameras.
[00:12:51.120 --> 00:12:54.600]   So we have these data set of videos from self-driving cars.
[00:12:54.600 --> 00:12:56.920]   And the cars also have a LIDAR sensor.
[00:12:56.920 --> 00:13:00.160]   It's a laser depth sensor, which makes it really good
[00:13:00.160 --> 00:13:02.160]   at detecting objects in the world.
[00:13:02.160 --> 00:13:03.960]   If a person's in front of a car, they're
[00:13:03.960 --> 00:13:04.840]   going to be at a certain depth.
[00:13:04.840 --> 00:13:07.160]   And everything else is going to be at a different depth.
[00:13:07.160 --> 00:13:09.480]   So it's easy to get a very fine-grained mask.
[00:13:09.480 --> 00:13:12.680]   And so we, again, another group uses information
[00:13:12.680 --> 00:13:13.800]   to extract these masks.
[00:13:13.800 --> 00:13:17.360]   We pulled out the human category and converted it
[00:13:17.360 --> 00:13:18.560]   into this data set.
[00:13:18.560 --> 00:13:21.320]   And again, we measure the position and area
[00:13:21.320 --> 00:13:23.640]   of these ground truth vectors.
[00:13:23.640 --> 00:13:28.560]   So this is, again, a further step closer towards real video.
[00:13:28.560 --> 00:13:30.960]   The downside of this compared to natural sprites
[00:13:30.960 --> 00:13:33.960]   is that with natural sprites, we have all of the parameters
[00:13:33.960 --> 00:13:35.280]   for generating the video.
[00:13:35.280 --> 00:13:39.560]   We know exactly what parameters went into the generating model.
[00:13:39.560 --> 00:13:44.200]   Whereas here, we only have the position and area.
[00:13:44.200 --> 00:13:48.240]   We're just limited by what we can measure.
[00:13:48.240 --> 00:13:50.400]   So now that we have these data sets,
[00:13:50.400 --> 00:13:53.560]   we wanted to build a model that exploits
[00:13:53.560 --> 00:13:56.640]   the statistics of this data or is
[00:13:56.640 --> 00:13:58.480]   constrained by the statistics of this data.
[00:13:58.480 --> 00:14:01.320]   So we measured those statistics.
[00:14:01.320 --> 00:14:03.600]   So these are looking at the time-varying statistics.
[00:14:03.600 --> 00:14:07.160]   This is looking at transitions of these measurements
[00:14:07.160 --> 00:14:08.640]   from one frame to the next.
[00:14:08.640 --> 00:14:11.000]   And we found that the distributions are all sparse.
[00:14:11.000 --> 00:14:13.840]   So what that means is that they're peaked at zero.
[00:14:13.840 --> 00:14:16.080]   And they have very heavy tails.
[00:14:16.080 --> 00:14:17.440]   So that means sharp changes could
[00:14:17.440 --> 00:14:19.160]   occur in some latent sources.
[00:14:19.160 --> 00:14:21.360]   But most of the other sources would
[00:14:21.360 --> 00:14:24.680]   remain unchanged between adjacent time points.
[00:14:24.680 --> 00:14:26.080]   So now when I'm saying sources, I
[00:14:26.080 --> 00:14:31.040]   mean properties like position, identity, and size.
[00:14:31.040 --> 00:14:33.500]   So as an example, if you imagine you're sitting in a traffic
[00:14:33.500 --> 00:14:36.040]   lane, someone's going across the crosswalk,
[00:14:36.040 --> 00:14:38.840]   that person's x position might change quite a bit in time.
[00:14:38.840 --> 00:14:42.360]   But their size and identity and shape are relatively constant.
[00:14:43.360 --> 00:14:48.120]   So using this idea, we decided to constrain our model
[00:14:48.120 --> 00:14:54.320]   by imposing a prior that adhered to this statistic that we see.
[00:14:54.320 --> 00:14:58.600]   So again, we have an autoencoder model.
[00:14:58.600 --> 00:15:01.080]   We get images x.
[00:15:01.080 --> 00:15:02.400]   I'm showing my pointer here.
[00:15:02.400 --> 00:15:03.860]   And then it goes through an encoder
[00:15:03.860 --> 00:15:05.160]   to get our latent code z.
[00:15:05.160 --> 00:15:08.360]   And what we do is we establish a prior on our model
[00:15:08.360 --> 00:15:12.920]   to impose this regularity that we saw in the natural data.
[00:15:12.920 --> 00:15:16.160]   And so this is represented here by this heat map.
[00:15:16.160 --> 00:15:19.080]   So the darker red indicates a higher probability.
[00:15:19.080 --> 00:15:20.920]   So for the first time step, we don't really
[00:15:20.920 --> 00:15:21.880]   know anything about it.
[00:15:21.880 --> 00:15:25.160]   So we have a very general prior that we place in the model.
[00:15:25.160 --> 00:15:26.920]   That's signified here.
[00:15:26.920 --> 00:15:30.080]   And the next time step, this is the encoding
[00:15:30.080 --> 00:15:31.200]   at that first time step.
[00:15:31.200 --> 00:15:33.000]   So now we actually have seen the object.
[00:15:33.000 --> 00:15:35.240]   And we know where it is, which we mark by the x.
[00:15:35.240 --> 00:15:38.960]   And we have some probability distribution around it.
[00:15:38.960 --> 00:15:42.640]   Given that information, we can enforce another prior
[00:15:42.640 --> 00:15:46.000]   of the time step t given time step t minus 1.
[00:15:46.000 --> 00:15:48.160]   And this is the one that we asked to be sparse.
[00:15:48.160 --> 00:15:50.560]   So we again say that we only want a few of the latents
[00:15:50.560 --> 00:15:51.320]   to change.
[00:15:51.320 --> 00:15:54.240]   We only expect a few of the latents to change.
[00:15:54.240 --> 00:15:56.040]   And now when we encode the next time step,
[00:15:56.040 --> 00:15:58.480]   because we have imposed this prior,
[00:15:58.480 --> 00:16:01.000]   we have a high likelihood of doing a good job
[00:16:01.000 --> 00:16:03.480]   at encoding the next time step.
[00:16:03.480 --> 00:16:05.320]   And I'm not going to go into it very much.
[00:16:05.320 --> 00:16:07.760]   But it turns out that having this sparse prior,
[00:16:07.760 --> 00:16:10.600]   because it is shaped the way it is,
[00:16:10.600 --> 00:16:13.040]   it allows us to prove identifiability
[00:16:13.040 --> 00:16:14.240]   with a very simple proof.
[00:16:14.240 --> 00:16:21.000]   So we applied this to both the data sets
[00:16:21.000 --> 00:16:24.280]   in the disentanglement loop, as well as our new data sets.
[00:16:24.280 --> 00:16:26.000]   And we found that we did a lot better
[00:16:26.000 --> 00:16:27.360]   than previous approaches.
[00:16:27.360 --> 00:16:28.440]   This is a really big plot.
[00:16:28.440 --> 00:16:29.400]   There's a lot going on.
[00:16:29.400 --> 00:16:32.960]   I'm just going to highlight the important points, most notably
[00:16:32.960 --> 00:16:36.240]   the red circle around our model, which does the best.
[00:16:36.240 --> 00:16:38.960]   And the way you can interpret these green boxes here
[00:16:38.960 --> 00:16:40.600]   is this is looking at the correlation
[00:16:40.600 --> 00:16:44.560]   between the latent components in z and the ground truth factors.
[00:16:44.560 --> 00:16:47.160]   So each column is a different ground truth factor.
[00:16:47.160 --> 00:16:49.080]   So ideally, if it was a perfect solution,
[00:16:49.080 --> 00:16:53.960]   you would just see a dark green row diagonal of 100,
[00:16:53.960 --> 00:16:56.320]   indicating that every ground truth factor is perfectly
[00:16:56.320 --> 00:16:59.440]   correlated with one of the latents.
[00:16:59.440 --> 00:17:01.320]   And another way to look at it is to just look
[00:17:01.320 --> 00:17:02.560]   at the values themselves.
[00:17:02.560 --> 00:17:04.000]   So here, what we're doing is we're
[00:17:04.000 --> 00:17:06.160]   changing those latent values.
[00:17:06.160 --> 00:17:07.840]   And we're looking at how z--
[00:17:07.840 --> 00:17:09.800]   sorry, we're changing the ground truth values.
[00:17:09.800 --> 00:17:12.200]   And we're looking at how z changes with respect to that.
[00:17:12.200 --> 00:17:15.360]   And so again, a perfect solution would be a diagonal line.
[00:17:15.360 --> 00:17:17.880]   Or in the case of shape, where there's discrete categories,
[00:17:17.880 --> 00:17:20.560]   we would just have separated points.
[00:17:20.560 --> 00:17:23.720]   And you can see that a solution like this, for example,
[00:17:23.720 --> 00:17:26.880]   it'd be very difficult to decode what the x position was,
[00:17:26.880 --> 00:17:28.400]   given a value of z.
[00:17:29.040 --> 00:17:32.520]   So the summary of the advantages to our approach,
[00:17:32.520 --> 00:17:33.720]   it's parsimonious.
[00:17:33.720 --> 00:17:36.440]   So our mathematical setup is simple.
[00:17:36.440 --> 00:17:38.560]   It can be applied to a lot of different data types,
[00:17:38.560 --> 00:17:42.120]   including videos with natural transitions.
[00:17:42.120 --> 00:17:43.160]   It's identifiable.
[00:17:43.160 --> 00:17:45.800]   So we prove that our model is more identifiable
[00:17:45.800 --> 00:17:47.520]   than previous approaches.
[00:17:47.520 --> 00:17:50.880]   By more, I mean we have fewer constraints.
[00:17:50.880 --> 00:17:53.880]   It's applicable to a broader set of data,
[00:17:53.880 --> 00:17:55.840]   broader set of data types.
[00:17:55.840 --> 00:17:59.120]   It's identifiable to a broader set of data,
[00:17:59.120 --> 00:18:02.320]   broader amount of data sets.
[00:18:02.320 --> 00:18:06.520]   And it's identifiable up to permutations,
[00:18:06.520 --> 00:18:09.200]   as opposed to up to something like a linear transformation,
[00:18:09.200 --> 00:18:11.720]   which previous approaches were.
[00:18:11.720 --> 00:18:13.280]   And then we also just empirically
[00:18:13.280 --> 00:18:15.720]   show improved disentanglement on these more constrained data
[00:18:15.720 --> 00:18:16.800]   sets.
[00:18:16.800 --> 00:18:20.800]   Ultimately, our goal is to move towards natural data.
[00:18:20.800 --> 00:18:23.720]   But we believe that we've offered these data sets
[00:18:23.720 --> 00:18:27.280]   to push the field in that direction.
[00:18:27.280 --> 00:18:28.720]   Yeah, and that's what I got.
[00:18:28.720 --> 00:18:32.840]   So this is the links that you can go to to check out more.
[00:18:32.840 --> 00:18:34.000]   Yeah, thanks a lot.
[00:18:34.000 --> 00:18:37.760]   Open to questions.
[00:18:37.760 --> 00:18:39.520]   Great, so if folks have questions,
[00:18:39.520 --> 00:18:42.160]   you can put them in the Zoom chat or the YouTube chat,
[00:18:42.160 --> 00:18:45.480]   and we'll pose them to Dylan.
[00:18:45.480 --> 00:18:48.760]   Also, Andrew, if you have questions,
[00:18:48.760 --> 00:18:51.560]   it'd be great to hear any questions that you have.
[00:18:51.560 --> 00:18:53.640]   I'll kick us off with a couple of questions.
[00:18:53.640 --> 00:18:56.920]   So the first question is, what do you
[00:18:56.920 --> 00:19:00.000]   think is needed to take these nonlinear disentanglement
[00:19:00.000 --> 00:19:02.200]   approaches and be able to apply them
[00:19:02.200 --> 00:19:05.600]   to full high-definition natural video?
[00:19:05.600 --> 00:19:07.600]   Is it just calculation?
[00:19:07.600 --> 00:19:10.120]   Is it just computation, rather?
[00:19:10.120 --> 00:19:15.120]   Or are there big algorithmic advances that need to be made?
[00:19:15.120 --> 00:19:16.520]   Yeah, that's a tough question.
[00:19:16.520 --> 00:19:19.800]   So it's not really clear what--
[00:19:19.800 --> 00:19:22.120]   so autoencoders by themselves are
[00:19:22.120 --> 00:19:26.720]   capable of encoding and decoding high-resolution images.
[00:19:26.720 --> 00:19:29.760]   When you stick them in this variational framework,
[00:19:29.760 --> 00:19:32.920]   so this probabilistic framework, where you're not just
[00:19:32.920 --> 00:19:36.080]   asking it to encode an image and then decode that image,
[00:19:36.080 --> 00:19:37.840]   but you're also asking for the encoding
[00:19:37.840 --> 00:19:39.640]   to have these certain properties,
[00:19:39.640 --> 00:19:43.240]   it seems like the scalability kind of falls off.
[00:19:43.240 --> 00:19:47.120]   And so this might come down to just the network's ability
[00:19:47.120 --> 00:19:50.600]   to approximate this probabilistic distribution,
[00:19:50.600 --> 00:19:54.160]   which is what it's trying to do with variational inference.
[00:19:54.160 --> 00:19:56.880]   And so one thing that we're working on now
[00:19:56.880 --> 00:20:00.640]   with continuing this is to modify the architecture
[00:20:00.640 --> 00:20:03.280]   to an architecture that can handle--
[00:20:03.280 --> 00:20:05.920]   like a recurrent architecture that can handle more complicated
[00:20:05.920 --> 00:20:11.120]   computation without having to have an extremely deep network.
[00:20:11.120 --> 00:20:15.720]   And so we're hoping that we can go farther with fewer layers
[00:20:15.720 --> 00:20:17.240]   to try to scale up the model.
[00:20:18.240 --> 00:20:19.240]   Interesting.
[00:20:19.240 --> 00:20:22.240]   So Joshua Clancy from YouTube asks,
[00:20:22.240 --> 00:20:24.120]   how did you implement those priors?
[00:20:24.120 --> 00:20:28.320]   So how are those priors enforced on the autoencoder?
[00:20:28.320 --> 00:20:28.920]   Sure, yeah.
[00:20:28.920 --> 00:20:31.160]   So it ends up just being a training loss,
[00:20:31.160 --> 00:20:34.160]   but it comes from a probabilistic description.
[00:20:34.160 --> 00:20:36.480]   So the variational autoencoder basically
[00:20:36.480 --> 00:20:42.520]   specifies that the encoder itself solves a lower bound
[00:20:42.520 --> 00:20:49.520]   on likelihood of the representation given the images.
[00:20:49.520 --> 00:20:52.680]   And so we end up with a loss function
[00:20:52.680 --> 00:20:56.880]   that is a term that asks that information is preserved
[00:20:56.880 --> 00:20:57.880]   as much as possible.
[00:20:57.880 --> 00:20:59.760]   So that's basically looking at the difference
[00:20:59.760 --> 00:21:02.200]   between x star and x.
[00:21:02.200 --> 00:21:05.720]   And then a second term that says that we want the latent space
[00:21:05.720 --> 00:21:09.680]   to look as much as possible, like draws random numbers
[00:21:09.680 --> 00:21:12.960]   from these two probability distributions.
[00:21:12.960 --> 00:21:14.160]   Where-- let's see.
[00:21:14.160 --> 00:21:15.040]   Go back to the slide.
[00:21:15.040 --> 00:21:17.280]   So the one probability distribution
[00:21:17.280 --> 00:21:19.920]   is just a standard Gaussian prior.
[00:21:19.920 --> 00:21:22.800]   So this is the standard prior that's used in all VAEs.
[00:21:22.800 --> 00:21:25.200]   And then we added a second probability distribution.
[00:21:25.200 --> 00:21:28.640]   Given the first time point, we encode the first image.
[00:21:28.640 --> 00:21:32.880]   We use that image to condition our second prior.
[00:21:32.880 --> 00:21:34.960]   And then when we feed in the second image,
[00:21:34.960 --> 00:21:36.480]   we have a loss term.
[00:21:36.480 --> 00:21:38.440]   And then when we feed in the second image,
[00:21:38.440 --> 00:21:42.160]   we have a loss term for the second image
[00:21:42.160 --> 00:21:43.280]   using the second prior.
[00:21:43.280 --> 00:21:45.680]   So it ends up just being the same KL divergence--
[00:21:45.680 --> 00:21:47.760]   or similar KL divergence term that you get
[00:21:47.760 --> 00:21:50.720]   in the standard VAE framework.
[00:21:50.720 --> 00:21:51.280]   Interesting.
[00:21:51.280 --> 00:21:52.680]   While we're on this slide, I think
[00:21:52.680 --> 00:21:54.680]   I just wanted to sort of check my understanding
[00:21:54.680 --> 00:21:56.120]   of your identifiability proof.
[00:21:56.120 --> 00:21:58.440]   The essence of it is basically that if you
[00:21:58.440 --> 00:22:00.840]   look at that prior t given t minus 1,
[00:22:00.840 --> 00:22:02.800]   you can see that it's axis aligned, right?
[00:22:02.800 --> 00:22:08.240]   That you see those two sharp lines oriented up and--
[00:22:08.240 --> 00:22:09.760]   horizontally and vertically.
[00:22:09.760 --> 00:22:11.800]   And that's what gives you identifiability,
[00:22:11.800 --> 00:22:13.840]   that the axes are really important, as opposed
[00:22:13.840 --> 00:22:17.680]   to that Gaussian prior where it's isotropic.
[00:22:17.680 --> 00:22:20.440]   And so there's no special directions of any kind.
[00:22:20.440 --> 00:22:21.160]   Yeah, exactly.
[00:22:21.160 --> 00:22:24.000]   So the prior that you're looking at here for t minus 1,
[00:22:24.000 --> 00:22:25.840]   that's just a standard Gaussian prior,
[00:22:25.840 --> 00:22:29.200]   which is used for beta VAEs and other VAE approaches
[00:22:29.200 --> 00:22:30.080]   to disentanglement.
[00:22:30.080 --> 00:22:31.600]   And the problem with this prior is
[00:22:31.600 --> 00:22:33.560]   it's rotationally symmetric.
[00:22:33.560 --> 00:22:35.560]   So what that means is in my latent space,
[00:22:35.560 --> 00:22:39.560]   I can multiply my z variable by a rotation matrix that
[00:22:39.560 --> 00:22:41.680]   can rotate at any arbitrary amount.
[00:22:41.680 --> 00:22:46.680]   And it would still be equally valid under our loss function.
[00:22:46.680 --> 00:22:49.320]   And so there's no way of knowing, given the loss,
[00:22:49.320 --> 00:22:51.640]   whether we actually found the right solution or not.
[00:22:51.640 --> 00:22:53.800]   Because the ground truth has one solution.
[00:22:53.800 --> 00:22:56.160]   And our latent has infinite solutions,
[00:22:56.160 --> 00:22:57.600]   depending on what rotation you do.
[00:22:57.600 --> 00:23:00.640]   And that's actually the core argument for ICA
[00:23:00.640 --> 00:23:03.560]   not being identifiable.
[00:23:03.560 --> 00:23:05.280]   And so yeah, the shape of our prior,
[00:23:05.280 --> 00:23:07.160]   the fact that we chose a sparse prior,
[00:23:07.160 --> 00:23:10.720]   it has these axis-aligned probability densities,
[00:23:10.720 --> 00:23:11.520]   marginals.
[00:23:11.520 --> 00:23:15.120]   That means that when you rotate it, it's not the same anymore.
[00:23:15.120 --> 00:23:18.440]   And so if it minimizes the loss, if it finds a solution,
[00:23:18.440 --> 00:23:20.280]   it is the right solution.
[00:23:20.280 --> 00:23:23.000]   And I should say that with this kind of stuff,
[00:23:23.000 --> 00:23:27.360]   identifiability is proven in this mathematical sense.
[00:23:27.360 --> 00:23:29.820]   And of course, we have to relax a lot of those constraints
[00:23:29.820 --> 00:23:32.000]   when we apply the model in practice.
[00:23:32.000 --> 00:23:34.560]   And so it was important for us to do a lot of empirical tests
[00:23:34.560 --> 00:23:35.560]   at the same time.
[00:23:35.560 --> 00:23:37.740]   We show identifiability, but we also
[00:23:37.740 --> 00:23:41.680]   want to make sure that when we relax the constraints that we
[00:23:41.680 --> 00:23:45.160]   needed to impose to show identifiability to actually run
[00:23:45.160 --> 00:23:48.160]   it on real data, we still see a good job at disentanglement.
[00:23:48.160 --> 00:23:51.560]   So when you look at the results, no, it's not perfect.
[00:23:51.560 --> 00:23:53.920]   But it's doing a lot better than everyone else.
[00:23:53.920 --> 00:23:57.640]   And in theory, given the right conditions,
[00:23:57.640 --> 00:23:58.920]   it should do perfectly.
[00:23:58.920 --> 00:24:02.360]   And we do test it on toy data, like very toy data,
[00:24:02.360 --> 00:24:04.360]   where it exactly matches all the conditions,
[00:24:04.360 --> 00:24:05.960]   and then it perfectly disentangles it.
[00:24:05.960 --> 00:24:11.760]   Cool.
[00:24:11.760 --> 00:24:13.000]   Yeah, this is interesting work.
[00:24:13.000 --> 00:24:15.800]   And I'm looking forward to see as you continue closer
[00:24:15.800 --> 00:24:18.160]   towards nonlinear disentanglement
[00:24:18.160 --> 00:24:20.120]   in natural data.
[00:24:20.120 --> 00:24:22.920]   And hopefully, folks take up these data sets
[00:24:22.920 --> 00:24:26.280]   that you put out there, that kitty mask data set,
[00:24:26.280 --> 00:24:29.760]   and improve on this, and make some steps
[00:24:29.760 --> 00:24:32.920]   towards better disentangling natural videos.
[00:24:32.920 --> 00:24:34.920]   Yeah, so a lot of the--
[00:24:34.920 --> 00:24:37.880]   Yash, and Lucas, and David, they did a lot of work
[00:24:37.880 --> 00:24:39.800]   on the GitHub repository.
[00:24:39.800 --> 00:24:42.240]   So if you're interested in trying this out,
[00:24:42.240 --> 00:24:45.360]   that same repo is where you go to get the data,
[00:24:45.360 --> 00:24:48.360]   and also where you go to test out our model.
[00:24:48.360 --> 00:24:50.800]   And it integrates pretty well with the disentanglement
[00:24:50.800 --> 00:24:53.280]   library already and the metrics that they use.
[00:24:53.280 --> 00:25:01.820]   [BLANK_AUDIO]
[00:25:01.820 --> 00:25:11.500]   [BLANK_AUDIO]

