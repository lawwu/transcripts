
[00:00:00.000 --> 00:00:05.000]   But I would not underestimate the difficulty of alignment of models that are actually
[00:00:05.000 --> 00:00:11.460]   Smarter than us of models that are capable of misrepresenting their intentions. Are you worried about spies?
[00:00:11.460 --> 00:00:13.800]   I'm really not worried about the way it's being leaked
[00:00:13.800 --> 00:00:18.840]   We will be able to become more enlightened because we'd interact with an AGI that will help us
[00:00:18.840 --> 00:00:24.380]   See the world more correctly like imagine talking to the best meditation teacher in history
[00:00:24.920 --> 00:00:30.880]   Microsoft has been a very very good partner for us. So I challenge the claim that next token prediction cannot surpass
[00:00:30.880 --> 00:00:38.580]   Human performance if your base neural net is smart enough. You just ask it like what would a person is like great
[00:00:38.580 --> 00:00:40.880]   insight and wisdom and capability do
[00:00:40.880 --> 00:00:45.200]   Okay today I have the pleasure of interviewing
[00:00:45.200 --> 00:00:52.760]   Ilya Sutskover who is the co-founder and chief scientist of open AI. Ilya, welcome to the Lunar Society. Thank you
[00:00:52.760 --> 00:00:53.960]   Happy to be here
[00:00:53.960 --> 00:00:55.960]   First question and no humility allowed
[00:00:55.960 --> 00:01:01.920]   There's many scientists or maybe not that many scientists who will make a big breakthrough in their field
[00:01:01.920 --> 00:01:06.180]   There's far fewer scientists who will make multiple independent breakthroughs that define their field
[00:01:06.180 --> 00:01:11.080]   Throughout their career. What is the difference? Well, like what distinguishes you from other researchers?
[00:01:11.080 --> 00:01:14.600]   Why have you been able to make multiple breakthroughs in your field? Well, thank you for the kind words
[00:01:14.600 --> 00:01:18.360]   It's hard to answer that question, I mean
[00:01:18.360 --> 00:01:22.140]   I tried really hard. I gave it everything you got and
[00:01:23.600 --> 00:01:25.600]   that
[00:01:25.600 --> 00:01:27.640]   Worked so far I
[00:01:27.640 --> 00:01:29.720]   Think that's all there is to it
[00:01:29.720 --> 00:01:31.360]   Got it
[00:01:31.360 --> 00:01:35.040]   What's the explanations for why there aren't more illicit uses of GPT?
[00:01:35.040 --> 00:01:41.320]   Why aren't more foreign governments using it to spread propaganda or scam grandmothers or something? I mean
[00:01:41.320 --> 00:01:44.520]   Maybe they haven't
[00:01:44.520 --> 00:01:49.600]   Really gotten to do it a lot, but it also wouldn't surprise me if some of it was going on right now
[00:01:50.440 --> 00:01:54.680]   Certainly, I imagine they'd be taking some of the open-source models and try and use them for that purpose
[00:01:54.680 --> 00:01:59.100]   Like I sure I would expect this would be
[00:01:59.100 --> 00:02:03.000]   Something they'd be interested in in the future. It's like technically possible
[00:02:03.000 --> 00:02:07.320]   They just haven't thought about it enough or haven't like done it at scale using their technology
[00:02:07.320 --> 00:02:10.840]   Or maybe it's happening, which is annoying. Would you be able to track it if it was happening?
[00:02:10.840 --> 00:02:17.720]   I think large-scale tracking is possible. Yes. I mean this requires of all special operations. It's possible. Mm-hmm
[00:02:18.280 --> 00:02:23.920]   Now there's some window in which AI is very economically valuable on the scale of airplanes
[00:02:23.920 --> 00:02:29.040]   Let's say well, we haven't reached a GI yet. How big is that window? I mean, I think this wind
[00:02:29.040 --> 00:02:33.560]   It's hard to give you a precise sense answer, but it's definitely going to be like it good multi-year window
[00:02:33.560 --> 00:02:36.120]   it's also a question of definition because
[00:02:36.120 --> 00:02:39.800]   AI before it becomes a GI
[00:02:39.800 --> 00:02:44.760]   Is going to be increasingly more valuable year after year
[00:02:45.800 --> 00:02:47.480]   I'd say in an exponential way
[00:02:47.480 --> 00:02:51.600]   So it's something in some sense it may feel like especially in hindsight
[00:02:51.600 --> 00:02:56.800]   It may feel like there was only one year or two years because those two years were larger than the previous years
[00:02:56.800 --> 00:02:59.040]   But I would say that already
[00:02:59.040 --> 00:03:02.760]   Last year there's been a fair
[00:03:02.760 --> 00:03:05.040]   amount of
[00:03:05.040 --> 00:03:09.760]   Economic value produced by AI next year is going to be larger and larger after that. So I
[00:03:10.480 --> 00:03:15.720]   Think like this is going to be a good multi-multi-year chunk of time, but that's going to be true
[00:03:15.720 --> 00:03:22.400]   I would say from now until AGI pretty much. Okay. Well, because I'm curious if there's a startup that's using your models, right?
[00:03:22.400 --> 00:03:26.200]   At some point if you have AGI, there's only one business in the world, right?
[00:03:26.200 --> 00:03:28.560]   It's open AI. How much window do they have?
[00:03:28.560 --> 00:03:34.400]   Does any business have where they're actually producing something that AGI can't produce? Yeah. Well, I mean, it's the same
[00:03:34.400 --> 00:03:39.200]   It's the same question is asking how long until AGI? Yeah, I think it's a hard question to answer
[00:03:39.200 --> 00:03:44.700]   I mean, I hesitate to give you a number also because there is this thing where effect where people who are
[00:03:44.700 --> 00:03:51.080]   Optimistic people who are working on the technology tend to underestimate the time it takes to get there
[00:03:51.080 --> 00:03:56.240]   But I think that the way I ground myself is by thinking about a self-driving car in particular
[00:03:56.240 --> 00:03:58.640]   there is an analogy where if you look at the
[00:03:58.640 --> 00:04:04.600]   So I have a Tesla and if you look at the self-driving behavior of it, it like it looks like it does everything
[00:04:06.040 --> 00:04:11.720]   It does everything. All right, but it's also clear that there is still a long way to go in terms of reliability and
[00:04:11.720 --> 00:04:17.720]   we might be in a similar place with respect to our models where it also looks like we can do everything and
[00:04:17.720 --> 00:04:20.160]   at the same time it will be
[00:04:20.160 --> 00:04:23.160]   We'll need to do some more work until we really
[00:04:23.160 --> 00:04:29.640]   Iron out all the issues and make it really good and really reliable and robust and well-behaved by 2030
[00:04:29.640 --> 00:04:35.320]   What percent of GDP is AI? Oh gosh hard to answer that question very hard to answer. Give me an over/under
[00:04:36.280 --> 00:04:41.400]   Like the problem is that my error bars are in log scale so I could imagine like I could imagine like a huge percentage
[00:04:41.400 --> 00:04:46.920]   I could imagine it like disappointing a small percentage at the same time. Okay, so let's take the counterfactual where it is a small percentage
[00:04:46.920 --> 00:04:49.400]   Let's say it's 2030 and you know
[00:04:49.400 --> 00:04:53.120]   Not that much economic value has been created by these other ones as unlikely as you think this might be
[00:04:53.120 --> 00:04:57.360]   What is what would you be your best explanation right now? Why something like this might happen?
[00:04:57.360 --> 00:05:03.760]   My best explanation. So I really don't think that's a likely possibility. Yeah
[00:05:04.400 --> 00:05:10.280]   So that's that's the preface to that to the comment, but if I were to take the premise of your question
[00:05:10.280 --> 00:05:15.280]   Well, like why were things disappointing in terms of the real-world impact and my answer would be
[00:05:15.280 --> 00:05:20.040]   Reliability if somehow it ends up being the case that
[00:05:20.040 --> 00:05:27.560]   You really want them to be reliable and they ended up not being reliable or if reliability did not to be harder
[00:05:27.560 --> 00:05:31.200]   Than we expect I really don't think that will be the case
[00:05:31.200 --> 00:05:35.440]   But if I had to pick one if I had to pick one and you tell me like hey
[00:05:35.440 --> 00:05:36.800]   Like why didn't things work out?
[00:05:36.800 --> 00:05:42.080]   It would be reliability that you still have to look over the answers and double-check everything
[00:05:42.080 --> 00:05:48.800]   That's just really puts a damper on the economic value that can be produced by those systems. They'll be technologically mature
[00:05:48.800 --> 00:05:54.600]   It's just a question of whether they'll be reliable enough. Yeah. Well in some sense not reliable means not technologically mature
[00:05:54.600 --> 00:05:56.600]   You see what I mean? Yeah fair enough
[00:05:57.000 --> 00:06:02.760]   What's after generative models, right? So before you're working on reinforcement learning, is this is this basically it?
[00:06:02.760 --> 00:06:05.320]   Is this a paradigm that gets us to AGI or is there something after this?
[00:06:05.320 --> 00:06:09.160]   I mean, I think this paradigm is gonna go really really far and I would not underestimate it
[00:06:09.160 --> 00:06:16.160]   I think it's quite likely that this exact paradigm is not going to be the quiet AGI form factor
[00:06:16.160 --> 00:06:20.000]   I mean, I hesitate to say precisely what the next paradigm will be
[00:06:21.280 --> 00:06:26.920]   But I think it will probably involve integration of all the different ideas that came that came in the past
[00:06:26.920 --> 00:06:29.640]   Is there some specific one you're referring to or I?
[00:06:29.640 --> 00:06:38.280]   Mean it's hard to be specific. So you could argue that the next token prediction can only help us match human performance
[00:06:38.280 --> 00:06:42.300]   And maybe not surpass it. What would it take to surpass human performance?
[00:06:42.300 --> 00:06:47.600]   So I challenge the claim that next token prediction cannot surpass human performance
[00:06:48.920 --> 00:06:54.960]   It looks like on the surface it cannot. It looks on the surface if you just learn to imitate
[00:06:54.960 --> 00:06:59.800]   To predict what people do it means that you can only copy people
[00:06:59.800 --> 00:07:05.760]   but the here is a counter argument for why it might not be quite so if your neural net is
[00:07:05.760 --> 00:07:11.760]   If your base neural net is smart enough, you just ask it like what would it what would a person with
[00:07:11.760 --> 00:07:14.760]   great insight and wisdom and capability do
[00:07:15.560 --> 00:07:20.040]   Maybe such a person doesn't exist. But there's a pretty good chance that the neural net will be able to
[00:07:20.040 --> 00:07:24.840]   Extrapolate how such a person would behave. Do you see what I mean?
[00:07:24.840 --> 00:07:25.540]   Yes
[00:07:25.540 --> 00:07:29.240]   although where would it get that sort of insight about what that's person would do if not from
[00:07:29.240 --> 00:07:35.680]   From the data of regular people because like if you think about it, what does it mean to predict the next token well enough?
[00:07:35.680 --> 00:07:39.840]   What does it mean? Actually, it's actually it's a much it's a deeper question than it seems
[00:07:39.840 --> 00:07:42.760]   predicting the next token well means that you
[00:07:43.640 --> 00:07:45.640]   understand
[00:07:45.640 --> 00:07:51.120]   The underlying reality that led to the creation of that token
[00:07:51.120 --> 00:07:56.200]   It's not statistics like it is statistics, but what is statistics?
[00:07:56.200 --> 00:08:02.200]   In order to want to understand those statistics to compress them you need to understand
[00:08:02.200 --> 00:08:09.040]   What is it about the world that creates this those statistics? And so then you say, okay. Well, I have all those people
[00:08:09.040 --> 00:08:11.160]   What is it about people that creates their behaviors?
[00:08:11.160 --> 00:08:12.880]   Well, they have you know
[00:08:12.880 --> 00:08:19.860]   They have thoughts and their feelings and they have ideas and they do things in certain ways all of those would be deduced
[00:08:19.860 --> 00:08:26.480]   From next token prediction and I'd argue that this should make it possible not
[00:08:26.480 --> 00:08:30.240]   Indefinitely, but to a pretty decent degree to say well
[00:08:30.240 --> 00:08:36.640]   Can you guess what you what you do if you took a person with like this characteristic and that characteristic like such a person doesn't exist
[00:08:37.360 --> 00:08:43.280]   But because you're so good at predicting the next token, you should still be able to guess what that person would do this hypothetical
[00:08:43.280 --> 00:08:45.120]   imaginary
[00:08:45.120 --> 00:08:49.600]   Person is far greater mental ability than the rest of us
[00:08:49.600 --> 00:08:53.120]   When we're doing reinforcement learning on these models how long before?
[00:08:53.120 --> 00:08:56.800]   Most of the data for the reinforcement learning is coming from AIs and not humans. I
[00:08:56.800 --> 00:09:00.400]   Mean
[00:09:00.400 --> 00:09:04.560]   Already most of the data for reinforcement learning is coming from AIs. Yeah. Well, it's like
[00:09:05.360 --> 00:09:11.040]   The humans are being used to train the reward function, but then the but then the reward function
[00:09:11.040 --> 00:09:17.920]   Enter and in its interaction with the model is automatic and all the data that's generated in the during the process of reinforcement learning
[00:09:17.920 --> 00:09:21.760]   It's created by AI. So like if you look at the current I
[00:09:21.760 --> 00:09:28.880]   Would say technique paradigm, which is in getting some significant attention because of chat GPT
[00:09:28.880 --> 00:09:34.680]   Reinforcement learning from human feedback. There is human feedback. The human feedback is being used to train
[00:09:35.000 --> 00:09:36.840]   the reward function and
[00:09:36.840 --> 00:09:40.520]   Then the reward function is being used to create the data, which trains the model
[00:09:40.520 --> 00:09:45.160]   Got it. And is there any hope of just removing a human from the loop and have it improve itself and some sort of alpha?
[00:09:45.160 --> 00:09:49.200]   Go away. Yeah, definitely. I mean, I feel like in some sense our hopes for
[00:09:49.200 --> 00:09:52.160]   like our plan like
[00:09:52.160 --> 00:09:55.800]   Very much. So the thing you really want is for
[00:09:55.800 --> 00:10:00.320]   The human teachers that tell you that teach the AI
[00:10:00.880 --> 00:10:04.640]   For them to collaborate with an AI so you might want to think about it
[00:10:04.640 --> 00:10:07.400]   and you might want to think of it as being in a world where
[00:10:07.400 --> 00:10:11.800]   The human teachers do 1% of the world and the work and the AI do 99% of the work
[00:10:11.800 --> 00:10:13.840]   You don't want it to be 100% AI
[00:10:13.840 --> 00:10:18.440]   But you do want it to be a human machine collaboration, which teaches the next machine
[00:10:18.440 --> 00:10:21.240]   So currently, I mean, I've had a chance to play around these models
[00:10:21.240 --> 00:10:27.640]   They seem bad at multi-step reasoning and they have been getting better. But what does it take to really surpass that barrier?
[00:10:27.640 --> 00:10:34.880]   I mean, I think dedicated training will get us there more more improvements to the base models. You'll get us there. Okay, but
[00:10:34.880 --> 00:10:41.720]   Like fundamentally I also don't feel like they're that bad at multi-step reasoning
[00:10:41.720 --> 00:10:46.320]   I actually think that they are bad at mental multi-step reasoning, but they're not allowed to think out loud
[00:10:46.320 --> 00:10:51.640]   But when they are allowed to think out loud, they're quite good and I expect this to improve
[00:10:51.640 --> 00:10:53.480]   significantly
[00:10:53.480 --> 00:11:00.280]   Both with better models and with special training. Hmm. Are you running out of reasoning tokens on the Internet? Are there enough of them? I
[00:11:00.280 --> 00:11:02.800]   Mean, you know, it's okay
[00:11:02.800 --> 00:11:04.520]   So for context on this question
[00:11:04.520 --> 00:11:10.400]   Like there is there are claims that indeed at some point we'll run out of tokens in general to train those models
[00:11:10.400 --> 00:11:13.920]   And yeah, I think this will happen one day and we'll by the time that happens
[00:11:13.920 --> 00:11:21.840]   We need to have other ways of training models other ways of productively improving their capabilities and sharpening their behavior making sure they're
[00:11:22.280 --> 00:11:29.280]   Doing exactly precisely what we want without more data. Well, I mean you haven't run out of data yet. There's more
[00:11:29.280 --> 00:11:33.760]   Yeah, I would say I would say the data situation is still quite good. There are still lots to go
[00:11:33.760 --> 00:11:39.720]   But at some point, yeah at some point data will run. Okay, where what is the most valuable source of data?
[00:11:39.720 --> 00:11:44.740]   Is it Reddit, Twitter, books? What would you train many other tokens of other varieties for?
[00:11:44.740 --> 00:11:48.280]   Generally speaking you'd like tokens which are
[00:11:48.760 --> 00:11:53.400]   Speaking about smarter things tokens, which are like more interesting. Yeah
[00:11:53.400 --> 00:11:58.800]   So, I mean all this all the sources which you mentioned they're valuable. Okay, so maybe not Twitter, but
[00:11:58.800 --> 00:12:04.000]   Do we need to go multimodal to get more tokens or do we still have enough text tokens left?
[00:12:04.000 --> 00:12:09.500]   I mean, I think that you can still go very far in text only but going multimodal seems like a very fruitful direction
[00:12:09.500 --> 00:12:14.880]   Mm-hmm. If you're comfortable talking about this, like where is the place where we haven't scraped the tokens yet? Oh, I mean
[00:12:16.200 --> 00:12:21.080]   Yeah, obviously, I mean I can't answer that question for us, but I'm sure I'm sure that for everyone
[00:12:21.080 --> 00:12:22.520]   There's a different answer to that question
[00:12:22.520 --> 00:12:29.480]   How many orders of magnitude improvement can we get just not from scale or not from data, but just from algorithmic improvements?
[00:12:29.480 --> 00:12:32.480]   Hard to answer, but I'm sure there is some
[00:12:32.480 --> 00:12:37.400]   Is some a lot or is some a little? I mean, it's only one way to find out. Okay
[00:12:37.400 --> 00:12:41.240]   Let me get to your like quick-fire opinions about these different research directions
[00:12:41.760 --> 00:12:48.120]   Retrieval transformers, so just like somehow storing the data outside of the model itself and retrieving it somehow. Seems promising
[00:12:48.120 --> 00:12:52.400]   What do you see of that as a path forward or? I think it seems promising
[00:12:52.400 --> 00:12:56.960]   Robotics, was it the right step for open AI to leave that behind?
[00:12:56.960 --> 00:13:05.200]   Yeah, it was. Like back then it really wasn't possible to continue working in robotics because there was so little data like
[00:13:05.200 --> 00:13:08.880]   Back then if you wanted to do in robot, if you wanted to work in robotics
[00:13:09.120 --> 00:13:14.480]   You needed to become a robotics company. You needed to really have a giant group of people
[00:13:14.480 --> 00:13:18.600]   working on building robots and maintaining them and having
[00:13:18.600 --> 00:13:24.640]   And even then like if you only if you're gonna have a hundred robots, it's a giant operations already
[00:13:24.640 --> 00:13:26.480]   But you're not gonna get that much data
[00:13:26.480 --> 00:13:28.480]   so in a world
[00:13:28.480 --> 00:13:33.120]   Where most of the progress comes from the combination of compute and data
[00:13:33.880 --> 00:13:38.840]   Right. That's where we've been where it was the combination of compute and data that drove the progress
[00:13:38.840 --> 00:13:43.120]   There was no path to data from robotics
[00:13:43.120 --> 00:13:45.280]   so
[00:13:45.280 --> 00:13:50.040]   Back in the day when you made a decision to stop working in robotics, there was no path forward
[00:13:50.040 --> 00:13:53.360]   Is there one now?
[00:13:53.360 --> 00:13:57.680]   So I'd say that now it is possible to create a path forward
[00:13:57.960 --> 00:14:04.280]   But one needs to really commit to the task of robotics. You really need to say I'm going to build
[00:14:04.280 --> 00:14:07.280]   like many
[00:14:07.280 --> 00:14:10.200]   thousands tens of thousands hundreds of thousands of robots and
[00:14:10.200 --> 00:14:16.280]   Somehow collect data from them and find a gradual path where the robots are doing something slightly more useful
[00:14:16.280 --> 00:14:21.640]   And then the data that they get from these robots and then the data that is obtained and used to train the models
[00:14:21.640 --> 00:14:25.000]   And they do something it's slightly more useful. So you could imagine it's kind of gradual path of improvement
[00:14:25.120 --> 00:14:28.520]   Where you build more robots they do more things you collect more data and so on
[00:14:28.520 --> 00:14:33.600]   But you really need to be committed to this path. If you say I want to make robotics happen
[00:14:33.600 --> 00:14:39.320]   That's what you need to do. I believe that there are companies who are thinking about such doing exactly that
[00:14:39.320 --> 00:14:46.160]   But I think that you need to really love robots and need to be really willing to solve all the physical and logistical
[00:14:46.160 --> 00:14:49.520]   Problems of dealing with them. It's not the same as software at all
[00:14:50.120 --> 00:14:55.360]   So I think one could make progress in robotics today with enough motivation
[00:14:55.360 --> 00:15:00.360]   What ideas are you excited to try but you can't because they don't work well on current hardware. I
[00:15:00.360 --> 00:15:06.000]   Don't think current hardware is a limitation. Okay, I think it's just not the case. Got it
[00:15:06.000 --> 00:15:08.280]   So but anything you want to try you can just spin it up
[00:15:08.280 --> 00:15:16.000]   I mean, of course like this the thing you might say well, I wish current hardware was cheaper or maybe it had higher
[00:15:17.120 --> 00:15:21.120]   Like maybe maybe it would be better if it was higher memory processor bandwidth, let's say
[00:15:21.120 --> 00:15:25.280]   But
[00:15:25.280 --> 00:15:33.240]   By and large hardware is just a limitation. Let's talk about alignment. Do you think we'll ever have a mathematical definition of alignment?
[00:15:33.240 --> 00:15:36.920]   Mathematical definition I think is unlikely. Uh-huh
[00:15:36.920 --> 00:15:44.520]   Like I do I do think that we will instead have multiple like like rather than rather than achieving one mathematical definition
[00:15:44.520 --> 00:15:46.520]   I think we'll achieve multiple
[00:15:46.520 --> 00:15:49.800]   definitions that look at alignment from different aspects and
[00:15:49.800 --> 00:15:54.240]   I think that this is how we will get the assurance that we want
[00:15:54.240 --> 00:15:59.660]   And by which I mean you can look at the behavior. You can look at the behavior in various tests
[00:15:59.660 --> 00:16:06.920]   In various adversarial stress situations. You can look at how the neural net operates from the inside. I
[00:16:06.920 --> 00:16:10.560]   Think you have to look at all several of these
[00:16:11.320 --> 00:16:15.360]   Factors at the same time and how sure do you have to be before you release a model in the while?
[00:16:15.360 --> 00:16:18.360]   Is it 100% 95% depends how capable the model is?
[00:16:18.360 --> 00:16:24.040]   The more capable the model is the more the more the higher the more confident you need to be
[00:16:24.040 --> 00:16:26.860]   Okay, so just say it's something that's almost AGI. Where is AGI?
[00:16:26.860 --> 00:16:30.960]   Well depends what your AGI can do. Keep in mind that AGI is an ambiguous term also like
[00:16:30.960 --> 00:16:33.520]   like your
[00:16:33.520 --> 00:16:36.880]   Average college undergrads is an AGI, right?
[00:16:39.280 --> 00:16:45.560]   You see what I mean, there's significant ambiguity in terms of what is meant by AGI. Mm-hmm. So depending on where you put this
[00:16:45.560 --> 00:16:51.880]   Mark, you need to be more or less confident. Well, you mentioned a few of the paths towards alignment earlier
[00:16:51.880 --> 00:16:53.840]   What is the one you think is most promising at this point?
[00:16:53.840 --> 00:17:00.000]   Like I think that it will be a combination. I really think that you will not want to have just one approach
[00:17:00.000 --> 00:17:02.000]   I think people want to have a
[00:17:03.280 --> 00:17:10.320]   Combination of approaches where we you spend a lot of compute but adversarially probing to find any mismatch between the
[00:17:10.320 --> 00:17:17.400]   Behavior that you wanted to teach in the behavior that it exhibits. We look inside into the neural net using another neural
[00:17:17.400 --> 00:17:22.120]   To understand how it how it operates on the inside. I think all of them will be necessary
[00:17:22.120 --> 00:17:24.920]   every approach like this
[00:17:24.920 --> 00:17:27.280]   reduces the probability of
[00:17:27.280 --> 00:17:28.960]   misalignment and
[00:17:28.960 --> 00:17:31.160]   you also want to be in a world where you're
[00:17:32.880 --> 00:17:37.960]   Degree of alignment keeps of increasing faster than the capability of the models
[00:17:37.960 --> 00:17:41.920]   I would say that right now our understanding of our models is still quite
[00:17:41.920 --> 00:17:50.360]   rudimentary we made some progress but much more progress is possible and so I would expect that ultimately the thing that will really succeed is when
[00:17:50.360 --> 00:17:51.600]   we will have a
[00:17:51.600 --> 00:17:53.600]   Small neural net that is well understood
[00:17:53.600 --> 00:17:58.820]   That's given the task to study the behavior of a large neural net that is not understood to verify
[00:17:59.200 --> 00:18:05.560]   By what point is most of the research being done by AI? I mean, so today when you use co-pilot, right?
[00:18:05.560 --> 00:18:10.040]   What fraction how do you do the how do you divide it up?
[00:18:10.040 --> 00:18:12.660]   So I expect at some point you ask your, you know
[00:18:12.660 --> 00:18:18.280]   Descendant of Chad GPT you say hey, like I'm thinking about this and this can you suggest fruitful ideas?
[00:18:18.280 --> 00:18:21.280]   I should try and you would actually get fruitful ideas. Mm-hmm
[00:18:21.280 --> 00:18:24.440]   I don't think that will make it possible for you to solve problems you couldn't solve before
[00:18:24.680 --> 00:18:29.120]   Got it, but it's somehow just telling the humans keeping them ideas a faster or something
[00:18:29.120 --> 00:18:31.680]   It's not itself interacting with the one example
[00:18:31.680 --> 00:18:33.680]   I mean you could you could slice it in in a variety of ways
[00:18:33.680 --> 00:18:39.760]   But I think the bottleneck there is what idea is good insights and that's something which the neural nets could help us with
[00:18:39.760 --> 00:18:45.880]   Mm-hmm, if you design some like a billion dollar prize for some sort of alignment research result or product
[00:18:45.880 --> 00:18:48.720]   What is like the concrete criteria would set for that billion dollar price?
[00:18:48.720 --> 00:18:52.040]   There's something that makes sense for such a price. I it's funny that you asked this
[00:18:52.040 --> 00:18:56.560]   I was actually thinking about this exact question. I haven't I haven't come up with an exact criteria yet
[00:18:56.560 --> 00:19:01.240]   maybe something that with the benefit like maybe a prize where
[00:19:01.240 --> 00:19:04.240]   We could say that
[00:19:04.240 --> 00:19:10.280]   Two years later or three or five years later. We'll look back and say it like that was the main result
[00:19:10.280 --> 00:19:14.680]   So rather than say that there is a prize committee that decides right away
[00:19:14.680 --> 00:19:19.800]   You wait for five years and then award it retroactively, but there's no concrete thing
[00:19:19.800 --> 00:19:24.480]   We can identify yet as it like you solve this particular problem and you're you made a lot of progress. I
[00:19:24.480 --> 00:19:27.880]   think a lot of progress yet, so I wouldn't say that this would be the
[00:19:27.880 --> 00:19:34.240]   The full thing. Mm-hmm. Do you think end-to-end training is the right architecture for?
[00:19:34.240 --> 00:19:38.920]   Bigger and bigger models or do we need better ways of just connecting things together?
[00:19:38.920 --> 00:19:43.080]   I think end-to-end training is very promising. I think connecting things together is very promising
[00:19:43.680 --> 00:19:49.520]   Everything is promising. So OpenAI is projecting revenues of a billion dollars in 2024
[00:19:49.520 --> 00:19:54.160]   That might very well be correct. But I'm just curious when you're talking about a new general-purpose technology
[00:19:54.160 --> 00:19:57.280]   How do you estimate how big a windfall it'll be?
[00:19:57.280 --> 00:19:59.920]   Like why that particular number?
[00:19:59.920 --> 00:20:05.760]   I mean you look at the current you look at the you know, we've already had a so we've had a product
[00:20:05.760 --> 00:20:12.720]   For quite a while now for back from the GPT three days from two years ago through the API and we've seen how it grew
[00:20:12.720 --> 00:20:16.000]   We've seen how the response to DALY has grown as well
[00:20:16.000 --> 00:20:20.320]   And so you see how the response to chat GPT is and I think all of this gives us information
[00:20:20.320 --> 00:20:25.440]   That allows us to make a relatively sensible extrapolation to 2024
[00:20:25.440 --> 00:20:29.440]   Maybe that would be that'd be one answer like you need to have data
[00:20:29.440 --> 00:20:34.560]   you can't come up with those things out of thin air because otherwise your error bars will be like off by
[00:20:34.560 --> 00:20:38.920]   Your error bars are going to be like a hundred X in each direction
[00:20:38.920 --> 00:20:41.680]   I mean that most exponentials don't stay exponential
[00:20:41.680 --> 00:20:46.520]   Especially when they get into bigger and bigger quantities, right? So how do you determine in this case that I
[00:20:46.520 --> 00:20:49.600]   mean, like would you bet against AI?
[00:20:49.600 --> 00:20:53.560]   Not after talking with you
[00:20:53.560 --> 00:20:57.920]   Let's talk about what like a post-AGI future looks like. So are people like you, you know
[00:20:57.920 --> 00:21:00.600]   I'm guessing you're working like 80-hour weeks towards this grand goal
[00:21:00.600 --> 00:21:05.480]   That you're really obsessed with. Are you gonna be satisfied in a world where you're basically living in an AI retirement home?
[00:21:05.480 --> 00:21:10.240]   Or like what is your what are you constantly doing after AGI comes?
[00:21:10.240 --> 00:21:17.240]   I think the question of what what I'll be doing or what people will be doing after AGI comes is a very tricky question
[00:21:17.240 --> 00:21:23.080]   You know, I think where will people find meaning but I think I think that that's something that AI could help us with
[00:21:23.080 --> 00:21:26.080]   like
[00:21:26.080 --> 00:21:29.040]   One thing I imagine is that
[00:21:29.040 --> 00:21:34.600]   We'll all be able to become more enlightened because we'd interact with an AGI that will help us
[00:21:35.320 --> 00:21:37.320]   See the world more correctly
[00:21:37.320 --> 00:21:43.280]   Become better on the inside as a result of interaction like imagine talking to the best meditation teacher in history
[00:21:43.280 --> 00:21:45.280]   I think that will be a helpful thing
[00:21:45.280 --> 00:21:47.680]   but I also think that
[00:21:47.680 --> 00:21:51.520]   Because the world will change a lot. It will be very hard for people to understand
[00:21:51.520 --> 00:21:58.080]   What is happening precisely and how to really contribute. One thing that I think
[00:21:59.160 --> 00:22:02.360]   Some people will choose to do is to become part AI in
[00:22:02.360 --> 00:22:09.880]   Order to really expand their minds and understanding to really be able to solve the hardest problems that society will face then
[00:22:09.880 --> 00:22:11.780]   Are you gonna become part of AI?
[00:22:11.780 --> 00:22:16.840]   Very tempting. It is tempting. Yeah. Well, do you think they'll be physically embodied humans and
[00:22:16.840 --> 00:22:18.720]   3,000?
[00:22:18.720 --> 00:22:21.400]   3,000. Oh, how do I know it's gonna have any 3,000?
[00:22:21.400 --> 00:22:22.600]   Like what does it look like?
[00:22:22.600 --> 00:22:27.480]   Are there still like humans walking around on earth or have you guys thought concretely about what you actually want this world to look like?
[00:22:27.800 --> 00:22:31.200]   3,000 well, I mean that that that the thing is here's the thing
[00:22:31.200 --> 00:22:36.360]   let me let me describe to you what I think is not quite right about the question like it implies like oh like we
[00:22:36.360 --> 00:22:39.280]   Get to decide how we want the world to look like. I
[00:22:39.280 --> 00:22:42.000]   Don't think that picture is correct. I think
[00:22:42.000 --> 00:22:46.720]   Change is the only constant and so of course even after a AGI is built
[00:22:46.720 --> 00:22:51.880]   it doesn't mean that the world will be static the world will continue to change the world will continue to evolve and
[00:22:53.240 --> 00:23:00.640]   it will go through all kinds of transformations and I really have no I don't think anyone has any idea of how the world will
[00:23:00.640 --> 00:23:01.760]   look like in
[00:23:01.760 --> 00:23:04.960]   3,000 but I do hope that there will be a lot of
[00:23:04.960 --> 00:23:11.360]   Descendants of human beings who will live happy fulfilled lives where they're free to do as their wish as they see fit
[00:23:11.360 --> 00:23:13.720]   Where they are the ones who are?
[00:23:13.720 --> 00:23:17.860]   Solving their own problems like one of the things which I would not want one one one world
[00:23:17.860 --> 00:23:20.480]   Which I would find very unexciting is one where you know
[00:23:20.480 --> 00:23:23.720]   We feel this powerful tool and then the government said, okay, so
[00:23:23.720 --> 00:23:28.760]   The AGI said that society shall be run in such a way and now we shall run society in such a way
[00:23:28.760 --> 00:23:30.880]   I'd much rather
[00:23:30.880 --> 00:23:38.440]   Have a world where people are still free to make their own mistakes and suffer their consequences and gradually evolve
[00:23:38.440 --> 00:23:42.200]   Morally and progress forward on their own through their own strength
[00:23:42.200 --> 00:23:45.960]   See what I mean with the AGI providing more like a base safety net
[00:23:45.960 --> 00:23:49.680]   How much time do you spend thinking about these kinds of things versus just doing the research that?
[00:23:50.320 --> 00:23:53.360]   I do think about those things a fair bit. Yeah, I think it's a very interesting questions
[00:23:53.360 --> 00:23:59.880]   So in what ways have the capabilities we have today in what ways have they surpassed where you expected them to be in 2015?
[00:23:59.880 --> 00:24:03.040]   And in what ways are they still not where you would expect them to be?
[00:24:03.040 --> 00:24:05.360]   by this point I
[00:24:05.360 --> 00:24:10.040]   Mean in fairness, they did surpass what I expected to be in 2015 in today in 2015
[00:24:10.040 --> 00:24:14.160]   I my thinking was a lot more. I just don't want to bet against deep learning
[00:24:14.160 --> 00:24:18.600]   I want to make the biggest possible bet on deep learning don't know how but it will figure it out
[00:24:18.680 --> 00:24:21.200]   But is there any specific way in which it's?
[00:24:21.200 --> 00:24:24.080]   Been more than you expected or less than you expected
[00:24:24.080 --> 00:24:27.160]   like some concrete prediction you had in 2015 that's been
[00:24:27.160 --> 00:24:29.400]   prounced
[00:24:29.400 --> 00:24:36.680]   You know, unfortunately, I don't remember concrete predictions. I made in 2015, but I definitely but I definitely think that overall in
[00:24:36.680 --> 00:24:40.360]   2015 I just want to
[00:24:40.360 --> 00:24:45.320]   Move to make the biggest bet possible on deep learning, but I didn't know exactly
[00:24:45.320 --> 00:24:49.640]   I didn't have a specific idea of how far things will go in seven years
[00:24:49.640 --> 00:24:56.280]   Well, I mean 2015 I did have all these best with people in 2016. Maybe 2017 that things will go really far
[00:24:56.280 --> 00:24:58.320]   but
[00:24:58.320 --> 00:24:59.520]   specifics
[00:24:59.520 --> 00:25:04.120]   So it's like it's both. It's both the case that it surprised me and I was making these
[00:25:04.120 --> 00:25:09.760]   Aggressive predictions, but I think maybe I believe them only only few only 50% on the inside. Uh-huh
[00:25:09.760 --> 00:25:13.280]   Well, what do you believe now that even most people at open AI would find far-fetched? I
[00:25:14.280 --> 00:25:20.120]   Mean, I think that at this because we communicate a lot at open AI people have a pretty good sense of what I think
[00:25:20.120 --> 00:25:25.220]   And so yeah, we reach the point open. I think we see eye to eye and all these questions
[00:25:25.220 --> 00:25:30.820]   So Google has you know, it's custom TPU hardware. It has all this data from all its users, you know, gmail
[00:25:30.820 --> 00:25:35.840]   What and so on it doesn't give it an advantage in terms of training bigger models and better models than you
[00:25:35.840 --> 00:25:41.240]   So I think like when the first at first when the TPU came out, I was really impressed and I thought wow
[00:25:41.240 --> 00:25:44.920]   This is amazing. But that's because I didn't quite understand hardware back then
[00:25:44.920 --> 00:25:51.920]   What really turned out to be the case is that TPUs and GPUs are almost the same thing
[00:25:51.920 --> 00:25:55.620]   They are very very similar. It's like I
[00:25:55.620 --> 00:26:00.680]   Think a GPU chip is a little bit bigger. I think a TPU chip is a little bit smaller
[00:26:00.680 --> 00:26:06.420]   It may be a little bit cheaper, but then they make more GPUs than TPUs. So I think the GPUs might be cheaper after all
[00:26:06.420 --> 00:26:09.560]   but fundamentally you have a big processor and
[00:26:10.480 --> 00:26:14.280]   you have a lot of memory and there is a bottleneck between those two and
[00:26:14.280 --> 00:26:21.360]   The problem that both the TPU and the GPU are trying to solve is that by the amount of time it takes you to move
[00:26:21.360 --> 00:26:27.840]   One floating point from the memory to the processor you can do several hundred floating point operations on the processor
[00:26:27.840 --> 00:26:30.440]   which means that you have to do some kind of batch processing and
[00:26:30.440 --> 00:26:35.200]   In this sense, both of these architectures are the same. So I really feel like
[00:26:35.880 --> 00:26:40.280]   Hardware, like in some sense, the only thing that matters about hardware is cost. Cost per flop
[00:26:40.280 --> 00:26:46.200]   Overall systems cost. Okay, there isn't much that much difference. Well, I actually don't know
[00:26:46.200 --> 00:26:49.280]   I mean, I don't know how much what the TPU costs are
[00:26:49.280 --> 00:26:51.280]   But I would suspect that
[00:26:51.280 --> 00:26:57.400]   Probably not. If anything, probably the TPUs are more expensive because there is less of them. When you're doing your work
[00:26:57.400 --> 00:26:59.680]   How much of the time is spent, you know, configuring
[00:26:59.680 --> 00:27:04.840]   The right initializations, making sure the training run goes well and getting the right hyperparameters
[00:27:04.840 --> 00:27:06.840]   And how much is it just coming up with whole new ideas?
[00:27:06.840 --> 00:27:11.160]   I would say it's a combination, but I think that coming up with... it's a combination
[00:27:11.160 --> 00:27:15.140]   But coming up with whole new ideas is actually not. It's like a
[00:27:15.140 --> 00:27:18.840]   Modest part of the work. Certainly coming up with new ideas is important
[00:27:18.840 --> 00:27:25.520]   But I think even more important is to understand the results, to understand the existing ideas, to understand what's going on
[00:27:25.520 --> 00:27:28.960]   Because normally you'd have these, you know, a neural net is a very complicated system, right?
[00:27:28.960 --> 00:27:33.360]   And you ran it and you get some behavior, which is hard to understand what's going on
[00:27:34.200 --> 00:27:39.720]   Understanding the results, figuring out what next experiment to run. A lot of the time is spent on that
[00:27:39.720 --> 00:27:47.920]   Understanding what could be wrong, what could have caused the neural net to produce a result, which was not expected
[00:27:47.920 --> 00:27:52.480]   I'd say a lot of time is spent as well, of course coming up with new ideas, but not new ideas
[00:27:52.480 --> 00:27:55.440]   I think like, I don't like this
[00:27:55.440 --> 00:28:01.760]   Framing as much. It's not that it's false, but I think the main activity is actually understanding
[00:28:03.080 --> 00:28:05.080]   What do you see as the difference between the two?
[00:28:05.080 --> 00:28:10.480]   So at least in my mind when you say come up with new ideas, I'm like, oh like what happened if it did such-and-such
[00:28:10.480 --> 00:28:16.320]   Whereas understanding it's more like what is this whole thing? You're like, what are the real underlying
[00:28:16.320 --> 00:28:21.480]   Phenomena that are going on? What are the underlying effects? Like why?
[00:28:21.480 --> 00:28:24.360]   Why are we doing things this way and not another way?
[00:28:24.360 --> 00:28:28.880]   And of course, this is very adjacent to what can be described as coming up with ideas
[00:28:28.880 --> 00:28:32.440]   But I think the understanding part is where the real action takes place
[00:28:33.120 --> 00:28:36.560]   Does that describe your entire career? Like if you think back on like ImageNet or something
[00:28:36.560 --> 00:28:38.680]   Was that more a new idea or was that more understanding?
[00:28:38.680 --> 00:28:43.800]   Well, I was definitely understanding, definitely understanding. It was a new understanding of very old things
[00:28:43.800 --> 00:28:46.680]   what is the experience of
[00:28:46.680 --> 00:28:51.800]   Training on Azure been like using Azure? Fantastic. I mean, yeah
[00:28:51.800 --> 00:28:55.120]   I mean Microsoft has been a very very good partner for us and they've really
[00:28:55.120 --> 00:28:57.880]   helped
[00:28:57.880 --> 00:29:02.880]   take Azure and make it bring it to a point where it's really good for ML and
[00:29:02.880 --> 00:29:10.600]   You're super happy with it. How how vulnerable is a whole AI ecosystem to something that might happen in Taiwan?
[00:29:10.600 --> 00:29:15.280]   So let's say there's like a tsunami in Taiwan or something. What happens to AI in general?
[00:29:15.280 --> 00:29:22.760]   Like it's definitely going to be a significant setback. It's not going to like it might be something equivalent to
[00:29:23.520 --> 00:29:26.440]   Like no one will be able to get more more computers for a few years
[00:29:26.440 --> 00:29:29.600]   But I expect computers will spring up like for example
[00:29:29.600 --> 00:29:33.360]   I believe that Intel has fabs just of the previous of like a few generations ago
[00:29:33.360 --> 00:29:39.640]   That means that if Intel wanted to they could produce something GPU like from like four years ago
[00:29:39.640 --> 00:29:43.420]   So yeah, it's not the best. Let's say I'm actually not sure about if
[00:29:43.420 --> 00:29:50.120]   If my statement about Intel is correct, but I do know that there are fabs outside of Taiwan
[00:29:50.240 --> 00:29:54.880]   That is not as good, but you can still use them and still go very far with them. It's just
[00:29:54.880 --> 00:30:00.040]   It just cost it's just a setback. Will inference get cost prohibitive as these models get bigger and bigger?
[00:30:00.040 --> 00:30:05.360]   So I have a different way of looking at this question. Yeah, it's not that inference will become cost prohibitive
[00:30:05.360 --> 00:30:10.480]   Inference of better models will indeed become more expensive
[00:30:10.480 --> 00:30:13.080]   But is it prohibitive?
[00:30:13.080 --> 00:30:15.080]   Well, it depends on how useful is it?
[00:30:15.520 --> 00:30:20.160]   Like if it is more useful than it is expensive, then it is not prohibitive like to give you an analogy
[00:30:20.160 --> 00:30:25.400]   Like suppose you want to talk to a lawyer you have some case you or need some advice or something
[00:30:25.400 --> 00:30:27.800]   You are perfectly happy to spend $500 an hour
[00:30:27.800 --> 00:30:33.600]   Right. So if your neural net could give you like really reliable legal advice
[00:30:33.600 --> 00:30:39.960]   You'd say I'm happy to spend $400 for that advice and suddenly inference becomes very much non-prohibitive
[00:30:40.160 --> 00:30:46.960]   Mm-hmm. The question is is can neural net produce an answer good enough at this cost?
[00:30:46.960 --> 00:30:55.000]   Yes, and you will just have different like price discrimination different. Yeah different models. I mean, it's already the case today
[00:30:55.000 --> 00:30:57.040]   So on our product
[00:30:57.040 --> 00:30:59.280]   the API
[00:30:59.280 --> 00:31:06.440]   We serve multiple neural nets of different sizes and different customers use different neural nets of different sizes depending on their use case
[00:31:07.040 --> 00:31:12.800]   Like if someone can take a small model and fine-tune it and get something that's satisfactory for them. They'll use that
[00:31:12.800 --> 00:31:17.440]   Yeah, but if someone wants to do something more complicated and more interesting, there is the biggest model
[00:31:17.440 --> 00:31:21.120]   How do you prevent these models from just becoming commodities where these different companies?
[00:31:21.120 --> 00:31:25.240]   Just they just bid each other's prices down until it's basically the cost of the GPU run
[00:31:25.240 --> 00:31:30.320]   Yeah, I think I think there is without question a force that's trying to create that and the answer is you got to keep on
[00:31:30.320 --> 00:31:32.520]   Making progress you got to keep improving the models
[00:31:32.520 --> 00:31:37.480]   You got to keep on coming up with new ideas and making our models better and more reliable
[00:31:37.480 --> 00:31:40.800]   More trustworthy so you can trust their answers
[00:31:40.800 --> 00:31:45.160]   All those things. Yeah, but let's say it's like 2025 and
[00:31:45.160 --> 00:31:49.640]   The model from 2024 or somebody just offering it at cost and it's like still pretty good
[00:31:49.640 --> 00:31:56.000]   Why would people use a new one from 2025 if the one from just a year older is, you know, even better?
[00:31:56.000 --> 00:31:59.840]   So there are several answers there for some use cases that may be true
[00:32:00.040 --> 00:32:04.440]   There will be a new model from 2025 which will be driving the more interesting use cases
[00:32:04.440 --> 00:32:09.800]   There's also going to be a question of inference cost like you can if you can do research to serve the same model at less
[00:32:09.800 --> 00:32:17.240]   Cost so there will be different the same model will be served will cost different amounts to serve
[00:32:17.240 --> 00:32:23.040]   For different companies. I can also imagine some degree of specialization to where some companies may try to
[00:32:23.040 --> 00:32:29.120]   Specialize in some area and be stronger in a narrower area compared to other companies and I think that too may
[00:32:29.800 --> 00:32:31.800]   That may be a response
[00:32:31.800 --> 00:32:38.720]   To commoditization to some degree. As over time do these different companies do their research directions converge or their diverge?
[00:32:38.720 --> 00:32:43.560]   Are they doing similar and similar things over time or are they going off branching off into different areas?
[00:32:43.560 --> 00:32:45.040]   So I'd say in the near term
[00:32:45.040 --> 00:32:51.720]   It looks like there is convergence in the like I expect there's going to be a convergence a divergence convergence behavior
[00:32:51.720 --> 00:32:58.040]   Where there is a lot of convergence on the near-term work. There's going to be some divergence on the longer term work
[00:32:58.240 --> 00:33:03.480]   But then once the longer term work starts to yield fruit, I think there will be convergence again. Got it
[00:33:03.480 --> 00:33:07.520]   When one of them finds the most promising area they everybody just that's right
[00:33:07.520 --> 00:33:13.880]   Now there is obviously less publishing now, so it will take longer before this promising direction gets rediscovered
[00:33:13.880 --> 00:33:15.880]   But that's how I'd imagine it. I think it's going to be
[00:33:15.880 --> 00:33:18.160]   convergence, divergence, convergence
[00:33:18.160 --> 00:33:20.400]   Yeah, we talked about this a little bit in the beginning
[00:33:20.400 --> 00:33:24.640]   But you know as foreign governments learn about how capable these models are
[00:33:24.960 --> 00:33:28.160]   How do you are you worried about spies or some sort of?
[00:33:28.160 --> 00:33:36.520]   Attack to get your weights or you know, somehow abuse these models and learn about them. Yeah, it's definitely something that
[00:33:36.520 --> 00:33:39.880]   You absolutely can't discount that. Yeah, and
[00:33:39.880 --> 00:33:48.080]   Yeah, something that we try to guard against the best of our ability, but it's going to be a problem for everyone who is building this
[00:33:48.080 --> 00:33:50.360]   How do you prevent your weights from leaking?
[00:33:50.360 --> 00:33:53.640]   I mean you have like really good security people and
[00:33:53.640 --> 00:33:58.840]   Like how many people have the if they wanted to just like a stage into the weights a machine
[00:33:58.840 --> 00:34:01.280]   How many people could do that? I mean
[00:34:01.280 --> 00:34:04.240]   Like what I can say is that
[00:34:04.240 --> 00:34:10.800]   The security people that we have the built-in they've done a really good job so that I'm really not worried about the weights being leaked
[00:34:10.800 --> 00:34:15.360]   Okay, got it. What kinds of emergent properties are expecting from these models at this scale?
[00:34:15.360 --> 00:34:18.480]   Is there something that just comes about de novo?
[00:34:19.560 --> 00:34:23.880]   I'm sure things will come. I'm sure really new surprising properties will come up. I would not be surprised
[00:34:23.880 --> 00:34:26.800]   The thing which I'm really excited about or the thing which I'd like to see is
[00:34:26.800 --> 00:34:33.320]   Reliability and controllability. I think that this will be very very important class of emergent properties
[00:34:33.320 --> 00:34:35.320]   If you have reliability and controllability
[00:34:35.320 --> 00:34:41.840]   I think that helps you solve a lot of problems reliability means you can trust the models output controllability means you can control it and
[00:34:41.840 --> 00:34:49.160]   We'll see but it'll be very cool if those emergent properties did exist. Is there somewhere you can predict it in advance?
[00:34:49.680 --> 00:34:52.320]   Like what will happen in this parameter count will have an ever America
[00:34:52.320 --> 00:34:56.640]   I think it's possible to make some predictions about specific specific capabilities
[00:34:56.640 --> 00:35:01.200]   So it's definitely not simple and you can't do it in a super fine-grained way at least today
[00:35:01.200 --> 00:35:04.120]   but I think getting better at that is really important and
[00:35:04.120 --> 00:35:06.760]   anyone who is interested in who has a
[00:35:06.760 --> 00:35:11.080]   Research ideas on how to do that. I think that can be a valuable contribution
[00:35:11.080 --> 00:35:15.280]   How seriously do you take these scaling laws if like there's a paper?
[00:35:15.280 --> 00:35:20.240]   This is like, oh you just increase you need this many orders of magnitude more to get all the reasoning out
[00:35:20.240 --> 00:35:22.800]   Like if you take that seriously, or do you think it breaks down at some point?
[00:35:22.800 --> 00:35:27.160]   well, the thing is that the scaling law tells you what happens as you
[00:35:27.160 --> 00:35:31.680]   What happens to your lock to your next word prediction accuracy, right?
[00:35:31.680 --> 00:35:36.200]   There is a whole separate challenge of linking next word prediction accuracy
[00:35:36.200 --> 00:35:38.800]   to reasoning capability.
[00:35:38.800 --> 00:35:44.600]   I do believe that indeed there is a link but this link is complicated
[00:35:45.120 --> 00:35:48.520]   And you may find that there are other things that can give us more
[00:35:48.520 --> 00:35:51.240]   reasoning per unit effort
[00:35:51.240 --> 00:35:57.520]   Like for example some special like, you know, you mentioned reasoning tokens and I think they can be helpful
[00:35:57.520 --> 00:36:02.160]   It can be there can be probably some things that
[00:36:02.160 --> 00:36:07.600]   You can is this is something you're considering just hiring humans to
[00:36:07.600 --> 00:36:11.600]   Generate tokens for you or is it all gonna come from stuff that already exists out there?
[00:36:11.600 --> 00:36:15.560]   I mean, I think that relying on people to teach our models to do things
[00:36:15.560 --> 00:36:20.960]   Especially, you know to make sure that they are well behaved and they don't produce false things
[00:36:20.960 --> 00:36:23.080]   I think it's an extremely sensible thing to do and
[00:36:23.080 --> 00:36:29.480]   Isn't it odd that we have the data we need at exactly the same time as we have the transformer at the exact same time
[00:36:29.480 --> 00:36:31.280]   that we have these GPUs like
[00:36:31.280 --> 00:36:34.520]   Is it odd to you that all these things happen at the same time or do you not see that way?
[00:36:34.520 --> 00:36:37.760]   I mean, it is definitely an interesting
[00:36:38.360 --> 00:36:41.560]   It is an interesting situation that is the case. I will say that
[00:36:41.560 --> 00:36:46.360]   It is odd and it is less odd on some level. Here's why it's less odd
[00:36:46.360 --> 00:36:55.000]   So what is the driving force behind the fact that the data exists that the GPUs exists that the transformer exists. So
[00:36:55.000 --> 00:36:59.720]   As the data exists because computers became better and cheaper
[00:36:59.720 --> 00:37:05.600]   We've got smaller and smaller transistors and suddenly at some point it became economical for every person to have a personal computer
[00:37:06.000 --> 00:37:08.840]   Once everyone has a personal computer, you really want to connect them with the network
[00:37:08.840 --> 00:37:13.920]   You get the internet. Once you have the internet you have suddenly data appearing in great quantities
[00:37:13.920 --> 00:37:18.720]   The GPUs were improving concurrently because you have more smaller and smaller transistors and you're looking for
[00:37:18.720 --> 00:37:24.800]   Things to do with them. Gaming turned out to be the thing that you could do and then at some point
[00:37:24.800 --> 00:37:29.000]   The gaming GPU and NVIDIA said wait a sec, Brian
[00:37:29.760 --> 00:37:37.120]   May turn it into a general-purpose GPU computer. Maybe someone will find it useful. Turns out it's good for neural nets. So
[00:37:37.120 --> 00:37:41.000]   It could have been the case that maybe
[00:37:41.000 --> 00:37:48.000]   The GPU would have arrived five years later or ten years later if what let's suppose gaming wasn't the thing
[00:37:48.000 --> 00:37:51.160]   It's kind of hard to imagine. What does it mean if gaming isn't a thing?
[00:37:51.160 --> 00:37:59.000]   But it could maybe there was a counterfactual world where GPUs arrived five years after the data or five years before the data
[00:37:59.480 --> 00:38:03.520]   In which case maybe things would move a little bit more. Things would have been as
[00:38:03.520 --> 00:38:06.040]   Ready to go as they are now
[00:38:06.040 --> 00:38:12.720]   But that's the picture which I imagine. All this progress in all these dimensions is very intertwined. It's not a coincidence that
[00:38:12.720 --> 00:38:19.920]   Like you don't get to pick and choose which dimension in which dimensions things improve if you see what I mean
[00:38:19.920 --> 00:38:22.440]   How inevitable is this kind of progress?
[00:38:22.440 --> 00:38:27.120]   So if like let's say you and Geoffrey Hinton and a few other pioneers if they were never born
[00:38:27.400 --> 00:38:31.360]   Does the deep learning revolution happen around the same time? How much does it delay?
[00:38:31.360 --> 00:38:37.280]   I think maybe there would have been some delay maybe like a year delay. It's really hard. It's really hard to tell
[00:38:37.280 --> 00:38:40.640]   I mean, I hesitate to give a lot a lot a longer answer because okay
[00:38:40.640 --> 00:38:44.960]   Well, then you'd have GPUs would keep on improving, right? Then at some point I
[00:38:44.960 --> 00:38:49.720]   Cannot see how someone would not have discovered it. But here's the other thing
[00:38:49.720 --> 00:38:54.840]   If okay, so let's suppose no one has done it. Computers keep getting faster and better
[00:38:55.160 --> 00:38:57.280]   Becomes easy and easy to train these neural nets
[00:38:57.280 --> 00:39:02.440]   Because you have bigger GPUs so it takes less engineering effort to train one
[00:39:02.440 --> 00:39:04.600]   You don't need to optimize your code as much, you know
[00:39:04.600 --> 00:39:06.600]   When the image in a data set came out
[00:39:06.600 --> 00:39:09.880]   It was huge and it was very very difficult to use now
[00:39:09.880 --> 00:39:15.800]   imagine wait for a few years and it becomes very easy to download and people can just tinker so I I would imagine that
[00:39:15.800 --> 00:39:17.760]   like a
[00:39:17.760 --> 00:39:24.680]   Modest number of years maximum. This would be my guess. I hesitate. I hesitate to give it to give a lot a longer answer though
[00:39:25.640 --> 00:39:27.640]   You know, you can't you can't
[00:39:27.640 --> 00:39:29.160]   run
[00:39:29.160 --> 00:39:31.160]   You can't rerun the world. You don't know
[00:39:31.160 --> 00:39:35.160]   Let's go back to alignment for a second as somebody who deeply understands these models
[00:39:35.160 --> 00:39:37.560]   What is your intuition of how hard alignment will be?
[00:39:37.560 --> 00:39:42.200]   Like I think because so here's what I would say. I think with the current level of capabilities
[00:39:42.200 --> 00:39:44.560]   I think we have a pretty good set of ideas of how to align them
[00:39:44.560 --> 00:39:49.880]   but I would not underestimate the difficulty of alignment of models that are actually
[00:39:50.400 --> 00:39:54.660]   Smarter than us of models that are capable of misrepresenting their intentions
[00:39:54.660 --> 00:39:57.720]   like I think I think it's something to
[00:39:57.720 --> 00:40:03.280]   To think it to think about a lot into research. I think this is one area also, by the way, you know
[00:40:03.280 --> 00:40:08.200]   Like oftentimes academic researchers asked me ask me where what's the best place where they can contribute?
[00:40:08.200 --> 00:40:13.120]   And I think alignment research is one place where I think academic researchers can make
[00:40:13.120 --> 00:40:15.080]   very meaningful contributions
[00:40:15.080 --> 00:40:19.520]   Other than that, do you think academia will come up with in weren't insights about actual capabilities?
[00:40:19.520 --> 00:40:23.280]   Or is that gonna be just the companies at this point? The companies will realize the capabilities
[00:40:23.280 --> 00:40:26.640]   I think it's very possible for academic research to come up with those insights
[00:40:26.640 --> 00:40:32.680]   I think it's just it doesn't seem to happen that much for some reason, but I don't I don't think there's anything
[00:40:32.680 --> 00:40:42.920]   Fundamental about academia like it's not like academia can't I think maybe they're just not thinking about the right problems or something because
[00:40:43.600 --> 00:40:47.400]   Maybe it's just easier to see what needs to be done inside these companies
[00:40:47.400 --> 00:40:52.640]   Mmm, I see but there's a possibility that somebody could just realize. Yeah, I totally think so
[00:40:52.640 --> 00:40:57.480]   Like why would I possibly rule this out? You see what I mean? Yeah, what are the concrete steps by which?
[00:40:57.480 --> 00:41:03.840]   These language models start actually impacting the world of atoms and not just the world of bits
[00:41:03.840 --> 00:41:09.840]   Well, you see I don't think that there is a distinction a clean distinction between the world of bits in the world of atoms
[00:41:10.320 --> 00:41:13.480]   suppose the neural net tells you that hey like here is like
[00:41:13.480 --> 00:41:16.560]   Something that you should do and it's going to improve your life
[00:41:16.560 --> 00:41:22.240]   But you need to like rearrange your apartment in a certain way and you go and you rearrange your apartment as a result
[00:41:22.240 --> 00:41:27.280]   If the neural net impact the world of atoms just yeah fair enough fair enough
[00:41:27.280 --> 00:41:31.120]   Do you think it'll take a couple of additional breakthroughs as important as a transformer?
[00:41:31.120 --> 00:41:36.800]   They get to superhuman AI or do you think we basically got the insights in the books somewhere?
[00:41:36.800 --> 00:41:39.040]   And we just need to implement them and connect them
[00:41:39.680 --> 00:41:43.880]   So I don't really see such a big distinction in those two cases and let me explain why
[00:41:43.880 --> 00:41:49.760]   Like I think what's what one of the ways in which progress has taken place in the past
[00:41:49.760 --> 00:41:55.720]   Is that we've understood that something had a property a
[00:41:55.720 --> 00:41:59.920]   Desirable property all along but you didn't realize
[00:41:59.920 --> 00:42:06.500]   So is that a breakthrough you can say yes, it is. Is that an implementation of something on the books also?
[00:42:06.500 --> 00:42:11.920]   Yes, so I am I my feeling is that a few of those are quite likely to happen
[00:42:11.920 --> 00:42:16.240]   But that in hindsight it will not feel like a breakthrough. Everybody's gonna say
[00:42:16.240 --> 00:42:21.440]   Oh, well, of course like it's totally obvious that such and such thing can and work
[00:42:21.440 --> 00:42:26.880]   You see with the transformer the reason it's been brought up as a big as a specific advance is because it's the kind of thing
[00:42:26.880 --> 00:42:31.760]   That was not obvious for almost anyone so people can say yeah, like it's not something which they knew about
[00:42:32.120 --> 00:42:37.080]   But if an advance comes from something like let's consider that as a the most fundamental advance of deep learning
[00:42:37.080 --> 00:42:44.700]   That the big neural network trained with backpropagation and do a lot of things like where's the novelty not in the neural network
[00:42:44.700 --> 00:42:46.940]   It's not in the backpropagation
[00:42:46.940 --> 00:42:54.740]   But then somehow it's the kind of but it was it is most definitely a giant conceptual breakthrough because for the longest time people just
[00:42:54.740 --> 00:42:56.340]   didn't see that
[00:42:56.340 --> 00:43:00.580]   But then now that everyone sees it everyone's gonna say well, of course like it's totally obvious big neural network
[00:43:01.180 --> 00:43:06.140]   Everyone knows that they can do it. What is your opinion of your former advisors new forward-forward algorithm? I
[00:43:06.140 --> 00:43:09.500]   Think that it's an attempt
[00:43:09.500 --> 00:43:11.340]   to
[00:43:11.340 --> 00:43:13.980]   brain and neural network without backpropagation and
[00:43:13.980 --> 00:43:17.340]   I think that this is especially interesting if you are
[00:43:17.340 --> 00:43:22.340]   Motivated to try to understand how the brain might be learning its connections
[00:43:22.340 --> 00:43:26.020]   The reason for that is that as far as I know
[00:43:27.980 --> 00:43:35.860]   Neuroscientists are really convinced that the brain cannot implement backpropagation because the signals in the synapses only move in one direction and
[00:43:35.860 --> 00:43:40.660]   So if you have a neuroscience motivation and you want to say, okay
[00:43:40.660 --> 00:43:46.220]   How can I come up with something that tries to approximate?
[00:43:46.220 --> 00:43:49.620]   approximate the good properties of backpropagation
[00:43:49.620 --> 00:43:54.840]   Without doing backpropagation. That's what the forward-forward algorithm is trying to do
[00:43:55.340 --> 00:44:00.740]   But if you are trying to just engineer a good system, there is no reason to not use backpropagation like
[00:44:00.740 --> 00:44:04.340]   It's it's it's the only algorithm
[00:44:04.340 --> 00:44:09.820]   hmm, I guess I've heard you in different contexts talk about the need like using humans as
[00:44:09.820 --> 00:44:15.020]   You know the existing example case that you know AGI exists, right?
[00:44:15.020 --> 00:44:18.820]   So at what point do you take the metaphor less seriously and feel?
[00:44:18.820 --> 00:44:23.720]   Don't feel the need to pursue it in terms of research because it is important to you as a sort of existence case
[00:44:24.720 --> 00:44:32.400]   Like at what point I stop caring caring about humans as an existence case of intelligence or as the sort of as
[00:44:32.400 --> 00:44:37.200]   Example in the model you want to follow in terms of pursuing intelligence in models
[00:44:37.200 --> 00:44:42.520]   I see. I mean like you gotta I think it's good to be inspired by humans
[00:44:42.520 --> 00:44:43.920]   I think it's good to be inspired by the brain
[00:44:43.920 --> 00:44:48.000]   I think there is an art into being inspired by humans in the brain correctly
[00:44:48.280 --> 00:44:52.720]   because it's very easy to latch on to an non-essential quality of
[00:44:52.720 --> 00:44:59.920]   Humans or of the brain and I think many people who I know who many people whose research is trying to be inspired by humans
[00:44:59.920 --> 00:45:03.240]   And by the brain often gets a little bit specific people get a little bit too
[00:45:03.240 --> 00:45:03.520]   Okay
[00:45:03.520 --> 00:45:06.920]   So like what cognitive science model should follow at the same time?
[00:45:06.920 --> 00:45:13.260]   Consider the idea of the neural network itself the idea of the artificial neuron this too is inspired by the brain
[00:45:13.260 --> 00:45:15.680]   But it turned out to be extremely fruitful
[00:45:15.920 --> 00:45:21.760]   So, how do you do this? You what what behaviors of human beings are essential that you say?
[00:45:21.760 --> 00:45:27.140]   Like this is something that proves to us that it's possible. What is inessential? No, actually, this is like some emergent
[00:45:27.140 --> 00:45:29.880]   phenomenon of something more basic and
[00:45:29.880 --> 00:45:39.560]   We just need to focus on our own on getting our own basics, right I would say I would say that it's like
[00:45:41.920 --> 00:45:48.920]   I think one should one can and should be inspired by human intelligence with care final question
[00:45:48.920 --> 00:45:52.920]   Why is there in your case such a strong correlation between being first?
[00:45:52.920 --> 00:45:58.280]   To the deep learning revolution and still being one of the top researchers you would think that these two things wouldn't be that correlated
[00:45:58.280 --> 00:46:03.920]   But why is that their correlation? I don't think those things are super correlated. Indeed. I feel like in my case. I
[00:46:03.920 --> 00:46:08.500]   Mean, honestly, it's hard to answer the question, you know, I just kept on
[00:46:10.000 --> 00:46:15.660]   Yep, I kept trying really hard and it turned out to have sufficed thus far. All right, so it's a perseverance
[00:46:15.660 --> 00:46:21.680]   I think it's a necessary but not a sufficient condition. Like, you know, many things need to come together in order to
[00:46:21.680 --> 00:46:24.200]   Really figure something out. Mm-hmm
[00:46:24.200 --> 00:46:27.120]   like you need to really go for it and
[00:46:27.120 --> 00:46:30.440]   also need to have the right way of looking at things and
[00:46:30.440 --> 00:46:34.880]   So it's hard. It's hard to give him like a really meaningful answer to this question
[00:46:35.640 --> 00:46:40.280]   Alright, Ilya. It has been a true pleasure. Thank you so much for coming out of Lunar Society
[00:46:40.280 --> 00:46:44.500]   I appreciate you bringing us to the offices. Thank you. Yeah, I really enjoyed it. Thank you very much
[00:46:44.500 --> 00:46:48.400]   Hey everybody. I hope you enjoyed that episode
[00:46:48.400 --> 00:46:55.320]   Just wanted to let you know that in order to help pay for the bills associated with this podcast
[00:46:55.320 --> 00:46:59.200]   I'm turning on paid subscriptions on my cep stack at
[00:47:00.000 --> 00:47:06.080]   Warkesh Patel calm no important content on this podcast will ever be paywalled
[00:47:06.080 --> 00:47:10.320]   So, please don't donate if you have to think twice before buying a cup of coffee
[00:47:10.320 --> 00:47:15.480]   But if you have the means and you have enjoyed this podcast or gotten some kind of value out of it
[00:47:15.480 --> 00:47:21.280]   I would really appreciate your support as always the most helpful thing you can do is to share the podcast
[00:47:21.280 --> 00:47:26.400]   Send it to people you think might enjoy it put it in Twitter your group chats, etc. Just splits the world
[00:47:27.480 --> 00:47:30.160]   Appreciate your listening. I'll see you next time Cheers
[00:47:30.160 --> 00:47:32.160]   You
[00:47:32.160 --> 00:47:34.160]   You
[00:47:34.160 --> 00:47:36.160]   You
[00:47:36.160 --> 00:47:40.160]   [music]

