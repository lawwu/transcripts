
[00:00:00.000 --> 00:00:02.580]   (gentle music)
[00:00:02.580 --> 00:00:11.360]   - Can you try to define intelligence?
[00:00:11.360 --> 00:00:16.200]   Like what does it mean to be more or less intelligent?
[00:00:16.200 --> 00:00:19.200]   Is it completely coupled to a particular problem
[00:00:19.200 --> 00:00:21.920]   or is there something a little bit more universal?
[00:00:21.920 --> 00:00:23.640]   - Yeah, I do believe all intelligence
[00:00:23.640 --> 00:00:25.280]   is specialized intelligence.
[00:00:25.280 --> 00:00:28.440]   Even human intelligence has some degree of generality.
[00:00:28.440 --> 00:00:31.560]   Well, all intelligence systems have some degree of generality
[00:00:31.560 --> 00:00:35.640]   but they're always specialized in one category of problems.
[00:00:35.640 --> 00:00:38.120]   So the human intelligence is specialized
[00:00:38.120 --> 00:00:39.800]   in the human experience.
[00:00:39.800 --> 00:00:41.800]   And that shows at various levels.
[00:00:41.800 --> 00:00:45.560]   That shows in some prior knowledge
[00:00:45.560 --> 00:00:48.260]   that's innate that we have at birth.
[00:00:48.260 --> 00:00:53.260]   Knowledge about things like agents, goal-driven behavior,
[00:00:53.260 --> 00:00:56.640]   visual priors about what makes an object,
[00:00:56.640 --> 00:00:59.720]   priors about time and so on.
[00:00:59.720 --> 00:01:01.520]   That shows also in the way we learn.
[00:01:01.520 --> 00:01:03.360]   For instance, it's very, very easy for us
[00:01:03.360 --> 00:01:04.780]   to pick up language.
[00:01:04.780 --> 00:01:08.240]   It's very, very easy for us to learn certain things
[00:01:08.240 --> 00:01:11.120]   because we are basically hard-coded to learn them.
[00:01:11.120 --> 00:01:14.480]   And we are specialized in solving certain kinds of problem
[00:01:14.480 --> 00:01:15.880]   and we are quite useless
[00:01:15.880 --> 00:01:17.600]   when it comes to other kinds of problems.
[00:01:17.600 --> 00:01:22.320]   For instance, we are not really designed
[00:01:22.320 --> 00:01:24.980]   to handle very long-term problems.
[00:01:24.980 --> 00:01:28.240]   We have no capability of seeing the very long-term.
[00:01:28.240 --> 00:01:33.060]   We don't have very much working memory.
[00:01:33.060 --> 00:01:36.240]   - So how do you think about long-term?
[00:01:36.240 --> 00:01:37.560]   Do you think long-term planning,
[00:01:37.560 --> 00:01:41.060]   we're talking about scale of years, millennia,
[00:01:41.060 --> 00:01:44.300]   what do you mean by long-term we're not very good?
[00:01:44.300 --> 00:01:45.920]   - Well, human intelligence is specialized
[00:01:45.920 --> 00:01:46.920]   in the human experience.
[00:01:46.920 --> 00:01:48.800]   And human experience is very short.
[00:01:48.800 --> 00:01:50.420]   Like one lifetime is short.
[00:01:50.420 --> 00:01:52.080]   Even within one lifetime,
[00:01:52.080 --> 00:01:56.220]   we have a very hard time envisioning things
[00:01:56.220 --> 00:01:57.380]   on a scale of years.
[00:01:57.380 --> 00:01:59.420]   Like it's very difficult to project yourself
[00:01:59.420 --> 00:02:02.340]   at a scale of five years, at a scale of 10 years and so on.
[00:02:02.340 --> 00:02:03.180]   - Right.
[00:02:03.180 --> 00:02:06.180]   - We can solve only fairly narrowly scoped problems.
[00:02:06.180 --> 00:02:08.500]   So when it comes to solving bigger problems,
[00:02:08.500 --> 00:02:09.940]   larger scale problems,
[00:02:09.940 --> 00:02:12.540]   we are not actually doing it on an individual level.
[00:02:12.540 --> 00:02:15.500]   So it's not actually our brain doing it.
[00:02:15.500 --> 00:02:19.260]   We have this thing called civilization, right?
[00:02:19.260 --> 00:02:22.840]   Which is itself a sort of problem solving system,
[00:02:22.840 --> 00:02:26.240]   a sort of artificial intelligence system, right?
[00:02:26.240 --> 00:02:28.320]   And it's not running on one brain,
[00:02:28.320 --> 00:02:30.320]   it's running on a network of brains.
[00:02:30.320 --> 00:02:31.840]   In fact, it's running on much more
[00:02:31.840 --> 00:02:32.960]   than a network of brains.
[00:02:32.960 --> 00:02:36.320]   It's running on a lot of infrastructure,
[00:02:36.320 --> 00:02:39.280]   like books and computers and the internet
[00:02:39.280 --> 00:02:42.000]   and human institutions and so on.
[00:02:42.000 --> 00:02:46.440]   And that is capable of handling problems
[00:02:46.440 --> 00:02:49.960]   on a much greater scale than any individual human.
[00:02:49.960 --> 00:02:53.820]   If you look at computer science, for instance,
[00:02:53.820 --> 00:02:56.080]   that's an institution that solves problems
[00:02:56.080 --> 00:02:58.780]   and it is superhuman, right?
[00:02:58.780 --> 00:03:00.400]   It operates on a greater scale,
[00:03:00.400 --> 00:03:03.120]   it can solve much bigger problems
[00:03:03.120 --> 00:03:05.320]   than an individual human could.
[00:03:05.320 --> 00:03:07.600]   And science itself, science as a system,
[00:03:07.600 --> 00:03:11.320]   as an institution is a kind of artificially intelligent
[00:03:11.320 --> 00:03:15.600]   problem solving algorithm that is superhuman.
[00:03:15.600 --> 00:03:19.000]   - Yeah, it's a, at least computer science
[00:03:19.000 --> 00:03:20.600]   is like a theorem prover.
[00:03:20.600 --> 00:03:23.960]   At a scale of thousands,
[00:03:23.960 --> 00:03:26.640]   maybe hundreds of thousands of human beings.
[00:03:26.640 --> 00:03:30.900]   At that scale, what do you think is an intelligent agent?
[00:03:30.900 --> 00:03:34.520]   So there's us humans at the individual level,
[00:03:34.520 --> 00:03:38.600]   there is millions, maybe billions of bacteria in our skin.
[00:03:38.600 --> 00:03:42.640]   There is, that's at the smaller scale.
[00:03:42.640 --> 00:03:45.400]   You can even go to the particle level
[00:03:45.400 --> 00:03:49.760]   as systems that behave, you can say intelligently
[00:03:49.760 --> 00:03:50.600]   in some ways.
[00:03:50.600 --> 00:03:54.080]   And then you can look at Earth as a single organism,
[00:03:54.080 --> 00:03:55.440]   you can look at our galaxy
[00:03:55.440 --> 00:03:57.600]   and even the universe as a single organism.
[00:03:57.600 --> 00:04:00.880]   Do you think, how do you think about scale
[00:04:00.880 --> 00:04:02.520]   and defining intelligent systems?
[00:04:02.520 --> 00:04:04.320]   And we're here at Google,
[00:04:04.320 --> 00:04:08.080]   there is millions of devices doing computation
[00:04:08.080 --> 00:04:09.640]   in a distributed way.
[00:04:09.640 --> 00:04:12.120]   How do you think about intelligence versus scale?
[00:04:12.120 --> 00:04:15.640]   - You can always characterize anything as a system.
[00:04:15.640 --> 00:04:19.840]   I think people who talk about things
[00:04:19.840 --> 00:04:23.640]   like intelligence explosion tend to focus on one agent
[00:04:23.640 --> 00:04:25.040]   is basically one brain,
[00:04:25.040 --> 00:04:27.240]   like one brain considered in isolation,
[00:04:27.240 --> 00:04:29.440]   like a brain a jar that's controlling a body
[00:04:29.440 --> 00:04:32.520]   in a very like top to bottom kind of fashion.
[00:04:32.520 --> 00:04:35.720]   And that body is pursuing goals into an environment.
[00:04:35.720 --> 00:04:36.960]   So it's a very hierarchical view.
[00:04:36.960 --> 00:04:39.120]   You have the brain at the top of the pyramid,
[00:04:39.120 --> 00:04:42.240]   then you have the body just plainly receiving orders
[00:04:42.240 --> 00:04:43.880]   and then the body is manipulating objects
[00:04:43.880 --> 00:04:45.160]   in environment and so on.
[00:04:45.160 --> 00:04:49.160]   So everything is subordinate to this one thing,
[00:04:49.160 --> 00:04:50.920]   this epicenter, which is the brain.
[00:04:50.920 --> 00:04:52.200]   But in real life,
[00:04:52.200 --> 00:04:55.480]   intelligent agents don't really work like this.
[00:04:55.480 --> 00:04:57.160]   There is no strong delimitation
[00:04:57.160 --> 00:04:59.640]   between the brain and the body to start with.
[00:04:59.640 --> 00:05:01.240]   You have to look not just at the brain,
[00:05:01.240 --> 00:05:02.760]   but at the nervous system.
[00:05:02.760 --> 00:05:05.080]   But then the nervous system and the body
[00:05:05.080 --> 00:05:07.000]   are naturally two separate entities.
[00:05:07.000 --> 00:05:10.240]   So you have to look at an entire animal as one agent,
[00:05:10.240 --> 00:05:13.280]   but then you start realizing as you observe an animal
[00:05:13.280 --> 00:05:16.480]   over any length of time,
[00:05:16.480 --> 00:05:19.440]   that a lot of the intelligence of an animal
[00:05:19.440 --> 00:05:20.880]   is actually externalized.
[00:05:20.880 --> 00:05:22.520]   That's especially true for humans.
[00:05:22.520 --> 00:05:25.160]   A lot of our intelligence is externalized.
[00:05:25.160 --> 00:05:26.640]   When you write down some notes,
[00:05:26.640 --> 00:05:28.240]   that is externalized intelligence.
[00:05:28.240 --> 00:05:30.240]   When you write a computer program,
[00:05:30.240 --> 00:05:32.280]   you are externalizing cognition.
[00:05:32.280 --> 00:05:33.600]   So it's externalizing books,
[00:05:33.600 --> 00:05:35.960]   it's externalized in computers,
[00:05:35.960 --> 00:05:38.400]   it's externalized in the internet, in other humans.
[00:05:38.400 --> 00:05:42.160]   It's externalized in language and so on.
[00:05:42.160 --> 00:05:47.160]   So there is no hard delimitation
[00:05:47.160 --> 00:05:49.400]   of what makes an intelligent agent.
[00:05:49.400 --> 00:05:50.680]   It's all about context.
[00:05:50.680 --> 00:05:55.560]   - Okay, but AlphaGo is better at Go
[00:05:55.560 --> 00:05:57.000]   than the best human player.
[00:05:57.000 --> 00:06:01.760]   There's levels of skill here.
[00:06:02.480 --> 00:06:07.000]   Do you think there's such a concept
[00:06:07.000 --> 00:06:10.880]   as intelligence explosion in a specific task?
[00:06:10.880 --> 00:06:13.480]   And then, well, yeah.
[00:06:13.480 --> 00:06:16.240]   Do you think it's possible to have a category of tasks
[00:06:16.240 --> 00:06:18.200]   on which you do have something
[00:06:18.200 --> 00:06:21.160]   like an exponential growth of ability
[00:06:21.160 --> 00:06:23.560]   to solve that particular problem?
[00:06:23.560 --> 00:06:26.480]   - I think if you consider a specific vertical,
[00:06:26.480 --> 00:06:29.880]   it's probably possible to some extent.
[00:06:31.440 --> 00:06:34.480]   I also don't think we have to speculate about it
[00:06:34.480 --> 00:06:38.440]   because we have real world examples
[00:06:38.440 --> 00:06:42.160]   of recursively self-improving intelligent systems.
[00:06:42.160 --> 00:06:47.040]   So for instance, science is a problem solving system,
[00:06:47.040 --> 00:06:48.720]   a knowledge generation system,
[00:06:48.720 --> 00:06:52.360]   like a system that experiences the world in some sense
[00:06:52.360 --> 00:06:56.280]   and then gradually understands it and can act on it.
[00:06:56.280 --> 00:06:58.240]   And that system is superhuman
[00:06:58.240 --> 00:07:01.720]   and it is clearly recursively self-improving
[00:07:01.720 --> 00:07:03.680]   because science feeds into technology.
[00:07:03.680 --> 00:07:06.320]   Technology can be used to build better tools,
[00:07:06.320 --> 00:07:09.000]   better computers, better instrumentation and so on,
[00:07:09.000 --> 00:07:11.960]   which in turn can make science faster.
[00:07:11.960 --> 00:07:16.680]   So science is probably the closest thing we have today
[00:07:16.680 --> 00:07:20.880]   to a recursively self-improving superhuman AI.
[00:07:20.880 --> 00:07:22.600]   And you can just observe,
[00:07:22.600 --> 00:07:26.440]   is scientific progress today exploding,
[00:07:26.440 --> 00:07:28.920]   which itself is an interesting question.
[00:07:28.920 --> 00:07:31.680]   And you can use that as a basis to try to understand
[00:07:31.680 --> 00:07:34.000]   what will happen with a superhuman AI
[00:07:34.000 --> 00:07:36.440]   that has science-like behavior.
[00:07:36.440 --> 00:07:37.280]   Thank you.
[00:07:37.280 --> 00:07:39.860]   (upbeat music)
[00:07:39.860 --> 00:07:42.440]   (upbeat music)
[00:07:42.440 --> 00:07:45.020]   (upbeat music)
[00:07:45.020 --> 00:07:47.600]   (upbeat music)
[00:07:47.600 --> 00:07:50.180]   (upbeat music)
[00:07:50.180 --> 00:07:52.760]   (upbeat music)
[00:07:52.760 --> 00:08:02.760]   [BLANK_AUDIO]

